[2018-01-21 12:32:17.240710 UTC] Starting env pool
[2018-01-21 12:32:17.281635 UTC] Starting iteration 0
[2018-01-21 12:32:17.281930 UTC] Start collecting samples
[2018-01-21 12:32:20.947076 UTC] Computing input variables for policy optimization
[2018-01-21 12:32:21.216618 UTC] Performing policy update
[2018-01-21 12:32:21.217131 UTC] Computing gradient in Euclidean space
[2018-01-21 12:32:21.307373 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:32:22.262161 UTC] Performing line search
[2018-01-21 12:32:22.397052 UTC] Updating baseline
[2018-01-21 12:32:23.757392 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.034621   |
| ActualImprovement    | 0.033489   |
| ImprovementRatio     | 0.96729    |
| MeanKL               | 0.0066362  |
| Entropy              | 8.5136     |
| Perplexity           | 4982.2     |
| AveragePolicyStd     | 1          |
| AveragePolicyStd[0]  | 1          |
| AveragePolicyStd[1]  | 1          |
| AveragePolicyStd[2]  | 1          |
| AveragePolicyStd[3]  | 1          |
| AveragePolicyStd[4]  | 1          |
| AveragePolicyStd[5]  | 1          |
| AverageReturn        | -12.003    |
| MinReturn            | -56.593    |
| MaxReturn            | -2.3802    |
| StdReturn            | 6.4706     |
| AverageEpisodeLength | 18         |
| MinEpisodeLength     | 12         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 5.5767     |
| TotalNEpisodes       | 266        |
| TotalNSamples        | 4865       |
| ExplainedVariance    | -0.0083088 |
-------------------------------------
[2018-01-21 12:32:23.895021 UTC] Saving snapshot
[2018-01-21 12:32:23.916393 UTC] Starting iteration 1
[2018-01-21 12:32:23.918601 UTC] Start collecting samples
[2018-01-21 12:32:28.660761 UTC] Computing input variables for policy optimization
[2018-01-21 12:32:29.024134 UTC] Performing policy update
[2018-01-21 12:32:29.025849 UTC] Computing gradient in Euclidean space
[2018-01-21 12:32:29.160717 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:32:30.253825 UTC] Performing line search
[2018-01-21 12:32:30.403426 UTC] Updating baseline
[2018-01-21 12:32:32.308435 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.038113  |
| ActualImprovement    | 0.037093  |
| ImprovementRatio     | 0.97322   |
| MeanKL               | 0.0065209 |
| Entropy              | 8.4811    |
| Perplexity           | 4822.8    |
| AveragePolicyStd     | 0.9946    |
| AveragePolicyStd[0]  | 0.99985   |
| AveragePolicyStd[1]  | 0.99382   |
| AveragePolicyStd[2]  | 0.99346   |
| AveragePolicyStd[3]  | 0.9942    |
| AveragePolicyStd[4]  | 0.99582   |
| AveragePolicyStd[5]  | 0.99044   |
| AverageReturn        | -10.754   |
| MinReturn            | -33.342   |
| MaxReturn            | 1.3604    |
| StdReturn            | 5.9738    |
| AverageEpisodeLength | 18.87     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 47        |
| StdEpisodeLength     | 6.3002    |
| TotalNEpisodes       | 551       |
| TotalNSamples        | 9886      |
| ExplainedVariance    | 0.22239   |
------------------------------------
[2018-01-21 12:32:32.507845 UTC] Saving snapshot
[2018-01-21 12:32:32.508311 UTC] Starting iteration 2
[2018-01-21 12:32:32.508677 UTC] Start collecting samples
[2018-01-21 12:32:38.905388 UTC] Computing input variables for policy optimization
[2018-01-21 12:32:39.345833 UTC] Performing policy update
[2018-01-21 12:32:39.346884 UTC] Computing gradient in Euclidean space
[2018-01-21 12:32:39.463640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:32:40.928470 UTC] Performing line search
[2018-01-21 12:32:41.133608 UTC] Updating baseline
[2018-01-21 12:32:43.139012 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.037799  |
| ActualImprovement    | 0.036918  |
| ImprovementRatio     | 0.97669   |
| MeanKL               | 0.0065489 |
| Entropy              | 8.4131    |
| Perplexity           | 4505.7    |
| AveragePolicyStd     | 0.98339   |
| AveragePolicyStd[0]  | 0.99054   |
| AveragePolicyStd[1]  | 0.98705   |
| AveragePolicyStd[2]  | 0.98269   |
| AveragePolicyStd[3]  | 0.98063   |
| AveragePolicyStd[4]  | 0.9819    |
| AveragePolicyStd[5]  | 0.97755   |
| AverageReturn        | -10.115   |
| MinReturn            | -25.284   |
| MaxReturn            | 1.566     |
| StdReturn            | 4.8503    |
| AverageEpisodeLength | 17.75     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 3.5451    |
| TotalNEpisodes       | 829       |
| TotalNSamples        | 14879     |
| ExplainedVariance    | 0.28955   |
------------------------------------
[2018-01-21 12:32:43.300476 UTC] Saving snapshot
[2018-01-21 12:32:43.300864 UTC] Starting iteration 3
[2018-01-21 12:32:43.301060 UTC] Start collecting samples
[2018-01-21 12:32:49.545015 UTC] Computing input variables for policy optimization
[2018-01-21 12:32:50.076244 UTC] Performing policy update
[2018-01-21 12:32:50.077075 UTC] Computing gradient in Euclidean space
[2018-01-21 12:32:50.221264 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:32:51.796344 UTC] Performing line search
[2018-01-21 12:32:52.030881 UTC] Updating baseline
[2018-01-21 12:32:54.202545 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.036312  |
| ActualImprovement    | 0.035296  |
| ImprovementRatio     | 0.972     |
| MeanKL               | 0.0065678 |
| Entropy              | 8.3292    |
| Perplexity           | 4143.1    |
| AveragePolicyStd     | 0.96976   |
| AveragePolicyStd[0]  | 0.984     |
| AveragePolicyStd[1]  | 0.971     |
| AveragePolicyStd[2]  | 0.97077   |
| AveragePolicyStd[3]  | 0.9642    |
| AveragePolicyStd[4]  | 0.97097   |
| AveragePolicyStd[5]  | 0.95762   |
| AverageReturn        | -8.7584   |
| MinReturn            | -25.107   |
| MaxReturn            | -0.047694 |
| StdReturn            | 4.2212    |
| AverageEpisodeLength | 17.25     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 32        |
| StdEpisodeLength     | 3.223     |
| TotalNEpisodes       | 1116      |
| TotalNSamples        | 19879     |
| ExplainedVariance    | 0.24705   |
------------------------------------
[2018-01-21 12:32:54.375195 UTC] Saving snapshot
[2018-01-21 12:32:54.375503 UTC] Starting iteration 4
[2018-01-21 12:32:54.375729 UTC] Start collecting samples
[2018-01-21 12:33:00.782725 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:01.258025 UTC] Performing policy update
[2018-01-21 12:33:01.258997 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:01.394834 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:03.029924 UTC] Performing line search
[2018-01-21 12:33:03.251943 UTC] Updating baseline
[2018-01-21 12:33:05.332175 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.035506  |
| ActualImprovement    | 0.034555  |
| ImprovementRatio     | 0.97321   |
| MeanKL               | 0.0066126 |
| Entropy              | 8.2723    |
| Perplexity           | 3913.8    |
| AveragePolicyStd     | 0.96063   |
| AveragePolicyStd[0]  | 0.97911   |
| AveragePolicyStd[1]  | 0.96724   |
| AveragePolicyStd[2]  | 0.96425   |
| AveragePolicyStd[3]  | 0.9511    |
| AveragePolicyStd[4]  | 0.95634   |
| AveragePolicyStd[5]  | 0.94575   |
| AverageReturn        | -7.8924   |
| MinReturn            | -29.924   |
| MaxReturn            | 0.42888   |
| StdReturn            | 4.575     |
| AverageEpisodeLength | 17.93     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 45        |
| StdEpisodeLength     | 4.1982    |
| TotalNEpisodes       | 1397      |
| TotalNSamples        | 24919     |
| ExplainedVariance    | 0.28326   |
------------------------------------
[2018-01-21 12:33:05.549700 UTC] Saving snapshot
[2018-01-21 12:33:05.549948 UTC] Starting iteration 5
[2018-01-21 12:33:05.550118 UTC] Start collecting samples
[2018-01-21 12:33:11.831875 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:12.310058 UTC] Performing policy update
[2018-01-21 12:33:12.310800 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:12.447144 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:14.001544 UTC] Performing line search
[2018-01-21 12:33:14.225631 UTC] Updating baseline
[2018-01-21 12:33:16.176726 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.034994  |
| ActualImprovement    | 0.034121  |
| ImprovementRatio     | 0.97506   |
| MeanKL               | 0.0066143 |
| Entropy              | 8.1755    |
| Perplexity           | 3552.8    |
| AveragePolicyStd     | 0.94532   |
| AveragePolicyStd[0]  | 0.96875   |
| AveragePolicyStd[1]  | 0.95903   |
| AveragePolicyStd[2]  | 0.94505   |
| AveragePolicyStd[3]  | 0.92959   |
| AveragePolicyStd[4]  | 0.94297   |
| AveragePolicyStd[5]  | 0.92652   |
| AverageReturn        | -7.2844   |
| MinReturn            | -16.813   |
| MaxReturn            | 4.483     |
| StdReturn            | 3.6839    |
| AverageEpisodeLength | 17.63     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 3.3931    |
| TotalNEpisodes       | 1681      |
| TotalNSamples        | 29954     |
| ExplainedVariance    | 0.28246   |
------------------------------------
[2018-01-21 12:33:16.376901 UTC] Saving snapshot
[2018-01-21 12:33:16.377157 UTC] Starting iteration 6
[2018-01-21 12:33:16.377312 UTC] Start collecting samples
[2018-01-21 12:33:22.894370 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:23.430376 UTC] Performing policy update
[2018-01-21 12:33:23.431025 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:23.580481 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:25.377634 UTC] Performing line search
[2018-01-21 12:33:25.635266 UTC] Updating baseline
[2018-01-21 12:33:27.739691 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.035998  |
| ActualImprovement    | 0.035428  |
| ImprovementRatio     | 0.98415   |
| MeanKL               | 0.0066444 |
| Entropy              | 8.0691    |
| Perplexity           | 3194.2    |
| AveragePolicyStd     | 0.92872   |
| AveragePolicyStd[0]  | 0.95008   |
| AveragePolicyStd[1]  | 0.94632   |
| AveragePolicyStd[2]  | 0.92988   |
| AveragePolicyStd[3]  | 0.90811   |
| AveragePolicyStd[4]  | 0.92713   |
| AveragePolicyStd[5]  | 0.91082   |
| AverageReturn        | -6.2117   |
| MinReturn            | -17.155   |
| MaxReturn            | 1.1742    |
| StdReturn            | 3.5942    |
| AverageEpisodeLength | 17.31     |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.8203    |
| TotalNEpisodes       | 1962      |
| TotalNSamples        | 34895     |
| ExplainedVariance    | 0.27471   |
------------------------------------
[2018-01-21 12:33:27.978507 UTC] Saving snapshot
[2018-01-21 12:33:27.978893 UTC] Starting iteration 7
[2018-01-21 12:33:27.979448 UTC] Start collecting samples
[2018-01-21 12:33:34.571642 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:35.007168 UTC] Performing policy update
[2018-01-21 12:33:35.007813 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:35.153963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:36.702285 UTC] Performing line search
[2018-01-21 12:33:36.901448 UTC] Updating baseline
[2018-01-21 12:33:38.949640 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.032649  |
| ActualImprovement    | 0.031842  |
| ImprovementRatio     | 0.9753    |
| MeanKL               | 0.0066129 |
| Entropy              | 7.9629    |
| Perplexity           | 2872.3    |
| AveragePolicyStd     | 0.91248   |
| AveragePolicyStd[0]  | 0.93519   |
| AveragePolicyStd[1]  | 0.93527   |
| AveragePolicyStd[2]  | 0.9114    |
| AveragePolicyStd[3]  | 0.89045   |
| AveragePolicyStd[4]  | 0.91351   |
| AveragePolicyStd[5]  | 0.88906   |
| AverageReturn        | -4.6518   |
| MinReturn            | -15.408   |
| MaxReturn            | 3.3876    |
| StdReturn            | 3.6242    |
| AverageEpisodeLength | 18.21     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 38        |
| StdEpisodeLength     | 3.8789    |
| TotalNEpisodes       | 2241      |
| TotalNSamples        | 39936     |
| ExplainedVariance    | 0.23246   |
------------------------------------
[2018-01-21 12:33:39.180881 UTC] Saving snapshot
[2018-01-21 12:33:39.181127 UTC] Starting iteration 8
[2018-01-21 12:33:39.181301 UTC] Start collecting samples
[2018-01-21 12:33:45.914168 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:46.381566 UTC] Performing policy update
[2018-01-21 12:33:46.387128 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:46.515585 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:48.047969 UTC] Performing line search
[2018-01-21 12:33:48.249718 UTC] Updating baseline
[2018-01-21 12:33:50.271844 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.030651  |
| ActualImprovement    | 0.030418  |
| ImprovementRatio     | 0.9924    |
| MeanKL               | 0.0066139 |
| Entropy              | 7.8487    |
| Perplexity           | 2562.3    |
| AveragePolicyStd     | 0.8953    |
| AveragePolicyStd[0]  | 0.91496   |
| AveragePolicyStd[1]  | 0.91797   |
| AveragePolicyStd[2]  | 0.89705   |
| AveragePolicyStd[3]  | 0.87384   |
| AveragePolicyStd[4]  | 0.90196   |
| AveragePolicyStd[5]  | 0.86604   |
| AverageReturn        | -4.326    |
| MinReturn            | -16.617   |
| MaxReturn            | 14.69     |
| StdReturn            | 4.1627    |
| AverageEpisodeLength | 17.97     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 36        |
| StdEpisodeLength     | 2.9579    |
| TotalNEpisodes       | 2520      |
| TotalNSamples        | 44974     |
| ExplainedVariance    | 0.23172   |
------------------------------------
[2018-01-21 12:33:50.508113 UTC] Saving snapshot
[2018-01-21 12:33:50.508357 UTC] Starting iteration 9
[2018-01-21 12:33:50.508512 UTC] Start collecting samples
[2018-01-21 12:33:57.051958 UTC] Computing input variables for policy optimization
[2018-01-21 12:33:57.454750 UTC] Performing policy update
[2018-01-21 12:33:57.456400 UTC] Computing gradient in Euclidean space
[2018-01-21 12:33:57.600174 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:33:59.275858 UTC] Performing line search
[2018-01-21 12:33:59.471321 UTC] Updating baseline
[2018-01-21 12:34:01.492872 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.032702  |
| ActualImprovement    | 0.031959  |
| ImprovementRatio     | 0.97728   |
| MeanKL               | 0.0064898 |
| Entropy              | 7.7385    |
| Perplexity           | 2295      |
| AveragePolicyStd     | 0.87906   |
| AveragePolicyStd[0]  | 0.90199   |
| AveragePolicyStd[1]  | 0.90438   |
| AveragePolicyStd[2]  | 0.8782    |
| AveragePolicyStd[3]  | 0.85894   |
| AveragePolicyStd[4]  | 0.88442   |
| AveragePolicyStd[5]  | 0.84645   |
| AverageReturn        | -3.7315   |
| MinReturn            | -11.728   |
| MaxReturn            | 4.0536    |
| StdReturn            | 3.143     |
| AverageEpisodeLength | 18.31     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 31        |
| StdEpisodeLength     | 2.7953    |
| TotalNEpisodes       | 2791      |
| TotalNSamples        | 49937     |
| ExplainedVariance    | 0.17806   |
------------------------------------
[2018-01-21 12:34:01.739537 UTC] Saving snapshot
[2018-01-21 12:34:01.739995 UTC] Starting iteration 10
[2018-01-21 12:34:01.740254 UTC] Start collecting samples
[2018-01-21 12:34:08.366679 UTC] Computing input variables for policy optimization
[2018-01-21 12:34:08.837698 UTC] Performing policy update
[2018-01-21 12:34:08.839045 UTC] Computing gradient in Euclidean space
[2018-01-21 12:34:08.965908 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:34:10.461800 UTC] Performing line search
[2018-01-21 12:34:10.677989 UTC] Updating baseline
[2018-01-21 12:34:12.658234 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.030577  |
| ActualImprovement    | 0.030054  |
| ImprovementRatio     | 0.98289   |
| MeanKL               | 0.0066494 |
| Entropy              | 7.6229    |
| Perplexity           | 2044.6    |
| AveragePolicyStd     | 0.8624    |
| AveragePolicyStd[0]  | 0.89587   |
| AveragePolicyStd[1]  | 0.89001   |
| AveragePolicyStd[2]  | 0.85675   |
| AveragePolicyStd[3]  | 0.84251   |
| AveragePolicyStd[4]  | 0.86403   |
| AveragePolicyStd[5]  | 0.82524   |
| AverageReturn        | -2.8181   |
| MinReturn            | -11.867   |
| MaxReturn            | 2.7385    |
| StdReturn            | 2.9038    |
| AverageEpisodeLength | 18.13     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 2.9922    |
| TotalNEpisodes       | 3067      |
| TotalNSamples        | 54907     |
| ExplainedVariance    | 0.21784   |
------------------------------------
[2018-01-21 12:34:12.891764 UTC] Saving snapshot
[2018-01-21 12:34:12.907340 UTC] Starting iteration 11
[2018-01-21 12:34:12.907634 UTC] Start collecting samples
[2018-01-21 12:34:19.645381 UTC] Computing input variables for policy optimization
[2018-01-21 12:34:20.193235 UTC] Performing policy update
[2018-01-21 12:34:20.193865 UTC] Computing gradient in Euclidean space
[2018-01-21 12:34:20.333520 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:34:21.874804 UTC] Performing line search
[2018-01-21 12:34:22.097434 UTC] Updating baseline
[2018-01-21 12:34:24.112013 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.030408  |
| ActualImprovement    | 0.029744  |
| ImprovementRatio     | 0.97817   |
| MeanKL               | 0.0065944 |
| Entropy              | 7.5024    |
| Perplexity           | 1812.3    |
| AveragePolicyStd     | 0.84525   |
| AveragePolicyStd[0]  | 0.87186   |
| AveragePolicyStd[1]  | 0.87761   |
| AveragePolicyStd[2]  | 0.84351   |
| AveragePolicyStd[3]  | 0.82522   |
| AveragePolicyStd[4]  | 0.84597   |
| AveragePolicyStd[5]  | 0.80734   |
| AverageReturn        | -1.9744   |
| MinReturn            | -11.042   |
| MaxReturn            | 7.4567    |
| StdReturn            | 3.4347    |
| AverageEpisodeLength | 18.42     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 33        |
| StdEpisodeLength     | 3.4444    |
| TotalNEpisodes       | 3340      |
| TotalNSamples        | 59988     |
| ExplainedVariance    | 0.12813   |
------------------------------------
[2018-01-21 12:34:24.420831 UTC] Saving snapshot
[2018-01-21 12:34:24.421087 UTC] Starting iteration 12
[2018-01-21 12:34:24.421238 UTC] Start collecting samples
[2018-01-21 12:34:31.401013 UTC] Computing input variables for policy optimization
[2018-01-21 12:34:31.815478 UTC] Performing policy update
[2018-01-21 12:34:31.816187 UTC] Computing gradient in Euclidean space
[2018-01-21 12:34:31.935281 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:34:33.422998 UTC] Performing line search
[2018-01-21 12:34:33.645852 UTC] Updating baseline
[2018-01-21 12:34:35.637926 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.031047  |
| ActualImprovement    | 0.030066  |
| ImprovementRatio     | 0.9684    |
| MeanKL               | 0.0067312 |
| Entropy              | 7.3867    |
| Perplexity           | 1614.4    |
| AveragePolicyStd     | 0.82908   |
| AveragePolicyStd[0]  | 0.86118   |
| AveragePolicyStd[1]  | 0.85499   |
| AveragePolicyStd[2]  | 0.82601   |
| AveragePolicyStd[3]  | 0.81013   |
| AveragePolicyStd[4]  | 0.82447   |
| AveragePolicyStd[5]  | 0.79767   |
| AverageReturn        | -1.2863   |
| MinReturn            | -11.302   |
| MaxReturn            | 10.036    |
| StdReturn            | 3.2888    |
| AverageEpisodeLength | 18.56     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 32        |
| StdEpisodeLength     | 3.1506    |
| TotalNEpisodes       | 3609      |
| TotalNSamples        | 64921     |
| ExplainedVariance    | 0.16523   |
------------------------------------
[2018-01-21 12:34:35.926097 UTC] Saving snapshot
[2018-01-21 12:34:35.926345 UTC] Starting iteration 13
[2018-01-21 12:34:35.926545 UTC] Start collecting samples
[2018-01-21 12:34:42.984076 UTC] Computing input variables for policy optimization
[2018-01-21 12:34:43.461988 UTC] Performing policy update
[2018-01-21 12:34:43.462606 UTC] Computing gradient in Euclidean space
[2018-01-21 12:34:43.611204 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:34:45.403659 UTC] Performing line search
[2018-01-21 12:34:45.612877 UTC] Updating baseline
[2018-01-21 12:34:47.877926 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.026707  |
| ActualImprovement    | 0.026198  |
| ImprovementRatio     | 0.98097   |
| MeanKL               | 0.0066699 |
| Entropy              | 7.2688    |
| Perplexity           | 1434.8    |
| AveragePolicyStd     | 0.81294   |
| AveragePolicyStd[0]  | 0.83824   |
| AveragePolicyStd[1]  | 0.84281   |
| AveragePolicyStd[2]  | 0.81568   |
| AveragePolicyStd[3]  | 0.78563   |
| AveragePolicyStd[4]  | 0.80686   |
| AveragePolicyStd[5]  | 0.7884    |
| AverageReturn        | -0.53733  |
| MinReturn            | -11.63    |
| MaxReturn            | 7.1697    |
| StdReturn            | 2.8719    |
| AverageEpisodeLength | 18.39     |
| MinEpisodeLength     | 15        |
| MaxEpisodeLength     | 30        |
| StdEpisodeLength     | 2.6149    |
| TotalNEpisodes       | 3881      |
| TotalNSamples        | 69986     |
| ExplainedVariance    | 0.16001   |
------------------------------------
[2018-01-21 12:34:48.151912 UTC] Saving snapshot
[2018-01-21 12:34:48.152232 UTC] Starting iteration 14
[2018-01-21 12:34:48.152416 UTC] Start collecting samples
[2018-01-21 12:34:54.825018 UTC] Computing input variables for policy optimization
[2018-01-21 12:34:55.146584 UTC] Performing policy update
[2018-01-21 12:34:55.147081 UTC] Computing gradient in Euclidean space
[2018-01-21 12:34:55.249677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:34:56.233912 UTC] Performing line search
[2018-01-21 12:34:56.373444 UTC] Updating baseline
[2018-01-21 12:34:57.782332 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.027788  |
| ActualImprovement    | 0.026922  |
| ImprovementRatio     | 0.96883   |
| MeanKL               | 0.0066522 |
| Entropy              | 7.1377    |
| Perplexity           | 1258.5    |
| AveragePolicyStd     | 0.79544   |
| AveragePolicyStd[0]  | 0.81941   |
| AveragePolicyStd[1]  | 0.82723   |
| AveragePolicyStd[2]  | 0.80025   |
| AveragePolicyStd[3]  | 0.76111   |
| AveragePolicyStd[4]  | 0.79513   |
| AveragePolicyStd[5]  | 0.76951   |
| AverageReturn        | -0.30965  |
| MinReturn            | -6.6066   |
| MaxReturn            | 7.2637    |
| StdReturn            | 2.8415    |
| AverageEpisodeLength | 18.26     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.3265    |
| TotalNEpisodes       | 4153      |
| TotalNSamples        | 74979     |
| ExplainedVariance    | 0.21088   |
------------------------------------
[2018-01-21 12:34:57.961997 UTC] Saving snapshot
[2018-01-21 12:34:57.962176 UTC] Starting iteration 15
[2018-01-21 12:34:57.962335 UTC] Start collecting samples
[2018-01-21 12:35:01.962746 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:02.226376 UTC] Performing policy update
[2018-01-21 12:35:02.226915 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:02.308598 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:03.293212 UTC] Performing line search
[2018-01-21 12:35:03.421713 UTC] Updating baseline
[2018-01-21 12:35:04.674761 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.025585  |
| ActualImprovement    | 0.025357  |
| ImprovementRatio     | 0.9911    |
| MeanKL               | 0.0065235 |
| Entropy              | 7.0122    |
| Perplexity           | 1110.1    |
| AveragePolicyStd     | 0.77906   |
| AveragePolicyStd[0]  | 0.8081    |
| AveragePolicyStd[1]  | 0.81457   |
| AveragePolicyStd[2]  | 0.78328   |
| AveragePolicyStd[3]  | 0.74781   |
| AveragePolicyStd[4]  | 0.77272   |
| AveragePolicyStd[5]  | 0.74787   |
| AverageReturn        | 0.62927   |
| MinReturn            | -6.9776   |
| MaxReturn            | 13.387    |
| StdReturn            | 3.097     |
| AverageEpisodeLength | 18.07     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 32        |
| StdEpisodeLength     | 2.6655    |
| TotalNEpisodes       | 4426      |
| TotalNSamples        | 79973     |
| ExplainedVariance    | 0.25799   |
------------------------------------
[2018-01-21 12:35:04.850179 UTC] Saving snapshot
[2018-01-21 12:35:04.850383 UTC] Starting iteration 16
[2018-01-21 12:35:04.850573 UTC] Start collecting samples
[2018-01-21 12:35:08.921104 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:09.180421 UTC] Performing policy update
[2018-01-21 12:35:09.180999 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:09.266640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:10.221504 UTC] Performing line search
[2018-01-21 12:35:10.354181 UTC] Updating baseline
[2018-01-21 12:35:11.650689 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.026165  |
| ActualImprovement    | 0.025663  |
| ImprovementRatio     | 0.98083   |
| MeanKL               | 0.0067108 |
| Entropy              | 6.89      |
| Perplexity           | 982.37    |
| AveragePolicyStd     | 0.76332   |
| AveragePolicyStd[0]  | 0.78108   |
| AveragePolicyStd[1]  | 0.80508   |
| AveragePolicyStd[2]  | 0.76857   |
| AveragePolicyStd[3]  | 0.72924   |
| AveragePolicyStd[4]  | 0.75375   |
| AveragePolicyStd[5]  | 0.74222   |
| AverageReturn        | 0.96582   |
| MinReturn            | -6.4413   |
| MaxReturn            | 11.917    |
| StdReturn            | 3.3317    |
| AverageEpisodeLength | 18.98     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 36        |
| StdEpisodeLength     | 3.4928    |
| TotalNEpisodes       | 4694      |
| TotalNSamples        | 85000     |
| ExplainedVariance    | 0.21884   |
------------------------------------
[2018-01-21 12:35:11.833702 UTC] Saving snapshot
[2018-01-21 12:35:11.833929 UTC] Starting iteration 17
[2018-01-21 12:35:11.834113 UTC] Start collecting samples
[2018-01-21 12:35:15.710953 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:15.973694 UTC] Performing policy update
[2018-01-21 12:35:15.974336 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:16.055098 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:17.037333 UTC] Performing line search
[2018-01-21 12:35:17.171413 UTC] Updating baseline
[2018-01-21 12:35:18.459750 UTC] Computing logging information
-----------------------------------
| Iteration            | 17       |
| ExpectedImprovement  | 0.024582 |
| ActualImprovement    | 0.02454  |
| ImprovementRatio     | 0.99828  |
| MeanKL               | 0.006587 |
| Entropy              | 6.745    |
| Perplexity           | 849.77   |
| AveragePolicyStd     | 0.74516  |
| AveragePolicyStd[0]  | 0.76941  |
| AveragePolicyStd[1]  | 0.78742  |
| AveragePolicyStd[2]  | 0.74514  |
| AveragePolicyStd[3]  | 0.70925  |
| AveragePolicyStd[4]  | 0.73397  |
| AveragePolicyStd[5]  | 0.72577  |
| AverageReturn        | 1.5125   |
| MinReturn            | -5.7357  |
| MaxReturn            | 10.127   |
| StdReturn            | 3.0336   |
| AverageEpisodeLength | 18.76    |
| MinEpisodeLength     | 16       |
| MaxEpisodeLength     | 33       |
| StdEpisodeLength     | 2.8394   |
| TotalNEpisodes       | 4963     |
| TotalNSamples        | 89992    |
| ExplainedVariance    | 0.27488  |
-----------------------------------
[2018-01-21 12:35:18.645747 UTC] Saving snapshot
[2018-01-21 12:35:18.645955 UTC] Starting iteration 18
[2018-01-21 12:35:18.646095 UTC] Start collecting samples
[2018-01-21 12:35:22.839248 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:23.098537 UTC] Performing policy update
[2018-01-21 12:35:23.099236 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:23.181489 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:24.140480 UTC] Performing line search
[2018-01-21 12:35:24.271416 UTC] Updating baseline
[2018-01-21 12:35:25.466139 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| ExpectedImprovement  | 0.028601  |
| ActualImprovement    | 0.028354  |
| ImprovementRatio     | 0.99136   |
| MeanKL               | 0.0065576 |
| Entropy              | 6.6121    |
| Perplexity           | 744.01    |
| AveragePolicyStd     | 0.72893   |
| AveragePolicyStd[0]  | 0.75499   |
| AveragePolicyStd[1]  | 0.77035   |
| AveragePolicyStd[2]  | 0.73555   |
| AveragePolicyStd[3]  | 0.68661   |
| AveragePolicyStd[4]  | 0.71587   |
| AveragePolicyStd[5]  | 0.71018   |
| AverageReturn        | 1.7592    |
| MinReturn            | -5.125    |
| MaxReturn            | 10.833    |
| StdReturn            | 2.7571    |
| AverageEpisodeLength | 18.67     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 2.4982    |
| TotalNEpisodes       | 5231      |
| TotalNSamples        | 94999     |
| ExplainedVariance    | 0.3657    |
------------------------------------
[2018-01-21 12:35:25.655468 UTC] Saving snapshot
[2018-01-21 12:35:25.655677 UTC] Starting iteration 19
[2018-01-21 12:35:25.655844 UTC] Start collecting samples
[2018-01-21 12:35:29.688956 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:29.971242 UTC] Performing policy update
[2018-01-21 12:35:29.972344 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:30.056793 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:31.014252 UTC] Performing line search
[2018-01-21 12:35:31.143013 UTC] Updating baseline
[2018-01-21 12:35:32.328113 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.024767  |
| ActualImprovement    | 0.024211  |
| ImprovementRatio     | 0.97751   |
| MeanKL               | 0.0065762 |
| Entropy              | 6.4881    |
| Perplexity           | 657.3     |
| AveragePolicyStd     | 0.71406   |
| AveragePolicyStd[0]  | 0.73865   |
| AveragePolicyStd[1]  | 0.75844   |
| AveragePolicyStd[2]  | 0.71134   |
| AveragePolicyStd[3]  | 0.67051   |
| AveragePolicyStd[4]  | 0.71025   |
| AveragePolicyStd[5]  | 0.69517   |
| AverageReturn        | 2.5393    |
| MinReturn            | -6.1711   |
| MaxReturn            | 11.648    |
| StdReturn            | 3.194     |
| AverageEpisodeLength | 19.37     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 36        |
| StdEpisodeLength     | 3.6322    |
| TotalNEpisodes       | 5492      |
| TotalNSamples        | 99995     |
| ExplainedVariance    | 0.30767   |
------------------------------------
[2018-01-21 12:35:32.526985 UTC] Saving snapshot
[2018-01-21 12:35:32.527204 UTC] Starting iteration 20
[2018-01-21 12:35:32.527372 UTC] Start collecting samples
[2018-01-21 12:35:36.937334 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:37.364146 UTC] Performing policy update
[2018-01-21 12:35:37.364856 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:37.476257 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:38.564231 UTC] Performing line search
[2018-01-21 12:35:38.711114 UTC] Updating baseline
[2018-01-21 12:35:39.882637 UTC] Computing logging information
-------------------------------------
| Iteration            | 20         |
| ExpectedImprovement  | 0.025324   |
| ActualImprovement    | 0.025059   |
| ImprovementRatio     | 0.98953    |
| MeanKL               | 0.0067046  |
| Entropy              | 6.3559     |
| Perplexity           | 575.87     |
| AveragePolicyStd     | 0.69851    |
| AveragePolicyStd[0]  | 0.71815    |
| AveragePolicyStd[1]  | 0.7487     |
| AveragePolicyStd[2]  | 0.6906     |
| AveragePolicyStd[3]  | 0.66494    |
| AveragePolicyStd[4]  | 0.69814    |
| AveragePolicyStd[5]  | 0.67053    |
| AverageReturn        | 3.04       |
| MinReturn            | -3.0106    |
| MaxReturn            | 13.678     |
| StdReturn            | 2.4698     |
| AverageEpisodeLength | 18.69      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 2.4564     |
| TotalNEpisodes       | 5757       |
| TotalNSamples        | 1.0501e+05 |
| ExplainedVariance    | 0.41406    |
-------------------------------------
[2018-01-21 12:35:40.090567 UTC] Saving snapshot
[2018-01-21 12:35:40.096980 UTC] Starting iteration 21
[2018-01-21 12:35:40.097227 UTC] Start collecting samples
[2018-01-21 12:35:44.180849 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:44.427949 UTC] Performing policy update
[2018-01-21 12:35:44.428463 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:44.506769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:45.490970 UTC] Performing line search
[2018-01-21 12:35:45.617971 UTC] Updating baseline
[2018-01-21 12:35:46.874820 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| ExpectedImprovement  | 0.023127   |
| ActualImprovement    | 0.023128   |
| ImprovementRatio     | 1          |
| MeanKL               | 0.0068322  |
| Entropy              | 6.2185     |
| Perplexity           | 501.95     |
| AveragePolicyStd     | 0.68294    |
| AveragePolicyStd[0]  | 0.70034    |
| AveragePolicyStd[1]  | 0.74475    |
| AveragePolicyStd[2]  | 0.67457    |
| AveragePolicyStd[3]  | 0.64559    |
| AveragePolicyStd[4]  | 0.68218    |
| AveragePolicyStd[5]  | 0.65019    |
| AverageReturn        | 3.6394     |
| MinReturn            | -2.6417    |
| MaxReturn            | 13.519     |
| StdReturn            | 2.9877     |
| AverageEpisodeLength | 19.33      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 2.8109     |
| TotalNEpisodes       | 6016       |
| TotalNSamples        | 1.1007e+05 |
| ExplainedVariance    | 0.36416    |
-------------------------------------
[2018-01-21 12:35:47.080830 UTC] Saving snapshot
[2018-01-21 12:35:47.081037 UTC] Starting iteration 22
[2018-01-21 12:35:47.081191 UTC] Start collecting samples
[2018-01-21 12:35:51.361275 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:51.606029 UTC] Performing policy update
[2018-01-21 12:35:51.606734 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:51.711871 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:52.689482 UTC] Performing line search
[2018-01-21 12:35:52.817002 UTC] Updating baseline
[2018-01-21 12:35:54.087964 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| ExpectedImprovement  | 0.023251  |
| ActualImprovement    | 0.023091  |
| ImprovementRatio     | 0.99308   |
| MeanKL               | 0.0066871 |
| Entropy              | 6.0763    |
| Perplexity           | 435.43    |
| AveragePolicyStd     | 0.6669    |
| AveragePolicyStd[0]  | 0.68133   |
| AveragePolicyStd[1]  | 0.72782   |
| AveragePolicyStd[2]  | 0.65565   |
| AveragePolicyStd[3]  | 0.6356    |
| AveragePolicyStd[4]  | 0.66589   |
| AveragePolicyStd[5]  | 0.63513   |
| AverageReturn        | 4.0248    |
| MinReturn            | -3.4395   |
| MaxReturn            | 9.2165    |
| StdReturn            | 2.5241    |
| AverageEpisodeLength | 18.75     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 27        |
| StdEpisodeLength     | 1.4448    |
| TotalNEpisodes       | 6275      |
| TotalNSamples        | 1.15e+05  |
| ExplainedVariance    | 0.41362   |
------------------------------------
[2018-01-21 12:35:54.308348 UTC] Saving snapshot
[2018-01-21 12:35:54.308583 UTC] Starting iteration 23
[2018-01-21 12:35:54.308731 UTC] Start collecting samples
[2018-01-21 12:35:58.491281 UTC] Computing input variables for policy optimization
[2018-01-21 12:35:58.745457 UTC] Performing policy update
[2018-01-21 12:35:58.746460 UTC] Computing gradient in Euclidean space
[2018-01-21 12:35:58.835965 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:35:59.796993 UTC] Performing line search
[2018-01-21 12:35:59.926837 UTC] Updating baseline
[2018-01-21 12:36:01.153553 UTC] Computing logging information
-------------------------------------
| Iteration            | 23         |
| ExpectedImprovement  | 0.023839   |
| ActualImprovement    | 0.023697   |
| ImprovementRatio     | 0.99406    |
| MeanKL               | 0.0066054  |
| Entropy              | 5.9533     |
| Perplexity           | 385.01     |
| AveragePolicyStd     | 0.65328    |
| AveragePolicyStd[0]  | 0.66519    |
| AveragePolicyStd[1]  | 0.7085     |
| AveragePolicyStd[2]  | 0.64251    |
| AveragePolicyStd[3]  | 0.61953    |
| AveragePolicyStd[4]  | 0.6556     |
| AveragePolicyStd[5]  | 0.62832    |
| AverageReturn        | 4.462      |
| MinReturn            | -4.8289    |
| MaxReturn            | 19.902     |
| StdReturn            | 3.5451     |
| AverageEpisodeLength | 19.46      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.3299     |
| TotalNEpisodes       | 6535       |
| TotalNSamples        | 1.2009e+05 |
| ExplainedVariance    | 0.34814    |
-------------------------------------
[2018-01-21 12:36:01.379658 UTC] Saving snapshot
[2018-01-21 12:36:01.379914 UTC] Starting iteration 24
[2018-01-21 12:36:01.380097 UTC] Start collecting samples
[2018-01-21 12:36:05.714419 UTC] Computing input variables for policy optimization
[2018-01-21 12:36:05.998579 UTC] Performing policy update
[2018-01-21 12:36:05.999083 UTC] Computing gradient in Euclidean space
[2018-01-21 12:36:06.089389 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:36:07.122310 UTC] Performing line search
[2018-01-21 12:36:07.308678 UTC] Updating baseline
[2018-01-21 12:36:08.647950 UTC] Computing logging information
-------------------------------------
| Iteration            | 24         |
| ExpectedImprovement  | 0.027278   |
| ActualImprovement    | 0.027056   |
| ImprovementRatio     | 0.99186    |
| MeanKL               | 0.0065953  |
| Entropy              | 5.8328     |
| Perplexity           | 341.3      |
| AveragePolicyStd     | 0.64035    |
| AveragePolicyStd[0]  | 0.654      |
| AveragePolicyStd[1]  | 0.69668    |
| AveragePolicyStd[2]  | 0.62944    |
| AveragePolicyStd[3]  | 0.60536    |
| AveragePolicyStd[4]  | 0.64267    |
| AveragePolicyStd[5]  | 0.61396    |
| AverageReturn        | 4.8008     |
| MinReturn            | -2.2947    |
| MaxReturn            | 12.949     |
| StdReturn            | 2.6363     |
| AverageEpisodeLength | 19.14      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.1355     |
| TotalNEpisodes       | 6793       |
| TotalNSamples        | 1.2508e+05 |
| ExplainedVariance    | 0.44537    |
-------------------------------------
[2018-01-21 12:36:08.880053 UTC] Saving snapshot
[2018-01-21 12:36:08.880339 UTC] Starting iteration 25
[2018-01-21 12:36:08.880527 UTC] Start collecting samples
[2018-01-21 12:36:13.789745 UTC] Computing input variables for policy optimization
[2018-01-21 12:36:14.045061 UTC] Performing policy update
[2018-01-21 12:36:14.046022 UTC] Computing gradient in Euclidean space
[2018-01-21 12:36:14.163727 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:36:15.212234 UTC] Performing line search
[2018-01-21 12:36:15.340167 UTC] Updating baseline
[2018-01-21 12:36:16.765129 UTC] Computing logging information
-------------------------------------
| Iteration            | 25         |
| ExpectedImprovement  | 0.023734   |
| ActualImprovement    | 0.023082   |
| ImprovementRatio     | 0.97253    |
| MeanKL               | 0.0064002  |
| Entropy              | 5.7335     |
| Perplexity           | 309.06     |
| AveragePolicyStd     | 0.63003    |
| AveragePolicyStd[0]  | 0.64029    |
| AveragePolicyStd[1]  | 0.6923     |
| AveragePolicyStd[2]  | 0.61681    |
| AveragePolicyStd[3]  | 0.58939    |
| AveragePolicyStd[4]  | 0.63887    |
| AveragePolicyStd[5]  | 0.60253    |
| AverageReturn        | 5.2919     |
| MinReturn            | -4.8419    |
| MaxReturn            | 16.785     |
| StdReturn            | 2.8641     |
| AverageEpisodeLength | 19.12      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 2.3549     |
| TotalNEpisodes       | 7052       |
| TotalNSamples        | 1.3011e+05 |
| ExplainedVariance    | 0.49737    |
-------------------------------------
[2018-01-21 12:36:17.037393 UTC] Saving snapshot
[2018-01-21 12:36:17.037616 UTC] Starting iteration 26
[2018-01-21 12:36:17.037756 UTC] Start collecting samples
[2018-01-21 12:36:24.063545 UTC] Computing input variables for policy optimization
[2018-01-21 12:36:24.508007 UTC] Performing policy update
[2018-01-21 12:36:24.508608 UTC] Computing gradient in Euclidean space
[2018-01-21 12:36:24.628053 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:36:26.161073 UTC] Performing line search
[2018-01-21 12:36:26.371338 UTC] Updating baseline
[2018-01-21 12:36:28.203390 UTC] Computing logging information
-------------------------------------
| Iteration            | 26         |
| ExpectedImprovement  | 0.020405   |
| ActualImprovement    | 0.02042    |
| ImprovementRatio     | 1.0007     |
| MeanKL               | 0.0066797  |
| Entropy              | 5.6116     |
| Perplexity           | 273.58     |
| AveragePolicyStd     | 0.61741    |
| AveragePolicyStd[0]  | 0.62797    |
| AveragePolicyStd[1]  | 0.68076    |
| AveragePolicyStd[2]  | 0.6067     |
| AveragePolicyStd[3]  | 0.57351    |
| AveragePolicyStd[4]  | 0.62213    |
| AveragePolicyStd[5]  | 0.59338    |
| AverageReturn        | 5.7958     |
| MinReturn            | -3.0856    |
| MaxReturn            | 15.513     |
| StdReturn            | 2.8249     |
| AverageEpisodeLength | 19.28      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.214      |
| TotalNEpisodes       | 7306       |
| TotalNSamples        | 1.3507e+05 |
| ExplainedVariance    | 0.53214    |
-------------------------------------
[2018-01-21 12:36:28.603785 UTC] Saving snapshot
[2018-01-21 12:36:28.604094 UTC] Starting iteration 27
[2018-01-21 12:36:28.604331 UTC] Start collecting samples
[2018-01-21 12:36:36.171908 UTC] Computing input variables for policy optimization
[2018-01-21 12:36:36.618710 UTC] Performing policy update
[2018-01-21 12:36:36.619603 UTC] Computing gradient in Euclidean space
[2018-01-21 12:36:36.760625 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:36:38.336902 UTC] Performing line search
[2018-01-21 12:36:38.535462 UTC] Updating baseline
[2018-01-21 12:36:40.686331 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| ExpectedImprovement  | 0.022959   |
| ActualImprovement    | 0.022569   |
| ImprovementRatio     | 0.983      |
| MeanKL               | 0.0066262  |
| Entropy              | 5.5112     |
| Perplexity           | 247.45     |
| AveragePolicyStd     | 0.60721    |
| AveragePolicyStd[0]  | 0.62101    |
| AveragePolicyStd[1]  | 0.67005    |
| AveragePolicyStd[2]  | 0.59793    |
| AveragePolicyStd[3]  | 0.56343    |
| AveragePolicyStd[4]  | 0.61039    |
| AveragePolicyStd[5]  | 0.58045    |
| AverageReturn        | 6.2183     |
| MinReturn            | -0.58634   |
| MaxReturn            | 17.973     |
| StdReturn            | 3.0125     |
| AverageEpisodeLength | 19.52      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 2.7331     |
| TotalNEpisodes       | 7562       |
| TotalNSamples        | 1.4009e+05 |
| ExplainedVariance    | 0.55108    |
-------------------------------------
[2018-01-21 12:36:41.116871 UTC] Saving snapshot
[2018-01-21 12:36:41.117245 UTC] Starting iteration 28
[2018-01-21 12:36:41.117471 UTC] Start collecting samples
[2018-01-21 12:36:48.992067 UTC] Computing input variables for policy optimization
[2018-01-21 12:36:49.443038 UTC] Performing policy update
[2018-01-21 12:36:49.443842 UTC] Computing gradient in Euclidean space
[2018-01-21 12:36:49.571441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:36:51.041342 UTC] Performing line search
[2018-01-21 12:36:51.237334 UTC] Updating baseline
[2018-01-21 12:36:53.244629 UTC] Computing logging information
-------------------------------------
| Iteration            | 28         |
| ExpectedImprovement  | 0.023903   |
| ActualImprovement    | 0.02344    |
| ImprovementRatio     | 0.98065    |
| MeanKL               | 0.006827   |
| Entropy              | 5.3818     |
| Perplexity           | 217.42     |
| AveragePolicyStd     | 0.59426    |
| AveragePolicyStd[0]  | 0.60607    |
| AveragePolicyStd[1]  | 0.65827    |
| AveragePolicyStd[2]  | 0.58151    |
| AveragePolicyStd[3]  | 0.55628    |
| AveragePolicyStd[4]  | 0.59725    |
| AveragePolicyStd[5]  | 0.56615    |
| AverageReturn        | 7.0421     |
| MinReturn            | 0.73761    |
| MaxReturn            | 13.887     |
| StdReturn            | 2.9691     |
| AverageEpisodeLength | 20.06      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.8664     |
| TotalNEpisodes       | 7811       |
| TotalNSamples        | 1.4505e+05 |
| ExplainedVariance    | 0.51628    |
-------------------------------------
[2018-01-21 12:36:53.632703 UTC] Saving snapshot
[2018-01-21 12:36:53.632992 UTC] Starting iteration 29
[2018-01-21 12:36:53.633171 UTC] Start collecting samples
[2018-01-21 12:37:01.592038 UTC] Computing input variables for policy optimization
[2018-01-21 12:37:02.033652 UTC] Performing policy update
[2018-01-21 12:37:02.034562 UTC] Computing gradient in Euclidean space
[2018-01-21 12:37:02.162422 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:37:03.681140 UTC] Performing line search
[2018-01-21 12:37:03.870110 UTC] Updating baseline
[2018-01-21 12:37:05.804386 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| ExpectedImprovement  | 0.023866  |
| ActualImprovement    | 0.024065  |
| ImprovementRatio     | 1.0083    |
| MeanKL               | 0.0067061 |
| Entropy              | 5.2677    |
| Perplexity           | 193.97    |
| AveragePolicyStd     | 0.58318   |
| AveragePolicyStd[0]  | 0.58937   |
| AveragePolicyStd[1]  | 0.6506    |
| AveragePolicyStd[2]  | 0.57149   |
| AveragePolicyStd[3]  | 0.53953   |
| AveragePolicyStd[4]  | 0.5888    |
| AveragePolicyStd[5]  | 0.55928   |
| AverageReturn        | 6.9011    |
| MinReturn            | -1.0428   |
| MaxReturn            | 12.971    |
| StdReturn            | 2.367     |
| AverageEpisodeLength | 19.4      |
| MinEpisodeLength     | 17        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 2         |
| TotalNEpisodes       | 8070      |
| TotalNSamples        | 1.501e+05 |
| ExplainedVariance    | 0.61346   |
------------------------------------
[2018-01-21 12:37:06.180814 UTC] Saving snapshot
[2018-01-21 12:37:06.181054 UTC] Starting iteration 30
[2018-01-21 12:37:06.181197 UTC] Start collecting samples
[2018-01-21 12:37:13.470396 UTC] Computing input variables for policy optimization
[2018-01-21 12:37:13.863190 UTC] Performing policy update
[2018-01-21 12:37:13.863828 UTC] Computing gradient in Euclidean space
[2018-01-21 12:37:13.979934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:37:15.444749 UTC] Performing line search
[2018-01-21 12:37:15.634560 UTC] Updating baseline
[2018-01-21 12:37:17.436212 UTC] Computing logging information
-------------------------------------
| Iteration            | 30         |
| ExpectedImprovement  | 0.020968   |
| ActualImprovement    | 0.02026    |
| ImprovementRatio     | 0.96624    |
| MeanKL               | 0.0065985  |
| Entropy              | 5.1559     |
| Perplexity           | 173.46     |
| AveragePolicyStd     | 0.57249    |
| AveragePolicyStd[0]  | 0.57286    |
| AveragePolicyStd[1]  | 0.64173    |
| AveragePolicyStd[2]  | 0.55469    |
| AveragePolicyStd[3]  | 0.53122    |
| AveragePolicyStd[4]  | 0.58588    |
| AveragePolicyStd[5]  | 0.54857    |
| AverageReturn        | 7.485      |
| MinReturn            | -5.9606    |
| MaxReturn            | 23.101     |
| StdReturn            | 3.4466     |
| AverageEpisodeLength | 19.76      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 30         |
| StdEpisodeLength     | 2.25       |
| TotalNEpisodes       | 8320       |
| TotalNSamples        | 1.5506e+05 |
| ExplainedVariance    | 0.53353    |
-------------------------------------
[2018-01-21 12:37:17.858778 UTC] Saving snapshot
[2018-01-21 12:37:17.870082 UTC] Starting iteration 31
[2018-01-21 12:37:17.870372 UTC] Start collecting samples
[2018-01-21 12:37:25.443136 UTC] Computing input variables for policy optimization
[2018-01-21 12:37:25.790704 UTC] Performing policy update
[2018-01-21 12:37:25.791763 UTC] Computing gradient in Euclidean space
[2018-01-21 12:37:25.974310 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:37:27.422228 UTC] Performing line search
[2018-01-21 12:37:27.621407 UTC] Updating baseline
[2018-01-21 12:37:29.461784 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| ExpectedImprovement  | 0.021413   |
| ActualImprovement    | 0.021585   |
| ImprovementRatio     | 1.008      |
| MeanKL               | 0.0066799  |
| Entropy              | 5.0219     |
| Perplexity           | 151.7      |
| AveragePolicyStd     | 0.55975    |
| AveragePolicyStd[0]  | 0.56181    |
| AveragePolicyStd[1]  | 0.62536    |
| AveragePolicyStd[2]  | 0.54214    |
| AveragePolicyStd[3]  | 0.52025    |
| AveragePolicyStd[4]  | 0.56733    |
| AveragePolicyStd[5]  | 0.5416     |
| AverageReturn        | 7.9589     |
| MinReturn            | 0.72517    |
| MaxReturn            | 21.152     |
| StdReturn            | 3.038      |
| AverageEpisodeLength | 19.92      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 2.5404     |
| TotalNEpisodes       | 8570       |
| TotalNSamples        | 1.6007e+05 |
| ExplainedVariance    | 0.53108    |
-------------------------------------
[2018-01-21 12:37:29.858777 UTC] Saving snapshot
[2018-01-21 12:37:29.859071 UTC] Starting iteration 32
[2018-01-21 12:37:29.859271 UTC] Start collecting samples
[2018-01-21 12:37:37.119776 UTC] Computing input variables for policy optimization
[2018-01-21 12:37:37.511783 UTC] Performing policy update
[2018-01-21 12:37:37.512528 UTC] Computing gradient in Euclidean space
[2018-01-21 12:37:37.631784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:37:39.012171 UTC] Performing line search
[2018-01-21 12:37:39.204337 UTC] Updating baseline
[2018-01-21 12:37:41.030224 UTC] Computing logging information
-------------------------------------
| Iteration            | 32         |
| ExpectedImprovement  | 0.021387   |
| ActualImprovement    | 0.021222   |
| ImprovementRatio     | 0.99227    |
| MeanKL               | 0.0065872  |
| Entropy              | 4.9029     |
| Perplexity           | 134.68     |
| AveragePolicyStd     | 0.54874    |
| AveragePolicyStd[0]  | 0.55223    |
| AveragePolicyStd[1]  | 0.61165    |
| AveragePolicyStd[2]  | 0.53459    |
| AveragePolicyStd[3]  | 0.50731    |
| AveragePolicyStd[4]  | 0.55391    |
| AveragePolicyStd[5]  | 0.53276    |
| AverageReturn        | 8.8177     |
| MinReturn            | 2.3144     |
| MaxReturn            | 21.49      |
| StdReturn            | 3.1624     |
| AverageEpisodeLength | 20.34      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 2.9333     |
| TotalNEpisodes       | 8819       |
| TotalNSamples        | 1.6512e+05 |
| ExplainedVariance    | 0.55693    |
-------------------------------------
[2018-01-21 12:37:41.445402 UTC] Saving snapshot
[2018-01-21 12:37:41.445650 UTC] Starting iteration 33
[2018-01-21 12:37:41.445805 UTC] Start collecting samples
[2018-01-21 12:37:49.004276 UTC] Computing input variables for policy optimization
[2018-01-21 12:37:49.377397 UTC] Performing policy update
[2018-01-21 12:37:49.378032 UTC] Computing gradient in Euclidean space
[2018-01-21 12:37:49.499295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:37:50.924344 UTC] Performing line search
[2018-01-21 12:37:51.120609 UTC] Updating baseline
[2018-01-21 12:37:52.882531 UTC] Computing logging information
-------------------------------------
| Iteration            | 33         |
| ExpectedImprovement  | 0.021457   |
| ActualImprovement    | 0.021195   |
| ImprovementRatio     | 0.9878     |
| MeanKL               | 0.0065799  |
| Entropy              | 4.8342     |
| Perplexity           | 125.74     |
| AveragePolicyStd     | 0.54258    |
| AveragePolicyStd[0]  | 0.54244    |
| AveragePolicyStd[1]  | 0.61143    |
| AveragePolicyStd[2]  | 0.52709    |
| AveragePolicyStd[3]  | 0.50712    |
| AveragePolicyStd[4]  | 0.54631    |
| AveragePolicyStd[5]  | 0.5211     |
| AverageReturn        | 8.8725     |
| MinReturn            | 3.0459     |
| MaxReturn            | 18.745     |
| StdReturn            | 2.8894     |
| AverageEpisodeLength | 20.03      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.3852     |
| TotalNEpisodes       | 9065       |
| TotalNSamples        | 1.7008e+05 |
| ExplainedVariance    | 0.67772    |
-------------------------------------
[2018-01-21 12:37:53.318946 UTC] Saving snapshot
[2018-01-21 12:37:53.319244 UTC] Starting iteration 34
[2018-01-21 12:37:53.319461 UTC] Start collecting samples
[2018-01-21 12:38:00.557330 UTC] Computing input variables for policy optimization
[2018-01-21 12:38:00.874135 UTC] Performing policy update
[2018-01-21 12:38:00.874793 UTC] Computing gradient in Euclidean space
[2018-01-21 12:38:00.997545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:38:02.426095 UTC] Performing line search
[2018-01-21 12:38:02.633923 UTC] Updating baseline
[2018-01-21 12:38:04.760180 UTC] Computing logging information
-------------------------------------
| Iteration            | 34         |
| ExpectedImprovement  | 0.022808   |
| ActualImprovement    | 0.022675   |
| ImprovementRatio     | 0.99417    |
| MeanKL               | 0.0066337  |
| Entropy              | 4.7452     |
| Perplexity           | 115.03     |
| AveragePolicyStd     | 0.53469    |
| AveragePolicyStd[0]  | 0.54124    |
| AveragePolicyStd[1]  | 0.60499    |
| AveragePolicyStd[2]  | 0.52084    |
| AveragePolicyStd[3]  | 0.49729    |
| AveragePolicyStd[4]  | 0.53326    |
| AveragePolicyStd[5]  | 0.51051    |
| AverageReturn        | 8.9949     |
| MinReturn            | 3.3963     |
| MaxReturn            | 21.939     |
| StdReturn            | 2.8865     |
| AverageEpisodeLength | 20.4       |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.3196     |
| TotalNEpisodes       | 9311       |
| TotalNSamples        | 1.7514e+05 |
| ExplainedVariance    | 0.56048    |
-------------------------------------
[2018-01-21 12:38:05.299485 UTC] Saving snapshot
[2018-01-21 12:38:05.299870 UTC] Starting iteration 35
[2018-01-21 12:38:05.300137 UTC] Start collecting samples
[2018-01-21 12:38:13.042482 UTC] Computing input variables for policy optimization
[2018-01-21 12:38:13.388616 UTC] Performing policy update
[2018-01-21 12:38:13.389685 UTC] Computing gradient in Euclidean space
[2018-01-21 12:38:13.509487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:38:14.921026 UTC] Performing line search
[2018-01-21 12:38:15.123283 UTC] Updating baseline
[2018-01-21 12:38:16.868132 UTC] Computing logging information
-------------------------------------
| Iteration            | 35         |
| ExpectedImprovement  | 0.022667   |
| ActualImprovement    | 0.022847   |
| ImprovementRatio     | 1.0079     |
| MeanKL               | 0.00681    |
| Entropy              | 4.6827     |
| Perplexity           | 108.06     |
| AveragePolicyStd     | 0.52928    |
| AveragePolicyStd[0]  | 0.53143    |
| AveragePolicyStd[1]  | 0.60599    |
| AveragePolicyStd[2]  | 0.51184    |
| AveragePolicyStd[3]  | 0.49549    |
| AveragePolicyStd[4]  | 0.52804    |
| AveragePolicyStd[5]  | 0.5029     |
| AverageReturn        | 9.4229     |
| MinReturn            | -2.7346    |
| MaxReturn            | 21.639     |
| StdReturn            | 3.5544     |
| AverageEpisodeLength | 20.85      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 4.1554     |
| TotalNEpisodes       | 9554       |
| TotalNSamples        | 1.8011e+05 |
| ExplainedVariance    | 0.5741     |
-------------------------------------
[2018-01-21 12:38:17.314159 UTC] Saving snapshot
[2018-01-21 12:38:17.314552 UTC] Starting iteration 36
[2018-01-21 12:38:17.314857 UTC] Start collecting samples
[2018-01-21 12:38:24.946986 UTC] Computing input variables for policy optimization
[2018-01-21 12:38:25.425781 UTC] Performing policy update
[2018-01-21 12:38:25.426450 UTC] Computing gradient in Euclidean space
[2018-01-21 12:38:25.562705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:38:26.946505 UTC] Performing line search
[2018-01-21 12:38:27.153800 UTC] Updating baseline
[2018-01-21 12:38:29.088164 UTC] Computing logging information
-------------------------------------
| Iteration            | 36         |
| ExpectedImprovement  | 0.021697   |
| ActualImprovement    | 0.021823   |
| ImprovementRatio     | 1.0058     |
| MeanKL               | 0.0068689  |
| Entropy              | 4.5767     |
| Perplexity           | 97.195     |
| AveragePolicyStd     | 0.51999    |
| AveragePolicyStd[0]  | 0.51978    |
| AveragePolicyStd[1]  | 0.59737    |
| AveragePolicyStd[2]  | 0.50233    |
| AveragePolicyStd[3]  | 0.49739    |
| AveragePolicyStd[4]  | 0.51098    |
| AveragePolicyStd[5]  | 0.49212    |
| AverageReturn        | 9.8765     |
| MinReturn            | 1.8657     |
| MaxReturn            | 23.405     |
| StdReturn            | 3.2911     |
| AverageEpisodeLength | 19.95      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 28         |
| StdEpisodeLength     | 2.0561     |
| TotalNEpisodes       | 9801       |
| TotalNSamples        | 1.8508e+05 |
| ExplainedVariance    | 0.64067    |
-------------------------------------
[2018-01-21 12:38:29.522561 UTC] Saving snapshot
[2018-01-21 12:38:29.522811 UTC] Starting iteration 37
[2018-01-21 12:38:29.522994 UTC] Start collecting samples
[2018-01-21 12:38:39.040464 UTC] Computing input variables for policy optimization
[2018-01-21 12:38:39.400955 UTC] Performing policy update
[2018-01-21 12:38:39.402030 UTC] Computing gradient in Euclidean space
[2018-01-21 12:38:39.527416 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:38:41.002054 UTC] Performing line search
[2018-01-21 12:38:41.217050 UTC] Updating baseline
[2018-01-21 12:38:43.094083 UTC] Computing logging information
-------------------------------------
| Iteration            | 37         |
| ExpectedImprovement  | 0.022589   |
| ActualImprovement    | 0.022499   |
| ImprovementRatio     | 0.99603    |
| MeanKL               | 0.0068605  |
| Entropy              | 4.4832     |
| Perplexity           | 88.518     |
| AveragePolicyStd     | 0.5121     |
| AveragePolicyStd[0]  | 0.50684    |
| AveragePolicyStd[1]  | 0.59443    |
| AveragePolicyStd[2]  | 0.49479    |
| AveragePolicyStd[3]  | 0.49122    |
| AveragePolicyStd[4]  | 0.50196    |
| AveragePolicyStd[5]  | 0.48336    |
| AverageReturn        | 10.167     |
| MinReturn            | 1.8371     |
| MaxReturn            | 26.396     |
| StdReturn            | 3.8827     |
| AverageEpisodeLength | 20.43      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 3.4907     |
| TotalNEpisodes       | 10048      |
| TotalNSamples        | 1.9015e+05 |
| ExplainedVariance    | 0.6048     |
-------------------------------------
[2018-01-21 12:38:43.529873 UTC] Saving snapshot
[2018-01-21 12:38:43.530123 UTC] Starting iteration 38
[2018-01-21 12:38:43.530306 UTC] Start collecting samples
[2018-01-21 12:38:51.099047 UTC] Computing input variables for policy optimization
[2018-01-21 12:38:51.460479 UTC] Performing policy update
[2018-01-21 12:38:51.461402 UTC] Computing gradient in Euclidean space
[2018-01-21 12:38:51.611406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:38:53.239765 UTC] Performing line search
[2018-01-21 12:38:53.475231 UTC] Updating baseline
[2018-01-21 12:38:55.876966 UTC] Computing logging information
-------------------------------------
| Iteration            | 38         |
| ExpectedImprovement  | 0.022147   |
| ActualImprovement    | 0.022105   |
| ImprovementRatio     | 0.99812    |
| MeanKL               | 0.0067652  |
| Entropy              | 4.3858     |
| Perplexity           | 80.303     |
| AveragePolicyStd     | 0.50381    |
| AveragePolicyStd[0]  | 0.49593    |
| AveragePolicyStd[1]  | 0.58299    |
| AveragePolicyStd[2]  | 0.49133    |
| AveragePolicyStd[3]  | 0.48483    |
| AveragePolicyStd[4]  | 0.49458    |
| AveragePolicyStd[5]  | 0.47319    |
| AverageReturn        | 10.22      |
| MinReturn            | 4.8223     |
| MaxReturn            | 19.584     |
| StdReturn            | 2.93       |
| AverageEpisodeLength | 19.96      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 28         |
| StdEpisodeLength     | 2.102      |
| TotalNEpisodes       | 10299      |
| TotalNSamples        | 1.9515e+05 |
| ExplainedVariance    | 0.71592    |
-------------------------------------
[2018-01-21 12:38:56.563244 UTC] Saving snapshot
[2018-01-21 12:38:56.563519 UTC] Starting iteration 39
[2018-01-21 12:38:56.563705 UTC] Start collecting samples
[2018-01-21 12:39:06.125132 UTC] Computing input variables for policy optimization
[2018-01-21 12:39:06.488618 UTC] Performing policy update
[2018-01-21 12:39:06.489108 UTC] Computing gradient in Euclidean space
[2018-01-21 12:39:06.613557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:39:08.072894 UTC] Performing line search
[2018-01-21 12:39:08.274022 UTC] Updating baseline
[2018-01-21 12:39:10.169385 UTC] Computing logging information
-------------------------------------
| Iteration            | 39         |
| ExpectedImprovement  | 0.023001   |
| ActualImprovement    | 0.022873   |
| ImprovementRatio     | 0.99444    |
| MeanKL               | 0.0067459  |
| Entropy              | 4.2892     |
| Perplexity           | 72.907     |
| AveragePolicyStd     | 0.49583    |
| AveragePolicyStd[0]  | 0.48493    |
| AveragePolicyStd[1]  | 0.5766     |
| AveragePolicyStd[2]  | 0.48359    |
| AveragePolicyStd[3]  | 0.47449    |
| AveragePolicyStd[4]  | 0.48799    |
| AveragePolicyStd[5]  | 0.46739    |
| AverageReturn        | 10.978     |
| MinReturn            | 3.8497     |
| MaxReturn            | 26.196     |
| StdReturn            | 3.7888     |
| AverageEpisodeLength | 20.21      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 2.6088     |
| TotalNEpisodes       | 10545      |
| TotalNSamples        | 2.0013e+05 |
| ExplainedVariance    | 0.65092    |
-------------------------------------
[2018-01-21 12:39:10.621230 UTC] Saving snapshot
[2018-01-21 12:39:10.621480 UTC] Starting iteration 40
[2018-01-21 12:39:10.621648 UTC] Start collecting samples
[2018-01-21 12:39:17.781755 UTC] Computing input variables for policy optimization
[2018-01-21 12:39:18.194198 UTC] Performing policy update
[2018-01-21 12:39:18.194853 UTC] Computing gradient in Euclidean space
[2018-01-21 12:39:18.327237 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:39:19.884367 UTC] Performing line search
[2018-01-21 12:39:20.079755 UTC] Updating baseline
[2018-01-21 12:39:21.831659 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| ExpectedImprovement  | 0.020735   |
| ActualImprovement    | 0.020714   |
| ImprovementRatio     | 0.99899    |
| MeanKL               | 0.0065065  |
| Entropy              | 4.2025     |
| Perplexity           | 66.852     |
| AveragePolicyStd     | 0.48879    |
| AveragePolicyStd[0]  | 0.48363    |
| AveragePolicyStd[1]  | 0.57027    |
| AveragePolicyStd[2]  | 0.47488    |
| AveragePolicyStd[3]  | 0.46496    |
| AveragePolicyStd[4]  | 0.47914    |
| AveragePolicyStd[5]  | 0.45988    |
| AverageReturn        | 11.098     |
| MinReturn            | 3.3735     |
| MaxReturn            | 30.342     |
| StdReturn            | 4.059      |
| AverageEpisodeLength | 20.53      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 36         |
| StdEpisodeLength     | 3.8948     |
| TotalNEpisodes       | 10790      |
| TotalNSamples        | 2.0514e+05 |
| ExplainedVariance    | 0.63727    |
-------------------------------------
[2018-01-21 12:39:22.365788 UTC] Saving snapshot
[2018-01-21 12:39:22.375984 UTC] Starting iteration 41
[2018-01-21 12:39:22.376216 UTC] Start collecting samples
[2018-01-21 12:39:30.054101 UTC] Computing input variables for policy optimization
[2018-01-21 12:39:30.419971 UTC] Performing policy update
[2018-01-21 12:39:30.420601 UTC] Computing gradient in Euclidean space
[2018-01-21 12:39:30.557932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:39:32.031055 UTC] Performing line search
[2018-01-21 12:39:32.260373 UTC] Updating baseline
[2018-01-21 12:39:34.166003 UTC] Computing logging information
-------------------------------------
| Iteration            | 41         |
| ExpectedImprovement  | 0.020872   |
| ActualImprovement    | 0.020768   |
| ImprovementRatio     | 0.99503    |
| MeanKL               | 0.0065664  |
| Entropy              | 4.1207     |
| Perplexity           | 61.602     |
| AveragePolicyStd     | 0.48213    |
| AveragePolicyStd[0]  | 0.47977    |
| AveragePolicyStd[1]  | 0.56096    |
| AveragePolicyStd[2]  | 0.46991    |
| AveragePolicyStd[3]  | 0.45771    |
| AveragePolicyStd[4]  | 0.46916    |
| AveragePolicyStd[5]  | 0.45529    |
| AverageReturn        | 11.665     |
| MinReturn            | 1.9211     |
| MaxReturn            | 23.933     |
| StdReturn            | 3.6618     |
| AverageEpisodeLength | 20.72      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.8986     |
| TotalNEpisodes       | 11030      |
| TotalNSamples        | 2.1012e+05 |
| ExplainedVariance    | 0.65421    |
-------------------------------------
[2018-01-21 12:39:34.699987 UTC] Saving snapshot
[2018-01-21 12:39:34.700230 UTC] Starting iteration 42
[2018-01-21 12:39:34.700389 UTC] Start collecting samples
[2018-01-21 12:39:42.995245 UTC] Computing input variables for policy optimization
[2018-01-21 12:39:43.436441 UTC] Performing policy update
[2018-01-21 12:39:43.437597 UTC] Computing gradient in Euclidean space
[2018-01-21 12:39:43.572138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:39:45.478948 UTC] Performing line search
[2018-01-21 12:39:45.671131 UTC] Updating baseline
[2018-01-21 12:39:47.457982 UTC] Computing logging information
-------------------------------------
| Iteration            | 42         |
| ExpectedImprovement  | 0.023446   |
| ActualImprovement    | 0.023285   |
| ImprovementRatio     | 0.99311    |
| MeanKL               | 0.0066027  |
| Entropy              | 4.0568     |
| Perplexity           | 57.792     |
| AveragePolicyStd     | 0.4771     |
| AveragePolicyStd[0]  | 0.47098    |
| AveragePolicyStd[1]  | 0.55804    |
| AveragePolicyStd[2]  | 0.4671     |
| AveragePolicyStd[3]  | 0.45196    |
| AveragePolicyStd[4]  | 0.46343    |
| AveragePolicyStd[5]  | 0.45111    |
| AverageReturn        | 11.471     |
| MinReturn            | 2.737      |
| MaxReturn            | 21.431     |
| StdReturn            | 3.5716     |
| AverageEpisodeLength | 20.86      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 40         |
| StdEpisodeLength     | 3.5469     |
| TotalNEpisodes       | 11273      |
| TotalNSamples        | 2.1514e+05 |
| ExplainedVariance    | 0.73398    |
-------------------------------------
[2018-01-21 12:39:47.938500 UTC] Saving snapshot
[2018-01-21 12:39:47.938757 UTC] Starting iteration 43
[2018-01-21 12:39:47.938921 UTC] Start collecting samples
[2018-01-21 12:39:56.804452 UTC] Computing input variables for policy optimization
[2018-01-21 12:39:57.207001 UTC] Performing policy update
[2018-01-21 12:39:57.207669 UTC] Computing gradient in Euclidean space
[2018-01-21 12:39:57.340926 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:39:58.871174 UTC] Performing line search
[2018-01-21 12:39:59.085463 UTC] Updating baseline
[2018-01-21 12:40:00.835110 UTC] Computing logging information
-------------------------------------
| Iteration            | 43         |
| ExpectedImprovement  | 0.021947   |
| ActualImprovement    | 0.022094   |
| ImprovementRatio     | 1.0067     |
| MeanKL               | 0.0067425  |
| Entropy              | 4.0041     |
| Perplexity           | 54.824     |
| AveragePolicyStd     | 0.4728     |
| AveragePolicyStd[0]  | 0.46748    |
| AveragePolicyStd[1]  | 0.54911    |
| AveragePolicyStd[2]  | 0.46191    |
| AveragePolicyStd[3]  | 0.45004    |
| AveragePolicyStd[4]  | 0.45932    |
| AveragePolicyStd[5]  | 0.44895    |
| AverageReturn        | 12.411     |
| MinReturn            | 3.7603     |
| MaxReturn            | 33.96      |
| StdReturn            | 4.6095     |
| AverageEpisodeLength | 21.31      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 40         |
| StdEpisodeLength     | 4.039      |
| TotalNEpisodes       | 11512      |
| TotalNSamples        | 2.2018e+05 |
| ExplainedVariance    | 0.63922    |
-------------------------------------
[2018-01-21 12:40:01.326944 UTC] Saving snapshot
[2018-01-21 12:40:01.327194 UTC] Starting iteration 44
[2018-01-21 12:40:01.327379 UTC] Start collecting samples
[2018-01-21 12:40:09.618827 UTC] Computing input variables for policy optimization
[2018-01-21 12:40:10.016846 UTC] Performing policy update
[2018-01-21 12:40:10.017463 UTC] Computing gradient in Euclidean space
[2018-01-21 12:40:10.167667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:40:11.658202 UTC] Performing line search
[2018-01-21 12:40:11.853125 UTC] Updating baseline
[2018-01-21 12:40:13.698286 UTC] Computing logging information
-------------------------------------
| Iteration            | 44         |
| ExpectedImprovement  | 0.02051    |
| ActualImprovement    | 0.020449   |
| ImprovementRatio     | 0.99701    |
| MeanKL               | 0.0066216  |
| Entropy              | 3.939      |
| Perplexity           | 51.367     |
| AveragePolicyStd     | 0.46765    |
| AveragePolicyStd[0]  | 0.46054    |
| AveragePolicyStd[1]  | 0.54158    |
| AveragePolicyStd[2]  | 0.45958    |
| AveragePolicyStd[3]  | 0.44237    |
| AveragePolicyStd[4]  | 0.45212    |
| AveragePolicyStd[5]  | 0.44971    |
| AverageReturn        | 12.942     |
| MinReturn            | 6.0109     |
| MaxReturn            | 29.776     |
| StdReturn            | 3.8475     |
| AverageEpisodeLength | 21.27      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.4055     |
| TotalNEpisodes       | 11746      |
| TotalNSamples        | 2.2522e+05 |
| ExplainedVariance    | 0.65933    |
-------------------------------------
[2018-01-21 12:40:14.210447 UTC] Saving snapshot
[2018-01-21 12:40:14.210752 UTC] Starting iteration 45
[2018-01-21 12:40:14.210956 UTC] Start collecting samples
[2018-01-21 12:40:21.685658 UTC] Computing input variables for policy optimization
[2018-01-21 12:40:22.070759 UTC] Performing policy update
[2018-01-21 12:40:22.071366 UTC] Computing gradient in Euclidean space
[2018-01-21 12:40:22.195203 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:40:23.637985 UTC] Performing line search
[2018-01-21 12:40:23.834259 UTC] Updating baseline
[2018-01-21 12:40:25.572254 UTC] Computing logging information
-------------------------------------
| Iteration            | 45         |
| ExpectedImprovement  | 0.023214   |
| ActualImprovement    | 0.02291    |
| ImprovementRatio     | 0.9869     |
| MeanKL               | 0.0066466  |
| Entropy              | 3.9074     |
| Perplexity           | 49.769     |
| AveragePolicyStd     | 0.46541    |
| AveragePolicyStd[0]  | 0.45839    |
| AveragePolicyStd[1]  | 0.54652    |
| AveragePolicyStd[2]  | 0.45158    |
| AveragePolicyStd[3]  | 0.43992    |
| AveragePolicyStd[4]  | 0.45188    |
| AveragePolicyStd[5]  | 0.44419    |
| AverageReturn        | 13.802     |
| MinReturn            | 5.9902     |
| MaxReturn            | 25.913     |
| StdReturn            | 4.1933     |
| AverageEpisodeLength | 22.83      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 46         |
| StdEpisodeLength     | 5.0101     |
| TotalNEpisodes       | 11968      |
| TotalNSamples        | 2.3019e+05 |
| ExplainedVariance    | 0.67599    |
-------------------------------------
[2018-01-21 12:40:26.064581 UTC] Saving snapshot
[2018-01-21 12:40:26.064849 UTC] Starting iteration 46
[2018-01-21 12:40:26.065029 UTC] Start collecting samples
[2018-01-21 12:40:33.208848 UTC] Computing input variables for policy optimization
[2018-01-21 12:40:33.537266 UTC] Performing policy update
[2018-01-21 12:40:33.537953 UTC] Computing gradient in Euclidean space
[2018-01-21 12:40:33.655386 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:40:35.075949 UTC] Performing line search
[2018-01-21 12:40:35.266404 UTC] Updating baseline
[2018-01-21 12:40:37.120537 UTC] Computing logging information
-------------------------------------
| Iteration            | 46         |
| ExpectedImprovement  | 0.021727   |
| ActualImprovement    | 0.021154   |
| ImprovementRatio     | 0.97365    |
| MeanKL               | 0.0064406  |
| Entropy              | 3.8449     |
| Perplexity           | 46.754     |
| AveragePolicyStd     | 0.46069    |
| AveragePolicyStd[0]  | 0.45487    |
| AveragePolicyStd[1]  | 0.54387    |
| AveragePolicyStd[2]  | 0.44779    |
| AveragePolicyStd[3]  | 0.43632    |
| AveragePolicyStd[4]  | 0.44465    |
| AveragePolicyStd[5]  | 0.43664    |
| AverageReturn        | 13.637     |
| MinReturn            | -0.97213   |
| MaxReturn            | 26.407     |
| StdReturn            | 4.3809     |
| AverageEpisodeLength | 22.79      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 45         |
| StdEpisodeLength     | 4.6825     |
| TotalNEpisodes       | 12193      |
| TotalNSamples        | 2.3523e+05 |
| ExplainedVariance    | 0.70951    |
-------------------------------------
[2018-01-21 12:40:37.600784 UTC] Saving snapshot
[2018-01-21 12:40:37.600981 UTC] Starting iteration 47
[2018-01-21 12:40:37.601106 UTC] Start collecting samples
[2018-01-21 12:40:44.985915 UTC] Computing input variables for policy optimization
[2018-01-21 12:40:45.339222 UTC] Performing policy update
[2018-01-21 12:40:45.339819 UTC] Computing gradient in Euclidean space
[2018-01-21 12:40:45.470906 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:40:46.980108 UTC] Performing line search
[2018-01-21 12:40:47.180410 UTC] Updating baseline
[2018-01-21 12:40:48.938881 UTC] Computing logging information
-------------------------------------
| Iteration            | 47         |
| ExpectedImprovement  | 0.021871   |
| ActualImprovement    | 0.021843   |
| ImprovementRatio     | 0.99871    |
| MeanKL               | 0.0065707  |
| Entropy              | 3.7694     |
| Perplexity           | 43.355     |
| AveragePolicyStd     | 0.45503    |
| AveragePolicyStd[0]  | 0.4484     |
| AveragePolicyStd[1]  | 0.54004    |
| AveragePolicyStd[2]  | 0.44439    |
| AveragePolicyStd[3]  | 0.43207    |
| AveragePolicyStd[4]  | 0.43619    |
| AveragePolicyStd[5]  | 0.42907    |
| AverageReturn        | 14.239     |
| MinReturn            | 6.9301     |
| MaxReturn            | 26.941     |
| StdReturn            | 4.1659     |
| AverageEpisodeLength | 23.22      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 41         |
| StdEpisodeLength     | 4.7108     |
| TotalNEpisodes       | 12407      |
| TotalNSamples        | 2.4016e+05 |
| ExplainedVariance    | 0.69624    |
-------------------------------------
[2018-01-21 12:40:49.440216 UTC] Saving snapshot
[2018-01-21 12:40:49.440415 UTC] Starting iteration 48
[2018-01-21 12:40:49.440557 UTC] Start collecting samples
[2018-01-21 12:40:57.592130 UTC] Computing input variables for policy optimization
[2018-01-21 12:40:58.003886 UTC] Performing policy update
[2018-01-21 12:40:58.004523 UTC] Computing gradient in Euclidean space
[2018-01-21 12:40:58.127597 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:40:59.556729 UTC] Performing line search
[2018-01-21 12:40:59.748804 UTC] Updating baseline
[2018-01-21 12:41:01.551725 UTC] Computing logging information
-------------------------------------
| Iteration            | 48         |
| ExpectedImprovement  | 0.020734   |
| ActualImprovement    | 0.020707   |
| ImprovementRatio     | 0.9987     |
| MeanKL               | 0.0067209  |
| Entropy              | 3.6984     |
| Perplexity           | 40.384     |
| AveragePolicyStd     | 0.44982    |
| AveragePolicyStd[0]  | 0.44066    |
| AveragePolicyStd[1]  | 0.53861    |
| AveragePolicyStd[2]  | 0.43835    |
| AveragePolicyStd[3]  | 0.4276     |
| AveragePolicyStd[4]  | 0.43008    |
| AveragePolicyStd[5]  | 0.42364    |
| AverageReturn        | 14.605     |
| MinReturn            | 7.2816     |
| MaxReturn            | 26.89      |
| StdReturn            | 3.6839     |
| AverageEpisodeLength | 23.54      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 4.6828     |
| TotalNEpisodes       | 12625      |
| TotalNSamples        | 2.4522e+05 |
| ExplainedVariance    | 0.71253    |
-------------------------------------
[2018-01-21 12:41:02.069748 UTC] Saving snapshot
[2018-01-21 12:41:02.069963 UTC] Starting iteration 49
[2018-01-21 12:41:02.070146 UTC] Start collecting samples
[2018-01-21 12:41:10.136887 UTC] Computing input variables for policy optimization
[2018-01-21 12:41:10.485765 UTC] Performing policy update
[2018-01-21 12:41:10.486836 UTC] Computing gradient in Euclidean space
[2018-01-21 12:41:10.610076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:41:12.000272 UTC] Performing line search
[2018-01-21 12:41:12.211140 UTC] Updating baseline
[2018-01-21 12:41:14.031967 UTC] Computing logging information
-------------------------------------
| Iteration            | 49         |
| ExpectedImprovement  | 0.021355   |
| ActualImprovement    | 0.02126    |
| ImprovementRatio     | 0.99559    |
| MeanKL               | 0.0066305  |
| Entropy              | 3.6582     |
| Perplexity           | 38.79      |
| AveragePolicyStd     | 0.44688    |
| AveragePolicyStd[0]  | 0.43808    |
| AveragePolicyStd[1]  | 0.53619    |
| AveragePolicyStd[2]  | 0.43907    |
| AveragePolicyStd[3]  | 0.42631    |
| AveragePolicyStd[4]  | 0.42346    |
| AveragePolicyStd[5]  | 0.41817    |
| AverageReturn        | 15.36      |
| MinReturn            | 5.3342     |
| MaxReturn            | 27.421     |
| StdReturn            | 4.8241     |
| AverageEpisodeLength | 23.69      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 42         |
| StdEpisodeLength     | 4.7428     |
| TotalNEpisodes       | 12833      |
| TotalNSamples        | 2.5021e+05 |
| ExplainedVariance    | 0.68058    |
-------------------------------------
[2018-01-21 12:41:14.588420 UTC] Saving snapshot
[2018-01-21 12:41:14.588728 UTC] Starting iteration 50
[2018-01-21 12:41:14.589076 UTC] Start collecting samples
[2018-01-21 12:41:22.816336 UTC] Computing input variables for policy optimization
[2018-01-21 12:41:23.248502 UTC] Performing policy update
[2018-01-21 12:41:23.249186 UTC] Computing gradient in Euclidean space
[2018-01-21 12:41:23.347462 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:41:24.799625 UTC] Performing line search
[2018-01-21 12:41:24.992952 UTC] Updating baseline
[2018-01-21 12:41:27.011258 UTC] Computing logging information
------------------------------------
| Iteration            | 50        |
| ExpectedImprovement  | 0.021675  |
| ActualImprovement    | 0.021712  |
| ImprovementRatio     | 1.0017    |
| MeanKL               | 0.0068853 |
| Entropy              | 3.5979    |
| Perplexity           | 36.521    |
| AveragePolicyStd     | 0.44244   |
| AveragePolicyStd[0]  | 0.43691   |
| AveragePolicyStd[1]  | 0.5309    |
| AveragePolicyStd[2]  | 0.43389   |
| AveragePolicyStd[3]  | 0.4226    |
| AveragePolicyStd[4]  | 0.41793   |
| AveragePolicyStd[5]  | 0.41239   |
| AverageReturn        | 15.596    |
| MinReturn            | 4.984     |
| MaxReturn            | 31.574    |
| StdReturn            | 5.2088    |
| AverageEpisodeLength | 24.16     |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 40        |
| StdEpisodeLength     | 5.2701    |
| TotalNEpisodes       | 13047     |
| TotalNSamples        | 2.553e+05 |
| ExplainedVariance    | 0.69879   |
------------------------------------
[2018-01-21 12:41:27.639456 UTC] Saving snapshot
[2018-01-21 12:41:27.920597 UTC] Starting iteration 51
[2018-01-21 12:41:27.920866 UTC] Start collecting samples
[2018-01-21 12:41:36.141536 UTC] Computing input variables for policy optimization
[2018-01-21 12:41:36.470978 UTC] Performing policy update
[2018-01-21 12:41:36.471580 UTC] Computing gradient in Euclidean space
[2018-01-21 12:41:36.592879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:41:38.031723 UTC] Performing line search
[2018-01-21 12:41:38.225407 UTC] Updating baseline
[2018-01-21 12:41:39.968454 UTC] Computing logging information
------------------------------------
| Iteration            | 51        |
| ExpectedImprovement  | 0.022984  |
| ActualImprovement    | 0.023177  |
| ImprovementRatio     | 1.0084    |
| MeanKL               | 0.00669   |
| Entropy              | 3.5463    |
| Perplexity           | 34.683    |
| AveragePolicyStd     | 0.43866   |
| AveragePolicyStd[0]  | 0.43616   |
| AveragePolicyStd[1]  | 0.526     |
| AveragePolicyStd[2]  | 0.43011   |
| AveragePolicyStd[3]  | 0.41795   |
| AveragePolicyStd[4]  | 0.41545   |
| AveragePolicyStd[5]  | 0.40628   |
| AverageReturn        | 16.08     |
| MinReturn            | 6.9437    |
| MaxReturn            | 37.802    |
| StdReturn            | 4.5288    |
| AverageEpisodeLength | 25.52     |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 50        |
| StdEpisodeLength     | 5.2278    |
| TotalNEpisodes       | 13244     |
| TotalNSamples        | 2.602e+05 |
| ExplainedVariance    | 0.70811   |
------------------------------------
[2018-01-21 12:41:40.576855 UTC] Saving snapshot
[2018-01-21 12:41:40.577353 UTC] Starting iteration 52
[2018-01-21 12:41:40.577752 UTC] Start collecting samples
[2018-01-21 12:41:48.723436 UTC] Computing input variables for policy optimization
[2018-01-21 12:41:49.062015 UTC] Performing policy update
[2018-01-21 12:41:49.062740 UTC] Computing gradient in Euclidean space
[2018-01-21 12:41:49.188161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:41:50.717026 UTC] Performing line search
[2018-01-21 12:41:50.917554 UTC] Updating baseline
[2018-01-21 12:41:52.748602 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| ExpectedImprovement  | 0.020144   |
| ActualImprovement    | 0.02021    |
| ImprovementRatio     | 1.0033     |
| MeanKL               | 0.0067296  |
| Entropy              | 3.5021     |
| Perplexity           | 33.185     |
| AveragePolicyStd     | 0.43554    |
| AveragePolicyStd[0]  | 0.4333     |
| AveragePolicyStd[1]  | 0.52432    |
| AveragePolicyStd[2]  | 0.4291     |
| AveragePolicyStd[3]  | 0.41317    |
| AveragePolicyStd[4]  | 0.41008    |
| AveragePolicyStd[5]  | 0.40326    |
| AverageReturn        | 17.103     |
| MinReturn            | 5.2533     |
| MaxReturn            | 33.518     |
| StdReturn            | 4.7469     |
| AverageEpisodeLength | 25.12      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 42         |
| StdEpisodeLength     | 4.5766     |
| TotalNEpisodes       | 13445      |
| TotalNSamples        | 2.6523e+05 |
| ExplainedVariance    | 0.7444     |
-------------------------------------
[2018-01-21 12:41:53.307522 UTC] Saving snapshot
[2018-01-21 12:41:53.307780 UTC] Starting iteration 53
[2018-01-21 12:41:53.307961 UTC] Start collecting samples
[2018-01-21 12:42:01.007179 UTC] Computing input variables for policy optimization
[2018-01-21 12:42:01.367226 UTC] Performing policy update
[2018-01-21 12:42:01.373947 UTC] Computing gradient in Euclidean space
[2018-01-21 12:42:01.548393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:42:03.007856 UTC] Performing line search
[2018-01-21 12:42:03.199621 UTC] Updating baseline
[2018-01-21 12:42:05.108587 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| ExpectedImprovement  | 0.020344   |
| ActualImprovement    | 0.020049   |
| ImprovementRatio     | 0.98553    |
| MeanKL               | 0.0068707  |
| Entropy              | 3.4501     |
| Perplexity           | 31.505     |
| AveragePolicyStd     | 0.43181    |
| AveragePolicyStd[0]  | 0.42888    |
| AveragePolicyStd[1]  | 0.52045    |
| AveragePolicyStd[2]  | 0.42629    |
| AveragePolicyStd[3]  | 0.41139    |
| AveragePolicyStd[4]  | 0.40357    |
| AveragePolicyStd[5]  | 0.40028    |
| AverageReturn        | 17.131     |
| MinReturn            | 6.0269     |
| MaxReturn            | 35.917     |
| StdReturn            | 4.5399     |
| AverageEpisodeLength | 26.05      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 43         |
| StdEpisodeLength     | 4.4953     |
| TotalNEpisodes       | 13639      |
| TotalNSamples        | 2.7022e+05 |
| ExplainedVariance    | 0.77354    |
-------------------------------------
[2018-01-21 12:42:05.675143 UTC] Saving snapshot
[2018-01-21 12:42:05.675380 UTC] Starting iteration 54
[2018-01-21 12:42:05.675529 UTC] Start collecting samples
[2018-01-21 12:42:13.205897 UTC] Computing input variables for policy optimization
[2018-01-21 12:42:13.696876 UTC] Performing policy update
[2018-01-21 12:42:13.697494 UTC] Computing gradient in Euclidean space
[2018-01-21 12:42:13.835615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:42:15.890717 UTC] Performing line search
[2018-01-21 12:42:16.150155 UTC] Updating baseline
[2018-01-21 12:42:18.579580 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| ExpectedImprovement  | 0.023032   |
| ActualImprovement    | 0.022859   |
| ImprovementRatio     | 0.99246    |
| MeanKL               | 0.0067938  |
| Entropy              | 3.3899     |
| Perplexity           | 29.662     |
| AveragePolicyStd     | 0.4276     |
| AveragePolicyStd[0]  | 0.4258     |
| AveragePolicyStd[1]  | 0.51674    |
| AveragePolicyStd[2]  | 0.42595    |
| AveragePolicyStd[3]  | 0.40497    |
| AveragePolicyStd[4]  | 0.39495    |
| AveragePolicyStd[5]  | 0.39717    |
| AverageReturn        | 17.551     |
| MinReturn            | 8.2306     |
| MaxReturn            | 27.126     |
| StdReturn            | 3.8071     |
| AverageEpisodeLength | 26.22      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 36         |
| StdEpisodeLength     | 3.5793     |
| TotalNEpisodes       | 13829      |
| TotalNSamples        | 2.7523e+05 |
| ExplainedVariance    | 0.84772    |
-------------------------------------
[2018-01-21 12:42:19.350183 UTC] Saving snapshot
[2018-01-21 12:42:19.350532 UTC] Starting iteration 55
[2018-01-21 12:42:19.350737 UTC] Start collecting samples
[2018-01-21 12:42:31.181353 UTC] Computing input variables for policy optimization
[2018-01-21 12:42:31.730210 UTC] Performing policy update
[2018-01-21 12:42:31.730916 UTC] Computing gradient in Euclidean space
[2018-01-21 12:42:31.948264 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:42:34.060537 UTC] Performing line search
[2018-01-21 12:42:34.312469 UTC] Updating baseline
[2018-01-21 12:42:36.710007 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| ExpectedImprovement  | 0.024352   |
| ActualImprovement    | 0.023813   |
| ImprovementRatio     | 0.97786    |
| MeanKL               | 0.0066064  |
| Entropy              | 3.3219     |
| Perplexity           | 27.712     |
| AveragePolicyStd     | 0.42299    |
| AveragePolicyStd[0]  | 0.42053    |
| AveragePolicyStd[1]  | 0.51608    |
| AveragePolicyStd[2]  | 0.42214    |
| AveragePolicyStd[3]  | 0.4006     |
| AveragePolicyStd[4]  | 0.38708    |
| AveragePolicyStd[5]  | 0.39152    |
| AverageReturn        | 19.583     |
| MinReturn            | 8.4729     |
| MaxReturn            | 36.85      |
| StdReturn            | 4.2929     |
| AverageEpisodeLength | 28.38      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 45         |
| StdEpisodeLength     | 4.5514     |
| TotalNEpisodes       | 14011      |
| TotalNSamples        | 2.8028e+05 |
| ExplainedVariance    | 0.82408    |
-------------------------------------
[2018-01-21 12:42:37.342616 UTC] Saving snapshot
[2018-01-21 12:42:37.343288 UTC] Starting iteration 56
[2018-01-21 12:42:37.343764 UTC] Start collecting samples
[2018-01-21 12:42:50.027860 UTC] Computing input variables for policy optimization
[2018-01-21 12:42:50.484064 UTC] Performing policy update
[2018-01-21 12:42:50.484733 UTC] Computing gradient in Euclidean space
[2018-01-21 12:42:50.681303 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:42:52.520047 UTC] Performing line search
[2018-01-21 12:42:52.722979 UTC] Updating baseline
[2018-01-21 12:42:55.219131 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| ExpectedImprovement  | 0.029575   |
| ActualImprovement    | 0.029563   |
| ImprovementRatio     | 0.99959    |
| MeanKL               | 0.006595   |
| Entropy              | 3.2606     |
| Perplexity           | 26.066     |
| AveragePolicyStd     | 0.41884    |
| AveragePolicyStd[0]  | 0.41883    |
| AveragePolicyStd[1]  | 0.513      |
| AveragePolicyStd[2]  | 0.41927    |
| AveragePolicyStd[3]  | 0.39642    |
| AveragePolicyStd[4]  | 0.37968    |
| AveragePolicyStd[5]  | 0.38586    |
| AverageReturn        | 19.802     |
| MinReturn            | 6.5507     |
| MaxReturn            | 31.696     |
| StdReturn            | 4.1819     |
| AverageEpisodeLength | 28.72      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 46         |
| StdEpisodeLength     | 4.5035     |
| TotalNEpisodes       | 14180      |
| TotalNSamples        | 2.8518e+05 |
| ExplainedVariance    | 0.83034    |
-------------------------------------
[2018-01-21 12:42:55.834209 UTC] Saving snapshot
[2018-01-21 12:42:55.834390 UTC] Starting iteration 57
[2018-01-21 12:42:55.834546 UTC] Start collecting samples
[2018-01-21 12:43:07.035146 UTC] Computing input variables for policy optimization
[2018-01-21 12:43:07.379600 UTC] Performing policy update
[2018-01-21 12:43:07.380378 UTC] Computing gradient in Euclidean space
[2018-01-21 12:43:07.537802 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:43:09.473314 UTC] Performing line search
[2018-01-21 12:43:09.704493 UTC] Updating baseline
[2018-01-21 12:43:11.756537 UTC] Computing logging information
------------------------------------
| Iteration            | 57        |
| ExpectedImprovement  | 0.027842  |
| ActualImprovement    | 0.027033  |
| ImprovementRatio     | 0.97094   |
| MeanKL               | 0.0067174 |
| Entropy              | 3.2233    |
| Perplexity           | 25.111    |
| AveragePolicyStd     | 0.41632   |
| AveragePolicyStd[0]  | 0.41802   |
| AveragePolicyStd[1]  | 0.51069   |
| AveragePolicyStd[2]  | 0.41713   |
| AveragePolicyStd[3]  | 0.39405   |
| AveragePolicyStd[4]  | 0.37504   |
| AveragePolicyStd[5]  | 0.38299   |
| AverageReturn        | 20.477    |
| MinReturn            | 9.7524    |
| MaxReturn            | 40.992    |
| StdReturn            | 5.1307    |
| AverageEpisodeLength | 29.7      |
| MinEpisodeLength     | 21        |
| MaxEpisodeLength     | 51        |
| StdEpisodeLength     | 5.7035    |
| TotalNEpisodes       | 14351     |
| TotalNSamples        | 2.902e+05 |
| ExplainedVariance    | 0.85169   |
------------------------------------
[2018-01-21 12:43:12.412038 UTC] Saving snapshot
[2018-01-21 12:43:12.412339 UTC] Starting iteration 58
[2018-01-21 12:43:12.412556 UTC] Start collecting samples
[2018-01-21 12:43:24.444485 UTC] Computing input variables for policy optimization
[2018-01-21 12:43:24.885533 UTC] Performing policy update
[2018-01-21 12:43:24.886765 UTC] Computing gradient in Euclidean space
[2018-01-21 12:43:25.024266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:43:26.763835 UTC] Performing line search
[2018-01-21 12:43:26.998379 UTC] Updating baseline
[2018-01-21 12:43:29.345323 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| ExpectedImprovement  | 0.027541   |
| ActualImprovement    | 0.026902   |
| ImprovementRatio     | 0.97682    |
| MeanKL               | 0.006572   |
| Entropy              | 3.1664     |
| Perplexity           | 23.723     |
| AveragePolicyStd     | 0.4126     |
| AveragePolicyStd[0]  | 0.41553    |
| AveragePolicyStd[1]  | 0.51002    |
| AveragePolicyStd[2]  | 0.41394    |
| AveragePolicyStd[3]  | 0.3891     |
| AveragePolicyStd[4]  | 0.37152    |
| AveragePolicyStd[5]  | 0.37545    |
| AverageReturn        | 21.918     |
| MinReturn            | 6.8355     |
| MaxReturn            | 41.255     |
| StdReturn            | 5.359      |
| AverageEpisodeLength | 31.44      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 55         |
| StdEpisodeLength     | 6.0833     |
| TotalNEpisodes       | 14508      |
| TotalNSamples        | 2.9517e+05 |
| ExplainedVariance    | 0.84239    |
-------------------------------------
[2018-01-21 12:43:30.037838 UTC] Saving snapshot
[2018-01-21 12:43:30.038068 UTC] Starting iteration 59
[2018-01-21 12:43:30.038230 UTC] Start collecting samples
[2018-01-21 12:43:37.409280 UTC] Computing input variables for policy optimization
[2018-01-21 12:43:37.719922 UTC] Performing policy update
[2018-01-21 12:43:37.720702 UTC] Computing gradient in Euclidean space
[2018-01-21 12:43:37.862266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:43:39.321373 UTC] Performing line search
[2018-01-21 12:43:39.517264 UTC] Updating baseline
[2018-01-21 12:43:41.317911 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| ExpectedImprovement  | 0.027476   |
| ActualImprovement    | 0.027416   |
| ImprovementRatio     | 0.99779    |
| MeanKL               | 0.0067688  |
| Entropy              | 3.1321     |
| Perplexity           | 22.921     |
| AveragePolicyStd     | 0.41046    |
| AveragePolicyStd[0]  | 0.41554    |
| AveragePolicyStd[1]  | 0.51178    |
| AveragePolicyStd[2]  | 0.41087    |
| AveragePolicyStd[3]  | 0.38381    |
| AveragePolicyStd[4]  | 0.36762    |
| AveragePolicyStd[5]  | 0.37315    |
| AverageReturn        | 23.301     |
| MinReturn            | 10.385     |
| MaxReturn            | 35.467     |
| StdReturn            | 5.2121     |
| AverageEpisodeLength | 34.14      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 55         |
| StdEpisodeLength     | 5.8975     |
| TotalNEpisodes       | 14657      |
| TotalNSamples        | 3.0019e+05 |
| ExplainedVariance    | 0.8484     |
-------------------------------------
[2018-01-21 12:43:41.914390 UTC] Saving snapshot
[2018-01-21 12:43:41.914644 UTC] Starting iteration 60
[2018-01-21 12:43:41.914930 UTC] Start collecting samples
[2018-01-21 12:43:49.843744 UTC] Computing input variables for policy optimization
[2018-01-21 12:43:50.179406 UTC] Performing policy update
[2018-01-21 12:43:50.180181 UTC] Computing gradient in Euclidean space
[2018-01-21 12:43:50.299113 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:43:51.916826 UTC] Performing line search
[2018-01-21 12:43:52.116173 UTC] Updating baseline
[2018-01-21 12:43:54.095497 UTC] Computing logging information
------------------------------------
| Iteration            | 60        |
| ExpectedImprovement  | 0.025138  |
| ActualImprovement    | 0.025056  |
| ImprovementRatio     | 0.99677   |
| MeanKL               | 0.0067252 |
| Entropy              | 3.0953    |
| Perplexity           | 22.094    |
| AveragePolicyStd     | 0.40817   |
| AveragePolicyStd[0]  | 0.41347   |
| AveragePolicyStd[1]  | 0.51217   |
| AveragePolicyStd[2]  | 0.41136   |
| AveragePolicyStd[3]  | 0.37922   |
| AveragePolicyStd[4]  | 0.3647    |
| AveragePolicyStd[5]  | 0.36809   |
| AverageReturn        | 25.612    |
| MinReturn            | 7.2978    |
| MaxReturn            | 48.576    |
| StdReturn            | 7.006     |
| AverageEpisodeLength | 36.11     |
| MinEpisodeLength     | 21        |
| MaxEpisodeLength     | 67        |
| StdEpisodeLength     | 7.4497    |
| TotalNEpisodes       | 14796     |
| TotalNSamples        | 3.052e+05 |
| ExplainedVariance    | 0.83104   |
------------------------------------
[2018-01-21 12:43:54.764231 UTC] Saving snapshot
[2018-01-21 12:43:54.773870 UTC] Starting iteration 61
[2018-01-21 12:43:54.774119 UTC] Start collecting samples
[2018-01-21 12:44:02.433966 UTC] Computing input variables for policy optimization
[2018-01-21 12:44:02.702970 UTC] Performing policy update
[2018-01-21 12:44:02.703583 UTC] Computing gradient in Euclidean space
[2018-01-21 12:44:02.833332 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:44:04.322308 UTC] Performing line search
[2018-01-21 12:44:04.520934 UTC] Updating baseline
[2018-01-21 12:44:06.182154 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| ExpectedImprovement  | 0.025562   |
| ActualImprovement    | 0.025509   |
| ImprovementRatio     | 0.99795    |
| MeanKL               | 0.0066622  |
| Entropy              | 3.0509     |
| Perplexity           | 21.134     |
| AveragePolicyStd     | 0.40527    |
| AveragePolicyStd[0]  | 0.41262    |
| AveragePolicyStd[1]  | 0.50977    |
| AveragePolicyStd[2]  | 0.40842    |
| AveragePolicyStd[3]  | 0.37635    |
| AveragePolicyStd[4]  | 0.36129    |
| AveragePolicyStd[5]  | 0.36314    |
| AverageReturn        | 31.078     |
| MinReturn            | 9.404      |
| MaxReturn            | 70.052     |
| StdReturn            | 9.9865     |
| AverageEpisodeLength | 40.3       |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 79         |
| StdEpisodeLength     | 9.901      |
| TotalNEpisodes       | 14917      |
| TotalNSamples        | 3.1008e+05 |
| ExplainedVariance    | 0.76635    |
-------------------------------------
[2018-01-21 12:44:06.735704 UTC] Saving snapshot
[2018-01-21 12:44:06.735951 UTC] Starting iteration 62
[2018-01-21 12:44:06.736103 UTC] Start collecting samples
[2018-01-21 12:44:14.108427 UTC] Computing input variables for policy optimization
[2018-01-21 12:44:14.396885 UTC] Performing policy update
[2018-01-21 12:44:14.397885 UTC] Computing gradient in Euclidean space
[2018-01-21 12:44:14.549535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:44:16.350195 UTC] Performing line search
[2018-01-21 12:44:16.582556 UTC] Updating baseline
[2018-01-21 12:44:18.428087 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| ExpectedImprovement  | 0.025384   |
| ActualImprovement    | 0.025411   |
| ImprovementRatio     | 1.0011     |
| MeanKL               | 0.0066934  |
| Entropy              | 2.9951     |
| Perplexity           | 19.987     |
| AveragePolicyStd     | 0.40162    |
| AveragePolicyStd[0]  | 0.41094    |
| AveragePolicyStd[1]  | 0.50672    |
| AveragePolicyStd[2]  | 0.40346    |
| AveragePolicyStd[3]  | 0.37305    |
| AveragePolicyStd[4]  | 0.35701    |
| AveragePolicyStd[5]  | 0.35853    |
| AverageReturn        | 31.968     |
| MinReturn            | 10.633     |
| MaxReturn            | 59.214     |
| StdReturn            | 10.007     |
| AverageEpisodeLength | 42.2       |
| MinEpisodeLength     | 25         |
| MaxEpisodeLength     | 64         |
| StdEpisodeLength     | 8.9465     |
| TotalNEpisodes       | 15035      |
| TotalNSamples        | 3.1507e+05 |
| ExplainedVariance    | 0.81089    |
-------------------------------------
[2018-01-21 12:44:19.047072 UTC] Saving snapshot
[2018-01-21 12:44:19.047328 UTC] Starting iteration 63
[2018-01-21 12:44:19.047491 UTC] Start collecting samples
[2018-01-21 12:44:26.223820 UTC] Computing input variables for policy optimization
[2018-01-21 12:44:26.452703 UTC] Performing policy update
[2018-01-21 12:44:26.453312 UTC] Computing gradient in Euclidean space
[2018-01-21 12:44:26.576800 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:44:28.064694 UTC] Performing line search
[2018-01-21 12:44:28.279601 UTC] Updating baseline
[2018-01-21 12:44:30.165068 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| ExpectedImprovement  | 0.026832   |
| ActualImprovement    | 0.026433   |
| ImprovementRatio     | 0.98512    |
| MeanKL               | 0.0068815  |
| Entropy              | 2.9519     |
| Perplexity           | 19.142     |
| AveragePolicyStd     | 0.39888    |
| AveragePolicyStd[0]  | 0.40707    |
| AveragePolicyStd[1]  | 0.50637    |
| AveragePolicyStd[2]  | 0.40088    |
| AveragePolicyStd[3]  | 0.36961    |
| AveragePolicyStd[4]  | 0.35488    |
| AveragePolicyStd[5]  | 0.35447    |
| AverageReturn        | 35.536     |
| MinReturn            | 12.869     |
| MaxReturn            | 71.302     |
| StdReturn            | 12.029     |
| AverageEpisodeLength | 46.13      |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 118        |
| StdEpisodeLength     | 13.029     |
| TotalNEpisodes       | 15144      |
| TotalNSamples        | 3.2007e+05 |
| ExplainedVariance    | 0.79753    |
-------------------------------------
[2018-01-21 12:44:30.753311 UTC] Saving snapshot
[2018-01-21 12:44:30.753493 UTC] Starting iteration 64
[2018-01-21 12:44:30.753598 UTC] Start collecting samples
[2018-01-21 12:44:38.188694 UTC] Computing input variables for policy optimization
[2018-01-21 12:44:38.447638 UTC] Performing policy update
[2018-01-21 12:44:38.448297 UTC] Computing gradient in Euclidean space
[2018-01-21 12:44:38.573870 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:44:40.207807 UTC] Performing line search
[2018-01-21 12:44:40.405589 UTC] Updating baseline
[2018-01-21 12:44:42.284902 UTC] Computing logging information
------------------------------------
| Iteration            | 64        |
| ExpectedImprovement  | 0.023014  |
| ActualImprovement    | 0.022772  |
| ImprovementRatio     | 0.98946   |
| MeanKL               | 0.0066484 |
| Entropy              | 2.9134    |
| Perplexity           | 18.418    |
| AveragePolicyStd     | 0.39656   |
| AveragePolicyStd[0]  | 0.40596   |
| AveragePolicyStd[1]  | 0.50683   |
| AveragePolicyStd[2]  | 0.39955   |
| AveragePolicyStd[3]  | 0.36597   |
| AveragePolicyStd[4]  | 0.35006   |
| AveragePolicyStd[5]  | 0.35102   |
| AverageReturn        | 41.486    |
| MinReturn            | 17.414    |
| MaxReturn            | 70.941    |
| StdReturn            | 11.58     |
| AverageEpisodeLength | 52.47     |
| MinEpisodeLength     | 27        |
| MaxEpisodeLength     | 97        |
| StdEpisodeLength     | 14.628    |
| TotalNEpisodes       | 15236     |
| TotalNSamples        | 3.25e+05  |
| ExplainedVariance    | 0.77193   |
------------------------------------
[2018-01-21 12:44:42.906820 UTC] Saving snapshot
[2018-01-21 12:44:42.907067 UTC] Starting iteration 65
[2018-01-21 12:44:42.907245 UTC] Start collecting samples
[2018-01-21 12:44:49.890577 UTC] Computing input variables for policy optimization
[2018-01-21 12:44:50.112870 UTC] Performing policy update
[2018-01-21 12:44:50.113483 UTC] Computing gradient in Euclidean space
[2018-01-21 12:44:50.250683 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:44:51.804505 UTC] Performing line search
[2018-01-21 12:44:52.000944 UTC] Updating baseline
[2018-01-21 12:44:54.313698 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| ExpectedImprovement  | 0.02061    |
| ActualImprovement    | 0.020145   |
| ImprovementRatio     | 0.97746    |
| MeanKL               | 0.0067571  |
| Entropy              | 2.8748     |
| Perplexity           | 17.721     |
| AveragePolicyStd     | 0.39419    |
| AveragePolicyStd[0]  | 0.40463    |
| AveragePolicyStd[1]  | 0.50691    |
| AveragePolicyStd[2]  | 0.39538    |
| AveragePolicyStd[3]  | 0.36353    |
| AveragePolicyStd[4]  | 0.34589    |
| AveragePolicyStd[5]  | 0.34882    |
| AverageReturn        | 43.378     |
| MinReturn            | 15.257     |
| MaxReturn            | 77.553     |
| StdReturn            | 12.981     |
| AverageEpisodeLength | 59.67      |
| MinEpisodeLength     | 29         |
| MaxEpisodeLength     | 120        |
| StdEpisodeLength     | 18.227     |
| TotalNEpisodes       | 15319      |
| TotalNSamples        | 3.2997e+05 |
| ExplainedVariance    | 0.7713     |
-------------------------------------
[2018-01-21 12:44:54.957868 UTC] Saving snapshot
[2018-01-21 12:44:54.958066 UTC] Starting iteration 66
[2018-01-21 12:44:54.958200 UTC] Start collecting samples
[2018-01-21 12:45:02.826569 UTC] Computing input variables for policy optimization
[2018-01-21 12:45:03.049525 UTC] Performing policy update
[2018-01-21 12:45:03.050425 UTC] Computing gradient in Euclidean space
[2018-01-21 12:45:03.180167 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:45:04.857096 UTC] Performing line search
[2018-01-21 12:45:05.080118 UTC] Updating baseline
[2018-01-21 12:45:07.370105 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| ExpectedImprovement  | 0.021474   |
| ActualImprovement    | 0.021048   |
| ImprovementRatio     | 0.98018    |
| MeanKL               | 0.0066335  |
| Entropy              | 2.8406     |
| Perplexity           | 17.126     |
| AveragePolicyStd     | 0.39213    |
| AveragePolicyStd[0]  | 0.40001    |
| AveragePolicyStd[1]  | 0.50863    |
| AveragePolicyStd[2]  | 0.39162    |
| AveragePolicyStd[3]  | 0.36351    |
| AveragePolicyStd[4]  | 0.34339    |
| AveragePolicyStd[5]  | 0.34562    |
| AverageReturn        | 47.749     |
| MinReturn            | 15.257     |
| MaxReturn            | 79.617     |
| StdReturn            | 13.049     |
| AverageEpisodeLength | 59.74      |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 104        |
| StdEpisodeLength     | 16.37      |
| TotalNEpisodes       | 15404      |
| TotalNSamples        | 3.3504e+05 |
| ExplainedVariance    | 0.83404    |
-------------------------------------
[2018-01-21 12:45:08.013375 UTC] Saving snapshot
[2018-01-21 12:45:08.013656 UTC] Starting iteration 67
[2018-01-21 12:45:08.013841 UTC] Start collecting samples
[2018-01-21 12:45:14.772794 UTC] Computing input variables for policy optimization
[2018-01-21 12:45:14.971161 UTC] Performing policy update
[2018-01-21 12:45:14.971760 UTC] Computing gradient in Euclidean space
[2018-01-21 12:45:15.090771 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:45:16.806447 UTC] Performing line search
[2018-01-21 12:45:17.115785 UTC] Updating baseline
[2018-01-21 12:45:19.098542 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| ExpectedImprovement  | 0.021936   |
| ActualImprovement    | 0.020969   |
| ImprovementRatio     | 0.95592    |
| MeanKL               | 0.0068176  |
| Entropy              | 2.7799     |
| Perplexity           | 16.117     |
| AveragePolicyStd     | 0.38831    |
| AveragePolicyStd[0]  | 0.39915    |
| AveragePolicyStd[1]  | 0.50383    |
| AveragePolicyStd[2]  | 0.38974    |
| AveragePolicyStd[3]  | 0.35882    |
| AveragePolicyStd[4]  | 0.33913    |
| AveragePolicyStd[5]  | 0.33918    |
| AverageReturn        | 48.458     |
| MinReturn            | 20.041     |
| MaxReturn            | 85.098     |
| StdReturn            | 13.692     |
| AverageEpisodeLength | 62.91      |
| MinEpisodeLength     | 34         |
| MaxEpisodeLength     | 117        |
| StdEpisodeLength     | 18.765     |
| TotalNEpisodes       | 15485      |
| TotalNSamples        | 3.4013e+05 |
| ExplainedVariance    | 0.83188    |
-------------------------------------
[2018-01-21 12:45:19.678560 UTC] Saving snapshot
[2018-01-21 12:45:19.678762 UTC] Starting iteration 68
[2018-01-21 12:45:19.678912 UTC] Start collecting samples
[2018-01-21 12:45:27.175368 UTC] Computing input variables for policy optimization
[2018-01-21 12:45:27.382512 UTC] Performing policy update
[2018-01-21 12:45:27.383248 UTC] Computing gradient in Euclidean space
[2018-01-21 12:45:27.528398 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:45:29.041441 UTC] Performing line search
[2018-01-21 12:45:29.250739 UTC] Updating baseline
[2018-01-21 12:45:31.280116 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| ExpectedImprovement  | 0.018626   |
| ActualImprovement    | 0.018562   |
| ImprovementRatio     | 0.99653    |
| MeanKL               | 0.0066299  |
| Entropy              | 2.7368     |
| Perplexity           | 15.438     |
| AveragePolicyStd     | 0.38569    |
| AveragePolicyStd[0]  | 0.39788    |
| AveragePolicyStd[1]  | 0.5016     |
| AveragePolicyStd[2]  | 0.38883    |
| AveragePolicyStd[3]  | 0.35654    |
| AveragePolicyStd[4]  | 0.33548    |
| AveragePolicyStd[5]  | 0.33384    |
| AverageReturn        | 51.484     |
| MinReturn            | 14.39      |
| MaxReturn            | 105.89     |
| StdReturn            | 15.391     |
| AverageEpisodeLength | 64.58      |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 151        |
| StdEpisodeLength     | 20.919     |
| TotalNEpisodes       | 15555      |
| TotalNSamples        | 3.4475e+05 |
| ExplainedVariance    | 0.77837    |
-------------------------------------
[2018-01-21 12:45:31.900303 UTC] Saving snapshot
[2018-01-21 12:45:31.900598 UTC] Starting iteration 69
[2018-01-21 12:45:31.900786 UTC] Start collecting samples
[2018-01-21 12:45:38.457800 UTC] Computing input variables for policy optimization
[2018-01-21 12:45:38.637228 UTC] Performing policy update
[2018-01-21 12:45:38.637826 UTC] Computing gradient in Euclidean space
[2018-01-21 12:45:38.749247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:45:40.105201 UTC] Performing line search
[2018-01-21 12:45:40.323208 UTC] Updating baseline
[2018-01-21 12:45:41.984353 UTC] Computing logging information
------------------------------------
| Iteration            | 69        |
| ExpectedImprovement  | 0.02022   |
| ActualImprovement    | 0.019761  |
| ImprovementRatio     | 0.97732   |
| MeanKL               | 0.0067239 |
| Entropy              | 2.7154    |
| Perplexity           | 15.111    |
| AveragePolicyStd     | 0.38462   |
| AveragePolicyStd[0]  | 0.39675   |
| AveragePolicyStd[1]  | 0.50522   |
| AveragePolicyStd[2]  | 0.38679   |
| AveragePolicyStd[3]  | 0.35522   |
| AveragePolicyStd[4]  | 0.33321   |
| AveragePolicyStd[5]  | 0.33051   |
| AverageReturn        | 54.883    |
| MinReturn            | 15.301    |
| MaxReturn            | 100.04    |
| StdReturn            | 13.574    |
| AverageEpisodeLength | 68.74     |
| MinEpisodeLength     | 38        |
| MaxEpisodeLength     | 180       |
| StdEpisodeLength     | 23.637    |
| TotalNEpisodes       | 15629     |
| TotalNSamples        | 3.498e+05 |
| ExplainedVariance    | 0.8748    |
------------------------------------
[2018-01-21 12:45:42.538033 UTC] Saving snapshot
[2018-01-21 12:45:42.538416 UTC] Starting iteration 70
[2018-01-21 12:45:42.538805 UTC] Start collecting samples
[2018-01-21 12:45:48.801486 UTC] Computing input variables for policy optimization
[2018-01-21 12:45:49.002289 UTC] Performing policy update
[2018-01-21 12:45:49.002935 UTC] Computing gradient in Euclidean space
[2018-01-21 12:45:49.117029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:45:50.608376 UTC] Performing line search
[2018-01-21 12:45:50.801110 UTC] Updating baseline
[2018-01-21 12:45:52.494030 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| ExpectedImprovement  | 0.020061   |
| ActualImprovement    | 0.019577   |
| ImprovementRatio     | 0.97588    |
| MeanKL               | 0.0067273  |
| Entropy              | 2.6818     |
| Perplexity           | 14.612     |
| AveragePolicyStd     | 0.38261    |
| AveragePolicyStd[0]  | 0.39226    |
| AveragePolicyStd[1]  | 0.50654    |
| AveragePolicyStd[2]  | 0.38221    |
| AveragePolicyStd[3]  | 0.35492    |
| AveragePolicyStd[4]  | 0.33015    |
| AveragePolicyStd[5]  | 0.32958    |
| AverageReturn        | 56.535     |
| MinReturn            | 17.374     |
| MaxReturn            | 111.32     |
| StdReturn            | 13.514     |
| AverageEpisodeLength | 69.86      |
| MinEpisodeLength     | 36         |
| MaxEpisodeLength     | 175        |
| StdEpisodeLength     | 23.634     |
| TotalNEpisodes       | 15703      |
| TotalNSamples        | 3.5511e+05 |
| ExplainedVariance    | 0.85645    |
-------------------------------------
[2018-01-21 12:45:53.097399 UTC] Saving snapshot
[2018-01-21 12:45:53.109947 UTC] Starting iteration 71
[2018-01-21 12:45:53.110592 UTC] Start collecting samples
[2018-01-21 12:45:59.973761 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:00.202876 UTC] Performing policy update
[2018-01-21 12:46:00.204019 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:00.356233 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:01.974365 UTC] Performing line search
[2018-01-21 12:46:02.211628 UTC] Updating baseline
[2018-01-21 12:46:04.503951 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| ExpectedImprovement  | 0.020527   |
| ActualImprovement    | 0.019815   |
| ImprovementRatio     | 0.96531    |
| MeanKL               | 0.0067775  |
| Entropy              | 2.6457     |
| Perplexity           | 14.094     |
| AveragePolicyStd     | 0.38013    |
| AveragePolicyStd[0]  | 0.39282    |
| AveragePolicyStd[1]  | 0.49989    |
| AveragePolicyStd[2]  | 0.37849    |
| AveragePolicyStd[3]  | 0.35097    |
| AveragePolicyStd[4]  | 0.3306     |
| AveragePolicyStd[5]  | 0.32802    |
| AverageReturn        | 57.479     |
| MinReturn            | 17.279     |
| MaxReturn            | 108.25     |
| StdReturn            | 12.103     |
| AverageEpisodeLength | 69.66      |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 142        |
| StdEpisodeLength     | 18.283     |
| TotalNEpisodes       | 15768      |
| TotalNSamples        | 3.5979e+05 |
| ExplainedVariance    | 0.8608     |
-------------------------------------
[2018-01-21 12:46:05.191634 UTC] Saving snapshot
[2018-01-21 12:46:05.191997 UTC] Starting iteration 72
[2018-01-21 12:46:05.192272 UTC] Start collecting samples
[2018-01-21 12:46:12.109014 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:12.313136 UTC] Performing policy update
[2018-01-21 12:46:12.314215 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:12.438030 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:13.873413 UTC] Performing line search
[2018-01-21 12:46:14.129737 UTC] Updating baseline
[2018-01-21 12:46:16.365776 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| ExpectedImprovement  | 0.016748   |
| ActualImprovement    | 0.016455   |
| ImprovementRatio     | 0.98251    |
| MeanKL               | 0.0068449  |
| Entropy              | 2.6202     |
| Perplexity           | 13.739     |
| AveragePolicyStd     | 0.37885    |
| AveragePolicyStd[0]  | 0.38991    |
| AveragePolicyStd[1]  | 0.50451    |
| AveragePolicyStd[2]  | 0.3763     |
| AveragePolicyStd[3]  | 0.34796    |
| AveragePolicyStd[4]  | 0.32716    |
| AveragePolicyStd[5]  | 0.32724    |
| AverageReturn        | 58.534     |
| MinReturn            | 23.094     |
| MaxReturn            | 99.517     |
| StdReturn            | 10.524     |
| AverageEpisodeLength | 67.94      |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 142        |
| StdEpisodeLength     | 18.473     |
| TotalNEpisodes       | 15844      |
| TotalNSamples        | 3.6485e+05 |
| ExplainedVariance    | 0.91423    |
-------------------------------------
[2018-01-21 12:46:17.022423 UTC] Saving snapshot
[2018-01-21 12:46:17.022716 UTC] Starting iteration 73
[2018-01-21 12:46:17.022906 UTC] Start collecting samples
[2018-01-21 12:46:23.831379 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:24.027158 UTC] Performing policy update
[2018-01-21 12:46:24.028056 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:24.141204 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:25.582973 UTC] Performing line search
[2018-01-21 12:46:25.775834 UTC] Updating baseline
[2018-01-21 12:46:27.616139 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| ExpectedImprovement  | 0.0202     |
| ActualImprovement    | 0.020041   |
| ImprovementRatio     | 0.99214    |
| MeanKL               | 0.0067241  |
| Entropy              | 2.6059     |
| Perplexity           | 13.544     |
| AveragePolicyStd     | 0.37804    |
| AveragePolicyStd[0]  | 0.3904     |
| AveragePolicyStd[1]  | 0.50439    |
| AveragePolicyStd[2]  | 0.37452    |
| AveragePolicyStd[3]  | 0.34776    |
| AveragePolicyStd[4]  | 0.3234     |
| AveragePolicyStd[5]  | 0.32776    |
| AverageReturn        | 60.587     |
| MinReturn            | 20.757     |
| MaxReturn            | 107.3      |
| StdReturn            | 13.829     |
| AverageEpisodeLength | 71.5       |
| MinEpisodeLength     | 37         |
| MaxEpisodeLength     | 158        |
| StdEpisodeLength     | 21.962     |
| TotalNEpisodes       | 15913      |
| TotalNSamples        | 3.6999e+05 |
| ExplainedVariance    | 0.85195    |
-------------------------------------
[2018-01-21 12:46:28.227763 UTC] Saving snapshot
[2018-01-21 12:46:28.228013 UTC] Starting iteration 74
[2018-01-21 12:46:28.228234 UTC] Start collecting samples
[2018-01-21 12:46:35.267371 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:35.480516 UTC] Performing policy update
[2018-01-21 12:46:35.481637 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:35.627275 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:37.246423 UTC] Performing line search
[2018-01-21 12:46:37.433027 UTC] Updating baseline
[2018-01-21 12:46:39.344536 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| ExpectedImprovement  | 0.020987   |
| ActualImprovement    | 0.020846   |
| ImprovementRatio     | 0.99328    |
| MeanKL               | 0.0064774  |
| Entropy              | 2.5885     |
| Perplexity           | 13.31      |
| AveragePolicyStd     | 0.3769     |
| AveragePolicyStd[0]  | 0.38849    |
| AveragePolicyStd[1]  | 0.5023     |
| AveragePolicyStd[2]  | 0.37343    |
| AveragePolicyStd[3]  | 0.34748    |
| AveragePolicyStd[4]  | 0.32079    |
| AveragePolicyStd[5]  | 0.32889    |
| AverageReturn        | 64.582     |
| MinReturn            | 18.319     |
| MaxReturn            | 113.53     |
| StdReturn            | 15.917     |
| AverageEpisodeLength | 80.22      |
| MinEpisodeLength     | 37         |
| MaxEpisodeLength     | 188        |
| StdEpisodeLength     | 28.133     |
| TotalNEpisodes       | 15970      |
| TotalNSamples        | 3.7486e+05 |
| ExplainedVariance    | 0.87043    |
-------------------------------------
[2018-01-21 12:46:39.996565 UTC] Saving snapshot
[2018-01-21 12:46:39.996869 UTC] Starting iteration 75
[2018-01-21 12:46:39.997085 UTC] Start collecting samples
[2018-01-21 12:46:46.098940 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:46.293754 UTC] Performing policy update
[2018-01-21 12:46:46.294388 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:46.415617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:47.881300 UTC] Performing line search
[2018-01-21 12:46:48.078374 UTC] Updating baseline
[2018-01-21 12:46:49.727518 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| ExpectedImprovement  | 0.017504   |
| ActualImprovement    | 0.017395   |
| ImprovementRatio     | 0.99379    |
| MeanKL               | 0.0067434  |
| Entropy              | 2.5534     |
| Perplexity           | 12.851     |
| AveragePolicyStd     | 0.37491    |
| AveragePolicyStd[0]  | 0.38658    |
| AveragePolicyStd[1]  | 0.50289    |
| AveragePolicyStd[2]  | 0.37205    |
| AveragePolicyStd[3]  | 0.34271    |
| AveragePolicyStd[4]  | 0.31855    |
| AveragePolicyStd[5]  | 0.32668    |
| AverageReturn        | 67.235     |
| MinReturn            | 12.974     |
| MaxReturn            | 113.53     |
| StdReturn            | 15.88      |
| AverageEpisodeLength | 82.77      |
| MinEpisodeLength     | 30         |
| MaxEpisodeLength     | 188        |
| StdEpisodeLength     | 27.998     |
| TotalNEpisodes       | 16035      |
| TotalNSamples        | 3.8011e+05 |
| ExplainedVariance    | 0.91752    |
-------------------------------------
[2018-01-21 12:46:50.304012 UTC] Saving snapshot
[2018-01-21 12:46:50.304301 UTC] Starting iteration 76
[2018-01-21 12:46:50.304508 UTC] Start collecting samples
[2018-01-21 12:46:57.732442 UTC] Computing input variables for policy optimization
[2018-01-21 12:46:57.902120 UTC] Performing policy update
[2018-01-21 12:46:57.902762 UTC] Computing gradient in Euclidean space
[2018-01-21 12:46:58.019038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:46:59.466786 UTC] Performing line search
[2018-01-21 12:46:59.661283 UTC] Updating baseline
[2018-01-21 12:47:01.448059 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| ExpectedImprovement  | 0.021071   |
| ActualImprovement    | 0.020453   |
| ImprovementRatio     | 0.97071    |
| MeanKL               | 0.006664   |
| Entropy              | 2.5479     |
| Perplexity           | 12.78      |
| AveragePolicyStd     | 0.37435    |
| AveragePolicyStd[0]  | 0.38456    |
| AveragePolicyStd[1]  | 0.50011    |
| AveragePolicyStd[2]  | 0.36984    |
| AveragePolicyStd[3]  | 0.34333    |
| AveragePolicyStd[4]  | 0.32026    |
| AveragePolicyStd[5]  | 0.32798    |
| AverageReturn        | 66.615     |
| MinReturn            | 12.974     |
| MaxReturn            | 155.42     |
| StdReturn            | 17.346     |
| AverageEpisodeLength | 80.36      |
| MinEpisodeLength     | 30         |
| MaxEpisodeLength     | 226        |
| StdEpisodeLength     | 27.597     |
| TotalNEpisodes       | 16098      |
| TotalNSamples        | 3.8515e+05 |
| ExplainedVariance    | 0.86792    |
-------------------------------------
[2018-01-21 12:47:02.046965 UTC] Saving snapshot
[2018-01-21 12:47:02.047333 UTC] Starting iteration 77
[2018-01-21 12:47:02.047553 UTC] Start collecting samples
[2018-01-21 12:47:08.208201 UTC] Computing input variables for policy optimization
[2018-01-21 12:47:08.378299 UTC] Performing policy update
[2018-01-21 12:47:08.379077 UTC] Computing gradient in Euclidean space
[2018-01-21 12:47:08.493272 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:47:09.973833 UTC] Performing line search
[2018-01-21 12:47:10.167846 UTC] Updating baseline
[2018-01-21 12:47:11.963105 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| ExpectedImprovement  | 0.023641   |
| ActualImprovement    | 0.023132   |
| ImprovementRatio     | 0.97848    |
| MeanKL               | 0.0066079  |
| Entropy              | 2.5251     |
| Perplexity           | 12.492     |
| AveragePolicyStd     | 0.37297    |
| AveragePolicyStd[0]  | 0.3823     |
| AveragePolicyStd[1]  | 0.4996     |
| AveragePolicyStd[2]  | 0.36671    |
| AveragePolicyStd[3]  | 0.34357    |
| AveragePolicyStd[4]  | 0.31884    |
| AveragePolicyStd[5]  | 0.3268     |
| AverageReturn        | 72.531     |
| MinReturn            | 28.735     |
| MaxReturn            | 189.06     |
| StdReturn            | 22.374     |
| AverageEpisodeLength | 88.7       |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 309        |
| StdEpisodeLength     | 39.653     |
| TotalNEpisodes       | 16146      |
| TotalNSamples        | 3.8978e+05 |
| ExplainedVariance    | 0.71466    |
-------------------------------------
[2018-01-21 12:47:12.592451 UTC] Saving snapshot
[2018-01-21 12:47:12.592701 UTC] Starting iteration 78
[2018-01-21 12:47:12.592882 UTC] Start collecting samples
[2018-01-21 12:47:22.706738 UTC] Computing input variables for policy optimization
[2018-01-21 12:47:22.980986 UTC] Performing policy update
[2018-01-21 12:47:22.982234 UTC] Computing gradient in Euclidean space
[2018-01-21 12:47:23.124109 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:47:24.639962 UTC] Performing line search
[2018-01-21 12:47:24.851517 UTC] Updating baseline
[2018-01-21 12:47:26.841661 UTC] Computing logging information
------------------------------------
| Iteration            | 78        |
| ExpectedImprovement  | 0.022247  |
| ActualImprovement    | 0.021152  |
| ImprovementRatio     | 0.95078   |
| MeanKL               | 0.0066539 |
| Entropy              | 2.512     |
| Perplexity           | 12.329    |
| AveragePolicyStd     | 0.37226   |
| AveragePolicyStd[0]  | 0.37981   |
| AveragePolicyStd[1]  | 0.5006    |
| AveragePolicyStd[2]  | 0.36772   |
| AveragePolicyStd[3]  | 0.34123   |
| AveragePolicyStd[4]  | 0.31726   |
| AveragePolicyStd[5]  | 0.32694   |
| AverageReturn        | 74.436    |
| MinReturn            | 2.3005    |
| MaxReturn            | 189.06    |
| StdReturn            | 21.342    |
| AverageEpisodeLength | 89.73     |
| MinEpisodeLength     | 24        |
| MaxEpisodeLength     | 309       |
| StdEpisodeLength     | 37.182    |
| TotalNEpisodes       | 16207     |
| TotalNSamples        | 3.949e+05 |
| ExplainedVariance    | 0.89936   |
------------------------------------
[2018-01-21 12:47:27.489373 UTC] Saving snapshot
[2018-01-21 12:47:27.489727 UTC] Starting iteration 79
[2018-01-21 12:47:27.489939 UTC] Start collecting samples
[2018-01-21 12:47:33.576649 UTC] Computing input variables for policy optimization
[2018-01-21 12:47:33.754586 UTC] Performing policy update
[2018-01-21 12:47:33.755654 UTC] Computing gradient in Euclidean space
[2018-01-21 12:47:33.876828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:47:35.355048 UTC] Performing line search
[2018-01-21 12:47:35.564139 UTC] Updating baseline
[2018-01-21 12:47:37.483843 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| ExpectedImprovement  | 0.022753   |
| ActualImprovement    | 0.022904   |
| ImprovementRatio     | 1.0066     |
| MeanKL               | 0.0065795  |
| Entropy              | 2.4956     |
| Perplexity           | 12.13      |
| AveragePolicyStd     | 0.37131    |
| AveragePolicyStd[0]  | 0.38437    |
| AveragePolicyStd[1]  | 0.49864    |
| AveragePolicyStd[2]  | 0.36483    |
| AveragePolicyStd[3]  | 0.33639    |
| AveragePolicyStd[4]  | 0.31442    |
| AveragePolicyStd[5]  | 0.32919    |
| AverageReturn        | 72.569     |
| MinReturn            | 2.3005     |
| MaxReturn            | 154.04     |
| StdReturn            | 20.421     |
| AverageEpisodeLength | 88.04      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 244        |
| StdEpisodeLength     | 34.999     |
| TotalNEpisodes       | 16263      |
| TotalNSamples        | 3.9998e+05 |
| ExplainedVariance    | 0.91895    |
-------------------------------------
[2018-01-21 12:47:38.156644 UTC] Saving snapshot
[2018-01-21 12:47:38.156899 UTC] Starting iteration 80
[2018-01-21 12:47:38.157085 UTC] Start collecting samples
[2018-01-21 12:47:43.904459 UTC] Computing input variables for policy optimization
[2018-01-21 12:47:44.073660 UTC] Performing policy update
[2018-01-21 12:47:44.074277 UTC] Computing gradient in Euclidean space
[2018-01-21 12:47:44.195532 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:47:45.637744 UTC] Performing line search
[2018-01-21 12:47:45.823434 UTC] Updating baseline
[2018-01-21 12:47:47.483586 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| ExpectedImprovement  | 0.018557   |
| ActualImprovement    | 0.018561   |
| ImprovementRatio     | 1.0002     |
| MeanKL               | 0.0066162  |
| Entropy              | 2.4883     |
| Perplexity           | 12.041     |
| AveragePolicyStd     | 0.37101    |
| AveragePolicyStd[0]  | 0.38509    |
| AveragePolicyStd[1]  | 0.5003     |
| AveragePolicyStd[2]  | 0.36386    |
| AveragePolicyStd[3]  | 0.33737    |
| AveragePolicyStd[4]  | 0.31457    |
| AveragePolicyStd[5]  | 0.32485    |
| AverageReturn        | 76.815     |
| MinReturn            | 37.462     |
| MaxReturn            | 166.77     |
| StdReturn            | 23.652     |
| AverageEpisodeLength | 96.31      |
| MinEpisodeLength     | 46         |
| MaxEpisodeLength     | 250        |
| StdEpisodeLength     | 42.961     |
| TotalNEpisodes       | 16307      |
| TotalNSamples        | 4.0453e+05 |
| ExplainedVariance    | 0.82153    |
-------------------------------------
[2018-01-21 12:47:48.127420 UTC] Saving snapshot
[2018-01-21 12:47:48.136820 UTC] Starting iteration 81
[2018-01-21 12:47:48.137053 UTC] Start collecting samples
[2018-01-21 12:47:53.738570 UTC] Computing input variables for policy optimization
[2018-01-21 12:47:53.935775 UTC] Performing policy update
[2018-01-21 12:47:53.936382 UTC] Computing gradient in Euclidean space
[2018-01-21 12:47:54.070041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:47:55.433018 UTC] Performing line search
[2018-01-21 12:47:55.616229 UTC] Updating baseline
[2018-01-21 12:47:57.399450 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| ExpectedImprovement  | 0.018395   |
| ActualImprovement    | 0.017944   |
| ImprovementRatio     | 0.97547    |
| MeanKL               | 0.0065801  |
| Entropy              | 2.4863     |
| Perplexity           | 12.017     |
| AveragePolicyStd     | 0.37077    |
| AveragePolicyStd[0]  | 0.37882    |
| AveragePolicyStd[1]  | 0.5014     |
| AveragePolicyStd[2]  | 0.36218    |
| AveragePolicyStd[3]  | 0.33871    |
| AveragePolicyStd[4]  | 0.31785    |
| AveragePolicyStd[5]  | 0.32569    |
| AverageReturn        | 81.127     |
| MinReturn            | 37.462     |
| MaxReturn            | 201.72     |
| StdReturn            | 27.355     |
| AverageEpisodeLength | 105.17     |
| MinEpisodeLength     | 50         |
| MaxEpisodeLength     | 336        |
| StdEpisodeLength     | 49.171     |
| TotalNEpisodes       | 16353      |
| TotalNSamples        | 4.0966e+05 |
| ExplainedVariance    | 0.84295    |
-------------------------------------
[2018-01-21 12:47:58.036105 UTC] Saving snapshot
[2018-01-21 12:47:58.036355 UTC] Starting iteration 82
[2018-01-21 12:47:58.036539 UTC] Start collecting samples
[2018-01-21 12:48:03.828625 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:03.992741 UTC] Performing policy update
[2018-01-21 12:48:03.993630 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:04.109354 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:05.164617 UTC] Performing line search
[2018-01-21 12:48:05.283055 UTC] Updating baseline
[2018-01-21 12:48:06.478330 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| ExpectedImprovement  | 0.021988   |
| ActualImprovement    | 0.020605   |
| ImprovementRatio     | 0.93709    |
| MeanKL               | 0.0066855  |
| Entropy              | 2.4431     |
| Perplexity           | 11.508     |
| AveragePolicyStd     | 0.36796    |
| AveragePolicyStd[0]  | 0.37594    |
| AveragePolicyStd[1]  | 0.49577    |
| AveragePolicyStd[2]  | 0.35852    |
| AveragePolicyStd[3]  | 0.33614    |
| AveragePolicyStd[4]  | 0.31702    |
| AveragePolicyStd[5]  | 0.32439    |
| AverageReturn        | 84.289     |
| MinReturn            | 14.362     |
| MaxReturn            | 201.72     |
| StdReturn            | 28.46      |
| AverageEpisodeLength | 111.63     |
| MinEpisodeLength     | 38         |
| MaxEpisodeLength     | 336        |
| StdEpisodeLength     | 48.175     |
| TotalNEpisodes       | 16402      |
| TotalNSamples        | 4.1488e+05 |
| ExplainedVariance    | 0.88002    |
-------------------------------------
[2018-01-21 12:48:06.871943 UTC] Saving snapshot
[2018-01-21 12:48:06.872140 UTC] Starting iteration 83
[2018-01-21 12:48:06.872273 UTC] Start collecting samples
[2018-01-21 12:48:10.194297 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:10.288916 UTC] Performing policy update
[2018-01-21 12:48:10.289479 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:10.363402 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:11.218085 UTC] Performing line search
[2018-01-21 12:48:11.331832 UTC] Updating baseline
[2018-01-21 12:48:12.433668 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| ExpectedImprovement  | 0.01928    |
| ActualImprovement    | 0.018778   |
| ImprovementRatio     | 0.974      |
| MeanKL               | 0.0068009  |
| Entropy              | 2.4039     |
| Perplexity           | 11.066     |
| AveragePolicyStd     | 0.36571    |
| AveragePolicyStd[0]  | 0.37331    |
| AveragePolicyStd[1]  | 0.49501    |
| AveragePolicyStd[2]  | 0.35565    |
| AveragePolicyStd[3]  | 0.33467    |
| AveragePolicyStd[4]  | 0.31271    |
| AveragePolicyStd[5]  | 0.32291    |
| AverageReturn        | 80.651     |
| MinReturn            | 14.362     |
| MaxReturn            | 146.73     |
| StdReturn            | 20.719     |
| AverageEpisodeLength | 102.95     |
| MinEpisodeLength     | 38         |
| MaxEpisodeLength     | 242        |
| StdEpisodeLength     | 34.763     |
| TotalNEpisodes       | 16449      |
| TotalNSamples        | 4.1958e+05 |
| ExplainedVariance    | 0.87272    |
-------------------------------------
[2018-01-21 12:48:12.794471 UTC] Saving snapshot
[2018-01-21 12:48:12.794666 UTC] Starting iteration 84
[2018-01-21 12:48:12.794791 UTC] Start collecting samples
[2018-01-21 12:48:15.669820 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:15.767567 UTC] Performing policy update
[2018-01-21 12:48:15.768073 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:15.842108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:16.690142 UTC] Performing line search
[2018-01-21 12:48:16.806312 UTC] Updating baseline
[2018-01-21 12:48:17.974824 UTC] Computing logging information
------------------------------------
| Iteration            | 84        |
| ExpectedImprovement  | 0.019332  |
| ActualImprovement    | 0.018574  |
| ImprovementRatio     | 0.9608    |
| MeanKL               | 0.0068685 |
| Entropy              | 2.3515    |
| Perplexity           | 10.501    |
| AveragePolicyStd     | 0.36273   |
| AveragePolicyStd[0]  | 0.37023   |
| AveragePolicyStd[1]  | 0.49389   |
| AveragePolicyStd[2]  | 0.3513    |
| AveragePolicyStd[3]  | 0.33204   |
| AveragePolicyStd[4]  | 0.3066    |
| AveragePolicyStd[5]  | 0.32232   |
| AverageReturn        | 83.996    |
| MinReturn            | 35.575    |
| MaxReturn            | 179.43    |
| StdReturn            | 23.608    |
| AverageEpisodeLength | 106.38    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 364       |
| StdEpisodeLength     | 45.288    |
| TotalNEpisodes       | 16494     |
| TotalNSamples        | 4.246e+05 |
| ExplainedVariance    | 0.87685   |
------------------------------------
[2018-01-21 12:48:18.331519 UTC] Saving snapshot
[2018-01-21 12:48:18.331709 UTC] Starting iteration 85
[2018-01-21 12:48:18.331862 UTC] Start collecting samples
[2018-01-21 12:48:21.260844 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:21.358184 UTC] Performing policy update
[2018-01-21 12:48:21.358718 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:21.431841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:22.275624 UTC] Performing line search
[2018-01-21 12:48:22.389723 UTC] Updating baseline
[2018-01-21 12:48:23.410410 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| ExpectedImprovement  | 0.020076   |
| ActualImprovement    | 0.020671   |
| ImprovementRatio     | 1.0296     |
| MeanKL               | 0.0066102  |
| Entropy              | 2.3366     |
| Perplexity           | 10.346     |
| AveragePolicyStd     | 0.3621     |
| AveragePolicyStd[0]  | 0.36914    |
| AveragePolicyStd[1]  | 0.49733    |
| AveragePolicyStd[2]  | 0.34912    |
| AveragePolicyStd[3]  | 0.33168    |
| AveragePolicyStd[4]  | 0.30409    |
| AveragePolicyStd[5]  | 0.32125    |
| AverageReturn        | 84.912     |
| MinReturn            | 53.22      |
| MaxReturn            | 179.43     |
| StdReturn            | 24.442     |
| AverageEpisodeLength | 105.87     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 364        |
| StdEpisodeLength     | 49.307     |
| TotalNEpisodes       | 16544      |
| TotalNSamples        | 4.2966e+05 |
| ExplainedVariance    | 0.94586    |
-------------------------------------
[2018-01-21 12:48:23.770038 UTC] Saving snapshot
[2018-01-21 12:48:23.770234 UTC] Starting iteration 86
[2018-01-21 12:48:23.770374 UTC] Start collecting samples
[2018-01-21 12:48:26.594590 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:26.688277 UTC] Performing policy update
[2018-01-21 12:48:26.688778 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:26.762607 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:27.646099 UTC] Performing line search
[2018-01-21 12:48:27.769010 UTC] Updating baseline
[2018-01-21 12:48:28.864311 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| ExpectedImprovement  | 0.020932   |
| ActualImprovement    | 0.019657   |
| ImprovementRatio     | 0.93913    |
| MeanKL               | 0.0066368  |
| Entropy              | 2.3185     |
| Perplexity           | 10.16      |
| AveragePolicyStd     | 0.36093    |
| AveragePolicyStd[0]  | 0.36706    |
| AveragePolicyStd[1]  | 0.49565    |
| AveragePolicyStd[2]  | 0.3465     |
| AveragePolicyStd[3]  | 0.32993    |
| AveragePolicyStd[4]  | 0.30552    |
| AveragePolicyStd[5]  | 0.32092    |
| AverageReturn        | 87.106     |
| MinReturn            | 52.584     |
| MaxReturn            | 216.32     |
| StdReturn            | 25.924     |
| AverageEpisodeLength | 107.99     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 331        |
| StdEpisodeLength     | 48.967     |
| TotalNEpisodes       | 16584      |
| TotalNSamples        | 4.3453e+05 |
| ExplainedVariance    | 0.89557    |
-------------------------------------
[2018-01-21 12:48:29.225535 UTC] Saving snapshot
[2018-01-21 12:48:29.225731 UTC] Starting iteration 87
[2018-01-21 12:48:29.225856 UTC] Start collecting samples
[2018-01-21 12:48:32.367670 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:32.488873 UTC] Performing policy update
[2018-01-21 12:48:32.489462 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:32.573127 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:33.427630 UTC] Performing line search
[2018-01-21 12:48:33.564606 UTC] Updating baseline
[2018-01-21 12:48:35.206809 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| ExpectedImprovement  | 0.023774   |
| ActualImprovement    | 0.022797   |
| ImprovementRatio     | 0.9589     |
| MeanKL               | 0.0065688  |
| Entropy              | 2.3        |
| Perplexity           | 9.9744     |
| AveragePolicyStd     | 0.35988    |
| AveragePolicyStd[0]  | 0.36656    |
| AveragePolicyStd[1]  | 0.49512    |
| AveragePolicyStd[2]  | 0.34464    |
| AveragePolicyStd[3]  | 0.32892    |
| AveragePolicyStd[4]  | 0.30457    |
| AveragePolicyStd[5]  | 0.31949    |
| AverageReturn        | 89.762     |
| MinReturn            | 39.139     |
| MaxReturn            | 216.32     |
| StdReturn            | 27.317     |
| AverageEpisodeLength | 111.78     |
| MinEpisodeLength     | 54         |
| MaxEpisodeLength     | 331        |
| StdEpisodeLength     | 46.408     |
| TotalNEpisodes       | 16630      |
| TotalNSamples        | 4.3962e+05 |
| ExplainedVariance    | 0.91458    |
-------------------------------------
[2018-01-21 12:48:35.818794 UTC] Saving snapshot
[2018-01-21 12:48:35.819373 UTC] Starting iteration 88
[2018-01-21 12:48:35.819797 UTC] Start collecting samples
[2018-01-21 12:48:39.410831 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:39.522830 UTC] Performing policy update
[2018-01-21 12:48:39.523338 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:39.642061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:40.575519 UTC] Performing line search
[2018-01-21 12:48:40.695823 UTC] Updating baseline
[2018-01-21 12:48:41.722908 UTC] Computing logging information
------------------------------------
| Iteration            | 88        |
| ExpectedImprovement  | 0.016516  |
| ActualImprovement    | 0.016285  |
| ImprovementRatio     | 0.986     |
| MeanKL               | 0.0067101 |
| Entropy              | 2.2855    |
| Perplexity           | 9.8309    |
| AveragePolicyStd     | 0.35894   |
| AveragePolicyStd[0]  | 0.36509   |
| AveragePolicyStd[1]  | 0.49238   |
| AveragePolicyStd[2]  | 0.34666   |
| AveragePolicyStd[3]  | 0.32674   |
| AveragePolicyStd[4]  | 0.30441   |
| AveragePolicyStd[5]  | 0.31835   |
| AverageReturn        | 95.484    |
| MinReturn            | 39.139    |
| MaxReturn            | 216.32    |
| StdReturn            | 29.273    |
| AverageEpisodeLength | 121.3     |
| MinEpisodeLength     | 64        |
| MaxEpisodeLength     | 331       |
| StdEpisodeLength     | 48.156    |
| TotalNEpisodes       | 16667     |
| TotalNSamples        | 4.444e+05 |
| ExplainedVariance    | 0.92857   |
------------------------------------
[2018-01-21 12:48:42.094384 UTC] Saving snapshot
[2018-01-21 12:48:42.094600 UTC] Starting iteration 89
[2018-01-21 12:48:42.094728 UTC] Start collecting samples
[2018-01-21 12:48:44.784045 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:44.869403 UTC] Performing policy update
[2018-01-21 12:48:44.869918 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:44.948040 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:45.836389 UTC] Performing line search
[2018-01-21 12:48:45.955107 UTC] Updating baseline
[2018-01-21 12:48:47.029255 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| ExpectedImprovement  | 0.018489   |
| ActualImprovement    | 0.018665   |
| ImprovementRatio     | 1.0095     |
| MeanKL               | 0.0067219  |
| Entropy              | 2.2758     |
| Perplexity           | 9.7357     |
| AveragePolicyStd     | 0.35832    |
| AveragePolicyStd[0]  | 0.36529    |
| AveragePolicyStd[1]  | 0.49115    |
| AveragePolicyStd[2]  | 0.34535    |
| AveragePolicyStd[3]  | 0.32384    |
| AveragePolicyStd[4]  | 0.30591    |
| AveragePolicyStd[5]  | 0.31835    |
| AverageReturn        | 101.84     |
| MinReturn            | 39.139     |
| MaxReturn            | 225.37     |
| StdReturn            | 33.638     |
| AverageEpisodeLength | 131.13     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 362        |
| StdEpisodeLength     | 59.573     |
| TotalNEpisodes       | 16697      |
| TotalNSamples        | 4.4903e+05 |
| ExplainedVariance    | 0.89655    |
-------------------------------------
[2018-01-21 12:48:47.391725 UTC] Saving snapshot
[2018-01-21 12:48:47.391943 UTC] Starting iteration 90
[2018-01-21 12:48:47.392088 UTC] Start collecting samples
[2018-01-21 12:48:50.289656 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:50.376869 UTC] Performing policy update
[2018-01-21 12:48:50.377387 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:50.449812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:51.370768 UTC] Performing line search
[2018-01-21 12:48:51.507055 UTC] Updating baseline
[2018-01-21 12:48:52.526594 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| ExpectedImprovement  | 0.019972   |
| ActualImprovement    | 0.019685   |
| ImprovementRatio     | 0.9856     |
| MeanKL               | 0.0067689  |
| Entropy              | 2.268      |
| Perplexity           | 9.66       |
| AveragePolicyStd     | 0.35768    |
| AveragePolicyStd[0]  | 0.3646     |
| AveragePolicyStd[1]  | 0.48809    |
| AveragePolicyStd[2]  | 0.34475    |
| AveragePolicyStd[3]  | 0.32145    |
| AveragePolicyStd[4]  | 0.30807    |
| AveragePolicyStd[5]  | 0.31912    |
| AverageReturn        | 110.6      |
| MinReturn            | 39.139     |
| MaxReturn            | 225.37     |
| StdReturn            | 37.927     |
| AverageEpisodeLength | 147.6      |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 362        |
| StdEpisodeLength     | 68.033     |
| TotalNEpisodes       | 16728      |
| TotalNSamples        | 4.5422e+05 |
| ExplainedVariance    | 0.86715    |
-------------------------------------
[2018-01-21 12:48:52.890391 UTC] Saving snapshot
[2018-01-21 12:48:52.896331 UTC] Starting iteration 91
[2018-01-21 12:48:52.896498 UTC] Start collecting samples
[2018-01-21 12:48:55.583105 UTC] Computing input variables for policy optimization
[2018-01-21 12:48:55.677130 UTC] Performing policy update
[2018-01-21 12:48:55.677686 UTC] Computing gradient in Euclidean space
[2018-01-21 12:48:55.752037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:48:56.587678 UTC] Performing line search
[2018-01-21 12:48:56.705393 UTC] Updating baseline
[2018-01-21 12:48:57.754282 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| ExpectedImprovement  | 0.018778   |
| ActualImprovement    | 0.019355   |
| ImprovementRatio     | 1.0307     |
| MeanKL               | 0.0067307  |
| Entropy              | 2.2402     |
| Perplexity           | 9.395      |
| AveragePolicyStd     | 0.35604    |
| AveragePolicyStd[0]  | 0.36411    |
| AveragePolicyStd[1]  | 0.48623    |
| AveragePolicyStd[2]  | 0.33993    |
| AveragePolicyStd[3]  | 0.32207    |
| AveragePolicyStd[4]  | 0.30701    |
| AveragePolicyStd[5]  | 0.31689    |
| AverageReturn        | 115.35     |
| MinReturn            | 53.934     |
| MaxReturn            | 293.07     |
| StdReturn            | 40.402     |
| AverageEpisodeLength | 156.61     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 468        |
| StdEpisodeLength     | 75.284     |
| TotalNEpisodes       | 16758      |
| TotalNSamples        | 4.5898e+05 |
| ExplainedVariance    | 0.82675    |
-------------------------------------
[2018-01-21 12:48:58.115583 UTC] Saving snapshot
[2018-01-21 12:48:58.115787 UTC] Starting iteration 92
[2018-01-21 12:48:58.115940 UTC] Start collecting samples
[2018-01-21 12:49:00.705434 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:00.789753 UTC] Performing policy update
[2018-01-21 12:49:00.790293 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:00.863059 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:01.699424 UTC] Performing line search
[2018-01-21 12:49:01.813870 UTC] Updating baseline
[2018-01-21 12:49:02.818526 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| ExpectedImprovement  | 0.021931   |
| ActualImprovement    | 0.021448   |
| ImprovementRatio     | 0.97798    |
| MeanKL               | 0.006716   |
| Entropy              | 2.2285     |
| Perplexity           | 9.2856     |
| AveragePolicyStd     | 0.35528    |
| AveragePolicyStd[0]  | 0.36439    |
| AveragePolicyStd[1]  | 0.48383    |
| AveragePolicyStd[2]  | 0.33744    |
| AveragePolicyStd[3]  | 0.32166    |
| AveragePolicyStd[4]  | 0.30433    |
| AveragePolicyStd[5]  | 0.32003    |
| AverageReturn        | 124.72     |
| MinReturn            | 68.9       |
| MaxReturn            | 293.07     |
| StdReturn            | 43.718     |
| AverageEpisodeLength | 171.46     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 468        |
| StdEpisodeLength     | 79.633     |
| TotalNEpisodes       | 16782      |
| TotalNSamples        | 4.6377e+05 |
| ExplainedVariance    | 0.89244    |
-------------------------------------
[2018-01-21 12:49:03.183210 UTC] Saving snapshot
[2018-01-21 12:49:03.183397 UTC] Starting iteration 93
[2018-01-21 12:49:03.183547 UTC] Start collecting samples
[2018-01-21 12:49:05.839554 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:05.926138 UTC] Performing policy update
[2018-01-21 12:49:05.926626 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:06.000422 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:06.840728 UTC] Performing line search
[2018-01-21 12:49:06.959787 UTC] Updating baseline
[2018-01-21 12:49:07.976521 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| ExpectedImprovement  | 0.02508    |
| ActualImprovement    | 0.023049   |
| ImprovementRatio     | 0.91903    |
| MeanKL               | 0.0065011  |
| Entropy              | 2.1997     |
| Perplexity           | 9.0228     |
| AveragePolicyStd     | 0.35356    |
| AveragePolicyStd[0]  | 0.35985    |
| AveragePolicyStd[1]  | 0.48184    |
| AveragePolicyStd[2]  | 0.33791    |
| AveragePolicyStd[3]  | 0.31841    |
| AveragePolicyStd[4]  | 0.30348    |
| AveragePolicyStd[5]  | 0.31987    |
| AverageReturn        | 130.82     |
| MinReturn            | 63.936     |
| MaxReturn            | 307.2      |
| StdReturn            | 52.223     |
| AverageEpisodeLength | 180.76     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 468        |
| StdEpisodeLength     | 91.336     |
| TotalNEpisodes       | 16810      |
| TotalNSamples        | 4.6938e+05 |
| ExplainedVariance    | 0.89893    |
-------------------------------------
[2018-01-21 12:49:08.339726 UTC] Saving snapshot
[2018-01-21 12:49:08.339929 UTC] Starting iteration 94
[2018-01-21 12:49:08.340067 UTC] Start collecting samples
[2018-01-21 12:49:10.971160 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:11.054029 UTC] Performing policy update
[2018-01-21 12:49:11.054572 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:11.126225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:11.964221 UTC] Performing line search
[2018-01-21 12:49:12.079061 UTC] Updating baseline
[2018-01-21 12:49:13.099553 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| ExpectedImprovement  | 0.020945   |
| ActualImprovement    | 0.020837   |
| ImprovementRatio     | 0.99485    |
| MeanKL               | 0.0067492  |
| Entropy              | 2.1902     |
| Perplexity           | 8.9366     |
| AveragePolicyStd     | 0.35313    |
| AveragePolicyStd[0]  | 0.35743    |
| AveragePolicyStd[1]  | 0.48426    |
| AveragePolicyStd[2]  | 0.33702    |
| AveragePolicyStd[3]  | 0.31725    |
| AveragePolicyStd[4]  | 0.30439    |
| AveragePolicyStd[5]  | 0.31842    |
| AverageReturn        | 135.65     |
| MinReturn            | 63.936     |
| MaxReturn            | 307.2      |
| StdReturn            | 52.119     |
| AverageEpisodeLength | 183.68     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 468        |
| StdEpisodeLength     | 90.627     |
| TotalNEpisodes       | 16837      |
| TotalNSamples        | 4.7381e+05 |
| ExplainedVariance    | 0.95564    |
-------------------------------------
[2018-01-21 12:49:13.461959 UTC] Saving snapshot
[2018-01-21 12:49:13.462147 UTC] Starting iteration 95
[2018-01-21 12:49:13.462291 UTC] Start collecting samples
[2018-01-21 12:49:16.123416 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:16.211138 UTC] Performing policy update
[2018-01-21 12:49:16.211574 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:16.282249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:17.118981 UTC] Performing line search
[2018-01-21 12:49:17.233883 UTC] Updating baseline
[2018-01-21 12:49:18.264572 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| ExpectedImprovement  | 0.021573   |
| ActualImprovement    | 0.021508   |
| ImprovementRatio     | 0.997      |
| MeanKL               | 0.0066577  |
| Entropy              | 2.1544     |
| Perplexity           | 8.6226     |
| AveragePolicyStd     | 0.35105    |
| AveragePolicyStd[0]  | 0.35613    |
| AveragePolicyStd[1]  | 0.48155    |
| AveragePolicyStd[2]  | 0.33455    |
| AveragePolicyStd[3]  | 0.31427    |
| AveragePolicyStd[4]  | 0.30303    |
| AveragePolicyStd[5]  | 0.31675    |
| AverageReturn        | 137.28     |
| MinReturn            | 63.936     |
| MaxReturn            | 307.2      |
| StdReturn            | 52.86      |
| AverageEpisodeLength | 184.56     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 466        |
| StdEpisodeLength     | 90.363     |
| TotalNEpisodes       | 16860      |
| TotalNSamples        | 4.7766e+05 |
| ExplainedVariance    | 0.94678    |
-------------------------------------
[2018-01-21 12:49:18.622346 UTC] Saving snapshot
[2018-01-21 12:49:18.622543 UTC] Starting iteration 96
[2018-01-21 12:49:18.622661 UTC] Start collecting samples
[2018-01-21 12:49:21.189879 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:21.267891 UTC] Performing policy update
[2018-01-21 12:49:21.268407 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:21.340983 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:22.181588 UTC] Performing line search
[2018-01-21 12:49:22.295799 UTC] Updating baseline
[2018-01-21 12:49:23.530566 UTC] Computing logging information
------------------------------------
| Iteration            | 96        |
| ExpectedImprovement  | 0.024998  |
| ActualImprovement    | 0.024     |
| ImprovementRatio     | 0.96008   |
| MeanKL               | 0.0067233 |
| Entropy              | 2.1364    |
| Perplexity           | 8.469     |
| AveragePolicyStd     | 0.35014   |
| AveragePolicyStd[0]  | 0.35508   |
| AveragePolicyStd[1]  | 0.48321   |
| AveragePolicyStd[2]  | 0.33058   |
| AveragePolicyStd[3]  | 0.31319   |
| AveragePolicyStd[4]  | 0.30313   |
| AveragePolicyStd[5]  | 0.31567   |
| AverageReturn        | 144.98    |
| MinReturn            | 63.936    |
| MaxReturn            | 591.21    |
| StdReturn            | 72.024    |
| AverageEpisodeLength | 193.91    |
| MinEpisodeLength     | 76        |
| MaxEpisodeLength     | 928       |
| StdEpisodeLength     | 118.97    |
| TotalNEpisodes       | 16879     |
| TotalNSamples        | 4.827e+05 |
| ExplainedVariance    | 0.81909   |
------------------------------------
[2018-01-21 12:49:23.897155 UTC] Saving snapshot
[2018-01-21 12:49:23.897346 UTC] Starting iteration 97
[2018-01-21 12:49:23.897504 UTC] Start collecting samples
[2018-01-21 12:49:26.374392 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:26.448454 UTC] Performing policy update
[2018-01-21 12:49:26.448961 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:26.520748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:27.395271 UTC] Performing line search
[2018-01-21 12:49:27.507829 UTC] Updating baseline
[2018-01-21 12:49:28.524661 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| ExpectedImprovement  | 0.018199   |
| ActualImprovement    | 0.018168   |
| ImprovementRatio     | 0.99828    |
| MeanKL               | 0.0067004  |
| Entropy              | 2.134      |
| Perplexity           | 8.4488     |
| AveragePolicyStd     | 0.34991    |
| AveragePolicyStd[0]  | 0.35556    |
| AveragePolicyStd[1]  | 0.48173    |
| AveragePolicyStd[2]  | 0.3278     |
| AveragePolicyStd[3]  | 0.31285    |
| AveragePolicyStd[4]  | 0.30525    |
| AveragePolicyStd[5]  | 0.31627    |
| AverageReturn        | 153.82     |
| MinReturn            | 63.936     |
| MaxReturn            | 591.21     |
| StdReturn            | 76.475     |
| AverageEpisodeLength | 209.78     |
| MinEpisodeLength     | 76         |
| MaxEpisodeLength     | 928        |
| StdEpisodeLength     | 125.77     |
| TotalNEpisodes       | 16891      |
| TotalNSamples        | 4.8635e+05 |
| ExplainedVariance    | 0.78894    |
-------------------------------------
[2018-01-21 12:49:28.888498 UTC] Saving snapshot
[2018-01-21 12:49:28.888695 UTC] Starting iteration 98
[2018-01-21 12:49:28.888838 UTC] Start collecting samples
[2018-01-21 12:49:31.382575 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:31.457737 UTC] Performing policy update
[2018-01-21 12:49:31.458307 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:31.531620 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:32.388542 UTC] Performing line search
[2018-01-21 12:49:32.508933 UTC] Updating baseline
[2018-01-21 12:49:33.707719 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| ExpectedImprovement  | 0.021134   |
| ActualImprovement    | 0.021012   |
| ImprovementRatio     | 0.99423    |
| MeanKL               | 0.0065143  |
| Entropy              | 2.0992     |
| Perplexity           | 8.16       |
| AveragePolicyStd     | 0.3478     |
| AveragePolicyStd[0]  | 0.35445    |
| AveragePolicyStd[1]  | 0.47704    |
| AveragePolicyStd[2]  | 0.32516    |
| AveragePolicyStd[3]  | 0.31237    |
| AveragePolicyStd[4]  | 0.30237    |
| AveragePolicyStd[5]  | 0.3154     |
| AverageReturn        | 164.32     |
| MinReturn            | 71.154     |
| MaxReturn            | 591.21     |
| StdReturn            | 95.037     |
| AverageEpisodeLength | 228.5      |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 928        |
| StdEpisodeLength     | 158.01     |
| TotalNEpisodes       | 16904      |
| TotalNSamples        | 4.9128e+05 |
| ExplainedVariance    | 0.47565    |
-------------------------------------
[2018-01-21 12:49:34.074206 UTC] Saving snapshot
[2018-01-21 12:49:34.074400 UTC] Starting iteration 99
[2018-01-21 12:49:34.074560 UTC] Start collecting samples
[2018-01-21 12:49:36.578987 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:36.657269 UTC] Performing policy update
[2018-01-21 12:49:36.657816 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:36.735912 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:37.590225 UTC] Performing line search
[2018-01-21 12:49:37.709575 UTC] Updating baseline
[2018-01-21 12:49:38.755378 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| ExpectedImprovement  | 0.020042   |
| ActualImprovement    | 0.020401   |
| ImprovementRatio     | 1.0179     |
| MeanKL               | 0.0066071  |
| Entropy              | 2.0914     |
| Perplexity           | 8.0963     |
| AveragePolicyStd     | 0.34743    |
| AveragePolicyStd[0]  | 0.35294    |
| AveragePolicyStd[1]  | 0.47828    |
| AveragePolicyStd[2]  | 0.32391    |
| AveragePolicyStd[3]  | 0.31294    |
| AveragePolicyStd[4]  | 0.30088    |
| AveragePolicyStd[5]  | 0.31565    |
| AverageReturn        | 180.79     |
| MinReturn            | 71.154     |
| MaxReturn            | 592.63     |
| StdReturn            | 114.14     |
| AverageEpisodeLength | 259.35     |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 998        |
| StdEpisodeLength     | 193.34     |
| TotalNEpisodes       | 16918      |
| TotalNSamples        | 4.9674e+05 |
| ExplainedVariance    | 0.842      |
-------------------------------------
[2018-01-21 12:49:39.117364 UTC] Saving snapshot
[2018-01-21 12:49:39.117549 UTC] Starting iteration 100
[2018-01-21 12:49:39.117690 UTC] Start collecting samples
[2018-01-21 12:49:41.717766 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:41.797914 UTC] Performing policy update
[2018-01-21 12:49:41.798441 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:41.872645 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:42.722044 UTC] Performing line search
[2018-01-21 12:49:42.838779 UTC] Updating baseline
[2018-01-21 12:49:43.912212 UTC] Computing logging information
-------------------------------------
| Iteration            | 100        |
| ExpectedImprovement  | 0.021245   |
| ActualImprovement    | 0.018398   |
| ImprovementRatio     | 0.866      |
| MeanKL               | 0.0065867  |
| Entropy              | 2.0929     |
| Perplexity           | 8.1084     |
| AveragePolicyStd     | 0.34776    |
| AveragePolicyStd[0]  | 0.35354    |
| AveragePolicyStd[1]  | 0.48219    |
| AveragePolicyStd[2]  | 0.32325    |
| AveragePolicyStd[3]  | 0.31289    |
| AveragePolicyStd[4]  | 0.29914    |
| AveragePolicyStd[5]  | 0.31555    |
| AverageReturn        | 200.8      |
| MinReturn            | 71.154     |
| MaxReturn            | 592.63     |
| StdReturn            | 124.99     |
| AverageEpisodeLength | 293        |
| MinEpisodeLength     | 82         |
| MaxEpisodeLength     | 998        |
| StdEpisodeLength     | 208.68     |
| TotalNEpisodes       | 16934      |
| TotalNSamples        | 5.0278e+05 |
| ExplainedVariance    | 0.82837    |
-------------------------------------
[2018-01-21 12:49:44.293187 UTC] Saving snapshot
[2018-01-21 12:49:44.298734 UTC] Starting iteration 101
[2018-01-21 12:49:44.298910 UTC] Start collecting samples
[2018-01-21 12:49:46.837463 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:46.916240 UTC] Performing policy update
[2018-01-21 12:49:46.916734 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:46.996114 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:47.871727 UTC] Performing line search
[2018-01-21 12:49:47.990033 UTC] Updating baseline
[2018-01-21 12:49:49.076321 UTC] Computing logging information
-------------------------------------
| Iteration            | 101        |
| ExpectedImprovement  | 0.016081   |
| ActualImprovement    | 0.016212   |
| ImprovementRatio     | 1.0081     |
| MeanKL               | 0.0067498  |
| Entropy              | 2.0555     |
| Perplexity           | 7.8108     |
| AveragePolicyStd     | 0.34565    |
| AveragePolicyStd[0]  | 0.3469     |
| AveragePolicyStd[1]  | 0.48139    |
| AveragePolicyStd[2]  | 0.32251    |
| AveragePolicyStd[3]  | 0.31111    |
| AveragePolicyStd[4]  | 0.29821    |
| AveragePolicyStd[5]  | 0.31374    |
| AverageReturn        | 217.78     |
| MinReturn            | 79.237     |
| MaxReturn            | 592.63     |
| StdReturn            | 123.83     |
| AverageEpisodeLength | 322.09     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 998        |
| StdEpisodeLength     | 208        |
| TotalNEpisodes       | 16947      |
| TotalNSamples        | 5.0744e+05 |
| ExplainedVariance    | 0.80123    |
-------------------------------------
[2018-01-21 12:49:49.447478 UTC] Saving snapshot
[2018-01-21 12:49:49.447680 UTC] Starting iteration 102
[2018-01-21 12:49:49.447822 UTC] Start collecting samples
[2018-01-21 12:49:51.991522 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:52.073794 UTC] Performing policy update
[2018-01-21 12:49:52.074350 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:52.187723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:53.523282 UTC] Performing line search
[2018-01-21 12:49:53.708822 UTC] Updating baseline
[2018-01-21 12:49:55.056214 UTC] Computing logging information
-------------------------------------
| Iteration            | 102        |
| ExpectedImprovement  | 0.022764   |
| ActualImprovement    | 0.021806   |
| ImprovementRatio     | 0.95793    |
| MeanKL               | 0.0065369  |
| Entropy              | 2.048      |
| Perplexity           | 7.7521     |
| AveragePolicyStd     | 0.34511    |
| AveragePolicyStd[0]  | 0.34379    |
| AveragePolicyStd[1]  | 0.48008    |
| AveragePolicyStd[2]  | 0.31979    |
| AveragePolicyStd[3]  | 0.3139     |
| AveragePolicyStd[4]  | 0.29926    |
| AveragePolicyStd[5]  | 0.31383    |
| AverageReturn        | 225.17     |
| MinReturn            | 79.237     |
| MaxReturn            | 592.63     |
| StdReturn            | 125.36     |
| AverageEpisodeLength | 334.97     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 998        |
| StdEpisodeLength     | 210.78     |
| TotalNEpisodes       | 16965      |
| TotalNSamples        | 5.1227e+05 |
| ExplainedVariance    | 0.89415    |
-------------------------------------
[2018-01-21 12:49:55.419104 UTC] Saving snapshot
[2018-01-21 12:49:55.419299 UTC] Starting iteration 103
[2018-01-21 12:49:55.419450 UTC] Start collecting samples
[2018-01-21 12:49:58.030052 UTC] Computing input variables for policy optimization
[2018-01-21 12:49:58.111817 UTC] Performing policy update
[2018-01-21 12:49:58.112342 UTC] Computing gradient in Euclidean space
[2018-01-21 12:49:58.183878 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:49:59.038365 UTC] Performing line search
[2018-01-21 12:49:59.150596 UTC] Updating baseline
[2018-01-21 12:50:00.382281 UTC] Computing logging information
-------------------------------------
| Iteration            | 103        |
| ExpectedImprovement  | 0.020861   |
| ActualImprovement    | 0.020814   |
| ImprovementRatio     | 0.99777    |
| MeanKL               | 0.0064374  |
| Entropy              | 2.0401     |
| Perplexity           | 7.6915     |
| AveragePolicyStd     | 0.34476    |
| AveragePolicyStd[0]  | 0.34134    |
| AveragePolicyStd[1]  | 0.48184    |
| AveragePolicyStd[2]  | 0.3174     |
| AveragePolicyStd[3]  | 0.31473    |
| AveragePolicyStd[4]  | 0.29935    |
| AveragePolicyStd[5]  | 0.31389    |
| AverageReturn        | 229.36     |
| MinReturn            | 79.237     |
| MaxReturn            | 609.7      |
| StdReturn            | 126.79     |
| AverageEpisodeLength | 341.4      |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.78     |
| TotalNEpisodes       | 16979      |
| TotalNSamples        | 5.1684e+05 |
| ExplainedVariance    | 0.91992    |
-------------------------------------
[2018-01-21 12:50:00.746574 UTC] Saving snapshot
[2018-01-21 12:50:00.746768 UTC] Starting iteration 104
[2018-01-21 12:50:00.746912 UTC] Start collecting samples
[2018-01-21 12:50:03.219210 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:03.306099 UTC] Performing policy update
[2018-01-21 12:50:03.306643 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:03.378492 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:04.227112 UTC] Performing line search
[2018-01-21 12:50:04.342596 UTC] Updating baseline
[2018-01-21 12:50:05.520594 UTC] Computing logging information
-------------------------------------
| Iteration            | 104        |
| ExpectedImprovement  | 0.028152   |
| ActualImprovement    | 0.02485    |
| ImprovementRatio     | 0.88272    |
| MeanKL               | 0.0065741  |
| Entropy              | 2.0421     |
| Perplexity           | 7.7071     |
| AveragePolicyStd     | 0.34493    |
| AveragePolicyStd[0]  | 0.34125    |
| AveragePolicyStd[1]  | 0.48316    |
| AveragePolicyStd[2]  | 0.31766    |
| AveragePolicyStd[3]  | 0.31288    |
| AveragePolicyStd[4]  | 0.30105    |
| AveragePolicyStd[5]  | 0.31356    |
| AverageReturn        | 234.23     |
| MinReturn            | 63.697     |
| MaxReturn            | 609.7      |
| StdReturn            | 132.81     |
| AverageEpisodeLength | 348.52     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.63     |
| TotalNEpisodes       | 16989      |
| TotalNSamples        | 5.2048e+05 |
| ExplainedVariance    | 0.92624    |
-------------------------------------
[2018-01-21 12:50:05.882156 UTC] Saving snapshot
[2018-01-21 12:50:05.882345 UTC] Starting iteration 105
[2018-01-21 12:50:05.882507 UTC] Start collecting samples
[2018-01-21 12:50:08.257061 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:08.326519 UTC] Performing policy update
[2018-01-21 12:50:08.327060 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:08.399708 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:09.255102 UTC] Performing line search
[2018-01-21 12:50:09.367064 UTC] Updating baseline
[2018-01-21 12:50:10.428905 UTC] Computing logging information
-------------------------------------
| Iteration            | 105        |
| ExpectedImprovement  | 0.023784   |
| ActualImprovement    | 0.025343   |
| ImprovementRatio     | 1.0656     |
| MeanKL               | 0.0065903  |
| Entropy              | 2.0233     |
| Perplexity           | 7.5629     |
| AveragePolicyStd     | 0.34379    |
| AveragePolicyStd[0]  | 0.33902    |
| AveragePolicyStd[1]  | 0.48121    |
| AveragePolicyStd[2]  | 0.31568    |
| AveragePolicyStd[3]  | 0.3126     |
| AveragePolicyStd[4]  | 0.30182    |
| AveragePolicyStd[5]  | 0.3124     |
| AverageReturn        | 240.45     |
| MinReturn            | 63.697     |
| MaxReturn            | 609.7      |
| StdReturn            | 130.22     |
| AverageEpisodeLength | 355.18     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.59     |
| TotalNEpisodes       | 16997      |
| TotalNSamples        | 5.2372e+05 |
| ExplainedVariance    | 0.69383    |
-------------------------------------
[2018-01-21 12:50:10.797755 UTC] Saving snapshot
[2018-01-21 12:50:10.797972 UTC] Starting iteration 106
[2018-01-21 12:50:10.798111 UTC] Start collecting samples
[2018-01-21 12:50:13.392744 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:13.475391 UTC] Performing policy update
[2018-01-21 12:50:13.475874 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:13.548799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:14.439038 UTC] Performing line search
[2018-01-21 12:50:14.551683 UTC] Updating baseline
[2018-01-21 12:50:15.654587 UTC] Computing logging information
-------------------------------------
| Iteration            | 106        |
| ExpectedImprovement  | 0.015177   |
| ActualImprovement    | 0.014445   |
| ImprovementRatio     | 0.95175    |
| MeanKL               | 0.0066803  |
| Entropy              | 2.0097     |
| Perplexity           | 7.4614     |
| AveragePolicyStd     | 0.34318    |
| AveragePolicyStd[0]  | 0.33976    |
| AveragePolicyStd[1]  | 0.48269    |
| AveragePolicyStd[2]  | 0.31452    |
| AveragePolicyStd[3]  | 0.30919    |
| AveragePolicyStd[4]  | 0.3019     |
| AveragePolicyStd[5]  | 0.31105    |
| AverageReturn        | 252.64     |
| MinReturn            | 63.697     |
| MaxReturn            | 635.59     |
| StdReturn            | 144.25     |
| AverageEpisodeLength | 370.19     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.41     |
| TotalNEpisodes       | 17012      |
| TotalNSamples        | 5.3181e+05 |
| ExplainedVariance    | 0.65345    |
-------------------------------------
[2018-01-21 12:50:16.031026 UTC] Saving snapshot
[2018-01-21 12:50:16.031209 UTC] Starting iteration 107
[2018-01-21 12:50:16.031349 UTC] Start collecting samples
[2018-01-21 12:50:18.443442 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:18.514164 UTC] Performing policy update
[2018-01-21 12:50:18.514746 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:18.588489 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:19.447912 UTC] Performing line search
[2018-01-21 12:50:19.561035 UTC] Updating baseline
[2018-01-21 12:50:20.607180 UTC] Computing logging information
-------------------------------------
| Iteration            | 107        |
| ExpectedImprovement  | 0.018262   |
| ActualImprovement    | 0.016542   |
| ImprovementRatio     | 0.90582    |
| MeanKL               | 0.0067203  |
| Entropy              | 1.9831     |
| Perplexity           | 7.2655     |
| AveragePolicyStd     | 0.34211    |
| AveragePolicyStd[0]  | 0.33748    |
| AveragePolicyStd[1]  | 0.48772    |
| AveragePolicyStd[2]  | 0.31325    |
| AveragePolicyStd[3]  | 0.30351    |
| AveragePolicyStd[4]  | 0.29885    |
| AveragePolicyStd[5]  | 0.31182    |
| AverageReturn        | 260.1      |
| MinReturn            | 63.697     |
| MaxReturn            | 635.59     |
| StdReturn            | 146.09     |
| AverageEpisodeLength | 382.51     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.34     |
| TotalNEpisodes       | 17022      |
| TotalNSamples        | 5.3609e+05 |
| ExplainedVariance    | 0.63877    |
-------------------------------------
[2018-01-21 12:50:20.974128 UTC] Saving snapshot
[2018-01-21 12:50:20.974319 UTC] Starting iteration 108
[2018-01-21 12:50:20.974480 UTC] Start collecting samples
[2018-01-21 12:50:23.449666 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:23.518771 UTC] Performing policy update
[2018-01-21 12:50:23.519293 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:23.591615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:24.505544 UTC] Performing line search
[2018-01-21 12:50:24.628486 UTC] Updating baseline
[2018-01-21 12:50:25.766204 UTC] Computing logging information
-------------------------------------
| Iteration            | 108        |
| ExpectedImprovement  | 0.015885   |
| ActualImprovement    | 0.015928   |
| ImprovementRatio     | 1.0027     |
| MeanKL               | 0.0066547  |
| Entropy              | 1.9878     |
| Perplexity           | 7.2992     |
| AveragePolicyStd     | 0.3423     |
| AveragePolicyStd[0]  | 0.33495    |
| AveragePolicyStd[1]  | 0.48755    |
| AveragePolicyStd[2]  | 0.31256    |
| AveragePolicyStd[3]  | 0.30379    |
| AveragePolicyStd[4]  | 0.30038    |
| AveragePolicyStd[5]  | 0.31454    |
| AverageReturn        | 266.75     |
| MinReturn            | 63.697     |
| MaxReturn            | 635.59     |
| StdReturn            | 146.74     |
| AverageEpisodeLength | 392.16     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.4      |
| TotalNEpisodes       | 17028      |
| TotalNSamples        | 5.3912e+05 |
| ExplainedVariance    | 0.92406    |
-------------------------------------
[2018-01-21 12:50:26.136796 UTC] Saving snapshot
[2018-01-21 12:50:26.136985 UTC] Starting iteration 109
[2018-01-21 12:50:26.137139 UTC] Start collecting samples
[2018-01-21 12:50:28.687335 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:28.758710 UTC] Performing policy update
[2018-01-21 12:50:28.759243 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:28.840849 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:29.697634 UTC] Performing line search
[2018-01-21 12:50:29.817196 UTC] Updating baseline
[2018-01-21 12:50:30.960422 UTC] Computing logging information
-------------------------------------
| Iteration            | 109        |
| ExpectedImprovement  | 0.013018   |
| ActualImprovement    | 0.01252    |
| ImprovementRatio     | 0.96181    |
| MeanKL               | 0.0068114  |
| Entropy              | 1.9952     |
| Perplexity           | 7.3534     |
| AveragePolicyStd     | 0.34276    |
| AveragePolicyStd[0]  | 0.3348     |
| AveragePolicyStd[1]  | 0.48866    |
| AveragePolicyStd[2]  | 0.31256    |
| AveragePolicyStd[3]  | 0.30375    |
| AveragePolicyStd[4]  | 0.29872    |
| AveragePolicyStd[5]  | 0.31809    |
| AverageReturn        | 273.83     |
| MinReturn            | 63.697     |
| MaxReturn            | 657.96     |
| StdReturn            | 155.49     |
| AverageEpisodeLength | 401.38     |
| MinEpisodeLength     | 111        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.98     |
| TotalNEpisodes       | 17037      |
| TotalNSamples        | 5.4398e+05 |
| ExplainedVariance    | 0.73095    |
-------------------------------------
[2018-01-21 12:50:31.324432 UTC] Saving snapshot
[2018-01-21 12:50:31.324642 UTC] Starting iteration 110
[2018-01-21 12:50:31.324771 UTC] Start collecting samples
[2018-01-21 12:50:33.813800 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:33.884320 UTC] Performing policy update
[2018-01-21 12:50:33.884825 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:33.972938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:34.868151 UTC] Performing line search
[2018-01-21 12:50:34.994664 UTC] Updating baseline
[2018-01-21 12:50:36.405566 UTC] Computing logging information
-------------------------------------
| Iteration            | 110        |
| ExpectedImprovement  | 0.017463   |
| ActualImprovement    | 0.016053   |
| ImprovementRatio     | 0.91926    |
| MeanKL               | 0.0067704  |
| Entropy              | 1.9934     |
| Perplexity           | 7.3405     |
| AveragePolicyStd     | 0.34266    |
| AveragePolicyStd[0]  | 0.3327     |
| AveragePolicyStd[1]  | 0.4886     |
| AveragePolicyStd[2]  | 0.30846    |
| AveragePolicyStd[3]  | 0.30419    |
| AveragePolicyStd[4]  | 0.29977    |
| AveragePolicyStd[5]  | 0.32223    |
| AverageReturn        | 289.76     |
| MinReturn            | 63.697     |
| MaxReturn            | 657.96     |
| StdReturn            | 169.34     |
| AverageEpisodeLength | 426.1      |
| MinEpisodeLength     | 114        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 271.98     |
| TotalNEpisodes       | 17044      |
| TotalNSamples        | 5.4867e+05 |
| ExplainedVariance    | 0.56909    |
-------------------------------------
[2018-01-21 12:50:36.780664 UTC] Saving snapshot
[2018-01-21 12:50:36.789734 UTC] Starting iteration 111
[2018-01-21 12:50:36.789925 UTC] Start collecting samples
[2018-01-21 12:50:39.498941 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:39.593870 UTC] Performing policy update
[2018-01-21 12:50:39.594413 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:39.690331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:40.613961 UTC] Performing line search
[2018-01-21 12:50:40.735048 UTC] Updating baseline
[2018-01-21 12:50:41.885531 UTC] Computing logging information
-------------------------------------
| Iteration            | 111        |
| ExpectedImprovement  | 0.015328   |
| ActualImprovement    | 0.014878   |
| ImprovementRatio     | 0.97062    |
| MeanKL               | 0.007032   |
| Entropy              | 1.9727     |
| Perplexity           | 7.1898     |
| AveragePolicyStd     | 0.34124    |
| AveragePolicyStd[0]  | 0.33161    |
| AveragePolicyStd[1]  | 0.48306    |
| AveragePolicyStd[2]  | 0.30948    |
| AveragePolicyStd[3]  | 0.30345    |
| AveragePolicyStd[4]  | 0.29859    |
| AveragePolicyStd[5]  | 0.32128    |
| AverageReturn        | 308.01     |
| MinReturn            | 63.697     |
| MaxReturn            | 659.09     |
| StdReturn            | 176.87     |
| AverageEpisodeLength | 452.62     |
| MinEpisodeLength     | 114        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 282.41     |
| TotalNEpisodes       | 17056      |
| TotalNSamples        | 5.5525e+05 |
| ExplainedVariance    | 0.72472    |
-------------------------------------
[2018-01-21 12:50:42.253386 UTC] Saving snapshot
[2018-01-21 12:50:42.253592 UTC] Starting iteration 112
[2018-01-21 12:50:42.253731 UTC] Start collecting samples
[2018-01-21 12:50:44.930233 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:45.001744 UTC] Performing policy update
[2018-01-21 12:50:45.002279 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:45.076988 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:45.990301 UTC] Performing line search
[2018-01-21 12:50:46.104525 UTC] Updating baseline
[2018-01-21 12:50:47.373950 UTC] Computing logging information
-------------------------------------
| Iteration            | 112        |
| ExpectedImprovement  | 0.020204   |
| ActualImprovement    | 0.020458   |
| ImprovementRatio     | 1.0126     |
| MeanKL               | 0.0065997  |
| Entropy              | 1.9644     |
| Perplexity           | 7.1303     |
| AveragePolicyStd     | 0.34044    |
| AveragePolicyStd[0]  | 0.33195    |
| AveragePolicyStd[1]  | 0.47675    |
| AveragePolicyStd[2]  | 0.30785    |
| AveragePolicyStd[3]  | 0.30391    |
| AveragePolicyStd[4]  | 0.30028    |
| AveragePolicyStd[5]  | 0.32189    |
| AverageReturn        | 325.47     |
| MinReturn            | 63.697     |
| MaxReturn            | 659.09     |
| StdReturn            | 176.56     |
| AverageEpisodeLength | 481.67     |
| MinEpisodeLength     | 114        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.79     |
| TotalNEpisodes       | 17066      |
| TotalNSamples        | 5.6112e+05 |
| ExplainedVariance    | 0.64823    |
-------------------------------------
[2018-01-21 12:50:47.776625 UTC] Saving snapshot
[2018-01-21 12:50:47.776830 UTC] Starting iteration 113
[2018-01-21 12:50:47.776974 UTC] Start collecting samples
[2018-01-21 12:50:50.181491 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:50.250158 UTC] Performing policy update
[2018-01-21 12:50:50.250676 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:50.325145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:51.201923 UTC] Performing line search
[2018-01-21 12:50:51.321296 UTC] Updating baseline
[2018-01-21 12:50:52.399086 UTC] Computing logging information
-------------------------------------
| Iteration            | 113        |
| ExpectedImprovement  | 0.015792   |
| ActualImprovement    | 0.015533   |
| ImprovementRatio     | 0.98358    |
| MeanKL               | 0.0067464  |
| Entropy              | 1.9525     |
| Perplexity           | 7.0459     |
| AveragePolicyStd     | 0.33964    |
| AveragePolicyStd[0]  | 0.32842    |
| AveragePolicyStd[1]  | 0.47395    |
| AveragePolicyStd[2]  | 0.30607    |
| AveragePolicyStd[3]  | 0.30467    |
| AveragePolicyStd[4]  | 0.30085    |
| AveragePolicyStd[5]  | 0.32386    |
| AverageReturn        | 336.8      |
| MinReturn            | 63.697     |
| MaxReturn            | 665.65     |
| StdReturn            | 176.7      |
| AverageEpisodeLength | 499.5      |
| MinEpisodeLength     | 114        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.26     |
| TotalNEpisodes       | 17072      |
| TotalNSamples        | 5.6422e+05 |
| ExplainedVariance    | 0.67775    |
-------------------------------------
[2018-01-21 12:50:52.792033 UTC] Saving snapshot
[2018-01-21 12:50:52.792227 UTC] Starting iteration 114
[2018-01-21 12:50:52.792380 UTC] Start collecting samples
[2018-01-21 12:50:55.307449 UTC] Computing input variables for policy optimization
[2018-01-21 12:50:55.387807 UTC] Performing policy update
[2018-01-21 12:50:55.388340 UTC] Computing gradient in Euclidean space
[2018-01-21 12:50:55.467358 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:50:56.317706 UTC] Performing line search
[2018-01-21 12:50:56.436994 UTC] Updating baseline
[2018-01-21 12:50:57.850768 UTC] Computing logging information
-------------------------------------
| Iteration            | 114        |
| ExpectedImprovement  | 0.020161   |
| ActualImprovement    | 0.019292   |
| ImprovementRatio     | 0.95691    |
| MeanKL               | 0.0066841  |
| Entropy              | 1.9619     |
| Perplexity           | 7.1127     |
| AveragePolicyStd     | 0.34038    |
| AveragePolicyStd[0]  | 0.32957    |
| AveragePolicyStd[1]  | 0.47767    |
| AveragePolicyStd[2]  | 0.3023     |
| AveragePolicyStd[3]  | 0.30414    |
| AveragePolicyStd[4]  | 0.30222    |
| AveragePolicyStd[5]  | 0.32637    |
| AverageReturn        | 349.38     |
| MinReturn            | 63.697     |
| MaxReturn            | 665.65     |
| StdReturn            | 179.9      |
| AverageEpisodeLength | 519.4      |
| MinEpisodeLength     | 114        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 284.31     |
| TotalNEpisodes       | 17079      |
| TotalNSamples        | 5.6878e+05 |
| ExplainedVariance    | 0.45964    |
-------------------------------------
[2018-01-21 12:50:58.226965 UTC] Saving snapshot
[2018-01-21 12:50:58.227156 UTC] Starting iteration 115
[2018-01-21 12:50:58.227275 UTC] Start collecting samples
[2018-01-21 12:51:00.577310 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:00.644813 UTC] Performing policy update
[2018-01-21 12:51:00.645357 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:00.717784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:01.556746 UTC] Performing line search
[2018-01-21 12:51:01.669952 UTC] Updating baseline
[2018-01-21 12:51:02.779249 UTC] Computing logging information
-------------------------------------
| Iteration            | 115        |
| ExpectedImprovement  | 0.015652   |
| ActualImprovement    | 0.014485   |
| ImprovementRatio     | 0.92548    |
| MeanKL               | 0.0068811  |
| Entropy              | 1.9649     |
| Perplexity           | 7.134      |
| AveragePolicyStd     | 0.34057    |
| AveragePolicyStd[0]  | 0.33158    |
| AveragePolicyStd[1]  | 0.47822    |
| AveragePolicyStd[2]  | 0.30398    |
| AveragePolicyStd[3]  | 0.30358    |
| AveragePolicyStd[4]  | 0.30123    |
| AveragePolicyStd[5]  | 0.32484    |
| AverageReturn        | 360.34     |
| MinReturn            | 16.928     |
| MaxReturn            | 674.8      |
| StdReturn            | 188.22     |
| AverageEpisodeLength | 536.52     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.05     |
| TotalNEpisodes       | 17084      |
| TotalNSamples        | 5.7281e+05 |
| ExplainedVariance    | 0.046616   |
-------------------------------------
[2018-01-21 12:51:03.147727 UTC] Saving snapshot
[2018-01-21 12:51:03.147937 UTC] Starting iteration 116
[2018-01-21 12:51:03.148079 UTC] Start collecting samples
[2018-01-21 12:51:05.562189 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:05.651951 UTC] Performing policy update
[2018-01-21 12:51:05.652497 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:05.732110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:06.660470 UTC] Performing line search
[2018-01-21 12:51:06.774181 UTC] Updating baseline
[2018-01-21 12:51:07.848321 UTC] Computing logging information
-------------------------------------
| Iteration            | 116        |
| ExpectedImprovement  | 0.015012   |
| ActualImprovement    | 0.014464   |
| ImprovementRatio     | 0.96349    |
| MeanKL               | 0.007006   |
| Entropy              | 1.9639     |
| Perplexity           | 7.1272     |
| AveragePolicyStd     | 0.34048    |
| AveragePolicyStd[0]  | 0.33564    |
| AveragePolicyStd[1]  | 0.47733    |
| AveragePolicyStd[2]  | 0.30314    |
| AveragePolicyStd[3]  | 0.30493    |
| AveragePolicyStd[4]  | 0.30176    |
| AveragePolicyStd[5]  | 0.32011    |
| AverageReturn        | 384.23     |
| MinReturn            | 16.928     |
| MaxReturn            | 674.8      |
| StdReturn            | 193.11     |
| AverageEpisodeLength | 574.27     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.51     |
| TotalNEpisodes       | 17090      |
| TotalNSamples        | 5.7834e+05 |
| ExplainedVariance    | -0.071299  |
-------------------------------------
[2018-01-21 12:51:08.212596 UTC] Saving snapshot
[2018-01-21 12:51:08.212779 UTC] Starting iteration 117
[2018-01-21 12:51:08.212913 UTC] Start collecting samples
[2018-01-21 12:51:10.632628 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:10.707314 UTC] Performing policy update
[2018-01-21 12:51:10.708273 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:10.794793 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:11.734989 UTC] Performing line search
[2018-01-21 12:51:11.897145 UTC] Updating baseline
[2018-01-21 12:51:13.082405 UTC] Computing logging information
-------------------------------------
| Iteration            | 117        |
| ExpectedImprovement  | 0.015734   |
| ActualImprovement    | 0.014768   |
| ImprovementRatio     | 0.93864    |
| MeanKL               | 0.0069738  |
| Entropy              | 1.9537     |
| Perplexity           | 7.0548     |
| AveragePolicyStd     | 0.34003    |
| AveragePolicyStd[0]  | 0.33126    |
| AveragePolicyStd[1]  | 0.47937    |
| AveragePolicyStd[2]  | 0.30472    |
| AveragePolicyStd[3]  | 0.3042     |
| AveragePolicyStd[4]  | 0.30042    |
| AveragePolicyStd[5]  | 0.32021    |
| AverageReturn        | 404.42     |
| MinReturn            | 16.928     |
| MaxReturn            | 674.8      |
| StdReturn            | 197.47     |
| AverageEpisodeLength | 605.99     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 311.88     |
| TotalNEpisodes       | 17098      |
| TotalNSamples        | 5.8458e+05 |
| ExplainedVariance    | 0.29479    |
-------------------------------------
[2018-01-21 12:51:13.447308 UTC] Saving snapshot
[2018-01-21 12:51:13.447514 UTC] Starting iteration 118
[2018-01-21 12:51:13.447651 UTC] Start collecting samples
[2018-01-21 12:51:16.108360 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:16.178167 UTC] Performing policy update
[2018-01-21 12:51:16.178727 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:16.252777 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:17.111114 UTC] Performing line search
[2018-01-21 12:51:17.223459 UTC] Updating baseline
[2018-01-21 12:51:18.274674 UTC] Computing logging information
-------------------------------------
| Iteration            | 118        |
| ExpectedImprovement  | 0.016147   |
| ActualImprovement    | 0.015427   |
| ImprovementRatio     | 0.95539    |
| MeanKL               | 0.006746   |
| Entropy              | 1.9489     |
| Perplexity           | 7.0206     |
| AveragePolicyStd     | 0.33937    |
| AveragePolicyStd[0]  | 0.32924    |
| AveragePolicyStd[1]  | 0.47273    |
| AveragePolicyStd[2]  | 0.30131    |
| AveragePolicyStd[3]  | 0.30828    |
| AveragePolicyStd[4]  | 0.30293    |
| AveragePolicyStd[5]  | 0.32176    |
| AverageReturn        | 405.66     |
| MinReturn            | 16.928     |
| MaxReturn            | 674.8      |
| StdReturn            | 198.77     |
| AverageEpisodeLength | 606.31     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.2      |
| TotalNEpisodes       | 17103      |
| TotalNSamples        | 5.8913e+05 |
| ExplainedVariance    | 0.34431    |
-------------------------------------
[2018-01-21 12:51:18.642769 UTC] Saving snapshot
[2018-01-21 12:51:18.642976 UTC] Starting iteration 119
[2018-01-21 12:51:18.643122 UTC] Start collecting samples
[2018-01-21 12:51:21.183194 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:21.254051 UTC] Performing policy update
[2018-01-21 12:51:21.254587 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:21.329932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:22.195763 UTC] Performing line search
[2018-01-21 12:51:22.309946 UTC] Updating baseline
[2018-01-21 12:51:23.517655 UTC] Computing logging information
-------------------------------------
| Iteration            | 119        |
| ExpectedImprovement  | 0.014508   |
| ActualImprovement    | 0.013613   |
| ImprovementRatio     | 0.93832    |
| MeanKL               | 0.0070217  |
| Entropy              | 1.9364     |
| Perplexity           | 6.9339     |
| AveragePolicyStd     | 0.33899    |
| AveragePolicyStd[0]  | 0.32907    |
| AveragePolicyStd[1]  | 0.47712    |
| AveragePolicyStd[2]  | 0.3004     |
| AveragePolicyStd[3]  | 0.30493    |
| AveragePolicyStd[4]  | 0.30185    |
| AveragePolicyStd[5]  | 0.32059    |
| AverageReturn        | 417.43     |
| MinReturn            | 16.928     |
| MaxReturn            | 678.27     |
| StdReturn            | 197.14     |
| AverageEpisodeLength | 622.43     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 309.34     |
| TotalNEpisodes       | 17110      |
| TotalNSamples        | 5.9339e+05 |
| ExplainedVariance    | 0.72763    |
-------------------------------------
[2018-01-21 12:51:23.886892 UTC] Saving snapshot
[2018-01-21 12:51:23.887078 UTC] Starting iteration 120
[2018-01-21 12:51:23.887198 UTC] Start collecting samples
[2018-01-21 12:51:26.300943 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:26.371867 UTC] Performing policy update
[2018-01-21 12:51:26.372389 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:26.447025 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:27.292362 UTC] Performing line search
[2018-01-21 12:51:27.407921 UTC] Updating baseline
[2018-01-21 12:51:28.506939 UTC] Computing logging information
-------------------------------------
| Iteration            | 120        |
| ExpectedImprovement  | 0.016068   |
| ActualImprovement    | 0.015872   |
| ImprovementRatio     | 0.98781    |
| MeanKL               | 0.0066206  |
| Entropy              | 1.9148     |
| Perplexity           | 6.7854     |
| AveragePolicyStd     | 0.33785    |
| AveragePolicyStd[0]  | 0.32798    |
| AveragePolicyStd[1]  | 0.47696    |
| AveragePolicyStd[2]  | 0.30349    |
| AveragePolicyStd[3]  | 0.30058    |
| AveragePolicyStd[4]  | 0.30091    |
| AveragePolicyStd[5]  | 0.31715    |
| AverageReturn        | 430        |
| MinReturn            | 16.928     |
| MaxReturn            | 678.27     |
| StdReturn            | 201.71     |
| AverageEpisodeLength | 640.9      |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.94     |
| TotalNEpisodes       | 17116      |
| TotalNSamples        | 5.9854e+05 |
| ExplainedVariance    | 0.37893    |
-------------------------------------
[2018-01-21 12:51:28.869985 UTC] Saving snapshot
[2018-01-21 12:51:28.875606 UTC] Starting iteration 121
[2018-01-21 12:51:28.875781 UTC] Start collecting samples
[2018-01-21 12:51:31.591459 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:31.661570 UTC] Performing policy update
[2018-01-21 12:51:31.662133 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:31.737406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:32.663425 UTC] Performing line search
[2018-01-21 12:51:32.782771 UTC] Updating baseline
[2018-01-21 12:51:33.857667 UTC] Computing logging information
-------------------------------------
| Iteration            | 121        |
| ExpectedImprovement  | 0.015538   |
| ActualImprovement    | 0.014393   |
| ImprovementRatio     | 0.92631    |
| MeanKL               | 0.0069996  |
| Entropy              | 1.9006     |
| Perplexity           | 6.6896     |
| AveragePolicyStd     | 0.33734    |
| AveragePolicyStd[0]  | 0.32749    |
| AveragePolicyStd[1]  | 0.47986    |
| AveragePolicyStd[2]  | 0.29764    |
| AveragePolicyStd[3]  | 0.29784    |
| AveragePolicyStd[4]  | 0.30103    |
| AveragePolicyStd[5]  | 0.32018    |
| AverageReturn        | 444.35     |
| MinReturn            | 16.928     |
| MaxReturn            | 678.27     |
| StdReturn            | 200.67     |
| AverageEpisodeLength | 662.74     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.44     |
| TotalNEpisodes       | 17120      |
| TotalNSamples        | 6.0194e+05 |
| ExplainedVariance    | 0.19816    |
-------------------------------------
[2018-01-21 12:51:34.248416 UTC] Saving snapshot
[2018-01-21 12:51:34.248626 UTC] Starting iteration 122
[2018-01-21 12:51:34.248768 UTC] Start collecting samples
[2018-01-21 12:51:37.450547 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:37.560982 UTC] Performing policy update
[2018-01-21 12:51:37.561735 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:37.637168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:38.542912 UTC] Performing line search
[2018-01-21 12:51:38.658000 UTC] Updating baseline
[2018-01-21 12:51:39.777781 UTC] Computing logging information
-------------------------------------
| Iteration            | 122        |
| ExpectedImprovement  | 0.02159    |
| ActualImprovement    | 0.019202   |
| ImprovementRatio     | 0.88938    |
| MeanKL               | 0.0065905  |
| Entropy              | 1.8924     |
| Perplexity           | 6.635      |
| AveragePolicyStd     | 0.33693    |
| AveragePolicyStd[0]  | 0.33129    |
| AveragePolicyStd[1]  | 0.47964    |
| AveragePolicyStd[2]  | 0.29872    |
| AveragePolicyStd[3]  | 0.29694    |
| AveragePolicyStd[4]  | 0.29941    |
| AveragePolicyStd[5]  | 0.31556    |
| AverageReturn        | 464.39     |
| MinReturn            | 10.081     |
| MaxReturn            | 678.27     |
| StdReturn            | 204.05     |
| AverageEpisodeLength | 693.93     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 319.03     |
| TotalNEpisodes       | 17128      |
| TotalNSamples        | 6.0851e+05 |
| ExplainedVariance    | 0.24147    |
-------------------------------------
[2018-01-21 12:51:40.155866 UTC] Saving snapshot
[2018-01-21 12:51:40.156071 UTC] Starting iteration 123
[2018-01-21 12:51:40.156220 UTC] Start collecting samples
[2018-01-21 12:51:42.612606 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:42.690196 UTC] Performing policy update
[2018-01-21 12:51:42.690794 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:42.764643 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:43.643723 UTC] Performing line search
[2018-01-21 12:51:43.760388 UTC] Updating baseline
[2018-01-21 12:51:44.817484 UTC] Computing logging information
-------------------------------------
| Iteration            | 123        |
| ExpectedImprovement  | 0.011888   |
| ActualImprovement    | 0.011706   |
| ImprovementRatio     | 0.98465    |
| MeanKL               | 0.0068831  |
| Entropy              | 1.8873     |
| Perplexity           | 6.6017     |
| AveragePolicyStd     | 0.33649    |
| AveragePolicyStd[0]  | 0.32943    |
| AveragePolicyStd[1]  | 0.47711    |
| AveragePolicyStd[2]  | 0.29789    |
| AveragePolicyStd[3]  | 0.2973     |
| AveragePolicyStd[4]  | 0.30187    |
| AveragePolicyStd[5]  | 0.31534    |
| AverageReturn        | 477.3      |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 203.65     |
| AverageEpisodeLength | 713.01     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 316.91     |
| TotalNEpisodes       | 17133      |
| TotalNSamples        | 6.1351e+05 |
| ExplainedVariance    | 0.085447   |
-------------------------------------
[2018-01-21 12:51:45.232754 UTC] Saving snapshot
[2018-01-21 12:51:45.232942 UTC] Starting iteration 124
[2018-01-21 12:51:45.233059 UTC] Start collecting samples
[2018-01-21 12:51:47.833960 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:47.910097 UTC] Performing policy update
[2018-01-21 12:51:47.910633 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:47.988082 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:48.871308 UTC] Performing line search
[2018-01-21 12:51:49.019136 UTC] Updating baseline
[2018-01-21 12:51:50.309381 UTC] Computing logging information
-------------------------------------
| Iteration            | 124        |
| ExpectedImprovement  | 0.012934   |
| ActualImprovement    | 0.011678   |
| ImprovementRatio     | 0.90285    |
| MeanKL               | 0.00749    |
| Entropy              | 1.9039     |
| Perplexity           | 6.7121     |
| AveragePolicyStd     | 0.33772    |
| AveragePolicyStd[0]  | 0.33047    |
| AveragePolicyStd[1]  | 0.48317    |
| AveragePolicyStd[2]  | 0.29747    |
| AveragePolicyStd[3]  | 0.29706    |
| AveragePolicyStd[4]  | 0.30234    |
| AveragePolicyStd[5]  | 0.31581    |
| AverageReturn        | 487.26     |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 201.89     |
| AverageEpisodeLength | 727.8      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 314.65     |
| TotalNEpisodes       | 17136      |
| TotalNSamples        | 6.1651e+05 |
| ExplainedVariance    | 0.03714    |
-------------------------------------
[2018-01-21 12:51:50.685477 UTC] Saving snapshot
[2018-01-21 12:51:50.685684 UTC] Starting iteration 125
[2018-01-21 12:51:50.685844 UTC] Start collecting samples
[2018-01-21 12:51:53.733568 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:53.808674 UTC] Performing policy update
[2018-01-21 12:51:53.809205 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:53.886957 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:51:54.745804 UTC] Performing line search
[2018-01-21 12:51:54.863686 UTC] Updating baseline
[2018-01-21 12:51:55.971055 UTC] Computing logging information
-------------------------------------
| Iteration            | 125        |
| ExpectedImprovement  | 0.013836   |
| ActualImprovement    | 0.013172   |
| ImprovementRatio     | 0.95197    |
| MeanKL               | 0.0069671  |
| Entropy              | 1.8991     |
| Perplexity           | 6.6801     |
| AveragePolicyStd     | 0.33733    |
| AveragePolicyStd[0]  | 0.33095    |
| AveragePolicyStd[1]  | 0.48028    |
| AveragePolicyStd[2]  | 0.29488    |
| AveragePolicyStd[3]  | 0.29685    |
| AveragePolicyStd[4]  | 0.3039     |
| AveragePolicyStd[5]  | 0.3171     |
| AverageReturn        | 501.62     |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 198.43     |
| AverageEpisodeLength | 749.61     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 309.43     |
| TotalNEpisodes       | 17145      |
| TotalNSamples        | 6.2418e+05 |
| ExplainedVariance    | 0.10469    |
-------------------------------------
[2018-01-21 12:51:56.359509 UTC] Saving snapshot
[2018-01-21 12:51:56.359733 UTC] Starting iteration 126
[2018-01-21 12:51:56.359868 UTC] Start collecting samples
[2018-01-21 12:51:59.039350 UTC] Computing input variables for policy optimization
[2018-01-21 12:51:59.116639 UTC] Performing policy update
[2018-01-21 12:51:59.117162 UTC] Computing gradient in Euclidean space
[2018-01-21 12:51:59.191324 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:00.070233 UTC] Performing line search
[2018-01-21 12:52:00.185369 UTC] Updating baseline
[2018-01-21 12:52:01.390188 UTC] Computing logging information
-------------------------------------
| Iteration            | 126        |
| ExpectedImprovement  | 0.01845    |
| ActualImprovement    | 0.017082   |
| ImprovementRatio     | 0.92586    |
| MeanKL               | 0.0069829  |
| Entropy              | 1.8674     |
| Perplexity           | 6.4716     |
| AveragePolicyStd     | 0.3356     |
| AveragePolicyStd[0]  | 0.32672    |
| AveragePolicyStd[1]  | 0.47924    |
| AveragePolicyStd[2]  | 0.2939     |
| AveragePolicyStd[3]  | 0.29752    |
| AveragePolicyStd[4]  | 0.30045    |
| AveragePolicyStd[5]  | 0.31577    |
| AverageReturn        | 511.3      |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 199.86     |
| AverageEpisodeLength | 764.6      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.52     |
| TotalNEpisodes       | 17152      |
| TotalNSamples        | 6.2974e+05 |
| ExplainedVariance    | 0.31196    |
-------------------------------------
[2018-01-21 12:52:01.759747 UTC] Saving snapshot
[2018-01-21 12:52:01.759938 UTC] Starting iteration 127
[2018-01-21 12:52:01.760058 UTC] Start collecting samples
[2018-01-21 12:52:04.618398 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:04.693207 UTC] Performing policy update
[2018-01-21 12:52:04.693781 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:04.775762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:05.707377 UTC] Performing line search
[2018-01-21 12:52:05.829243 UTC] Updating baseline
[2018-01-21 12:52:06.975116 UTC] Computing logging information
-------------------------------------
| Iteration            | 127        |
| ExpectedImprovement  | 0.017796   |
| ActualImprovement    | 0.017635   |
| ImprovementRatio     | 0.99099    |
| MeanKL               | 0.0066972  |
| Entropy              | 1.8556     |
| Perplexity           | 6.3958     |
| AveragePolicyStd     | 0.33491    |
| AveragePolicyStd[0]  | 0.3275     |
| AveragePolicyStd[1]  | 0.47703    |
| AveragePolicyStd[2]  | 0.29053    |
| AveragePolicyStd[3]  | 0.29787    |
| AveragePolicyStd[4]  | 0.29926    |
| AveragePolicyStd[5]  | 0.31727    |
| AverageReturn        | 514.01     |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 202.62     |
| AverageEpisodeLength | 766.62     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 313.83     |
| TotalNEpisodes       | 17162      |
| TotalNSamples        | 6.3522e+05 |
| ExplainedVariance    | 0.76295    |
-------------------------------------
[2018-01-21 12:52:07.358392 UTC] Saving snapshot
[2018-01-21 12:52:07.358594 UTC] Starting iteration 128
[2018-01-21 12:52:07.358751 UTC] Start collecting samples
[2018-01-21 12:52:10.426936 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:10.526941 UTC] Performing policy update
[2018-01-21 12:52:10.527700 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:10.609554 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:11.569102 UTC] Performing line search
[2018-01-21 12:52:11.689851 UTC] Updating baseline
[2018-01-21 12:52:12.753242 UTC] Computing logging information
-------------------------------------
| Iteration            | 128        |
| ExpectedImprovement  | 0.015354   |
| ActualImprovement    | 0.014565   |
| ImprovementRatio     | 0.94857    |
| MeanKL               | 0.0069083  |
| Entropy              | 1.8623     |
| Perplexity           | 6.4384     |
| AveragePolicyStd     | 0.33515    |
| AveragePolicyStd[0]  | 0.32969    |
| AveragePolicyStd[1]  | 0.47447    |
| AveragePolicyStd[2]  | 0.28777    |
| AveragePolicyStd[3]  | 0.2996     |
| AveragePolicyStd[4]  | 0.3009     |
| AveragePolicyStd[5]  | 0.31844    |
| AverageReturn        | 513.34     |
| MinReturn            | 10.081     |
| MaxReturn            | 690.02     |
| StdReturn            | 205.54     |
| AverageEpisodeLength | 763.55     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.74     |
| TotalNEpisodes       | 17168      |
| TotalNSamples        | 6.3902e+05 |
| ExplainedVariance    | 0.18673    |
-------------------------------------
[2018-01-21 12:52:13.138082 UTC] Saving snapshot
[2018-01-21 12:52:13.138273 UTC] Starting iteration 129
[2018-01-21 12:52:13.138439 UTC] Start collecting samples
[2018-01-21 12:52:16.635972 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:16.736562 UTC] Performing policy update
[2018-01-21 12:52:16.737174 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:16.848731 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:18.145162 UTC] Performing line search
[2018-01-21 12:52:18.352860 UTC] Updating baseline
[2018-01-21 12:52:20.286710 UTC] Computing logging information
-------------------------------------
| Iteration            | 129        |
| ExpectedImprovement  | 0.015539   |
| ActualImprovement    | 0.014382   |
| ImprovementRatio     | 0.92552    |
| MeanKL               | 0.0070606  |
| Entropy              | 1.853      |
| Perplexity           | 6.3792     |
| AveragePolicyStd     | 0.33442    |
| AveragePolicyStd[0]  | 0.33075    |
| AveragePolicyStd[1]  | 0.46966    |
| AveragePolicyStd[2]  | 0.2866     |
| AveragePolicyStd[3]  | 0.30177    |
| AveragePolicyStd[4]  | 0.29913    |
| AveragePolicyStd[5]  | 0.3186     |
| AverageReturn        | 531.69     |
| MinReturn            | 10.081     |
| MaxReturn            | 695.01     |
| StdReturn            | 197.97     |
| AverageEpisodeLength | 791.39     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 306.17     |
| TotalNEpisodes       | 17173      |
| TotalNSamples        | 6.4365e+05 |
| ExplainedVariance    | 0.14163    |
-------------------------------------
[2018-01-21 12:52:20.985497 UTC] Saving snapshot
[2018-01-21 12:52:20.986081 UTC] Starting iteration 130
[2018-01-21 12:52:20.986528 UTC] Start collecting samples
[2018-01-21 12:52:26.092676 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:26.220982 UTC] Performing policy update
[2018-01-21 12:52:26.221634 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:26.346069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:27.830964 UTC] Performing line search
[2018-01-21 12:52:28.022679 UTC] Updating baseline
[2018-01-21 12:52:29.874859 UTC] Computing logging information
-------------------------------------
| Iteration            | 130        |
| ExpectedImprovement  | 0.014716   |
| ActualImprovement    | 0.01483    |
| ImprovementRatio     | 1.0077     |
| MeanKL               | 0.0071299  |
| Entropy              | 1.8193     |
| Perplexity           | 6.1678     |
| AveragePolicyStd     | 0.33253    |
| AveragePolicyStd[0]  | 0.32985    |
| AveragePolicyStd[1]  | 0.46698    |
| AveragePolicyStd[2]  | 0.28666    |
| AveragePolicyStd[3]  | 0.29852    |
| AveragePolicyStd[4]  | 0.29743    |
| AveragePolicyStd[5]  | 0.31577    |
| AverageReturn        | 535.11     |
| MinReturn            | 10.081     |
| MaxReturn            | 702.38     |
| StdReturn            | 197.93     |
| AverageEpisodeLength | 792.73     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.98     |
| TotalNEpisodes       | 17181      |
| TotalNSamples        | 6.5006e+05 |
| ExplainedVariance    | 0.24641    |
-------------------------------------
[2018-01-21 12:52:30.565060 UTC] Saving snapshot
[2018-01-21 12:52:30.600833 UTC] Starting iteration 131
[2018-01-21 12:52:30.601060 UTC] Start collecting samples
[2018-01-21 12:52:35.632469 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:35.771797 UTC] Performing policy update
[2018-01-21 12:52:35.772442 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:35.892865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:37.335460 UTC] Performing line search
[2018-01-21 12:52:37.534048 UTC] Updating baseline
[2018-01-21 12:52:39.381457 UTC] Computing logging information
-------------------------------------
| Iteration            | 131        |
| ExpectedImprovement  | 0.017112   |
| ActualImprovement    | 0.016979   |
| ImprovementRatio     | 0.9922     |
| MeanKL               | 0.006894   |
| Entropy              | 1.8258     |
| Perplexity           | 6.2076     |
| AveragePolicyStd     | 0.3329     |
| AveragePolicyStd[0]  | 0.32836    |
| AveragePolicyStd[1]  | 0.46814    |
| AveragePolicyStd[2]  | 0.28699    |
| AveragePolicyStd[3]  | 0.29983    |
| AveragePolicyStd[4]  | 0.29837    |
| AveragePolicyStd[5]  | 0.31569    |
| AverageReturn        | 531.69     |
| MinReturn            | 10.081     |
| MaxReturn            | 708.21     |
| StdReturn            | 197.24     |
| AverageEpisodeLength | 783.9      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 304.3      |
| TotalNEpisodes       | 17188      |
| TotalNSamples        | 6.5472e+05 |
| ExplainedVariance    | 0.74168    |
-------------------------------------
[2018-01-21 12:52:40.073347 UTC] Saving snapshot
[2018-01-21 12:52:40.073681 UTC] Starting iteration 132
[2018-01-21 12:52:40.073878 UTC] Start collecting samples
[2018-01-21 12:52:45.001775 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:45.132125 UTC] Performing policy update
[2018-01-21 12:52:45.132727 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:45.250315 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:46.684844 UTC] Performing line search
[2018-01-21 12:52:46.882267 UTC] Updating baseline
[2018-01-21 12:52:48.707048 UTC] Computing logging information
-------------------------------------
| Iteration            | 132        |
| ExpectedImprovement  | 0.015737   |
| ActualImprovement    | 0.01537    |
| ImprovementRatio     | 0.97665    |
| MeanKL               | 0.0072834  |
| Entropy              | 1.821      |
| Perplexity           | 6.1782     |
| AveragePolicyStd     | 0.33268    |
| AveragePolicyStd[0]  | 0.32814    |
| AveragePolicyStd[1]  | 0.46808    |
| AveragePolicyStd[2]  | 0.2844     |
| AveragePolicyStd[3]  | 0.30124    |
| AveragePolicyStd[4]  | 0.29841    |
| AveragePolicyStd[5]  | 0.31579    |
| AverageReturn        | 529.62     |
| MinReturn            | 10.081     |
| MaxReturn            | 708.21     |
| StdReturn            | 198.5      |
| AverageEpisodeLength | 778.62     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 305.73     |
| TotalNEpisodes       | 17193      |
| TotalNSamples        | 6.5803e+05 |
| ExplainedVariance    | 0.59476    |
-------------------------------------
[2018-01-21 12:52:49.320811 UTC] Saving snapshot
[2018-01-21 12:52:49.321077 UTC] Starting iteration 133
[2018-01-21 12:52:49.321221 UTC] Start collecting samples
[2018-01-21 12:52:54.314220 UTC] Computing input variables for policy optimization
[2018-01-21 12:52:54.439266 UTC] Performing policy update
[2018-01-21 12:52:54.439920 UTC] Computing gradient in Euclidean space
[2018-01-21 12:52:54.565235 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:52:55.956470 UTC] Performing line search
[2018-01-21 12:52:56.148125 UTC] Updating baseline
[2018-01-21 12:52:57.923979 UTC] Computing logging information
-------------------------------------
| Iteration            | 133        |
| ExpectedImprovement  | 0.017559   |
| ActualImprovement    | 0.016443   |
| ImprovementRatio     | 0.93642    |
| MeanKL               | 0.0066191  |
| Entropy              | 1.8087     |
| Perplexity           | 6.1024     |
| AveragePolicyStd     | 0.33201    |
| AveragePolicyStd[0]  | 0.32617    |
| AveragePolicyStd[1]  | 0.4672     |
| AveragePolicyStd[2]  | 0.28295    |
| AveragePolicyStd[3]  | 0.30109    |
| AveragePolicyStd[4]  | 0.29728    |
| AveragePolicyStd[5]  | 0.31736    |
| AverageReturn        | 526.24     |
| MinReturn            | 10.081     |
| MaxReturn            | 708.21     |
| StdReturn            | 201.09     |
| AverageEpisodeLength | 771.74     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 308.9      |
| TotalNEpisodes       | 17200      |
| TotalNSamples        | 6.6376e+05 |
| ExplainedVariance    | 0.25424    |
-------------------------------------
[2018-01-21 12:52:58.617102 UTC] Saving snapshot
[2018-01-21 12:52:58.617333 UTC] Starting iteration 134
[2018-01-21 12:52:58.617478 UTC] Start collecting samples
[2018-01-21 12:53:03.802104 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:03.957077 UTC] Performing policy update
[2018-01-21 12:53:03.957928 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:04.078778 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:05.485219 UTC] Performing line search
[2018-01-21 12:53:05.684680 UTC] Updating baseline
[2018-01-21 12:53:07.637881 UTC] Computing logging information
------------------------------------
| Iteration            | 134       |
| ExpectedImprovement  | 0.020902  |
| ActualImprovement    | 0.018828  |
| ImprovementRatio     | 0.90077   |
| MeanKL               | 0.0067517 |
| Entropy              | 1.809     |
| Perplexity           | 6.1045    |
| AveragePolicyStd     | 0.33197   |
| AveragePolicyStd[0]  | 0.3238    |
| AveragePolicyStd[1]  | 0.46739   |
| AveragePolicyStd[2]  | 0.28464   |
| AveragePolicyStd[3]  | 0.30082   |
| AveragePolicyStd[4]  | 0.29992   |
| AveragePolicyStd[5]  | 0.31526   |
| AverageReturn        | 530.78    |
| MinReturn            | 10.081    |
| MaxReturn            | 708.21    |
| StdReturn            | 198.22    |
| AverageEpisodeLength | 778.49    |
| MinEpisodeLength     | 19        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 304.56    |
| TotalNEpisodes       | 17207     |
| TotalNSamples        | 6.694e+05 |
| ExplainedVariance    | 0.84033   |
------------------------------------
[2018-01-21 12:53:08.334948 UTC] Saving snapshot
[2018-01-21 12:53:08.335176 UTC] Starting iteration 135
[2018-01-21 12:53:08.335313 UTC] Start collecting samples
[2018-01-21 12:53:13.228335 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:13.348736 UTC] Performing policy update
[2018-01-21 12:53:13.349391 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:13.467484 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:14.950934 UTC] Performing line search
[2018-01-21 12:53:15.149694 UTC] Updating baseline
[2018-01-21 12:53:17.284321 UTC] Computing logging information
-------------------------------------
| Iteration            | 135        |
| ExpectedImprovement  | 0.019949   |
| ActualImprovement    | 0.018223   |
| ImprovementRatio     | 0.91348    |
| MeanKL               | 0.0064588  |
| Entropy              | 1.7995     |
| Perplexity           | 6.0468     |
| AveragePolicyStd     | 0.33143    |
| AveragePolicyStd[0]  | 0.32233    |
| AveragePolicyStd[1]  | 0.46694    |
| AveragePolicyStd[2]  | 0.28466    |
| AveragePolicyStd[3]  | 0.30234    |
| AveragePolicyStd[4]  | 0.29988    |
| AveragePolicyStd[5]  | 0.31244    |
| AverageReturn        | 525.16     |
| MinReturn            | 10.081     |
| MaxReturn            | 708.21     |
| StdReturn            | 203.93     |
| AverageEpisodeLength | 769.43     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.57     |
| TotalNEpisodes       | 17214      |
| TotalNSamples        | 6.7349e+05 |
| ExplainedVariance    | 0.47414    |
-------------------------------------
[2018-01-21 12:53:17.923044 UTC] Saving snapshot
[2018-01-21 12:53:17.923304 UTC] Starting iteration 136
[2018-01-21 12:53:17.923500 UTC] Start collecting samples
[2018-01-21 12:53:22.825368 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:22.976189 UTC] Performing policy update
[2018-01-21 12:53:22.977019 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:23.105889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:24.529899 UTC] Performing line search
[2018-01-21 12:53:24.721041 UTC] Updating baseline
[2018-01-21 12:53:26.510163 UTC] Computing logging information
-------------------------------------
| Iteration            | 136        |
| ExpectedImprovement  | 0.011786   |
| ActualImprovement    | 0.011476   |
| ImprovementRatio     | 0.97368    |
| MeanKL               | 0.0070139  |
| Entropy              | 1.7873     |
| Perplexity           | 5.9736     |
| AveragePolicyStd     | 0.33058    |
| AveragePolicyStd[0]  | 0.32201    |
| AveragePolicyStd[1]  | 0.46313    |
| AveragePolicyStd[2]  | 0.28515    |
| AveragePolicyStd[3]  | 0.30178    |
| AveragePolicyStd[4]  | 0.29967    |
| AveragePolicyStd[5]  | 0.31177    |
| AverageReturn        | 526.85     |
| MinReturn            | 10.081     |
| MaxReturn            | 721.96     |
| StdReturn            | 205.77     |
| AverageEpisodeLength | 769.16     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 310.91     |
| TotalNEpisodes       | 17219      |
| TotalNSamples        | 6.7786e+05 |
| ExplainedVariance    | 0.15584    |
-------------------------------------
[2018-01-21 12:53:27.159294 UTC] Saving snapshot
[2018-01-21 12:53:27.159608 UTC] Starting iteration 137
[2018-01-21 12:53:27.159789 UTC] Start collecting samples
[2018-01-21 12:53:32.106890 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:32.248690 UTC] Performing policy update
[2018-01-21 12:53:32.249283 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:32.360914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:33.770128 UTC] Performing line search
[2018-01-21 12:53:33.968103 UTC] Updating baseline
[2018-01-21 12:53:35.731047 UTC] Computing logging information
-------------------------------------
| Iteration            | 137        |
| ExpectedImprovement  | 0.012831   |
| ActualImprovement    | 0.0119     |
| ImprovementRatio     | 0.9274     |
| MeanKL               | 0.0071648  |
| Entropy              | 1.7885     |
| Perplexity           | 5.9808     |
| AveragePolicyStd     | 0.33064    |
| AveragePolicyStd[0]  | 0.32316    |
| AveragePolicyStd[1]  | 0.46253    |
| AveragePolicyStd[2]  | 0.28335    |
| AveragePolicyStd[3]  | 0.30203    |
| AveragePolicyStd[4]  | 0.30033    |
| AveragePolicyStd[5]  | 0.31246    |
| AverageReturn        | 526.46     |
| MinReturn            | 10.081     |
| MaxReturn            | 721.96     |
| StdReturn            | 209.07     |
| AverageEpisodeLength | 765.58     |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.72     |
| TotalNEpisodes       | 17226      |
| TotalNSamples        | 6.8405e+05 |
| ExplainedVariance    | 0.36533    |
-------------------------------------
[2018-01-21 12:53:36.346729 UTC] Saving snapshot
[2018-01-21 12:53:36.346968 UTC] Starting iteration 138
[2018-01-21 12:53:36.347129 UTC] Start collecting samples
[2018-01-21 12:53:41.256695 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:41.383467 UTC] Performing policy update
[2018-01-21 12:53:41.384546 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:41.508139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:42.913032 UTC] Performing line search
[2018-01-21 12:53:43.114117 UTC] Updating baseline
[2018-01-21 12:53:44.933630 UTC] Computing logging information
-------------------------------------
| Iteration            | 138        |
| ExpectedImprovement  | 0.016703   |
| ActualImprovement    | 0.01646    |
| ImprovementRatio     | 0.98546    |
| MeanKL               | 0.0067721  |
| Entropy              | 1.7868     |
| Perplexity           | 5.9706     |
| AveragePolicyStd     | 0.3303     |
| AveragePolicyStd[0]  | 0.3194     |
| AveragePolicyStd[1]  | 0.45842    |
| AveragePolicyStd[2]  | 0.28269    |
| AveragePolicyStd[3]  | 0.30567    |
| AveragePolicyStd[4]  | 0.30192    |
| AveragePolicyStd[5]  | 0.31371    |
| AverageReturn        | 518.07     |
| MinReturn            | 81.83      |
| MaxReturn            | 721.96     |
| StdReturn            | 208.92     |
| AverageEpisodeLength | 750.42     |
| MinEpisodeLength     | 127        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.19     |
| TotalNEpisodes       | 17234      |
| TotalNSamples        | 6.8956e+05 |
| ExplainedVariance    | 0.40446    |
-------------------------------------
[2018-01-21 12:53:45.566517 UTC] Saving snapshot
[2018-01-21 12:53:45.566782 UTC] Starting iteration 139
[2018-01-21 12:53:45.566990 UTC] Start collecting samples
[2018-01-21 12:53:50.853088 UTC] Computing input variables for policy optimization
[2018-01-21 12:53:51.001323 UTC] Performing policy update
[2018-01-21 12:53:51.002005 UTC] Computing gradient in Euclidean space
[2018-01-21 12:53:51.125036 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:53:52.553597 UTC] Performing line search
[2018-01-21 12:53:52.744955 UTC] Updating baseline
[2018-01-21 12:53:54.515887 UTC] Computing logging information
-------------------------------------
| Iteration            | 139        |
| ExpectedImprovement  | 0.012764   |
| ActualImprovement    | 0.012293   |
| ImprovementRatio     | 0.96313    |
| MeanKL               | 0.007205   |
| Entropy              | 1.7725     |
| Perplexity           | 5.8855     |
| AveragePolicyStd     | 0.32948    |
| AveragePolicyStd[0]  | 0.31744    |
| AveragePolicyStd[1]  | 0.45683    |
| AveragePolicyStd[2]  | 0.28207    |
| AveragePolicyStd[3]  | 0.30643    |
| AveragePolicyStd[4]  | 0.30126    |
| AveragePolicyStd[5]  | 0.31284    |
| AverageReturn        | 507.95     |
| MinReturn            | 81.83      |
| MaxReturn            | 721.96     |
| StdReturn            | 211.25     |
| AverageEpisodeLength | 730.84     |
| MinEpisodeLength     | 127        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 313.37     |
| TotalNEpisodes       | 17243      |
| TotalNSamples        | 6.9572e+05 |
| ExplainedVariance    | 0.57324    |
-------------------------------------
[2018-01-21 12:53:55.142720 UTC] Saving snapshot
[2018-01-21 12:53:55.142972 UTC] Starting iteration 140
[2018-01-21 12:53:55.143146 UTC] Start collecting samples
[2018-01-21 12:54:00.175541 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:00.309037 UTC] Performing policy update
[2018-01-21 12:54:00.310113 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:00.434086 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:01.912157 UTC] Performing line search
[2018-01-21 12:54:02.118313 UTC] Updating baseline
[2018-01-21 12:54:04.480281 UTC] Computing logging information
-------------------------------------
| Iteration            | 140        |
| ExpectedImprovement  | 0.02074    |
| ActualImprovement    | 0.017442   |
| ImprovementRatio     | 0.841      |
| MeanKL               | 0.0066455  |
| Entropy              | 1.7682     |
| Perplexity           | 5.8601     |
| AveragePolicyStd     | 0.32945    |
| AveragePolicyStd[0]  | 0.31838    |
| AveragePolicyStd[1]  | 0.46024    |
| AveragePolicyStd[2]  | 0.28195    |
| AveragePolicyStd[3]  | 0.30488    |
| AveragePolicyStd[4]  | 0.29992    |
| AveragePolicyStd[5]  | 0.31135    |
| AverageReturn        | 499.56     |
| MinReturn            | 81.83      |
| MaxReturn            | 721.96     |
| StdReturn            | 218.44     |
| AverageEpisodeLength | 716.53     |
| MinEpisodeLength     | 127        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321.34     |
| TotalNEpisodes       | 17248      |
| TotalNSamples        | 6.9883e+05 |
| ExplainedVariance    | 0.26527    |
-------------------------------------
[2018-01-21 12:54:05.168834 UTC] Saving snapshot
[2018-01-21 12:54:05.178663 UTC] Starting iteration 141
[2018-01-21 12:54:05.178892 UTC] Start collecting samples
[2018-01-21 12:54:09.972547 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:10.100791 UTC] Performing policy update
[2018-01-21 12:54:10.101419 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:10.219879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:11.705565 UTC] Performing line search
[2018-01-21 12:54:11.914746 UTC] Updating baseline
[2018-01-21 12:54:14.121277 UTC] Computing logging information
-------------------------------------
| Iteration            | 141        |
| ExpectedImprovement  | 0.012077   |
| ActualImprovement    | 0.011997   |
| ImprovementRatio     | 0.99341    |
| MeanKL               | 0.0070994  |
| Entropy              | 1.7456     |
| Perplexity           | 5.7296     |
| AveragePolicyStd     | 0.32809    |
| AveragePolicyStd[0]  | 0.31722    |
| AveragePolicyStd[1]  | 0.45579    |
| AveragePolicyStd[2]  | 0.2796     |
| AveragePolicyStd[3]  | 0.30457    |
| AveragePolicyStd[4]  | 0.30009    |
| AveragePolicyStd[5]  | 0.31125    |
| AverageReturn        | 499.92     |
| MinReturn            | 81.83      |
| MaxReturn            | 721.96     |
| StdReturn            | 218.75     |
| AverageEpisodeLength | 716.53     |
| MinEpisodeLength     | 127        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 321.34     |
| TotalNEpisodes       | 17249      |
| TotalNSamples        | 6.9983e+05 |
| ExplainedVariance    | 0.24126    |
-------------------------------------
[2018-01-21 12:54:14.783629 UTC] Saving snapshot
[2018-01-21 12:54:14.783862 UTC] Starting iteration 142
[2018-01-21 12:54:14.784007 UTC] Start collecting samples
[2018-01-21 12:54:19.680073 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:19.837504 UTC] Performing policy update
[2018-01-21 12:54:19.838180 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:19.960720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:21.430147 UTC] Performing line search
[2018-01-21 12:54:21.626562 UTC] Updating baseline
[2018-01-21 12:54:23.426105 UTC] Computing logging information
------------------------------------
| Iteration            | 142       |
| ExpectedImprovement  | 0.014566  |
| ActualImprovement    | 0.014109  |
| ImprovementRatio     | 0.96863   |
| MeanKL               | 0.0068137 |
| Entropy              | 1.7409    |
| Perplexity           | 5.7026    |
| AveragePolicyStd     | 0.32788   |
| AveragePolicyStd[0]  | 0.31441   |
| AveragePolicyStd[1]  | 0.45677   |
| AveragePolicyStd[2]  | 0.28058   |
| AveragePolicyStd[3]  | 0.3055    |
| AveragePolicyStd[4]  | 0.29799   |
| AveragePolicyStd[5]  | 0.31201   |
| AverageReturn        | 531.68    |
| MinReturn            | 81.83     |
| MaxReturn            | 741.58    |
| StdReturn            | 211.51    |
| AverageEpisodeLength | 759.42    |
| MinEpisodeLength     | 130       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 309.5     |
| TotalNEpisodes       | 17264     |
| TotalNSamples        | 7.119e+05 |
| ExplainedVariance    | 0.35033   |
------------------------------------
[2018-01-21 12:54:24.052485 UTC] Saving snapshot
[2018-01-21 12:54:24.052816 UTC] Starting iteration 143
[2018-01-21 12:54:24.053112 UTC] Start collecting samples
[2018-01-21 12:54:28.940357 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:29.078113 UTC] Performing policy update
[2018-01-21 12:54:29.078763 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:29.210812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:30.667027 UTC] Performing line search
[2018-01-21 12:54:30.863092 UTC] Updating baseline
[2018-01-21 12:54:32.752094 UTC] Computing logging information
-------------------------------------
| Iteration            | 143        |
| ExpectedImprovement  | 0.021737   |
| ActualImprovement    | 0.019858   |
| ImprovementRatio     | 0.91358    |
| MeanKL               | 0.0068293  |
| Entropy              | 1.7391     |
| Perplexity           | 5.692      |
| AveragePolicyStd     | 0.3278     |
| AveragePolicyStd[0]  | 0.31416    |
| AveragePolicyStd[1]  | 0.45642    |
| AveragePolicyStd[2]  | 0.27757    |
| AveragePolicyStd[3]  | 0.30757    |
| AveragePolicyStd[4]  | 0.30009    |
| AveragePolicyStd[5]  | 0.311      |
| AverageReturn        | 524.58     |
| MinReturn            | 81.83      |
| MaxReturn            | 741.58     |
| StdReturn            | 215.79     |
| AverageEpisodeLength | 747.94     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.4      |
| TotalNEpisodes       | 17269      |
| TotalNSamples        | 7.1482e+05 |
| ExplainedVariance    | 0.46732    |
-------------------------------------
[2018-01-21 12:54:33.455452 UTC] Saving snapshot
[2018-01-21 12:54:33.455705 UTC] Starting iteration 144
[2018-01-21 12:54:33.455868 UTC] Start collecting samples
[2018-01-21 12:54:38.543711 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:38.696842 UTC] Performing policy update
[2018-01-21 12:54:38.697572 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:38.817785 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:40.203441 UTC] Performing line search
[2018-01-21 12:54:40.396396 UTC] Updating baseline
[2018-01-21 12:54:42.226057 UTC] Computing logging information
-------------------------------------
| Iteration            | 144        |
| ExpectedImprovement  | 0.020095   |
| ActualImprovement    | 0.020505   |
| ImprovementRatio     | 1.0204     |
| MeanKL               | 0.0066089  |
| Entropy              | 1.7218     |
| Perplexity           | 5.5947     |
| AveragePolicyStd     | 0.32675    |
| AveragePolicyStd[0]  | 0.31467    |
| AveragePolicyStd[1]  | 0.45296    |
| AveragePolicyStd[2]  | 0.27677    |
| AveragePolicyStd[3]  | 0.30753    |
| AveragePolicyStd[4]  | 0.29908    |
| AveragePolicyStd[5]  | 0.30949    |
| AverageReturn        | 516.36     |
| MinReturn            | 81.83      |
| MaxReturn            | 741.58     |
| StdReturn            | 216.77     |
| AverageEpisodeLength | 734.61     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.56     |
| TotalNEpisodes       | 17273      |
| TotalNSamples        | 7.1711e+05 |
| ExplainedVariance    | 0.72136    |
-------------------------------------
[2018-01-21 12:54:42.929779 UTC] Saving snapshot
[2018-01-21 12:54:42.930055 UTC] Starting iteration 145
[2018-01-21 12:54:42.930228 UTC] Start collecting samples
[2018-01-21 12:54:47.904009 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:48.050513 UTC] Performing policy update
[2018-01-21 12:54:48.051314 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:48.173038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:49.543323 UTC] Performing line search
[2018-01-21 12:54:49.731145 UTC] Updating baseline
[2018-01-21 12:54:51.487963 UTC] Computing logging information
-------------------------------------
| Iteration            | 145        |
| ExpectedImprovement  | 0.015015   |
| ActualImprovement    | 0.014832   |
| ImprovementRatio     | 0.98781    |
| MeanKL               | 0.0070569  |
| Entropy              | 1.7189     |
| Perplexity           | 5.5786     |
| AveragePolicyStd     | 0.32659    |
| AveragePolicyStd[0]  | 0.31298    |
| AveragePolicyStd[1]  | 0.4519     |
| AveragePolicyStd[2]  | 0.274      |
| AveragePolicyStd[3]  | 0.30907    |
| AveragePolicyStd[4]  | 0.30097    |
| AveragePolicyStd[5]  | 0.31061    |
| AverageReturn        | 515.37     |
| MinReturn            | 81.83      |
| MaxReturn            | 741.58     |
| StdReturn            | 219.19     |
| AverageEpisodeLength | 732.23     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 317.83     |
| TotalNEpisodes       | 17284      |
| TotalNSamples        | 7.2622e+05 |
| ExplainedVariance    | 0.27001    |
-------------------------------------
[2018-01-21 12:54:52.154617 UTC] Saving snapshot
[2018-01-21 12:54:52.154870 UTC] Starting iteration 146
[2018-01-21 12:54:52.155028 UTC] Start collecting samples
[2018-01-21 12:54:57.073486 UTC] Computing input variables for policy optimization
[2018-01-21 12:54:57.243507 UTC] Performing policy update
[2018-01-21 12:54:57.245899 UTC] Computing gradient in Euclidean space
[2018-01-21 12:54:57.362168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:54:58.778739 UTC] Performing line search
[2018-01-21 12:54:58.966271 UTC] Updating baseline
[2018-01-21 12:55:00.678862 UTC] Computing logging information
-------------------------------------
| Iteration            | 146        |
| ExpectedImprovement  | 0.017412   |
| ActualImprovement    | 0.017779   |
| ImprovementRatio     | 1.0211     |
| MeanKL               | 0.0072405  |
| Entropy              | 1.7085     |
| Perplexity           | 5.5204     |
| AveragePolicyStd     | 0.326      |
| AveragePolicyStd[0]  | 0.31161    |
| AveragePolicyStd[1]  | 0.45082    |
| AveragePolicyStd[2]  | 0.27324    |
| AveragePolicyStd[3]  | 0.31041    |
| AveragePolicyStd[4]  | 0.30293    |
| AveragePolicyStd[5]  | 0.30699    |
| AverageReturn        | 522.31     |
| MinReturn            | 81.83      |
| MaxReturn            | 750.84     |
| StdReturn            | 216.29     |
| AverageEpisodeLength | 741.05     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.4      |
| TotalNEpisodes       | 17290      |
| TotalNSamples        | 7.3027e+05 |
| ExplainedVariance    | 0.7463     |
-------------------------------------
[2018-01-21 12:55:01.367124 UTC] Saving snapshot
[2018-01-21 12:55:01.367355 UTC] Starting iteration 147
[2018-01-21 12:55:01.367500 UTC] Start collecting samples
[2018-01-21 12:55:06.478859 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:06.606199 UTC] Performing policy update
[2018-01-21 12:55:06.606867 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:06.719726 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:08.151506 UTC] Performing line search
[2018-01-21 12:55:08.353731 UTC] Updating baseline
[2018-01-21 12:55:10.133657 UTC] Computing logging information
-------------------------------------
| Iteration            | 147        |
| ExpectedImprovement  | 0.014892   |
| ActualImprovement    | 0.015363   |
| ImprovementRatio     | 1.0316     |
| MeanKL               | 0.0070206  |
| Entropy              | 1.7063     |
| Perplexity           | 5.5084     |
| AveragePolicyStd     | 0.32585    |
| AveragePolicyStd[0]  | 0.31228    |
| AveragePolicyStd[1]  | 0.45024    |
| AveragePolicyStd[2]  | 0.2738     |
| AveragePolicyStd[3]  | 0.30962    |
| AveragePolicyStd[4]  | 0.30187    |
| AveragePolicyStd[5]  | 0.30728    |
| AverageReturn        | 513.77     |
| MinReturn            | 81.83      |
| MaxReturn            | 750.84     |
| StdReturn            | 216.16     |
| AverageEpisodeLength | 726.55     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 311.95     |
| TotalNEpisodes       | 17296      |
| TotalNSamples        | 7.3292e+05 |
| ExplainedVariance    | 0.82447    |
-------------------------------------
[2018-01-21 12:55:10.820313 UTC] Saving snapshot
[2018-01-21 12:55:10.820581 UTC] Starting iteration 148
[2018-01-21 12:55:10.820754 UTC] Start collecting samples
[2018-01-21 12:55:16.122236 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:16.252849 UTC] Performing policy update
[2018-01-21 12:55:16.253577 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:16.385610 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:17.813845 UTC] Performing line search
[2018-01-21 12:55:18.035806 UTC] Updating baseline
[2018-01-21 12:55:20.161303 UTC] Computing logging information
-------------------------------------
| Iteration            | 148        |
| ExpectedImprovement  | 0.015559   |
| ActualImprovement    | 0.014573   |
| ImprovementRatio     | 0.93664    |
| MeanKL               | 0.0070026  |
| Entropy              | 1.6838     |
| Perplexity           | 5.3861     |
| AveragePolicyStd     | 0.32473    |
| AveragePolicyStd[0]  | 0.31287    |
| AveragePolicyStd[1]  | 0.44939    |
| AveragePolicyStd[2]  | 0.27015    |
| AveragePolicyStd[3]  | 0.30869    |
| AveragePolicyStd[4]  | 0.30229    |
| AveragePolicyStd[5]  | 0.30501    |
| AverageReturn        | 518.67     |
| MinReturn            | 81.83      |
| MaxReturn            | 750.84     |
| StdReturn            | 217.75     |
| AverageEpisodeLength | 731.59     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.25     |
| TotalNEpisodes       | 17301      |
| TotalNSamples        | 7.3792e+05 |
| ExplainedVariance    | 0.32364    |
-------------------------------------
[2018-01-21 12:55:20.791736 UTC] Saving snapshot
[2018-01-21 12:55:20.791955 UTC] Starting iteration 149
[2018-01-21 12:55:20.792127 UTC] Start collecting samples
[2018-01-21 12:55:25.716607 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:25.849136 UTC] Performing policy update
[2018-01-21 12:55:25.849750 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:25.964317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:27.385832 UTC] Performing line search
[2018-01-21 12:55:27.587039 UTC] Updating baseline
[2018-01-21 12:55:29.333664 UTC] Computing logging information
-------------------------------------
| Iteration            | 149        |
| ExpectedImprovement  | 0.013343   |
| ActualImprovement    | 0.013182   |
| ImprovementRatio     | 0.9879     |
| MeanKL               | 0.0076329  |
| Entropy              | 1.6883     |
| Perplexity           | 5.4105     |
| AveragePolicyStd     | 0.32512    |
| AveragePolicyStd[0]  | 0.31221    |
| AveragePolicyStd[1]  | 0.45219    |
| AveragePolicyStd[2]  | 0.27012    |
| AveragePolicyStd[3]  | 0.31014    |
| AveragePolicyStd[4]  | 0.30287    |
| AveragePolicyStd[5]  | 0.30316    |
| AverageReturn        | 526.65     |
| MinReturn            | 81.83      |
| MaxReturn            | 750.84     |
| StdReturn            | 220.96     |
| AverageEpisodeLength | 740.94     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.25     |
| TotalNEpisodes       | 17306      |
| TotalNSamples        | 7.4292e+05 |
| ExplainedVariance    | -0.39984   |
-------------------------------------
[2018-01-21 12:55:29.980148 UTC] Saving snapshot
[2018-01-21 12:55:29.980373 UTC] Starting iteration 150
[2018-01-21 12:55:29.980556 UTC] Start collecting samples
[2018-01-21 12:55:35.070062 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:35.194578 UTC] Performing policy update
[2018-01-21 12:55:35.195167 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:35.320325 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:36.745738 UTC] Performing line search
[2018-01-21 12:55:36.949699 UTC] Updating baseline
[2018-01-21 12:55:39.130667 UTC] Computing logging information
-------------------------------------
| Iteration            | 150        |
| ExpectedImprovement  | 0.018765   |
| ActualImprovement    | 0.017042   |
| ImprovementRatio     | 0.90818    |
| MeanKL               | 0.0069371  |
| Entropy              | 1.6899     |
| Perplexity           | 5.4188     |
| AveragePolicyStd     | 0.3255     |
| AveragePolicyStd[0]  | 0.30889    |
| AveragePolicyStd[1]  | 0.45734    |
| AveragePolicyStd[2]  | 0.26896    |
| AveragePolicyStd[3]  | 0.31237    |
| AveragePolicyStd[4]  | 0.30287    |
| AveragePolicyStd[5]  | 0.30257    |
| AverageReturn        | 548.04     |
| MinReturn            | 85.952     |
| MaxReturn            | 760.75     |
| StdReturn            | 213.6      |
| AverageEpisodeLength | 769.52     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 304.45     |
| TotalNEpisodes       | 17313      |
| TotalNSamples        | 7.4944e+05 |
| ExplainedVariance    | 0.078915   |
-------------------------------------
[2018-01-21 12:55:39.748266 UTC] Saving snapshot
[2018-01-21 12:55:39.761260 UTC] Starting iteration 151
[2018-01-21 12:55:39.761592 UTC] Start collecting samples
[2018-01-21 12:55:45.073439 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:45.216412 UTC] Performing policy update
[2018-01-21 12:55:45.217251 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:45.338708 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:46.781573 UTC] Performing line search
[2018-01-21 12:55:46.990352 UTC] Updating baseline
[2018-01-21 12:55:48.915842 UTC] Computing logging information
-------------------------------------
| Iteration            | 151        |
| ExpectedImprovement  | 0.017255   |
| ActualImprovement    | 0.016448   |
| ImprovementRatio     | 0.95323    |
| MeanKL               | 0.0069668  |
| Entropy              | 1.6443     |
| Perplexity           | 5.1773     |
| AveragePolicyStd     | 0.32312    |
| AveragePolicyStd[0]  | 0.30786    |
| AveragePolicyStd[1]  | 0.45453    |
| AveragePolicyStd[2]  | 0.26511    |
| AveragePolicyStd[3]  | 0.31115    |
| AveragePolicyStd[4]  | 0.3014     |
| AveragePolicyStd[5]  | 0.29869    |
| AverageReturn        | 538.93     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 220.48     |
| AverageEpisodeLength | 755.62     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 312.35     |
| TotalNEpisodes       | 17320      |
| TotalNSamples        | 7.5442e+05 |
| ExplainedVariance    | 0.17059    |
-------------------------------------
[2018-01-21 12:55:49.552348 UTC] Saving snapshot
[2018-01-21 12:55:49.552606 UTC] Starting iteration 152
[2018-01-21 12:55:49.552774 UTC] Start collecting samples
[2018-01-21 12:55:54.727850 UTC] Computing input variables for policy optimization
[2018-01-21 12:55:54.903823 UTC] Performing policy update
[2018-01-21 12:55:54.904600 UTC] Computing gradient in Euclidean space
[2018-01-21 12:55:55.091864 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:55:56.532701 UTC] Performing line search
[2018-01-21 12:55:56.770164 UTC] Updating baseline
[2018-01-21 12:55:58.700695 UTC] Computing logging information
-------------------------------------
| Iteration            | 152        |
| ExpectedImprovement  | 0.016336   |
| ActualImprovement    | 0.015232   |
| ImprovementRatio     | 0.93241    |
| MeanKL               | 0.0068607  |
| Entropy              | 1.6485     |
| Perplexity           | 5.1991     |
| AveragePolicyStd     | 0.32326    |
| AveragePolicyStd[0]  | 0.30663    |
| AveragePolicyStd[1]  | 0.4533     |
| AveragePolicyStd[2]  | 0.26508    |
| AveragePolicyStd[3]  | 0.31136    |
| AveragePolicyStd[4]  | 0.30393    |
| AveragePolicyStd[5]  | 0.29928    |
| AverageReturn        | 535.56     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 225.08     |
| AverageEpisodeLength | 749.7      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 318.11     |
| TotalNEpisodes       | 17325      |
| TotalNSamples        | 7.5802e+05 |
| ExplainedVariance    | 0.19679    |
-------------------------------------
[2018-01-21 12:55:59.401220 UTC] Saving snapshot
[2018-01-21 12:55:59.401515 UTC] Starting iteration 153
[2018-01-21 12:55:59.401711 UTC] Start collecting samples
[2018-01-21 12:56:04.449426 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:04.559449 UTC] Performing policy update
[2018-01-21 12:56:04.560215 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:04.685471 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:06.264930 UTC] Performing line search
[2018-01-21 12:56:06.541375 UTC] Updating baseline
[2018-01-21 12:56:08.380852 UTC] Computing logging information
-------------------------------------
| Iteration            | 153        |
| ExpectedImprovement  | 0.015566   |
| ActualImprovement    | 0.014558   |
| ImprovementRatio     | 0.93524    |
| MeanKL               | 0.0069785  |
| Entropy              | 1.6428     |
| Perplexity           | 5.1694     |
| AveragePolicyStd     | 0.3231     |
| AveragePolicyStd[0]  | 0.30714    |
| AveragePolicyStd[1]  | 0.45545    |
| AveragePolicyStd[2]  | 0.26564    |
| AveragePolicyStd[3]  | 0.31167    |
| AveragePolicyStd[4]  | 0.30313    |
| AveragePolicyStd[5]  | 0.29555    |
| AverageReturn        | 542.99     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 224.89     |
| AverageEpisodeLength | 757.22     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 315.03     |
| TotalNEpisodes       | 17330      |
| TotalNSamples        | 7.6302e+05 |
| ExplainedVariance    | -0.030028  |
-------------------------------------
[2018-01-21 12:56:09.029957 UTC] Saving snapshot
[2018-01-21 12:56:09.030290 UTC] Starting iteration 154
[2018-01-21 12:56:09.030522 UTC] Start collecting samples
[2018-01-21 12:56:15.431148 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:15.615086 UTC] Performing policy update
[2018-01-21 12:56:15.615760 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:15.751096 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:17.258049 UTC] Performing line search
[2018-01-21 12:56:17.515177 UTC] Updating baseline
[2018-01-21 12:56:19.452006 UTC] Computing logging information
-------------------------------------
| Iteration            | 154        |
| ExpectedImprovement  | 0.016166   |
| ActualImprovement    | 0.014977   |
| ImprovementRatio     | 0.92647    |
| MeanKL               | 0.0069671  |
| Entropy              | 1.6297     |
| Perplexity           | 5.1021     |
| AveragePolicyStd     | 0.32263    |
| AveragePolicyStd[0]  | 0.30766    |
| AveragePolicyStd[1]  | 0.45796    |
| AveragePolicyStd[2]  | 0.26446    |
| AveragePolicyStd[3]  | 0.31285    |
| AveragePolicyStd[4]  | 0.30069    |
| AveragePolicyStd[5]  | 0.29215    |
| AverageReturn        | 565.63     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 216.27     |
| AverageEpisodeLength | 789.91     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 302.19     |
| TotalNEpisodes       | 17336      |
| TotalNSamples        | 7.6902e+05 |
| ExplainedVariance    | 0.11054    |
-------------------------------------
[2018-01-21 12:56:20.159376 UTC] Saving snapshot
[2018-01-21 12:56:20.159616 UTC] Starting iteration 155
[2018-01-21 12:56:20.159767 UTC] Start collecting samples
[2018-01-21 12:56:25.310580 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:25.430819 UTC] Performing policy update
[2018-01-21 12:56:25.431659 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:25.552301 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:27.101534 UTC] Performing line search
[2018-01-21 12:56:27.297479 UTC] Updating baseline
[2018-01-21 12:56:29.219311 UTC] Computing logging information
-------------------------------------
| Iteration            | 155        |
| ExpectedImprovement  | 0.013815   |
| ActualImprovement    | 0.013639   |
| ImprovementRatio     | 0.98724    |
| MeanKL               | 0.0069596  |
| Entropy              | 1.6172     |
| Perplexity           | 5.0392     |
| AveragePolicyStd     | 0.32196    |
| AveragePolicyStd[0]  | 0.30575    |
| AveragePolicyStd[1]  | 0.45716    |
| AveragePolicyStd[2]  | 0.26509    |
| AveragePolicyStd[3]  | 0.31249    |
| AveragePolicyStd[4]  | 0.30207    |
| AveragePolicyStd[5]  | 0.28918    |
| AverageReturn        | 570.78     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 216.4      |
| AverageEpisodeLength | 796.01     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 301.14     |
| TotalNEpisodes       | 17340      |
| TotalNSamples        | 7.7302e+05 |
| ExplainedVariance    | -0.073347  |
-------------------------------------
[2018-01-21 12:56:29.954035 UTC] Saving snapshot
[2018-01-21 12:56:29.954375 UTC] Starting iteration 156
[2018-01-21 12:56:29.954559 UTC] Start collecting samples
[2018-01-21 12:56:35.053748 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:35.184186 UTC] Performing policy update
[2018-01-21 12:56:35.184784 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:35.311933 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:36.734730 UTC] Performing line search
[2018-01-21 12:56:36.924359 UTC] Updating baseline
[2018-01-21 12:56:38.771832 UTC] Computing logging information
-------------------------------------
| Iteration            | 156        |
| ExpectedImprovement  | 0.016911   |
| ActualImprovement    | 0.015934   |
| ImprovementRatio     | 0.94225    |
| MeanKL               | 0.0067787  |
| Entropy              | 1.6282     |
| Perplexity           | 5.0945     |
| AveragePolicyStd     | 0.32254    |
| AveragePolicyStd[0]  | 0.30531    |
| AveragePolicyStd[1]  | 0.4581     |
| AveragePolicyStd[2]  | 0.26647    |
| AveragePolicyStd[3]  | 0.31411    |
| AveragePolicyStd[4]  | 0.30231    |
| AveragePolicyStd[5]  | 0.28892    |
| AverageReturn        | 581.88     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 208.93     |
| AverageEpisodeLength | 810.26     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 289.94     |
| TotalNEpisodes       | 17348      |
| TotalNSamples        | 7.7986e+05 |
| ExplainedVariance    | 0.31573    |
-------------------------------------
[2018-01-21 12:56:39.404254 UTC] Saving snapshot
[2018-01-21 12:56:39.404503 UTC] Starting iteration 157
[2018-01-21 12:56:39.404687 UTC] Start collecting samples
[2018-01-21 12:56:44.244587 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:44.389294 UTC] Performing policy update
[2018-01-21 12:56:44.390176 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:44.514258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:45.959960 UTC] Performing line search
[2018-01-21 12:56:46.160358 UTC] Updating baseline
[2018-01-21 12:56:47.912925 UTC] Computing logging information
-------------------------------------
| Iteration            | 157        |
| ExpectedImprovement  | 0.014343   |
| ActualImprovement    | 0.013826   |
| ImprovementRatio     | 0.96395    |
| MeanKL               | 0.0071691  |
| Entropy              | 1.6045     |
| Perplexity           | 4.9755     |
| AveragePolicyStd     | 0.32133    |
| AveragePolicyStd[0]  | 0.30245    |
| AveragePolicyStd[1]  | 0.45608    |
| AveragePolicyStd[2]  | 0.26324    |
| AveragePolicyStd[3]  | 0.3165     |
| AveragePolicyStd[4]  | 0.30154    |
| AveragePolicyStd[5]  | 0.28817    |
| AverageReturn        | 586.75     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 208.91     |
| AverageEpisodeLength | 815.39     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.71     |
| TotalNEpisodes       | 17352      |
| TotalNSamples        | 7.8386e+05 |
| ExplainedVariance    | 0.1538     |
-------------------------------------
[2018-01-21 12:56:48.605950 UTC] Saving snapshot
[2018-01-21 12:56:48.606246 UTC] Starting iteration 158
[2018-01-21 12:56:48.606451 UTC] Start collecting samples
[2018-01-21 12:56:53.689336 UTC] Computing input variables for policy optimization
[2018-01-21 12:56:53.814504 UTC] Performing policy update
[2018-01-21 12:56:53.815581 UTC] Computing gradient in Euclidean space
[2018-01-21 12:56:53.938961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:56:55.339572 UTC] Performing line search
[2018-01-21 12:56:55.536044 UTC] Updating baseline
[2018-01-21 12:56:57.437853 UTC] Computing logging information
-------------------------------------
| Iteration            | 158        |
| ExpectedImprovement  | 0.017346   |
| ActualImprovement    | 0.016426   |
| ImprovementRatio     | 0.94697    |
| MeanKL               | 0.0066794  |
| Entropy              | 1.595      |
| Perplexity           | 4.9283     |
| AveragePolicyStd     | 0.321      |
| AveragePolicyStd[0]  | 0.3037     |
| AveragePolicyStd[1]  | 0.45745    |
| AveragePolicyStd[2]  | 0.26063    |
| AveragePolicyStd[3]  | 0.31742    |
| AveragePolicyStd[4]  | 0.29837    |
| AveragePolicyStd[5]  | 0.28846    |
| AverageReturn        | 587.29     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 210.09     |
| AverageEpisodeLength | 815.22     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 288.89     |
| TotalNEpisodes       | 17356      |
| TotalNSamples        | 7.8736e+05 |
| ExplainedVariance    | 0.27516    |
-------------------------------------
[2018-01-21 12:56:58.080064 UTC] Saving snapshot
[2018-01-21 12:56:58.080338 UTC] Starting iteration 159
[2018-01-21 12:56:58.080519 UTC] Start collecting samples
[2018-01-21 12:57:03.409320 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:03.552897 UTC] Performing policy update
[2018-01-21 12:57:03.553511 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:03.682964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:05.148746 UTC] Performing line search
[2018-01-21 12:57:05.348768 UTC] Updating baseline
[2018-01-21 12:57:07.153866 UTC] Computing logging information
-------------------------------------
| Iteration            | 159        |
| ExpectedImprovement  | 0.014769   |
| ActualImprovement    | 0.014343   |
| ImprovementRatio     | 0.97112    |
| MeanKL               | 0.0073313  |
| Entropy              | 1.5767     |
| Perplexity           | 4.8391     |
| AveragePolicyStd     | 0.31992    |
| AveragePolicyStd[0]  | 0.30414    |
| AveragePolicyStd[1]  | 0.45405    |
| AveragePolicyStd[2]  | 0.2603     |
| AveragePolicyStd[3]  | 0.31715    |
| AveragePolicyStd[4]  | 0.29703    |
| AveragePolicyStd[5]  | 0.28683    |
| AverageReturn        | 596.42     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 206.2      |
| AverageEpisodeLength | 826.85     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.28     |
| TotalNEpisodes       | 17363      |
| TotalNSamples        | 7.9358e+05 |
| ExplainedVariance    | 0.22763    |
-------------------------------------
[2018-01-21 12:57:07.773241 UTC] Saving snapshot
[2018-01-21 12:57:07.773535 UTC] Starting iteration 160
[2018-01-21 12:57:07.773701 UTC] Start collecting samples
[2018-01-21 12:57:13.036332 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:13.165023 UTC] Performing policy update
[2018-01-21 12:57:13.166034 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:13.297568 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:14.750551 UTC] Performing line search
[2018-01-21 12:57:14.950188 UTC] Updating baseline
[2018-01-21 12:57:16.766152 UTC] Computing logging information
-------------------------------------
| Iteration            | 160        |
| ExpectedImprovement  | 0.016238   |
| ActualImprovement    | 0.01577    |
| ImprovementRatio     | 0.97118    |
| MeanKL               | 0.0070986  |
| Entropy              | 1.5661     |
| Perplexity           | 4.7881     |
| AveragePolicyStd     | 0.31921    |
| AveragePolicyStd[0]  | 0.30314    |
| AveragePolicyStd[1]  | 0.45016    |
| AveragePolicyStd[2]  | 0.2593     |
| AveragePolicyStd[3]  | 0.31852    |
| AveragePolicyStd[4]  | 0.29826    |
| AveragePolicyStd[5]  | 0.28588    |
| AverageReturn        | 605.11     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 203.09     |
| AverageEpisodeLength | 836.87     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.58     |
| TotalNEpisodes       | 17370      |
| TotalNSamples        | 7.9902e+05 |
| ExplainedVariance    | 0.49146    |
-------------------------------------
[2018-01-21 12:57:17.395248 UTC] Saving snapshot
[2018-01-21 12:57:17.404939 UTC] Starting iteration 161
[2018-01-21 12:57:17.405176 UTC] Start collecting samples
[2018-01-21 12:57:22.499015 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:22.627870 UTC] Performing policy update
[2018-01-21 12:57:22.628605 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:22.744552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:24.141212 UTC] Performing line search
[2018-01-21 12:57:24.324574 UTC] Updating baseline
[2018-01-21 12:57:26.261631 UTC] Computing logging information
-------------------------------------
| Iteration            | 161        |
| ExpectedImprovement  | 0.01672    |
| ActualImprovement    | 0.014772   |
| ImprovementRatio     | 0.88346    |
| MeanKL               | 0.0068012  |
| Entropy              | 1.5543     |
| Perplexity           | 4.7316     |
| AveragePolicyStd     | 0.31838    |
| AveragePolicyStd[0]  | 0.30139    |
| AveragePolicyStd[1]  | 0.4456     |
| AveragePolicyStd[2]  | 0.26005    |
| AveragePolicyStd[3]  | 0.32128    |
| AveragePolicyStd[4]  | 0.29706    |
| AveragePolicyStd[5]  | 0.28492    |
| AverageReturn        | 613.15     |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 198.34     |
| AverageEpisodeLength | 849.3      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 270.86     |
| TotalNEpisodes       | 17375      |
| TotalNSamples        | 8.0327e+05 |
| ExplainedVariance    | 0.38416    |
-------------------------------------
[2018-01-21 12:57:26.903046 UTC] Saving snapshot
[2018-01-21 12:57:26.903289 UTC] Starting iteration 162
[2018-01-21 12:57:26.903433 UTC] Start collecting samples
[2018-01-21 12:57:31.997045 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:32.128203 UTC] Performing policy update
[2018-01-21 12:57:32.128848 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:32.245294 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:33.773327 UTC] Performing line search
[2018-01-21 12:57:33.973898 UTC] Updating baseline
[2018-01-21 12:57:35.859469 UTC] Computing logging information
-------------------------------------
| Iteration            | 162        |
| ExpectedImprovement  | 0.014501   |
| ActualImprovement    | 0.013757   |
| ImprovementRatio     | 0.94871    |
| MeanKL               | 0.007111   |
| Entropy              | 1.5371     |
| Perplexity           | 4.651      |
| AveragePolicyStd     | 0.31739    |
| AveragePolicyStd[0]  | 0.30016    |
| AveragePolicyStd[1]  | 0.4423     |
| AveragePolicyStd[2]  | 0.25858    |
| AveragePolicyStd[3]  | 0.32185    |
| AveragePolicyStd[4]  | 0.29662    |
| AveragePolicyStd[5]  | 0.28484    |
| AverageReturn        | 613.7      |
| MinReturn            | 12.679     |
| MaxReturn            | 760.75     |
| StdReturn            | 196.45     |
| AverageEpisodeLength | 848.48     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.43     |
| TotalNEpisodes       | 17383      |
| TotalNSamples        | 8.1007e+05 |
| ExplainedVariance    | 0.3949     |
-------------------------------------
[2018-01-21 12:57:36.559811 UTC] Saving snapshot
[2018-01-21 12:57:36.560076 UTC] Starting iteration 163
[2018-01-21 12:57:36.560252 UTC] Start collecting samples
[2018-01-21 12:57:42.049245 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:42.171831 UTC] Performing policy update
[2018-01-21 12:57:42.172488 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:42.293344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:43.722959 UTC] Performing line search
[2018-01-21 12:57:43.940079 UTC] Updating baseline
[2018-01-21 12:57:45.715258 UTC] Computing logging information
-------------------------------------
| Iteration            | 163        |
| ExpectedImprovement  | 0.017744   |
| ActualImprovement    | 0.016818   |
| ImprovementRatio     | 0.9478     |
| MeanKL               | 0.0067987  |
| Entropy              | 1.5174     |
| Perplexity           | 4.5603     |
| AveragePolicyStd     | 0.31656    |
| AveragePolicyStd[0]  | 0.29824    |
| AveragePolicyStd[1]  | 0.44454    |
| AveragePolicyStd[2]  | 0.25719    |
| AveragePolicyStd[3]  | 0.31985    |
| AveragePolicyStd[4]  | 0.29613    |
| AveragePolicyStd[5]  | 0.28339    |
| AverageReturn        | 611.21     |
| MinReturn            | 10.456     |
| MaxReturn            | 760.75     |
| StdReturn            | 207.92     |
| AverageEpisodeLength | 844.15     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 284.1      |
| TotalNEpisodes       | 17390      |
| TotalNSamples        | 8.1468e+05 |
| ExplainedVariance    | 0.19525    |
-------------------------------------
[2018-01-21 12:57:46.367198 UTC] Saving snapshot
[2018-01-21 12:57:46.367522 UTC] Starting iteration 164
[2018-01-21 12:57:46.367728 UTC] Start collecting samples
[2018-01-21 12:57:51.676946 UTC] Computing input variables for policy optimization
[2018-01-21 12:57:51.821187 UTC] Performing policy update
[2018-01-21 12:57:51.821847 UTC] Computing gradient in Euclidean space
[2018-01-21 12:57:51.951270 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:57:53.456482 UTC] Performing line search
[2018-01-21 12:57:53.645982 UTC] Updating baseline
[2018-01-21 12:57:55.461038 UTC] Computing logging information
-------------------------------------
| Iteration            | 164        |
| ExpectedImprovement  | 0.012435   |
| ActualImprovement    | 0.012294   |
| ImprovementRatio     | 0.98865    |
| MeanKL               | 0.0075632  |
| Entropy              | 1.5065     |
| Perplexity           | 4.5108     |
| AveragePolicyStd     | 0.31602    |
| AveragePolicyStd[0]  | 0.29815    |
| AveragePolicyStd[1]  | 0.44518    |
| AveragePolicyStd[2]  | 0.25785    |
| AveragePolicyStd[3]  | 0.31803    |
| AveragePolicyStd[4]  | 0.29424    |
| AveragePolicyStd[5]  | 0.2827     |
| AverageReturn        | 622.48     |
| MinReturn            | 10.456     |
| MaxReturn            | 760.75     |
| StdReturn            | 202.66     |
| AverageEpisodeLength | 860.58     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.32     |
| TotalNEpisodes       | 17393      |
| TotalNSamples        | 8.1768e+05 |
| ExplainedVariance    | 0.18138    |
-------------------------------------
[2018-01-21 12:57:56.119478 UTC] Saving snapshot
[2018-01-21 12:57:56.119800 UTC] Starting iteration 165
[2018-01-21 12:57:56.120048 UTC] Start collecting samples
[2018-01-21 12:58:01.401906 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:01.530655 UTC] Performing policy update
[2018-01-21 12:58:01.531267 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:01.655855 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:03.106009 UTC] Performing line search
[2018-01-21 12:58:03.313940 UTC] Updating baseline
[2018-01-21 12:58:05.491664 UTC] Computing logging information
-------------------------------------
| Iteration            | 165        |
| ExpectedImprovement  | 0.018856   |
| ActualImprovement    | 0.016894   |
| ImprovementRatio     | 0.89597    |
| MeanKL               | 0.0067458  |
| Entropy              | 1.4977     |
| Perplexity           | 4.4715     |
| AveragePolicyStd     | 0.31553    |
| AveragePolicyStd[0]  | 0.29628    |
| AveragePolicyStd[1]  | 0.4439     |
| AveragePolicyStd[2]  | 0.25783    |
| AveragePolicyStd[3]  | 0.3178     |
| AveragePolicyStd[4]  | 0.29566    |
| AveragePolicyStd[5]  | 0.28169    |
| AverageReturn        | 630.43     |
| MinReturn            | 10.456     |
| MaxReturn            | 760.75     |
| StdReturn            | 196.05     |
| AverageEpisodeLength | 870.3      |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 266.06     |
| TotalNEpisodes       | 17399      |
| TotalNSamples        | 8.2294e+05 |
| ExplainedVariance    | 0.30398    |
-------------------------------------
[2018-01-21 12:58:06.105815 UTC] Saving snapshot
[2018-01-21 12:58:06.106170 UTC] Starting iteration 166
[2018-01-21 12:58:06.106346 UTC] Start collecting samples
[2018-01-21 12:58:11.438361 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:11.576570 UTC] Performing policy update
[2018-01-21 12:58:11.592689 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:11.774533 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:13.168403 UTC] Performing line search
[2018-01-21 12:58:13.359512 UTC] Updating baseline
[2018-01-21 12:58:15.175854 UTC] Computing logging information
-------------------------------------
| Iteration            | 166        |
| ExpectedImprovement  | 0.013674   |
| ActualImprovement    | 0.012618   |
| ImprovementRatio     | 0.92279    |
| MeanKL               | 0.0075946  |
| Entropy              | 1.4809     |
| Perplexity           | 4.3968     |
| AveragePolicyStd     | 0.31465    |
| AveragePolicyStd[0]  | 0.29557    |
| AveragePolicyStd[1]  | 0.44292    |
| AveragePolicyStd[2]  | 0.25631    |
| AveragePolicyStd[3]  | 0.31582    |
| AveragePolicyStd[4]  | 0.29462    |
| AveragePolicyStd[5]  | 0.28266    |
| AverageReturn        | 623.92     |
| MinReturn            | 10.456     |
| MaxReturn            | 760.75     |
| StdReturn            | 199.9      |
| AverageEpisodeLength | 859.9      |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 271.82     |
| TotalNEpisodes       | 17407      |
| TotalNSamples        | 8.2990e+05 |
| ExplainedVariance    | 0.27528    |
-------------------------------------
[2018-01-21 12:58:15.857007 UTC] Saving snapshot
[2018-01-21 12:58:15.857295 UTC] Starting iteration 167
[2018-01-21 12:58:15.857491 UTC] Start collecting samples
[2018-01-21 12:58:21.176637 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:21.324084 UTC] Performing policy update
[2018-01-21 12:58:21.324789 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:21.451212 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:22.948653 UTC] Performing line search
[2018-01-21 12:58:23.141748 UTC] Updating baseline
[2018-01-21 12:58:25.096197 UTC] Computing logging information
-------------------------------------
| Iteration            | 167        |
| ExpectedImprovement  | 0.017315   |
| ActualImprovement    | 0.01603    |
| ImprovementRatio     | 0.92581    |
| MeanKL               | 0.0067367  |
| Entropy              | 1.4571     |
| Perplexity           | 4.2934     |
| AveragePolicyStd     | 0.31351    |
| AveragePolicyStd[0]  | 0.29525    |
| AveragePolicyStd[1]  | 0.44217    |
| AveragePolicyStd[2]  | 0.25383    |
| AveragePolicyStd[3]  | 0.31548    |
| AveragePolicyStd[4]  | 0.29375    |
| AveragePolicyStd[5]  | 0.28061    |
| AverageReturn        | 612.63     |
| MinReturn            | 10.456     |
| MaxReturn            | 755.77     |
| StdReturn            | 208.99     |
| AverageEpisodeLength | 842.14     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 285.23     |
| TotalNEpisodes       | 17414      |
| TotalNSamples        | 8.3398e+05 |
| ExplainedVariance    | 0.64556    |
-------------------------------------
[2018-01-21 12:58:25.804375 UTC] Saving snapshot
[2018-01-21 12:58:25.804609 UTC] Starting iteration 168
[2018-01-21 12:58:25.804754 UTC] Start collecting samples
[2018-01-21 12:58:31.687003 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:31.811511 UTC] Performing policy update
[2018-01-21 12:58:31.811950 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:31.949396 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:33.425370 UTC] Performing line search
[2018-01-21 12:58:33.612898 UTC] Updating baseline
[2018-01-21 12:58:35.567806 UTC] Computing logging information
-------------------------------------
| Iteration            | 168        |
| ExpectedImprovement  | 0.013612   |
| ActualImprovement    | 0.013205   |
| ImprovementRatio     | 0.97011    |
| MeanKL               | 0.0069717  |
| Entropy              | 1.4444     |
| Perplexity           | 4.2392     |
| AveragePolicyStd     | 0.31286    |
| AveragePolicyStd[0]  | 0.2945     |
| AveragePolicyStd[1]  | 0.44152    |
| AveragePolicyStd[2]  | 0.25255    |
| AveragePolicyStd[3]  | 0.31341    |
| AveragePolicyStd[4]  | 0.29297    |
| AveragePolicyStd[5]  | 0.28219    |
| AverageReturn        | 613.18     |
| MinReturn            | 10.456     |
| MaxReturn            | 755.77     |
| StdReturn            | 209.27     |
| AverageEpisodeLength | 842.14     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 285.23     |
| TotalNEpisodes       | 17418      |
| TotalNSamples        | 8.3798e+05 |
| ExplainedVariance    | 0.24458    |
-------------------------------------
[2018-01-21 12:58:36.176954 UTC] Saving snapshot
[2018-01-21 12:58:36.177192 UTC] Starting iteration 169
[2018-01-21 12:58:36.177347 UTC] Start collecting samples
[2018-01-21 12:58:41.644546 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:41.770492 UTC] Performing policy update
[2018-01-21 12:58:41.771488 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:41.890924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:43.432586 UTC] Performing line search
[2018-01-21 12:58:43.620727 UTC] Updating baseline
[2018-01-21 12:58:45.964184 UTC] Computing logging information
-------------------------------------
| Iteration            | 169        |
| ExpectedImprovement  | 0.015077   |
| ActualImprovement    | 0.014494   |
| ImprovementRatio     | 0.96132    |
| MeanKL               | 0.0067112  |
| Entropy              | 1.4445     |
| Perplexity           | 4.2398     |
| AveragePolicyStd     | 0.31282    |
| AveragePolicyStd[0]  | 0.29327    |
| AveragePolicyStd[1]  | 0.44023    |
| AveragePolicyStd[2]  | 0.25138    |
| AveragePolicyStd[3]  | 0.31379    |
| AveragePolicyStd[4]  | 0.29497    |
| AveragePolicyStd[5]  | 0.28329    |
| AverageReturn        | 618.99     |
| MinReturn            | 10.456     |
| MaxReturn            | 765.12     |
| StdReturn            | 206.43     |
| AverageEpisodeLength | 847.94     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 280.46     |
| TotalNEpisodes       | 17424      |
| TotalNSamples        | 8.4278e+05 |
| ExplainedVariance    | 0.15276    |
-------------------------------------
[2018-01-21 12:58:46.588835 UTC] Saving snapshot
[2018-01-21 12:58:46.589131 UTC] Starting iteration 170
[2018-01-21 12:58:46.589314 UTC] Start collecting samples
[2018-01-21 12:58:51.851109 UTC] Computing input variables for policy optimization
[2018-01-21 12:58:51.989688 UTC] Performing policy update
[2018-01-21 12:58:51.990764 UTC] Computing gradient in Euclidean space
[2018-01-21 12:58:52.121865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:58:53.763164 UTC] Performing line search
[2018-01-21 12:58:53.960266 UTC] Updating baseline
[2018-01-21 12:58:56.115804 UTC] Computing logging information
-------------------------------------
| Iteration            | 170        |
| ExpectedImprovement  | 0.014912   |
| ActualImprovement    | 0.014554   |
| ImprovementRatio     | 0.976      |
| MeanKL               | 0.0072401  |
| Entropy              | 1.4539     |
| Perplexity           | 4.2796     |
| AveragePolicyStd     | 0.31324    |
| AveragePolicyStd[0]  | 0.29517    |
| AveragePolicyStd[1]  | 0.4402     |
| AveragePolicyStd[2]  | 0.25234    |
| AveragePolicyStd[3]  | 0.31253    |
| AveragePolicyStd[4]  | 0.29649    |
| AveragePolicyStd[5]  | 0.28273    |
| AverageReturn        | 626.35     |
| MinReturn            | 10.456     |
| MaxReturn            | 765.12     |
| StdReturn            | 197.97     |
| AverageEpisodeLength | 857.59     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.68     |
| TotalNEpisodes       | 17431      |
| TotalNSamples        | 8.4978e+05 |
| ExplainedVariance    | -0.16051   |
-------------------------------------
[2018-01-21 12:58:56.924310 UTC] Saving snapshot
[2018-01-21 12:58:56.933766 UTC] Starting iteration 171
[2018-01-21 12:58:56.934001 UTC] Start collecting samples
[2018-01-21 12:59:02.337781 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:02.473057 UTC] Performing policy update
[2018-01-21 12:59:02.481226 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:02.607447 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:04.129248 UTC] Performing line search
[2018-01-21 12:59:04.327360 UTC] Updating baseline
[2018-01-21 12:59:06.959646 UTC] Computing logging information
-------------------------------------
| Iteration            | 171        |
| ExpectedImprovement  | 0.015068   |
| ActualImprovement    | 0.014404   |
| ImprovementRatio     | 0.95594    |
| MeanKL               | 0.0072885  |
| Entropy              | 1.443      |
| Perplexity           | 4.2334     |
| AveragePolicyStd     | 0.31272    |
| AveragePolicyStd[0]  | 0.29469    |
| AveragePolicyStd[1]  | 0.43931    |
| AveragePolicyStd[2]  | 0.2503     |
| AveragePolicyStd[3]  | 0.31287    |
| AveragePolicyStd[4]  | 0.29623    |
| AveragePolicyStd[5]  | 0.28293    |
| AverageReturn        | 624.17     |
| MinReturn            | 10.456     |
| MaxReturn            | 769.36     |
| StdReturn            | 199.58     |
| AverageEpisodeLength | 853.12     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 269.99     |
| TotalNEpisodes       | 17435      |
| TotalNSamples        | 8.5333e+05 |
| ExplainedVariance    | 0.29238    |
-------------------------------------
[2018-01-21 12:59:07.680791 UTC] Saving snapshot
[2018-01-21 12:59:07.681048 UTC] Starting iteration 172
[2018-01-21 12:59:07.681223 UTC] Start collecting samples
[2018-01-21 12:59:13.152307 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:13.322941 UTC] Performing policy update
[2018-01-21 12:59:13.323604 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:13.441527 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:14.878510 UTC] Performing line search
[2018-01-21 12:59:15.069575 UTC] Updating baseline
[2018-01-21 12:59:17.005498 UTC] Computing logging information
------------------------------------
| Iteration            | 172       |
| ExpectedImprovement  | 0.015707  |
| ActualImprovement    | 0.014751  |
| ImprovementRatio     | 0.93911   |
| MeanKL               | 0.0072058 |
| Entropy              | 1.4293    |
| Perplexity           | 4.1759    |
| AveragePolicyStd     | 0.31216   |
| AveragePolicyStd[0]  | 0.29484   |
| AveragePolicyStd[1]  | 0.43954   |
| AveragePolicyStd[2]  | 0.24763   |
| AveragePolicyStd[3]  | 0.31286   |
| AveragePolicyStd[4]  | 0.2968    |
| AveragePolicyStd[5]  | 0.28127   |
| AverageReturn        | 620.97    |
| MinReturn            | 10.456    |
| MaxReturn            | 769.36    |
| StdReturn            | 202.89    |
| AverageEpisodeLength | 846.8     |
| MinEpisodeLength     | 23        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 273.85    |
| TotalNEpisodes       | 17441     |
| TotalNSamples        | 8.587e+05 |
| ExplainedVariance    | 0.033416  |
------------------------------------
[2018-01-21 12:59:17.694313 UTC] Saving snapshot
[2018-01-21 12:59:17.694660 UTC] Starting iteration 173
[2018-01-21 12:59:17.694890 UTC] Start collecting samples
[2018-01-21 12:59:22.819850 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:22.959181 UTC] Performing policy update
[2018-01-21 12:59:22.959961 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:23.076751 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:24.516851 UTC] Performing line search
[2018-01-21 12:59:24.707222 UTC] Updating baseline
[2018-01-21 12:59:26.794265 UTC] Computing logging information
------------------------------------
| Iteration            | 173       |
| ExpectedImprovement  | 0.013607  |
| ActualImprovement    | 0.013442  |
| ImprovementRatio     | 0.98782   |
| MeanKL               | 0.0073913 |
| Entropy              | 1.4273    |
| Perplexity           | 4.1674    |
| AveragePolicyStd     | 0.31212   |
| AveragePolicyStd[0]  | 0.29354   |
| AveragePolicyStd[1]  | 0.43987   |
| AveragePolicyStd[2]  | 0.24737   |
| AveragePolicyStd[3]  | 0.31429   |
| AveragePolicyStd[4]  | 0.29852   |
| AveragePolicyStd[5]  | 0.27913   |
| AverageReturn        | 631.73    |
| MinReturn            | 10.456    |
| MaxReturn            | 797.71    |
| StdReturn            | 199.39    |
| AverageEpisodeLength | 858.45    |
| MinEpisodeLength     | 23        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 268.26    |
| TotalNEpisodes       | 17448     |
| TotalNSamples        | 8.657e+05 |
| ExplainedVariance    | 0.14593   |
------------------------------------
[2018-01-21 12:59:27.444161 UTC] Saving snapshot
[2018-01-21 12:59:27.444399 UTC] Starting iteration 174
[2018-01-21 12:59:27.444547 UTC] Start collecting samples
[2018-01-21 12:59:32.216232 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:32.353213 UTC] Performing policy update
[2018-01-21 12:59:32.353845 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:32.480525 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:33.948397 UTC] Performing line search
[2018-01-21 12:59:34.164071 UTC] Updating baseline
[2018-01-21 12:59:36.314959 UTC] Computing logging information
-------------------------------------
| Iteration            | 174        |
| ExpectedImprovement  | 0.014817   |
| ActualImprovement    | 0.014577   |
| ImprovementRatio     | 0.9838     |
| MeanKL               | 0.0069548  |
| Entropy              | 1.4272     |
| Perplexity           | 4.167      |
| AveragePolicyStd     | 0.31221    |
| AveragePolicyStd[0]  | 0.29212    |
| AveragePolicyStd[1]  | 0.44125    |
| AveragePolicyStd[2]  | 0.24705    |
| AveragePolicyStd[3]  | 0.31385    |
| AveragePolicyStd[4]  | 0.30019    |
| AveragePolicyStd[5]  | 0.27879    |
| AverageReturn        | 631.12     |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 199.59     |
| AverageEpisodeLength | 856.34     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 267.97     |
| TotalNEpisodes       | 17451      |
| TotalNSamples        | 8.6849e+05 |
| ExplainedVariance    | 0.10564    |
-------------------------------------
[2018-01-21 12:59:36.960064 UTC] Saving snapshot
[2018-01-21 12:59:36.960313 UTC] Starting iteration 175
[2018-01-21 12:59:36.960490 UTC] Start collecting samples
[2018-01-21 12:59:42.066588 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:42.203418 UTC] Performing policy update
[2018-01-21 12:59:42.204402 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:42.335213 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:43.816118 UTC] Performing line search
[2018-01-21 12:59:44.016514 UTC] Updating baseline
[2018-01-21 12:59:46.300466 UTC] Computing logging information
-------------------------------------
| Iteration            | 175        |
| ExpectedImprovement  | 0.019073   |
| ActualImprovement    | 0.017116   |
| ImprovementRatio     | 0.89739    |
| MeanKL               | 0.0067686  |
| Entropy              | 1.4208     |
| Perplexity           | 4.1405     |
| AveragePolicyStd     | 0.31173    |
| AveragePolicyStd[0]  | 0.29248    |
| AveragePolicyStd[1]  | 0.43807    |
| AveragePolicyStd[2]  | 0.24662    |
| AveragePolicyStd[3]  | 0.31394    |
| AveragePolicyStd[4]  | 0.30067    |
| AveragePolicyStd[5]  | 0.27863    |
| AverageReturn        | 627.3      |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 205.48     |
| AverageEpisodeLength | 851.1      |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.93     |
| TotalNEpisodes       | 17456      |
| TotalNSamples        | 8.7247e+05 |
| ExplainedVariance    | 0.090438   |
-------------------------------------
[2018-01-21 12:59:46.959729 UTC] Saving snapshot
[2018-01-21 12:59:46.960360 UTC] Starting iteration 176
[2018-01-21 12:59:46.960860 UTC] Start collecting samples
[2018-01-21 12:59:52.059938 UTC] Computing input variables for policy optimization
[2018-01-21 12:59:52.201968 UTC] Performing policy update
[2018-01-21 12:59:52.202698 UTC] Computing gradient in Euclidean space
[2018-01-21 12:59:52.323013 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 12:59:53.763846 UTC] Performing line search
[2018-01-21 12:59:53.957774 UTC] Updating baseline
[2018-01-21 12:59:56.040358 UTC] Computing logging information
-------------------------------------
| Iteration            | 176        |
| ExpectedImprovement  | 0.015739   |
| ActualImprovement    | 0.014726   |
| ImprovementRatio     | 0.93562    |
| MeanKL               | 0.0068556  |
| Entropy              | 1.3995     |
| Perplexity           | 4.053      |
| AveragePolicyStd     | 0.31067    |
| AveragePolicyStd[0]  | 0.29218    |
| AveragePolicyStd[1]  | 0.43685    |
| AveragePolicyStd[2]  | 0.24558    |
| AveragePolicyStd[3]  | 0.31374    |
| AveragePolicyStd[4]  | 0.299      |
| AveragePolicyStd[5]  | 0.27665    |
| AverageReturn        | 627.79     |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 206.73     |
| AverageEpisodeLength | 850.43     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.52     |
| TotalNEpisodes       | 17463      |
| TotalNSamples        | 8.7863e+05 |
| ExplainedVariance    | 0.17944    |
-------------------------------------
[2018-01-21 12:59:56.739334 UTC] Saving snapshot
[2018-01-21 12:59:56.739639 UTC] Starting iteration 177
[2018-01-21 12:59:56.739840 UTC] Start collecting samples
[2018-01-21 13:00:01.553174 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:01.685090 UTC] Performing policy update
[2018-01-21 13:00:01.685737 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:01.815709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:03.242213 UTC] Performing line search
[2018-01-21 13:00:03.430160 UTC] Updating baseline
[2018-01-21 13:00:05.561184 UTC] Computing logging information
-------------------------------------
| Iteration            | 177        |
| ExpectedImprovement  | 0.014071   |
| ActualImprovement    | 0.013487   |
| ImprovementRatio     | 0.95853    |
| MeanKL               | 0.0074251  |
| Entropy              | 1.3713     |
| Perplexity           | 3.9405     |
| AveragePolicyStd     | 0.309      |
| AveragePolicyStd[0]  | 0.29001    |
| AveragePolicyStd[1]  | 0.43129    |
| AveragePolicyStd[2]  | 0.24532    |
| AveragePolicyStd[3]  | 0.31277    |
| AveragePolicyStd[4]  | 0.29845    |
| AveragePolicyStd[5]  | 0.27613    |
| AverageReturn        | 635.59     |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 203.57     |
| AverageEpisodeLength | 858.56     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.63     |
| TotalNEpisodes       | 17469      |
| TotalNSamples        | 8.8463e+05 |
| ExplainedVariance    | 0.20309    |
-------------------------------------
[2018-01-21 13:00:06.182901 UTC] Saving snapshot
[2018-01-21 13:00:06.183196 UTC] Starting iteration 178
[2018-01-21 13:00:06.183459 UTC] Start collecting samples
[2018-01-21 13:00:11.112710 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:11.262205 UTC] Performing policy update
[2018-01-21 13:00:11.262865 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:11.391454 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:12.807593 UTC] Performing line search
[2018-01-21 13:00:12.999704 UTC] Updating baseline
[2018-01-21 13:00:14.767357 UTC] Computing logging information
-------------------------------------
| Iteration            | 178        |
| ExpectedImprovement  | 0.016952   |
| ActualImprovement    | 0.016477   |
| ImprovementRatio     | 0.97202    |
| MeanKL               | 0.0067245  |
| Entropy              | 1.3727     |
| Perplexity           | 3.9459     |
| AveragePolicyStd     | 0.30901    |
| AveragePolicyStd[0]  | 0.29064    |
| AveragePolicyStd[1]  | 0.43031    |
| AveragePolicyStd[2]  | 0.24561    |
| AveragePolicyStd[3]  | 0.3128     |
| AveragePolicyStd[4]  | 0.29903    |
| AveragePolicyStd[5]  | 0.27566    |
| AverageReturn        | 632.15     |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 204.02     |
| AverageEpisodeLength | 851.65     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.65     |
| TotalNEpisodes       | 17474      |
| TotalNSamples        | 8.8814e+05 |
| ExplainedVariance    | 0.69284    |
-------------------------------------
[2018-01-21 13:00:15.455742 UTC] Saving snapshot
[2018-01-21 13:00:15.455998 UTC] Starting iteration 179
[2018-01-21 13:00:15.456176 UTC] Start collecting samples
[2018-01-21 13:00:20.140793 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:20.268217 UTC] Performing policy update
[2018-01-21 13:00:20.268906 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:20.387948 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:21.795190 UTC] Performing line search
[2018-01-21 13:00:21.983017 UTC] Updating baseline
[2018-01-21 13:00:24.170332 UTC] Computing logging information
-------------------------------------
| Iteration            | 179        |
| ExpectedImprovement  | 0.016284   |
| ActualImprovement    | 0.015703   |
| ImprovementRatio     | 0.96437    |
| MeanKL               | 0.006902   |
| Entropy              | 1.3575     |
| Perplexity           | 3.8863     |
| AveragePolicyStd     | 0.30823    |
| AveragePolicyStd[0]  | 0.28838    |
| AveragePolicyStd[1]  | 0.4314     |
| AveragePolicyStd[2]  | 0.24684    |
| AveragePolicyStd[3]  | 0.30966    |
| AveragePolicyStd[4]  | 0.29569    |
| AveragePolicyStd[5]  | 0.27743    |
| AverageReturn        | 644.68     |
| MinReturn            | 10.456     |
| MaxReturn            | 797.71     |
| StdReturn            | 197.67     |
| AverageEpisodeLength | 868.46     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 263.2      |
| TotalNEpisodes       | 17479      |
| TotalNSamples        | 8.9314e+05 |
| ExplainedVariance    | 0.27       |
-------------------------------------
[2018-01-21 13:00:24.794498 UTC] Saving snapshot
[2018-01-21 13:00:24.794737 UTC] Starting iteration 180
[2018-01-21 13:00:24.794891 UTC] Start collecting samples
[2018-01-21 13:00:29.611467 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:29.758206 UTC] Performing policy update
[2018-01-21 13:00:29.758826 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:29.903715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:31.332142 UTC] Performing line search
[2018-01-21 13:00:31.528597 UTC] Updating baseline
[2018-01-21 13:00:33.515745 UTC] Computing logging information
-------------------------------------
| Iteration            | 180        |
| ExpectedImprovement  | 0.017476   |
| ActualImprovement    | 0.016165   |
| ImprovementRatio     | 0.92498    |
| MeanKL               | 0.0068704  |
| Entropy              | 1.3462     |
| Perplexity           | 3.8428     |
| AveragePolicyStd     | 0.3074     |
| AveragePolicyStd[0]  | 0.289      |
| AveragePolicyStd[1]  | 0.42624    |
| AveragePolicyStd[2]  | 0.24678    |
| AveragePolicyStd[3]  | 0.30895    |
| AveragePolicyStd[4]  | 0.2952     |
| AveragePolicyStd[5]  | 0.2782     |
| AverageReturn        | 641.88     |
| MinReturn            | 10.456     |
| MaxReturn            | 818.98     |
| StdReturn            | 209.1      |
| AverageEpisodeLength | 860.98     |
| MinEpisodeLength     | 23         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.06     |
| TotalNEpisodes       | 17485      |
| TotalNSamples        | 8.9817e+05 |
| ExplainedVariance    | 0.036233   |
-------------------------------------
[2018-01-21 13:00:34.213597 UTC] Saving snapshot
[2018-01-21 13:00:34.223017 UTC] Starting iteration 181
[2018-01-21 13:00:34.223261 UTC] Start collecting samples
[2018-01-21 13:00:39.355113 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:39.506281 UTC] Performing policy update
[2018-01-21 13:00:39.507170 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:39.631927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:41.049629 UTC] Performing line search
[2018-01-21 13:00:41.236923 UTC] Updating baseline
[2018-01-21 13:00:43.133828 UTC] Computing logging information
-------------------------------------
| Iteration            | 181        |
| ExpectedImprovement  | 0.01336    |
| ActualImprovement    | 0.01259    |
| ImprovementRatio     | 0.94236    |
| MeanKL               | 0.0071989  |
| Entropy              | 1.3285     |
| Perplexity           | 3.7753     |
| AveragePolicyStd     | 0.30658    |
| AveragePolicyStd[0]  | 0.2885     |
| AveragePolicyStd[1]  | 0.42638    |
| AveragePolicyStd[2]  | 0.24488    |
| AveragePolicyStd[3]  | 0.30737    |
| AveragePolicyStd[4]  | 0.29314    |
| AveragePolicyStd[5]  | 0.27919    |
| AverageReturn        | 658.23     |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 192.36     |
| AverageEpisodeLength | 880.47     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 252.38     |
| TotalNEpisodes       | 17491      |
| TotalNSamples        | 9.0373e+05 |
| ExplainedVariance    | 0.31172    |
-------------------------------------
[2018-01-21 13:00:43.786854 UTC] Saving snapshot
[2018-01-21 13:00:43.787130 UTC] Starting iteration 182
[2018-01-21 13:00:43.787320 UTC] Start collecting samples
[2018-01-21 13:00:48.697750 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:48.826379 UTC] Performing policy update
[2018-01-21 13:00:48.827063 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:48.944304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:50.372980 UTC] Performing line search
[2018-01-21 13:00:50.566903 UTC] Updating baseline
[2018-01-21 13:00:52.372107 UTC] Computing logging information
-------------------------------------
| Iteration            | 182        |
| ExpectedImprovement  | 0.015013   |
| ActualImprovement    | 0.013555   |
| ImprovementRatio     | 0.90289    |
| MeanKL               | 0.0072409  |
| Entropy              | 1.3273     |
| Perplexity           | 3.7707     |
| AveragePolicyStd     | 0.30647    |
| AveragePolicyStd[0]  | 0.28743    |
| AveragePolicyStd[1]  | 0.42618    |
| AveragePolicyStd[2]  | 0.24657    |
| AveragePolicyStd[3]  | 0.30673    |
| AveragePolicyStd[4]  | 0.29478    |
| AveragePolicyStd[5]  | 0.27713    |
| AverageReturn        | 662.86     |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 191.6      |
| AverageEpisodeLength | 884.13     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.94     |
| TotalNEpisodes       | 17497      |
| TotalNSamples        | 9.0936e+05 |
| ExplainedVariance    | 0.32523    |
-------------------------------------
[2018-01-21 13:00:53.049880 UTC] Saving snapshot
[2018-01-21 13:00:53.050132 UTC] Starting iteration 183
[2018-01-21 13:00:53.050331 UTC] Start collecting samples
[2018-01-21 13:00:57.887769 UTC] Computing input variables for policy optimization
[2018-01-21 13:00:58.022910 UTC] Performing policy update
[2018-01-21 13:00:58.023527 UTC] Computing gradient in Euclidean space
[2018-01-21 13:00:58.138693 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:00:59.538107 UTC] Performing line search
[2018-01-21 13:00:59.727837 UTC] Updating baseline
[2018-01-21 13:01:01.690600 UTC] Computing logging information
-------------------------------------
| Iteration            | 183        |
| ExpectedImprovement  | 0.0135     |
| ActualImprovement    | 0.012941   |
| ImprovementRatio     | 0.95861    |
| MeanKL               | 0.0072805  |
| Entropy              | 1.3067     |
| Perplexity           | 3.6941     |
| AveragePolicyStd     | 0.30527    |
| AveragePolicyStd[0]  | 0.28608    |
| AveragePolicyStd[1]  | 0.42191    |
| AveragePolicyStd[2]  | 0.24541    |
| AveragePolicyStd[3]  | 0.30578    |
| AveragePolicyStd[4]  | 0.29396    |
| AveragePolicyStd[5]  | 0.27848    |
| AverageReturn        | 663.2      |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 195.08     |
| AverageEpisodeLength | 882.92     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 252.93     |
| TotalNEpisodes       | 17502      |
| TotalNSamples        | 9.1356e+05 |
| ExplainedVariance    | 0.23187    |
-------------------------------------
[2018-01-21 13:01:02.339271 UTC] Saving snapshot
[2018-01-21 13:01:02.339504 UTC] Starting iteration 184
[2018-01-21 13:01:02.339647 UTC] Start collecting samples
[2018-01-21 13:01:07.578924 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:07.718847 UTC] Performing policy update
[2018-01-21 13:01:07.719776 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:07.837904 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:09.239551 UTC] Performing line search
[2018-01-21 13:01:09.437922 UTC] Updating baseline
[2018-01-21 13:01:11.508706 UTC] Computing logging information
-------------------------------------
| Iteration            | 184        |
| ExpectedImprovement  | 0.012901   |
| ActualImprovement    | 0.012613   |
| ImprovementRatio     | 0.97768    |
| MeanKL               | 0.0075699  |
| Entropy              | 1.3108     |
| Perplexity           | 3.709      |
| AveragePolicyStd     | 0.30523    |
| AveragePolicyStd[0]  | 0.28554    |
| AveragePolicyStd[1]  | 0.41812    |
| AveragePolicyStd[2]  | 0.24606    |
| AveragePolicyStd[3]  | 0.30464    |
| AveragePolicyStd[4]  | 0.29637    |
| AveragePolicyStd[5]  | 0.28067    |
| AverageReturn        | 668        |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 195.22     |
| AverageEpisodeLength | 886.54     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 251.99     |
| TotalNEpisodes       | 17507      |
| TotalNSamples        | 9.1856e+05 |
| ExplainedVariance    | 0.05263    |
-------------------------------------
[2018-01-21 13:01:12.210166 UTC] Saving snapshot
[2018-01-21 13:01:12.210420 UTC] Starting iteration 185
[2018-01-21 13:01:12.210615 UTC] Start collecting samples
[2018-01-21 13:01:17.145965 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:17.271648 UTC] Performing policy update
[2018-01-21 13:01:17.290976 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:17.418692 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:18.799615 UTC] Performing line search
[2018-01-21 13:01:19.012672 UTC] Updating baseline
[2018-01-21 13:01:20.952148 UTC] Computing logging information
-------------------------------------
| Iteration            | 185        |
| ExpectedImprovement  | 0.012776   |
| ActualImprovement    | 0.012377   |
| ImprovementRatio     | 0.96875    |
| MeanKL               | 0.0074352  |
| Entropy              | 1.3182     |
| Perplexity           | 3.7365     |
| AveragePolicyStd     | 0.30593    |
| AveragePolicyStd[0]  | 0.28468    |
| AveragePolicyStd[1]  | 0.42365    |
| AveragePolicyStd[2]  | 0.24479    |
| AveragePolicyStd[3]  | 0.305      |
| AveragePolicyStd[4]  | 0.29711    |
| AveragePolicyStd[5]  | 0.28033    |
| AverageReturn        | 689.74     |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 174.12     |
| AverageEpisodeLength | 913.97     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.01     |
| TotalNEpisodes       | 17513      |
| TotalNSamples        | 9.2438e+05 |
| ExplainedVariance    | 0.19777    |
-------------------------------------
[2018-01-21 13:01:21.612990 UTC] Saving snapshot
[2018-01-21 13:01:21.613241 UTC] Starting iteration 186
[2018-01-21 13:01:21.613421 UTC] Start collecting samples
[2018-01-21 13:01:26.389577 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:26.527778 UTC] Performing policy update
[2018-01-21 13:01:26.528413 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:26.639412 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:28.044303 UTC] Performing line search
[2018-01-21 13:01:28.238520 UTC] Updating baseline
[2018-01-21 13:01:30.235473 UTC] Computing logging information
-------------------------------------
| Iteration            | 186        |
| ExpectedImprovement  | 0.016245   |
| ActualImprovement    | 0.014749   |
| ImprovementRatio     | 0.90791    |
| MeanKL               | 0.0067911  |
| Entropy              | 1.316      |
| Perplexity           | 3.7283     |
| AveragePolicyStd     | 0.30598    |
| AveragePolicyStd[0]  | 0.28436    |
| AveragePolicyStd[1]  | 0.42637    |
| AveragePolicyStd[2]  | 0.2443     |
| AveragePolicyStd[3]  | 0.30342    |
| AveragePolicyStd[4]  | 0.29801    |
| AveragePolicyStd[5]  | 0.27941    |
| AverageReturn        | 686.27     |
| MinReturn            | 13.794     |
| MaxReturn            | 818.98     |
| StdReturn            | 178.35     |
| AverageEpisodeLength | 908.08     |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.37     |
| TotalNEpisodes       | 17518      |
| TotalNSamples        | 9.2879e+05 |
| ExplainedVariance    | 0.21549    |
-------------------------------------
[2018-01-21 13:01:30.872559 UTC] Saving snapshot
[2018-01-21 13:01:30.872843 UTC] Starting iteration 187
[2018-01-21 13:01:30.873044 UTC] Start collecting samples
[2018-01-21 13:01:36.051038 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:36.218015 UTC] Performing policy update
[2018-01-21 13:01:36.218715 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:36.343492 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:37.750358 UTC] Performing line search
[2018-01-21 13:01:37.960124 UTC] Updating baseline
[2018-01-21 13:01:39.901828 UTC] Computing logging information
-------------------------------------
| Iteration            | 187        |
| ExpectedImprovement  | 0.018293   |
| ActualImprovement    | 0.015721   |
| ImprovementRatio     | 0.85939    |
| MeanKL               | 0.0067178  |
| Entropy              | 1.2862     |
| Perplexity           | 3.6189     |
| AveragePolicyStd     | 0.30438    |
| AveragePolicyStd[0]  | 0.28565    |
| AveragePolicyStd[1]  | 0.42237    |
| AveragePolicyStd[2]  | 0.24295    |
| AveragePolicyStd[3]  | 0.30143    |
| AveragePolicyStd[4]  | 0.29809    |
| AveragePolicyStd[5]  | 0.27579    |
| AverageReturn        | 688.99     |
| MinReturn            | 9.4866     |
| MaxReturn            | 818.98     |
| StdReturn            | 180.42     |
| AverageEpisodeLength | 910.26     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.1      |
| TotalNEpisodes       | 17523      |
| TotalNSamples        | 9.3281e+05 |
| ExplainedVariance    | -0.16114   |
-------------------------------------
[2018-01-21 13:01:40.578984 UTC] Saving snapshot
[2018-01-21 13:01:40.579271 UTC] Starting iteration 188
[2018-01-21 13:01:40.579498 UTC] Start collecting samples
[2018-01-21 13:01:45.402946 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:45.534127 UTC] Performing policy update
[2018-01-21 13:01:45.534761 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:45.659539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:47.076251 UTC] Performing line search
[2018-01-21 13:01:47.265247 UTC] Updating baseline
[2018-01-21 13:01:49.316208 UTC] Computing logging information
------------------------------------
| Iteration            | 188       |
| ExpectedImprovement  | 0.014094  |
| ActualImprovement    | 0.013517  |
| ImprovementRatio     | 0.95902   |
| MeanKL               | 0.0070899 |
| Entropy              | 1.2712    |
| Perplexity           | 3.5651    |
| AveragePolicyStd     | 0.30365   |
| AveragePolicyStd[0]  | 0.28374   |
| AveragePolicyStd[1]  | 0.42223   |
| AveragePolicyStd[2]  | 0.24318   |
| AveragePolicyStd[3]  | 0.30127   |
| AveragePolicyStd[4]  | 0.29653   |
| AveragePolicyStd[5]  | 0.27493   |
| AverageReturn        | 686       |
| MinReturn            | 9.4866    |
| MaxReturn            | 818.98    |
| StdReturn            | 181.82    |
| AverageEpisodeLength | 905.17    |
| MinEpisodeLength     | 21        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 233.86    |
| TotalNEpisodes       | 17530     |
| TotalNSamples        | 9.393e+05 |
| ExplainedVariance    | 0.34417   |
------------------------------------
[2018-01-21 13:01:49.993798 UTC] Saving snapshot
[2018-01-21 13:01:49.994007 UTC] Starting iteration 189
[2018-01-21 13:01:49.994176 UTC] Start collecting samples
[2018-01-21 13:01:55.016957 UTC] Computing input variables for policy optimization
[2018-01-21 13:01:55.166826 UTC] Performing policy update
[2018-01-21 13:01:55.169146 UTC] Computing gradient in Euclidean space
[2018-01-21 13:01:55.290749 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:01:56.715577 UTC] Performing line search
[2018-01-21 13:01:56.901025 UTC] Updating baseline
[2018-01-21 13:01:58.954157 UTC] Computing logging information
-------------------------------------
| Iteration            | 189        |
| ExpectedImprovement  | 0.016174   |
| ActualImprovement    | 0.015372   |
| ImprovementRatio     | 0.95042    |
| MeanKL               | 0.0071478  |
| Entropy              | 1.2634     |
| Perplexity           | 3.5374     |
| AveragePolicyStd     | 0.30326    |
| AveragePolicyStd[0]  | 0.28536    |
| AveragePolicyStd[1]  | 0.4212     |
| AveragePolicyStd[2]  | 0.24231    |
| AveragePolicyStd[3]  | 0.30138    |
| AveragePolicyStd[4]  | 0.29666    |
| AveragePolicyStd[5]  | 0.27267    |
| AverageReturn        | 675.33     |
| MinReturn            | 9.4866     |
| MaxReturn            | 818.98     |
| StdReturn            | 199.39     |
| AverageEpisodeLength | 888.64     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 258.53     |
| TotalNEpisodes       | 17539      |
| TotalNSamples        | 9.4557e+05 |
| ExplainedVariance    | 0.32551    |
-------------------------------------
[2018-01-21 13:01:59.596882 UTC] Saving snapshot
[2018-01-21 13:01:59.597150 UTC] Starting iteration 190
[2018-01-21 13:01:59.597324 UTC] Start collecting samples
[2018-01-21 13:02:04.495115 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:04.642556 UTC] Performing policy update
[2018-01-21 13:02:04.643215 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:04.787083 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:06.277767 UTC] Performing line search
[2018-01-21 13:02:06.476690 UTC] Updating baseline
[2018-01-21 13:02:08.346038 UTC] Computing logging information
-------------------------------------
| Iteration            | 190        |
| ExpectedImprovement  | 0.019418   |
| ActualImprovement    | 0.018356   |
| ImprovementRatio     | 0.9453     |
| MeanKL               | 0.0067841  |
| Entropy              | 1.2678     |
| Perplexity           | 3.5531     |
| AveragePolicyStd     | 0.3034     |
| AveragePolicyStd[0]  | 0.28379    |
| AveragePolicyStd[1]  | 0.42099    |
| AveragePolicyStd[2]  | 0.24539    |
| AveragePolicyStd[3]  | 0.30191    |
| AveragePolicyStd[4]  | 0.29718    |
| AveragePolicyStd[5]  | 0.27113    |
| AverageReturn        | 650.74     |
| MinReturn            | 9.4866     |
| MaxReturn            | 818.98     |
| StdReturn            | 226.96     |
| AverageEpisodeLength | 853.22     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 293.7      |
| TotalNEpisodes       | 17547      |
| TotalNSamples        | 9.5002e+05 |
| ExplainedVariance    | 0.45781    |
-------------------------------------
[2018-01-21 13:02:08.939364 UTC] Saving snapshot
[2018-01-21 13:02:08.947826 UTC] Starting iteration 191
[2018-01-21 13:02:08.948032 UTC] Start collecting samples
[2018-01-21 13:02:13.812181 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:13.969251 UTC] Performing policy update
[2018-01-21 13:02:13.970157 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:14.095393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:15.543677 UTC] Performing line search
[2018-01-21 13:02:15.740780 UTC] Updating baseline
[2018-01-21 13:02:17.782942 UTC] Computing logging information
-------------------------------------
| Iteration            | 191        |
| ExpectedImprovement  | 0.014354   |
| ActualImprovement    | 0.012991   |
| ImprovementRatio     | 0.90506    |
| MeanKL               | 0.0071816  |
| Entropy              | 1.2613     |
| Perplexity           | 3.53       |
| AveragePolicyStd     | 0.30296    |
| AveragePolicyStd[0]  | 0.28323    |
| AveragePolicyStd[1]  | 0.41882    |
| AveragePolicyStd[2]  | 0.24552    |
| AveragePolicyStd[3]  | 0.30243    |
| AveragePolicyStd[4]  | 0.29594    |
| AveragePolicyStd[5]  | 0.27181    |
| AverageReturn        | 645.01     |
| MinReturn            | 9.4866     |
| MaxReturn            | 818.98     |
| StdReturn            | 230.55     |
| AverageEpisodeLength | 846.55     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 297.53     |
| TotalNEpisodes       | 17552      |
| TotalNSamples        | 9.5406e+05 |
| ExplainedVariance    | 0.20354    |
-------------------------------------
[2018-01-21 13:02:18.423111 UTC] Saving snapshot
[2018-01-21 13:02:18.423309 UTC] Starting iteration 192
[2018-01-21 13:02:18.423465 UTC] Start collecting samples
[2018-01-21 13:02:23.304016 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:23.433053 UTC] Performing policy update
[2018-01-21 13:02:23.434184 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:23.558799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:24.983783 UTC] Performing line search
[2018-01-21 13:02:25.172136 UTC] Updating baseline
[2018-01-21 13:02:27.226017 UTC] Computing logging information
-------------------------------------
| Iteration            | 192        |
| ExpectedImprovement  | 0.012427   |
| ActualImprovement    | 0.012439   |
| ImprovementRatio     | 1.001      |
| MeanKL               | 0.0071192  |
| Entropy              | 1.2628     |
| Perplexity           | 3.5354     |
| AveragePolicyStd     | 0.30319    |
| AveragePolicyStd[0]  | 0.28341    |
| AveragePolicyStd[1]  | 0.42129    |
| AveragePolicyStd[2]  | 0.24423    |
| AveragePolicyStd[3]  | 0.3024     |
| AveragePolicyStd[4]  | 0.29512    |
| AveragePolicyStd[5]  | 0.27267    |
| AverageReturn        | 654.16     |
| MinReturn            | 9.4866     |
| MaxReturn            | 818.98     |
| StdReturn            | 223.56     |
| AverageEpisodeLength | 855.95     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 287.2      |
| TotalNEpisodes       | 17556      |
| TotalNSamples        | 9.5806e+05 |
| ExplainedVariance    | 0.073759   |
-------------------------------------
[2018-01-21 13:02:27.891572 UTC] Saving snapshot
[2018-01-21 13:02:27.891885 UTC] Starting iteration 193
[2018-01-21 13:02:27.892111 UTC] Start collecting samples
[2018-01-21 13:02:32.662944 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:32.810107 UTC] Performing policy update
[2018-01-21 13:02:32.811243 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:32.941934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:34.342736 UTC] Performing line search
[2018-01-21 13:02:34.540062 UTC] Updating baseline
[2018-01-21 13:02:36.459652 UTC] Computing logging information
-------------------------------------
| Iteration            | 193        |
| ExpectedImprovement  | 0.0099452  |
| ActualImprovement    | 0.0096851  |
| ImprovementRatio     | 0.97385    |
| MeanKL               | 0.0076483  |
| Entropy              | 1.2807     |
| Perplexity           | 3.5993     |
| AveragePolicyStd     | 0.30419    |
| AveragePolicyStd[0]  | 0.28592    |
| AveragePolicyStd[1]  | 0.42349    |
| AveragePolicyStd[2]  | 0.24349    |
| AveragePolicyStd[3]  | 0.30315    |
| AveragePolicyStd[4]  | 0.29625    |
| AveragePolicyStd[5]  | 0.27284    |
| AverageReturn        | 662.24     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 218.39     |
| AverageEpisodeLength | 862.68     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.32     |
| TotalNEpisodes       | 17562      |
| TotalNSamples        | 9.6389e+05 |
| ExplainedVariance    | 0.33678    |
-------------------------------------
[2018-01-21 13:02:37.149887 UTC] Saving snapshot
[2018-01-21 13:02:37.150148 UTC] Starting iteration 194
[2018-01-21 13:02:37.150370 UTC] Start collecting samples
[2018-01-21 13:02:42.304171 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:42.463699 UTC] Performing policy update
[2018-01-21 13:02:42.464660 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:42.591862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:43.960766 UTC] Performing line search
[2018-01-21 13:02:44.142117 UTC] Updating baseline
[2018-01-21 13:02:45.926883 UTC] Computing logging information
-------------------------------------
| Iteration            | 194        |
| ExpectedImprovement  | 0.013891   |
| ActualImprovement    | 0.013266   |
| ImprovementRatio     | 0.955      |
| MeanKL               | 0.0072707  |
| Entropy              | 1.3174     |
| Perplexity           | 3.7336     |
| AveragePolicyStd     | 0.30577    |
| AveragePolicyStd[0]  | 0.28883    |
| AveragePolicyStd[1]  | 0.42168    |
| AveragePolicyStd[2]  | 0.24652    |
| AveragePolicyStd[3]  | 0.3042     |
| AveragePolicyStd[4]  | 0.29849    |
| AveragePolicyStd[5]  | 0.27488    |
| AverageReturn        | 662.91     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 218.75     |
| AverageEpisodeLength | 862.68     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.32     |
| TotalNEpisodes       | 17566      |
| TotalNSamples        | 9.6789e+05 |
| ExplainedVariance    | 0.20731    |
-------------------------------------
[2018-01-21 13:02:46.618124 UTC] Saving snapshot
[2018-01-21 13:02:46.618362 UTC] Starting iteration 195
[2018-01-21 13:02:46.618546 UTC] Start collecting samples
[2018-01-21 13:02:51.701477 UTC] Computing input variables for policy optimization
[2018-01-21 13:02:51.837466 UTC] Performing policy update
[2018-01-21 13:02:51.838076 UTC] Computing gradient in Euclidean space
[2018-01-21 13:02:51.960850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:02:53.377642 UTC] Performing line search
[2018-01-21 13:02:53.580954 UTC] Updating baseline
[2018-01-21 13:02:55.431901 UTC] Computing logging information
-------------------------------------
| Iteration            | 195        |
| ExpectedImprovement  | 0.012928   |
| ActualImprovement    | 0.012401   |
| ImprovementRatio     | 0.95928    |
| MeanKL               | 0.0071519  |
| Entropy              | 1.3117     |
| Perplexity           | 3.7124     |
| AveragePolicyStd     | 0.30547    |
| AveragePolicyStd[0]  | 0.28965    |
| AveragePolicyStd[1]  | 0.42075    |
| AveragePolicyStd[2]  | 0.24559    |
| AveragePolicyStd[3]  | 0.30443    |
| AveragePolicyStd[4]  | 0.29781    |
| AveragePolicyStd[5]  | 0.2746     |
| AverageReturn        | 667.69     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 219.58     |
| AverageEpisodeLength | 866.41     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.33     |
| TotalNEpisodes       | 17571      |
| TotalNSamples        | 9.7289e+05 |
| ExplainedVariance    | 0.064879   |
-------------------------------------
[2018-01-21 13:02:56.061549 UTC] Saving snapshot
[2018-01-21 13:02:56.061931 UTC] Starting iteration 196
[2018-01-21 13:02:56.062229 UTC] Start collecting samples
[2018-01-21 13:03:01.069739 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:01.204420 UTC] Performing policy update
[2018-01-21 13:03:01.205019 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:01.336428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:02.759064 UTC] Performing line search
[2018-01-21 13:03:02.949985 UTC] Updating baseline
[2018-01-21 13:03:04.987966 UTC] Computing logging information
-------------------------------------
| Iteration            | 196        |
| ExpectedImprovement  | 0.015635   |
| ActualImprovement    | 0.015265   |
| ImprovementRatio     | 0.97633    |
| MeanKL               | 0.0068099  |
| Entropy              | 1.3291     |
| Perplexity           | 3.7775     |
| AveragePolicyStd     | 0.3064     |
| AveragePolicyStd[0]  | 0.29162    |
| AveragePolicyStd[1]  | 0.42144    |
| AveragePolicyStd[2]  | 0.24485    |
| AveragePolicyStd[3]  | 0.30596    |
| AveragePolicyStd[4]  | 0.30039    |
| AveragePolicyStd[5]  | 0.27415    |
| AverageReturn        | 674.16     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 221.12     |
| AverageEpisodeLength | 870.07     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 278.25     |
| TotalNEpisodes       | 17578      |
| TotalNSamples        | 9.7915e+05 |
| ExplainedVariance    | 0.16976    |
-------------------------------------
[2018-01-21 13:03:05.669539 UTC] Saving snapshot
[2018-01-21 13:03:05.669737 UTC] Starting iteration 197
[2018-01-21 13:03:05.669945 UTC] Start collecting samples
[2018-01-21 13:03:10.580897 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:10.715021 UTC] Performing policy update
[2018-01-21 13:03:10.716807 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:10.835788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:12.211399 UTC] Performing line search
[2018-01-21 13:03:12.401030 UTC] Updating baseline
[2018-01-21 13:03:14.138718 UTC] Computing logging information
-------------------------------------
| Iteration            | 197        |
| ExpectedImprovement  | 0.015289   |
| ActualImprovement    | 0.014731   |
| ImprovementRatio     | 0.96348    |
| MeanKL               | 0.0069429  |
| Entropy              | 1.3414     |
| Perplexity           | 3.8243     |
| AveragePolicyStd     | 0.3069     |
| AveragePolicyStd[0]  | 0.29323    |
| AveragePolicyStd[1]  | 0.4191     |
| AveragePolicyStd[2]  | 0.24443    |
| AveragePolicyStd[3]  | 0.30698    |
| AveragePolicyStd[4]  | 0.30253    |
| AveragePolicyStd[5]  | 0.27515    |
| AverageReturn        | 682.68     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 211.56     |
| AverageEpisodeLength | 879.8      |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.31     |
| TotalNEpisodes       | 17582      |
| TotalNSamples        | 9.8315e+05 |
| ExplainedVariance    | -0.31503   |
-------------------------------------
[2018-01-21 13:03:14.772203 UTC] Saving snapshot
[2018-01-21 13:03:14.772451 UTC] Starting iteration 198
[2018-01-21 13:03:14.772624 UTC] Start collecting samples
[2018-01-21 13:03:19.681327 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:19.814957 UTC] Performing policy update
[2018-01-21 13:03:19.815555 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:19.958082 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:21.360789 UTC] Performing line search
[2018-01-21 13:03:21.548124 UTC] Updating baseline
[2018-01-21 13:03:23.240089 UTC] Computing logging information
-------------------------------------
| Iteration            | 198        |
| ExpectedImprovement  | 0.014491   |
| ActualImprovement    | 0.013848   |
| ImprovementRatio     | 0.95562    |
| MeanKL               | 0.0074333  |
| Entropy              | 1.3121     |
| Perplexity           | 3.7139     |
| AveragePolicyStd     | 0.30548    |
| AveragePolicyStd[0]  | 0.29063    |
| AveragePolicyStd[1]  | 0.41798    |
| AveragePolicyStd[2]  | 0.24219    |
| AveragePolicyStd[3]  | 0.30587    |
| AveragePolicyStd[4]  | 0.30105    |
| AveragePolicyStd[5]  | 0.27516    |
| AverageReturn        | 680.81     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 212.92     |
| AverageEpisodeLength | 875.87     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 266.41     |
| TotalNEpisodes       | 17588      |
| TotalNSamples        | 9.8875e+05 |
| ExplainedVariance    | 0.14561    |
-------------------------------------
[2018-01-21 13:03:23.941865 UTC] Saving snapshot
[2018-01-21 13:03:23.942132 UTC] Starting iteration 199
[2018-01-21 13:03:23.942314 UTC] Start collecting samples
[2018-01-21 13:03:28.743586 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:28.859753 UTC] Performing policy update
[2018-01-21 13:03:28.860816 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:28.984900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:30.393421 UTC] Performing line search
[2018-01-21 13:03:30.583367 UTC] Updating baseline
[2018-01-21 13:03:32.477947 UTC] Computing logging information
-------------------------------------
| Iteration            | 199        |
| ExpectedImprovement  | 0.014452   |
| ActualImprovement    | 0.013841   |
| ImprovementRatio     | 0.95772    |
| MeanKL               | 0.0075468  |
| Entropy              | 1.2931     |
| Perplexity           | 3.6441     |
| AveragePolicyStd     | 0.30443    |
| AveragePolicyStd[0]  | 0.28933    |
| AveragePolicyStd[1]  | 0.41458    |
| AveragePolicyStd[2]  | 0.2409     |
| AveragePolicyStd[3]  | 0.30788    |
| AveragePolicyStd[4]  | 0.29813    |
| AveragePolicyStd[5]  | 0.27576    |
| AverageReturn        | 687.71     |
| MinReturn            | 9.4866     |
| MaxReturn            | 826.39     |
| StdReturn            | 212.05     |
| AverageEpisodeLength | 881.51     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.11     |
| TotalNEpisodes       | 17594      |
| TotalNSamples        | 9.9475e+05 |
| ExplainedVariance    | 0.025648   |
-------------------------------------
[2018-01-21 13:03:33.150520 UTC] Saving snapshot
[2018-01-21 13:03:33.150769 UTC] Starting iteration 200
[2018-01-21 13:03:33.150939 UTC] Start collecting samples
[2018-01-21 13:03:38.012429 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:38.215013 UTC] Performing policy update
[2018-01-21 13:03:38.215821 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:38.342606 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:39.773254 UTC] Performing line search
[2018-01-21 13:03:39.959891 UTC] Updating baseline
[2018-01-21 13:03:41.891932 UTC] Computing logging information
-------------------------------------
| Iteration            | 200        |
| ExpectedImprovement  | 0.014532   |
| ActualImprovement    | 0.013988   |
| ImprovementRatio     | 0.96256    |
| MeanKL               | 0.0070141  |
| Entropy              | 1.2922     |
| Perplexity           | 3.6408     |
| AveragePolicyStd     | 0.30435    |
| AveragePolicyStd[0]  | 0.28999    |
| AveragePolicyStd[1]  | 0.41382    |
| AveragePolicyStd[2]  | 0.24105    |
| AveragePolicyStd[3]  | 0.30747    |
| AveragePolicyStd[4]  | 0.29853    |
| AveragePolicyStd[5]  | 0.27522    |
| AverageReturn        | 690.96     |
| MinReturn            | 9.4866     |
| MaxReturn            | 830.58     |
| StdReturn            | 212.39     |
| AverageEpisodeLength | 883.89     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 265.04     |
| TotalNEpisodes       | 17598      |
| TotalNSamples        | 9.9875e+05 |
| ExplainedVariance    | 0.17684    |
-------------------------------------
[2018-01-21 13:03:42.522942 UTC] Saving snapshot
[2018-01-21 13:03:42.532525 UTC] Starting iteration 201
[2018-01-21 13:03:42.532757 UTC] Start collecting samples
[2018-01-21 13:03:47.300036 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:47.428133 UTC] Performing policy update
[2018-01-21 13:03:47.428789 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:47.545475 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:48.978389 UTC] Performing line search
[2018-01-21 13:03:49.177422 UTC] Updating baseline
[2018-01-21 13:03:51.639743 UTC] Computing logging information
-------------------------------------
| Iteration            | 201        |
| ExpectedImprovement  | 0.013643   |
| ActualImprovement    | 0.01291    |
| ImprovementRatio     | 0.94629    |
| MeanKL               | 0.0075264  |
| Entropy              | 1.2788     |
| Perplexity           | 3.5922     |
| AveragePolicyStd     | 0.3039     |
| AveragePolicyStd[0]  | 0.28946    |
| AveragePolicyStd[1]  | 0.41441    |
| AveragePolicyStd[2]  | 0.23685    |
| AveragePolicyStd[3]  | 0.30909    |
| AveragePolicyStd[4]  | 0.29913    |
| AveragePolicyStd[5]  | 0.27448    |
| AverageReturn        | 693.2      |
| MinReturn            | 9.4866     |
| MaxReturn            | 830.58     |
| StdReturn            | 207.23     |
| AverageEpisodeLength | 885.84     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 259.02     |
| TotalNEpisodes       | 17604      |
| TotalNSamples        | 1.0041e+06 |
| ExplainedVariance    | 0.19445    |
-------------------------------------
[2018-01-21 13:03:52.262949 UTC] Saving snapshot
[2018-01-21 13:03:52.263126 UTC] Starting iteration 202
[2018-01-21 13:03:52.263231 UTC] Start collecting samples
[2018-01-21 13:03:57.063775 UTC] Computing input variables for policy optimization
[2018-01-21 13:03:57.207157 UTC] Performing policy update
[2018-01-21 13:03:57.207867 UTC] Computing gradient in Euclidean space
[2018-01-21 13:03:57.326577 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:03:58.730978 UTC] Performing line search
[2018-01-21 13:03:58.917353 UTC] Updating baseline
[2018-01-21 13:04:00.680755 UTC] Computing logging information
-------------------------------------
| Iteration            | 202        |
| ExpectedImprovement  | 0.015566   |
| ActualImprovement    | 0.014633   |
| ImprovementRatio     | 0.94006    |
| MeanKL               | 0.0069007  |
| Entropy              | 1.254      |
| Perplexity           | 3.5045     |
| AveragePolicyStd     | 0.30256    |
| AveragePolicyStd[0]  | 0.28809    |
| AveragePolicyStd[1]  | 0.41165    |
| AveragePolicyStd[2]  | 0.23667    |
| AveragePolicyStd[3]  | 0.30888    |
| AveragePolicyStd[4]  | 0.29547    |
| AveragePolicyStd[5]  | 0.2746     |
| AverageReturn        | 675.2      |
| MinReturn            | 9.4866     |
| MaxReturn            | 830.58     |
| StdReturn            | 229.75     |
| AverageEpisodeLength | 859.91     |
| MinEpisodeLength     | 21         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 287.97     |
| TotalNEpisodes       | 17611      |
| TotalNSamples        | 1.0086e+06 |
| ExplainedVariance    | 0.32741    |
-------------------------------------
[2018-01-21 13:04:01.303026 UTC] Saving snapshot
[2018-01-21 13:04:01.303314 UTC] Starting iteration 203
[2018-01-21 13:04:01.303493 UTC] Start collecting samples
[2018-01-21 13:04:06.274875 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:06.407093 UTC] Performing policy update
[2018-01-21 13:04:06.407757 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:06.527206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:04:07.958975 UTC] Performing line search
[2018-01-21 13:04:08.158541 UTC] Updating baseline
[2018-01-21 13:04:10.240936 UTC] Computing logging information
------------------------------------
| Iteration            | 203       |
| ExpectedImprovement  | 0.014769  |
| ActualImprovement    | 0.013986  |
| ImprovementRatio     | 0.947     |
| MeanKL               | 0.0072879 |
| Entropy              | 1.2446    |
| Perplexity           | 3.4716    |
| AveragePolicyStd     | 0.30203   |
| AveragePolicyStd[0]  | 0.28907   |
| AveragePolicyStd[1]  | 0.41033   |
| AveragePolicyStd[2]  | 0.23711   |
| AveragePolicyStd[3]  | 0.30784   |
| AveragePolicyStd[4]  | 0.29541   |
| AveragePolicyStd[5]  | 0.27245   |
| AverageReturn        | 678.62    |
| MinReturn            | 9.4866    |
| MaxReturn            | 830.58    |
| StdReturn            | 230.13    |
| AverageEpisodeLength | 861.82    |
| MinEpisodeLength     | 21        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 288.16    |
| TotalNEpisodes       | 17617     |
| TotalNSamples        | 1.014e+06 |
| ExplainedVariance    | 0.29333   |
------------------------------------
[2018-01-21 13:04:10.838446 UTC] Saving snapshot
[2018-01-21 13:04:10.838735 UTC] Starting iteration 204
[2018-01-21 13:04:10.838952 UTC] Start collecting samples
[2018-01-21 13:04:15.765864 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:15.917162 UTC] Performing policy update
[2018-01-21 13:04:15.917791 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:16.038353 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:04:17.454632 UTC] Performing line search
[2018-01-21 13:04:17.653294 UTC] Updating baseline
[2018-01-21 13:04:19.539728 UTC] Computing logging information
-------------------------------------
| Iteration            | 204        |
| ExpectedImprovement  | 0.014119   |
| ActualImprovement    | 0.013681   |
| ImprovementRatio     | 0.96893    |
| MeanKL               | 0.0074296  |
| Entropy              | 1.2593     |
| Perplexity           | 3.523      |
| AveragePolicyStd     | 0.30272    |
| AveragePolicyStd[0]  | 0.29063    |
| AveragePolicyStd[1]  | 0.41022    |
| AveragePolicyStd[2]  | 0.23751    |
| AveragePolicyStd[3]  | 0.30796    |
| AveragePolicyStd[4]  | 0.2967     |
| AveragePolicyStd[5]  | 0.27329    |
| AverageReturn        | 686.42     |
| MinReturn            | 13.028     |
| MaxReturn            | 834.03     |
| StdReturn            | 221.61     |
| AverageEpisodeLength | 868.4      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 276.15     |
| TotalNEpisodes       | 17623      |
| TotalNSamples        | 1.0196e+06 |
| ExplainedVariance    | 0.1498     |
-------------------------------------
[2018-01-21 13:04:20.157511 UTC] Saving snapshot
[2018-01-21 13:04:20.157748 UTC] Starting iteration 205
[2018-01-21 13:04:20.157898 UTC] Start collecting samples
[2018-01-21 13:04:25.058369 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:25.180416 UTC] Performing policy update
[2018-01-21 13:04:25.181104 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:25.311466 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:04:26.747596 UTC] Performing line search
[2018-01-21 13:04:26.930874 UTC] Updating baseline
[2018-01-21 13:04:28.735100 UTC] Computing logging information
-------------------------------------
| Iteration            | 205        |
| ExpectedImprovement  | 0.011486   |
| ActualImprovement    | 0.011142   |
| ImprovementRatio     | 0.96998    |
| MeanKL               | 0.0073094  |
| Entropy              | 1.2592     |
| Perplexity           | 3.5225     |
| AveragePolicyStd     | 0.3026     |
| AveragePolicyStd[0]  | 0.28994    |
| AveragePolicyStd[1]  | 0.40867    |
| AveragePolicyStd[2]  | 0.23942    |
| AveragePolicyStd[3]  | 0.30981    |
| AveragePolicyStd[4]  | 0.29579    |
| AveragePolicyStd[5]  | 0.27196    |
| AverageReturn        | 687.37     |
| MinReturn            | 13.028     |
| MaxReturn            | 834.03     |
| StdReturn            | 222.28     |
| AverageEpisodeLength | 866.61     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 275.89     |
| TotalNEpisodes       | 17627      |
| TotalNSamples        | 1.0235e+06 |
| ExplainedVariance    | 0.22582    |
-------------------------------------
[2018-01-21 13:04:29.440787 UTC] Saving snapshot
[2018-01-21 13:04:29.441168 UTC] Starting iteration 206
[2018-01-21 13:04:29.441352 UTC] Start collecting samples
[2018-01-21 13:04:35.053187 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:35.210809 UTC] Performing policy update
[2018-01-21 13:04:35.211850 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:35.339093 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:04:36.876986 UTC] Performing line search
[2018-01-21 13:04:37.120248 UTC] Updating baseline
[2018-01-21 13:04:39.479838 UTC] Computing logging information
-------------------------------------
| Iteration            | 206        |
| ExpectedImprovement  | 0.018256   |
| ActualImprovement    | 0.016823   |
| ImprovementRatio     | 0.92151    |
| MeanKL               | 0.0069858  |
| Entropy              | 1.263      |
| Perplexity           | 3.5361     |
| AveragePolicyStd     | 0.30275    |
| AveragePolicyStd[0]  | 0.28868    |
| AveragePolicyStd[1]  | 0.40676    |
| AveragePolicyStd[2]  | 0.23872    |
| AveragePolicyStd[3]  | 0.31153    |
| AveragePolicyStd[4]  | 0.29855    |
| AveragePolicyStd[5]  | 0.27222    |
| AverageReturn        | 688.09     |
| MinReturn            | 13.028     |
| MaxReturn            | 834.03     |
| StdReturn            | 227.03     |
| AverageEpisodeLength | 864.61     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 281.76     |
| TotalNEpisodes       | 17634      |
| TotalNSamples        | 1.0291e+06 |
| ExplainedVariance    | 0.19821    |
-------------------------------------
[2018-01-21 13:04:40.296997 UTC] Saving snapshot
[2018-01-21 13:04:40.297249 UTC] Starting iteration 207
[2018-01-21 13:04:40.297432 UTC] Start collecting samples
[2018-01-21 13:04:45.790415 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:45.925493 UTC] Performing policy update
[2018-01-21 13:04:45.926183 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:46.047566 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:04:47.625097 UTC] Performing line search
[2018-01-21 13:04:47.851851 UTC] Updating baseline
[2018-01-21 13:04:50.024381 UTC] Computing logging information
-------------------------------------
| Iteration            | 207        |
| ExpectedImprovement  | 0.015259   |
| ActualImprovement    | 0.014493   |
| ImprovementRatio     | 0.94979    |
| MeanKL               | 0.0071173  |
| Entropy              | 1.2352     |
| Perplexity           | 3.439      |
| AveragePolicyStd     | 0.30136    |
| AveragePolicyStd[0]  | 0.28617    |
| AveragePolicyStd[1]  | 0.40518    |
| AveragePolicyStd[2]  | 0.23707    |
| AveragePolicyStd[3]  | 0.30911    |
| AveragePolicyStd[4]  | 0.29859    |
| AveragePolicyStd[5]  | 0.27206    |
| AverageReturn        | 706.08     |
| MinReturn            | 13.718     |
| MaxReturn            | 834.03     |
| StdReturn            | 216.21     |
| AverageEpisodeLength | 884.22     |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 268.29     |
| TotalNEpisodes       | 17641      |
| TotalNSamples        | 1.0352e+06 |
| ExplainedVariance    | 0.13817    |
-------------------------------------
[2018-01-21 13:04:50.755465 UTC] Saving snapshot
[2018-01-21 13:04:50.755722 UTC] Starting iteration 208
[2018-01-21 13:04:50.755907 UTC] Start collecting samples
[2018-01-21 13:04:58.431093 UTC] Computing input variables for policy optimization
[2018-01-21 13:04:58.643876 UTC] Performing policy update
[2018-01-21 13:04:58.645087 UTC] Computing gradient in Euclidean space
[2018-01-21 13:04:58.806641 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:00.727047 UTC] Performing line search
[2018-01-21 13:05:00.969308 UTC] Updating baseline
[2018-01-21 13:05:03.435471 UTC] Computing logging information
-------------------------------------
| Iteration            | 208        |
| ExpectedImprovement  | 0.012712   |
| ActualImprovement    | 0.012183   |
| ImprovementRatio     | 0.95842    |
| MeanKL               | 0.0070763  |
| Entropy              | 1.2238     |
| Perplexity           | 3.4        |
| AveragePolicyStd     | 0.30099    |
| AveragePolicyStd[0]  | 0.28672    |
| AveragePolicyStd[1]  | 0.40841    |
| AveragePolicyStd[2]  | 0.23602    |
| AveragePolicyStd[3]  | 0.30918    |
| AveragePolicyStd[4]  | 0.29364    |
| AveragePolicyStd[5]  | 0.27198    |
| AverageReturn        | 717.9      |
| MinReturn            | 99.922     |
| MaxReturn            | 834.03     |
| StdReturn            | 195.54     |
| AverageEpisodeLength | 898.37     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 243.01     |
| TotalNEpisodes       | 17644      |
| TotalNSamples        | 1.0377e+06 |
| ExplainedVariance    | 0.29313    |
-------------------------------------
[2018-01-21 13:05:04.242467 UTC] Saving snapshot
[2018-01-21 13:05:04.242801 UTC] Starting iteration 209
[2018-01-21 13:05:04.243067 UTC] Start collecting samples
[2018-01-21 13:05:12.027291 UTC] Computing input variables for policy optimization
[2018-01-21 13:05:12.197640 UTC] Performing policy update
[2018-01-21 13:05:12.198476 UTC] Computing gradient in Euclidean space
[2018-01-21 13:05:12.319358 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:13.975662 UTC] Performing line search
[2018-01-21 13:05:14.174623 UTC] Updating baseline
[2018-01-21 13:05:16.680791 UTC] Computing logging information
-------------------------------------
| Iteration            | 209        |
| ExpectedImprovement  | 0.021891   |
| ActualImprovement    | 0.019817   |
| ImprovementRatio     | 0.90525    |
| MeanKL               | 0.0067202  |
| Entropy              | 1.2252     |
| Perplexity           | 3.4047     |
| AveragePolicyStd     | 0.30106    |
| AveragePolicyStd[0]  | 0.28473    |
| AveragePolicyStd[1]  | 0.40762    |
| AveragePolicyStd[2]  | 0.23609    |
| AveragePolicyStd[3]  | 0.31194    |
| AveragePolicyStd[4]  | 0.29471    |
| AveragePolicyStd[5]  | 0.27128    |
| AverageReturn        | 727.38     |
| MinReturn            | 96.932     |
| MaxReturn            | 834.03     |
| StdReturn            | 191.56     |
| AverageEpisodeLength | 907.69     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.83     |
| TotalNEpisodes       | 17651      |
| TotalNSamples        | 1.0438e+06 |
| ExplainedVariance    | 0.055159   |
-------------------------------------
[2018-01-21 13:05:17.428177 UTC] Saving snapshot
[2018-01-21 13:05:17.428387 UTC] Starting iteration 210
[2018-01-21 13:05:17.428562 UTC] Start collecting samples
[2018-01-21 13:05:22.864731 UTC] Computing input variables for policy optimization
[2018-01-21 13:05:23.015285 UTC] Performing policy update
[2018-01-21 13:05:23.015926 UTC] Computing gradient in Euclidean space
[2018-01-21 13:05:23.135535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:24.732047 UTC] Performing line search
[2018-01-21 13:05:24.953926 UTC] Updating baseline
[2018-01-21 13:05:26.969360 UTC] Computing logging information
-------------------------------------
| Iteration            | 210        |
| ExpectedImprovement  | 0.015816   |
| ActualImprovement    | 0.013555   |
| ImprovementRatio     | 0.85701    |
| MeanKL               | 0.0069254  |
| Entropy              | 1.213      |
| Perplexity           | 3.3635     |
| AveragePolicyStd     | 0.30034    |
| AveragePolicyStd[0]  | 0.28343    |
| AveragePolicyStd[1]  | 0.40624    |
| AveragePolicyStd[2]  | 0.2371     |
| AveragePolicyStd[3]  | 0.30956    |
| AveragePolicyStd[4]  | 0.29348    |
| AveragePolicyStd[5]  | 0.27221    |
| AverageReturn        | 729.04     |
| MinReturn            | 96.932     |
| MaxReturn            | 838.42     |
| StdReturn            | 192.18     |
| AverageEpisodeLength | 907.69     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.83     |
| TotalNEpisodes       | 17656      |
| TotalNSamples        | 1.0488e+06 |
| ExplainedVariance    | 0.01045    |
-------------------------------------
[2018-01-21 13:05:27.618977 UTC] Saving snapshot
[2018-01-21 13:05:27.626335 UTC] Starting iteration 211
[2018-01-21 13:05:27.626563 UTC] Start collecting samples
[2018-01-21 13:05:33.013786 UTC] Computing input variables for policy optimization
[2018-01-21 13:05:33.179100 UTC] Performing policy update
[2018-01-21 13:05:33.180288 UTC] Computing gradient in Euclidean space
[2018-01-21 13:05:33.302815 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:34.913616 UTC] Performing line search
[2018-01-21 13:05:35.113812 UTC] Updating baseline
[2018-01-21 13:05:37.276477 UTC] Computing logging information
-------------------------------------
| Iteration            | 211        |
| ExpectedImprovement  | 0.015719   |
| ActualImprovement    | 0.014746   |
| ImprovementRatio     | 0.9381     |
| MeanKL               | 0.0072662  |
| Entropy              | 1.1937     |
| Perplexity           | 3.2992     |
| AveragePolicyStd     | 0.29928    |
| AveragePolicyStd[0]  | 0.27978    |
| AveragePolicyStd[1]  | 0.40576    |
| AveragePolicyStd[2]  | 0.23993    |
| AveragePolicyStd[3]  | 0.30592    |
| AveragePolicyStd[4]  | 0.29398    |
| AveragePolicyStd[5]  | 0.27034    |
| AverageReturn        | 723.11     |
| MinReturn            | 96.932     |
| MaxReturn            | 841.69     |
| StdReturn            | 199.21     |
| AverageEpisodeLength | 899.55     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.09     |
| TotalNEpisodes       | 17662      |
| TotalNSamples        | 1.0538e+06 |
| ExplainedVariance    | 0.38158    |
-------------------------------------
[2018-01-21 13:05:37.926880 UTC] Saving snapshot
[2018-01-21 13:05:37.927148 UTC] Starting iteration 212
[2018-01-21 13:05:37.927386 UTC] Start collecting samples
[2018-01-21 13:05:43.087426 UTC] Computing input variables for policy optimization
[2018-01-21 13:05:43.211072 UTC] Performing policy update
[2018-01-21 13:05:43.211694 UTC] Computing gradient in Euclidean space
[2018-01-21 13:05:43.353404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:44.775352 UTC] Performing line search
[2018-01-21 13:05:44.962331 UTC] Updating baseline
[2018-01-21 13:05:47.238055 UTC] Computing logging information
-------------------------------------
| Iteration            | 212        |
| ExpectedImprovement  | 0.014885   |
| ActualImprovement    | 0.014383   |
| ImprovementRatio     | 0.96629    |
| MeanKL               | 0.0073576  |
| Entropy              | 1.1816     |
| Perplexity           | 3.2597     |
| AveragePolicyStd     | 0.29877    |
| AveragePolicyStd[0]  | 0.28099    |
| AveragePolicyStd[1]  | 0.40681    |
| AveragePolicyStd[2]  | 0.23871    |
| AveragePolicyStd[3]  | 0.3047     |
| AveragePolicyStd[4]  | 0.29095    |
| AveragePolicyStd[5]  | 0.27047    |
| AverageReturn        | 724.48     |
| MinReturn            | 96.932     |
| MaxReturn            | 841.69     |
| StdReturn            | 199.76     |
| AverageEpisodeLength | 899.55     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.09     |
| TotalNEpisodes       | 17667      |
| TotalNSamples        | 1.0588e+06 |
| ExplainedVariance    | 0.045652   |
-------------------------------------
[2018-01-21 13:05:48.057195 UTC] Saving snapshot
[2018-01-21 13:05:48.057550 UTC] Starting iteration 213
[2018-01-21 13:05:48.057958 UTC] Start collecting samples
[2018-01-21 13:05:54.091430 UTC] Computing input variables for policy optimization
[2018-01-21 13:05:54.214402 UTC] Performing policy update
[2018-01-21 13:05:54.215323 UTC] Computing gradient in Euclidean space
[2018-01-21 13:05:54.340601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:05:55.970600 UTC] Performing line search
[2018-01-21 13:05:56.177732 UTC] Updating baseline
[2018-01-21 13:05:58.105255 UTC] Computing logging information
-------------------------------------
| Iteration            | 213        |
| ExpectedImprovement  | 0.014375   |
| ActualImprovement    | 0.013695   |
| ImprovementRatio     | 0.95268    |
| MeanKL               | 0.0073493  |
| Entropy              | 1.1785     |
| Perplexity           | 3.2494     |
| AveragePolicyStd     | 0.29861    |
| AveragePolicyStd[0]  | 0.28028    |
| AveragePolicyStd[1]  | 0.40644    |
| AveragePolicyStd[2]  | 0.23777    |
| AveragePolicyStd[3]  | 0.3031     |
| AveragePolicyStd[4]  | 0.29187    |
| AveragePolicyStd[5]  | 0.27217    |
| AverageReturn        | 725.41     |
| MinReturn            | 96.932     |
| MaxReturn            | 841.69     |
| StdReturn            | 200.14     |
| AverageEpisodeLength | 899.55     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 246.09     |
| TotalNEpisodes       | 17671      |
| TotalNSamples        | 1.0628e+06 |
| ExplainedVariance    | 0.082505   |
-------------------------------------
[2018-01-21 13:05:58.788391 UTC] Saving snapshot
[2018-01-21 13:05:58.788636 UTC] Starting iteration 214
[2018-01-21 13:05:58.788782 UTC] Start collecting samples
[2018-01-21 13:06:04.043385 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:04.184463 UTC] Performing policy update
[2018-01-21 13:06:04.185074 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:04.416976 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:05.921099 UTC] Performing line search
[2018-01-21 13:06:06.111022 UTC] Updating baseline
[2018-01-21 13:06:08.020872 UTC] Computing logging information
-------------------------------------
| Iteration            | 214        |
| ExpectedImprovement  | 0.014132   |
| ActualImprovement    | 0.013359   |
| ImprovementRatio     | 0.94529    |
| MeanKL               | 0.0072155  |
| Entropy              | 1.1715     |
| Perplexity           | 3.227      |
| AveragePolicyStd     | 0.29827    |
| AveragePolicyStd[0]  | 0.28017    |
| AveragePolicyStd[1]  | 0.40515    |
| AveragePolicyStd[2]  | 0.23725    |
| AveragePolicyStd[3]  | 0.30597    |
| AveragePolicyStd[4]  | 0.29082    |
| AveragePolicyStd[5]  | 0.27029    |
| AverageReturn        | 733.1      |
| MinReturn            | 96.932     |
| MaxReturn            | 841.69     |
| StdReturn            | 192.54     |
| AverageEpisodeLength | 907.02     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.54     |
| TotalNEpisodes       | 17677      |
| TotalNSamples        | 1.0688e+06 |
| ExplainedVariance    | -0.03694   |
-------------------------------------
[2018-01-21 13:06:08.715722 UTC] Saving snapshot
[2018-01-21 13:06:08.715990 UTC] Starting iteration 215
[2018-01-21 13:06:08.716155 UTC] Start collecting samples
[2018-01-21 13:06:14.052940 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:14.192570 UTC] Performing policy update
[2018-01-21 13:06:14.193252 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:14.309147 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:15.774740 UTC] Performing line search
[2018-01-21 13:06:15.987654 UTC] Updating baseline
[2018-01-21 13:06:17.989570 UTC] Computing logging information
-------------------------------------
| Iteration            | 215        |
| ExpectedImprovement  | 0.013426   |
| ActualImprovement    | 0.013077   |
| ImprovementRatio     | 0.97401    |
| MeanKL               | 0.0072708  |
| Entropy              | 1.154      |
| Perplexity           | 3.171      |
| AveragePolicyStd     | 0.2975     |
| AveragePolicyStd[0]  | 0.28078    |
| AveragePolicyStd[1]  | 0.40496    |
| AveragePolicyStd[2]  | 0.23486    |
| AveragePolicyStd[3]  | 0.30302    |
| AveragePolicyStd[4]  | 0.29203    |
| AveragePolicyStd[5]  | 0.26933    |
| AverageReturn        | 735.39     |
| MinReturn            | 96.932     |
| MaxReturn            | 846.03     |
| StdReturn            | 193.46     |
| AverageEpisodeLength | 907.02     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.54     |
| TotalNEpisodes       | 17683      |
| TotalNSamples        | 1.0748e+06 |
| ExplainedVariance    | 0.075976   |
-------------------------------------
[2018-01-21 13:06:18.665666 UTC] Saving snapshot
[2018-01-21 13:06:18.666180 UTC] Starting iteration 216
[2018-01-21 13:06:18.666581 UTC] Start collecting samples
[2018-01-21 13:06:23.994475 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:24.140109 UTC] Performing policy update
[2018-01-21 13:06:24.141009 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:24.270673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:25.737887 UTC] Performing line search
[2018-01-21 13:06:25.927216 UTC] Updating baseline
[2018-01-21 13:06:28.066325 UTC] Computing logging information
-------------------------------------
| Iteration            | 216        |
| ExpectedImprovement  | 0.017766   |
| ActualImprovement    | 0.015584   |
| ImprovementRatio     | 0.87716    |
| MeanKL               | 0.0067292  |
| Entropy              | 1.171      |
| Perplexity           | 3.2253     |
| AveragePolicyStd     | 0.2985     |
| AveragePolicyStd[0]  | 0.28233    |
| AveragePolicyStd[1]  | 0.40808    |
| AveragePolicyStd[2]  | 0.23295    |
| AveragePolicyStd[3]  | 0.30254    |
| AveragePolicyStd[4]  | 0.29304    |
| AveragePolicyStd[5]  | 0.27206    |
| AverageReturn        | 732.95     |
| MinReturn            | 96.932     |
| MaxReturn            | 853.61     |
| StdReturn            | 200.13     |
| AverageEpisodeLength | 903.08     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 245.61     |
| TotalNEpisodes       | 17688      |
| TotalNSamples        | 1.0791e+06 |
| ExplainedVariance    | 0.17759    |
-------------------------------------
[2018-01-21 13:06:28.766411 UTC] Saving snapshot
[2018-01-21 13:06:28.766699 UTC] Starting iteration 217
[2018-01-21 13:06:28.766884 UTC] Start collecting samples
[2018-01-21 13:06:33.870277 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:34.004709 UTC] Performing policy update
[2018-01-21 13:06:34.006081 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:34.132409 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:35.728686 UTC] Performing line search
[2018-01-21 13:06:35.939878 UTC] Updating baseline
[2018-01-21 13:06:38.112730 UTC] Computing logging information
--------------------------------------
| Iteration            | 217         |
| ExpectedImprovement  | 0.014589    |
| ActualImprovement    | 0.013747    |
| ImprovementRatio     | 0.94225     |
| MeanKL               | 0.0072902   |
| Entropy              | 1.1612      |
| Perplexity           | 3.1937      |
| AveragePolicyStd     | 0.29798     |
| AveragePolicyStd[0]  | 0.2814      |
| AveragePolicyStd[1]  | 0.40782     |
| AveragePolicyStd[2]  | 0.23381     |
| AveragePolicyStd[3]  | 0.3024      |
| AveragePolicyStd[4]  | 0.29055     |
| AveragePolicyStd[5]  | 0.2719      |
| AverageReturn        | 734.41      |
| MinReturn            | 96.932      |
| MaxReturn            | 853.61      |
| StdReturn            | 200.74      |
| AverageEpisodeLength | 903.08      |
| MinEpisodeLength     | 112         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 245.61      |
| TotalNEpisodes       | 17693       |
| TotalNSamples        | 1.0841e+06  |
| ExplainedVariance    | -0.00068404 |
--------------------------------------
[2018-01-21 13:06:38.855525 UTC] Saving snapshot
[2018-01-21 13:06:38.855746 UTC] Starting iteration 218
[2018-01-21 13:06:38.855930 UTC] Start collecting samples
[2018-01-21 13:06:43.788808 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:43.935927 UTC] Performing policy update
[2018-01-21 13:06:43.936553 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:44.074806 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:45.486494 UTC] Performing line search
[2018-01-21 13:06:45.693747 UTC] Updating baseline
[2018-01-21 13:06:47.717267 UTC] Computing logging information
-------------------------------------
| Iteration            | 218        |
| ExpectedImprovement  | 0.01529    |
| ActualImprovement    | 0.013909   |
| ImprovementRatio     | 0.90973    |
| MeanKL               | 0.0071766  |
| Entropy              | 1.1477     |
| Perplexity           | 3.1509     |
| AveragePolicyStd     | 0.29726    |
| AveragePolicyStd[0]  | 0.28285    |
| AveragePolicyStd[1]  | 0.40519    |
| AveragePolicyStd[2]  | 0.23297    |
| AveragePolicyStd[3]  | 0.30432    |
| AveragePolicyStd[4]  | 0.28749    |
| AveragePolicyStd[5]  | 0.27074    |
| AverageReturn        | 735.41     |
| MinReturn            | 96.932     |
| MaxReturn            | 853.61     |
| StdReturn            | 201.11     |
| AverageEpisodeLength | 903.15     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 245.64     |
| TotalNEpisodes       | 17697      |
| TotalNSamples        | 1.0881e+06 |
| ExplainedVariance    | -0.021169  |
-------------------------------------
[2018-01-21 13:06:48.359636 UTC] Saving snapshot
[2018-01-21 13:06:48.359910 UTC] Starting iteration 219
[2018-01-21 13:06:48.360079 UTC] Start collecting samples
[2018-01-21 13:06:53.560566 UTC] Computing input variables for policy optimization
[2018-01-21 13:06:53.683936 UTC] Performing policy update
[2018-01-21 13:06:53.684549 UTC] Computing gradient in Euclidean space
[2018-01-21 13:06:53.803860 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:06:55.244967 UTC] Performing line search
[2018-01-21 13:06:55.469057 UTC] Updating baseline
[2018-01-21 13:06:57.203683 UTC] Computing logging information
-------------------------------------
| Iteration            | 219        |
| ExpectedImprovement  | 0.014792   |
| ActualImprovement    | 0.013777   |
| ImprovementRatio     | 0.93141    |
| MeanKL               | 0.0072374  |
| Entropy              | 1.1405     |
| Perplexity           | 3.1283     |
| AveragePolicyStd     | 0.297      |
| AveragePolicyStd[0]  | 0.28341    |
| AveragePolicyStd[1]  | 0.40367    |
| AveragePolicyStd[2]  | 0.22918    |
| AveragePolicyStd[3]  | 0.30632    |
| AveragePolicyStd[4]  | 0.28711    |
| AveragePolicyStd[5]  | 0.2723     |
| AverageReturn        | 738.67     |
| MinReturn            | 96.932     |
| MaxReturn            | 858.6      |
| StdReturn            | 202.2      |
| AverageEpisodeLength | 903.98     |
| MinEpisodeLength     | 112        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 245.82     |
| TotalNEpisodes       | 17703      |
| TotalNSamples        | 1.0941e+06 |
| ExplainedVariance    | 0.018399   |
-------------------------------------
[2018-01-21 13:06:57.863852 UTC] Saving snapshot
[2018-01-21 13:06:57.864087 UTC] Starting iteration 220
[2018-01-21 13:06:57.864264 UTC] Start collecting samples
[2018-01-21 13:07:02.784035 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:02.933758 UTC] Performing policy update
[2018-01-21 13:07:02.934364 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:03.087502 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:04.505220 UTC] Performing line search
[2018-01-21 13:07:04.708548 UTC] Updating baseline
[2018-01-21 13:07:06.809593 UTC] Computing logging information
-------------------------------------
| Iteration            | 220        |
| ExpectedImprovement  | 0.013955   |
| ActualImprovement    | 0.01296    |
| ImprovementRatio     | 0.92869    |
| MeanKL               | 0.0073869  |
| Entropy              | 1.1287     |
| Perplexity           | 3.0917     |
| AveragePolicyStd     | 0.29633    |
| AveragePolicyStd[0]  | 0.28378    |
| AveragePolicyStd[1]  | 0.40168    |
| AveragePolicyStd[2]  | 0.22896    |
| AveragePolicyStd[3]  | 0.30478    |
| AveragePolicyStd[4]  | 0.28631    |
| AveragePolicyStd[5]  | 0.27248    |
| AverageReturn        | 756.51     |
| MinReturn            | 96.932     |
| MaxReturn            | 858.6      |
| StdReturn            | 177.91     |
| AverageEpisodeLength | 924.43     |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.24     |
| TotalNEpisodes       | 17709      |
| TotalNSamples        | 1.0998e+06 |
| ExplainedVariance    | 0.069244   |
-------------------------------------
[2018-01-21 13:07:07.414139 UTC] Saving snapshot
[2018-01-21 13:07:07.420655 UTC] Starting iteration 221
[2018-01-21 13:07:07.420843 UTC] Start collecting samples
[2018-01-21 13:07:12.403829 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:12.531830 UTC] Performing policy update
[2018-01-21 13:07:12.532542 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:12.657678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:14.145742 UTC] Performing line search
[2018-01-21 13:07:14.384800 UTC] Updating baseline
[2018-01-21 13:07:16.325280 UTC] Computing logging information
-------------------------------------
| Iteration            | 221        |
| ExpectedImprovement  | 0.013712   |
| ActualImprovement    | 0.013163   |
| ImprovementRatio     | 0.95993    |
| MeanKL               | 0.0072219  |
| Entropy              | 1.1358     |
| Perplexity           | 3.1137     |
| AveragePolicyStd     | 0.29667    |
| AveragePolicyStd[0]  | 0.28457    |
| AveragePolicyStd[1]  | 0.40173    |
| AveragePolicyStd[2]  | 0.229      |
| AveragePolicyStd[3]  | 0.30613    |
| AveragePolicyStd[4]  | 0.28498    |
| AveragePolicyStd[5]  | 0.27363    |
| AverageReturn        | 764.35     |
| MinReturn            | 96.932     |
| MaxReturn            | 858.6      |
| StdReturn            | 167.15     |
| AverageEpisodeLength | 932.79     |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.33     |
| TotalNEpisodes       | 17712      |
| TotalNSamples        | 1.1028e+06 |
| ExplainedVariance    | 0.032133   |
-------------------------------------
[2018-01-21 13:07:16.996411 UTC] Saving snapshot
[2018-01-21 13:07:16.996667 UTC] Starting iteration 222
[2018-01-21 13:07:16.996848 UTC] Start collecting samples
[2018-01-21 13:07:22.267090 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:22.410327 UTC] Performing policy update
[2018-01-21 13:07:22.411317 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:22.537704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:23.976840 UTC] Performing line search
[2018-01-21 13:07:24.166661 UTC] Updating baseline
[2018-01-21 13:07:26.338637 UTC] Computing logging information
-------------------------------------
| Iteration            | 222        |
| ExpectedImprovement  | 0.015156   |
| ActualImprovement    | 0.014585   |
| ImprovementRatio     | 0.96231    |
| MeanKL               | 0.0071889  |
| Entropy              | 1.1362     |
| Perplexity           | 3.1149     |
| AveragePolicyStd     | 0.29678    |
| AveragePolicyStd[0]  | 0.28333    |
| AveragePolicyStd[1]  | 0.40206    |
| AveragePolicyStd[2]  | 0.22763    |
| AveragePolicyStd[3]  | 0.30768    |
| AveragePolicyStd[4]  | 0.28536    |
| AveragePolicyStd[5]  | 0.2746     |
| AverageReturn        | 768.63     |
| MinReturn            | 96.932     |
| MaxReturn            | 862.63     |
| StdReturn            | 166.39     |
| AverageEpisodeLength | 936.46     |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.67     |
| TotalNEpisodes       | 17720      |
| TotalNSamples        | 1.1103e+06 |
| ExplainedVariance    | -0.026154  |
-------------------------------------
[2018-01-21 13:07:26.982027 UTC] Saving snapshot
[2018-01-21 13:07:26.982399 UTC] Starting iteration 223
[2018-01-21 13:07:26.982577 UTC] Start collecting samples
[2018-01-21 13:07:32.009367 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:32.165822 UTC] Performing policy update
[2018-01-21 13:07:32.166949 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:32.288467 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:33.728490 UTC] Performing line search
[2018-01-21 13:07:33.926633 UTC] Updating baseline
[2018-01-21 13:07:35.939815 UTC] Computing logging information
-------------------------------------
| Iteration            | 223        |
| ExpectedImprovement  | 0.014293   |
| ActualImprovement    | 0.013802   |
| ImprovementRatio     | 0.96563    |
| MeanKL               | 0.0073773  |
| Entropy              | 1.1206     |
| Perplexity           | 3.0665     |
| AveragePolicyStd     | 0.2959     |
| AveragePolicyStd[0]  | 0.28285    |
| AveragePolicyStd[1]  | 0.39861    |
| AveragePolicyStd[2]  | 0.22683    |
| AveragePolicyStd[3]  | 0.30726    |
| AveragePolicyStd[4]  | 0.28573    |
| AveragePolicyStd[5]  | 0.27412    |
| AverageReturn        | 764.31     |
| MinReturn            | 96.932     |
| MaxReturn            | 862.63     |
| StdReturn            | 177.53     |
| AverageEpisodeLength | 930.74     |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.91     |
| TotalNEpisodes       | 17726      |
| TotalNSamples        | 1.1155e+06 |
| ExplainedVariance    | 0.01827    |
-------------------------------------
[2018-01-21 13:07:36.623228 UTC] Saving snapshot
[2018-01-21 13:07:36.623453 UTC] Starting iteration 224
[2018-01-21 13:07:36.623638 UTC] Start collecting samples
[2018-01-21 13:07:41.618148 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:41.751432 UTC] Performing policy update
[2018-01-21 13:07:41.752437 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:41.875420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:43.343598 UTC] Performing line search
[2018-01-21 13:07:43.527018 UTC] Updating baseline
[2018-01-21 13:07:45.239657 UTC] Computing logging information
-------------------------------------
| Iteration            | 224        |
| ExpectedImprovement  | 0.01784    |
| ActualImprovement    | 0.016314   |
| ImprovementRatio     | 0.91444    |
| MeanKL               | 0.0071038  |
| Entropy              | 1.1249     |
| Perplexity           | 3.0799     |
| AveragePolicyStd     | 0.29622    |
| AveragePolicyStd[0]  | 0.28212    |
| AveragePolicyStd[1]  | 0.4007     |
| AveragePolicyStd[2]  | 0.22721    |
| AveragePolicyStd[3]  | 0.30752    |
| AveragePolicyStd[4]  | 0.28801    |
| AveragePolicyStd[5]  | 0.27175    |
| AverageReturn        | 765.22     |
| MinReturn            | 96.932     |
| MaxReturn            | 862.63     |
| StdReturn            | 177.81     |
| AverageEpisodeLength | 930.74     |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.91     |
| TotalNEpisodes       | 17728      |
| TotalNSamples        | 1.1175e+06 |
| ExplainedVariance    | 0.015298   |
-------------------------------------
[2018-01-21 13:07:45.962827 UTC] Saving snapshot
[2018-01-21 13:07:45.963104 UTC] Starting iteration 225
[2018-01-21 13:07:45.963327 UTC] Start collecting samples
[2018-01-21 13:07:50.911735 UTC] Computing input variables for policy optimization
[2018-01-21 13:07:51.074326 UTC] Performing policy update
[2018-01-21 13:07:51.075194 UTC] Computing gradient in Euclidean space
[2018-01-21 13:07:51.203515 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:07:52.629121 UTC] Performing line search
[2018-01-21 13:07:52.816451 UTC] Updating baseline
[2018-01-21 13:07:55.354144 UTC] Computing logging information
-------------------------------------
| Iteration            | 225        |
| ExpectedImprovement  | 0.013611   |
| ActualImprovement    | 0.013279   |
| ImprovementRatio     | 0.97564    |
| MeanKL               | 0.0072247  |
| Entropy              | 1.1165     |
| Perplexity           | 3.0541     |
| AveragePolicyStd     | 0.29587    |
| AveragePolicyStd[0]  | 0.28129    |
| AveragePolicyStd[1]  | 0.40094    |
| AveragePolicyStd[2]  | 0.22649    |
| AveragePolicyStd[3]  | 0.3077     |
| AveragePolicyStd[4]  | 0.28829    |
| AveragePolicyStd[5]  | 0.27053    |
| AverageReturn        | 778.29     |
| MinReturn            | 96.932     |
| MaxReturn            | 871.53     |
| StdReturn            | 161.75     |
| AverageEpisodeLength | 944.6      |
| MinEpisodeLength     | 120        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.67     |
| TotalNEpisodes       | 17733      |
| TotalNSamples        | 1.1225e+06 |
| ExplainedVariance    | 0.039479   |
-------------------------------------
[2018-01-21 13:07:55.965348 UTC] Saving snapshot
[2018-01-21 13:07:55.965661 UTC] Starting iteration 226
[2018-01-21 13:07:55.965884 UTC] Start collecting samples
[2018-01-21 13:08:01.011436 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:01.162813 UTC] Performing policy update
[2018-01-21 13:08:01.163889 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:01.286942 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:02.696362 UTC] Performing line search
[2018-01-21 13:08:02.881696 UTC] Updating baseline
[2018-01-21 13:08:05.019221 UTC] Computing logging information
-------------------------------------
| Iteration            | 226        |
| ExpectedImprovement  | 0.018049   |
| ActualImprovement    | 0.016244   |
| ImprovementRatio     | 0.89999    |
| MeanKL               | 0.0069036  |
| Entropy              | 1.1123     |
| Perplexity           | 3.0413     |
| AveragePolicyStd     | 0.29563    |
| AveragePolicyStd[0]  | 0.27973    |
| AveragePolicyStd[1]  | 0.39996    |
| AveragePolicyStd[2]  | 0.22604    |
| AveragePolicyStd[3]  | 0.30727    |
| AveragePolicyStd[4]  | 0.28879    |
| AveragePolicyStd[5]  | 0.27201    |
| AverageReturn        | 777.51     |
| MinReturn            | 10.339     |
| MaxReturn            | 871.53     |
| StdReturn            | 174.45     |
| AverageEpisodeLength | 940.39     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.29     |
| TotalNEpisodes       | 17743      |
| TotalNSamples        | 1.1307e+06 |
| ExplainedVariance    | 0.26035    |
-------------------------------------
[2018-01-21 13:08:05.663416 UTC] Saving snapshot
[2018-01-21 13:08:05.663613 UTC] Starting iteration 227
[2018-01-21 13:08:05.663783 UTC] Start collecting samples
[2018-01-21 13:08:10.511045 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:10.652182 UTC] Performing policy update
[2018-01-21 13:08:10.653072 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:10.786738 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:12.248329 UTC] Performing line search
[2018-01-21 13:08:12.448861 UTC] Updating baseline
[2018-01-21 13:08:14.574579 UTC] Computing logging information
-------------------------------------
| Iteration            | 227        |
| ExpectedImprovement  | 0.01476    |
| ActualImprovement    | 0.014672   |
| ImprovementRatio     | 0.99403    |
| MeanKL               | 0.0073648  |
| Entropy              | 1.1107     |
| Perplexity           | 3.0364     |
| AveragePolicyStd     | 0.29553    |
| AveragePolicyStd[0]  | 0.2795     |
| AveragePolicyStd[1]  | 0.39928    |
| AveragePolicyStd[2]  | 0.2258     |
| AveragePolicyStd[3]  | 0.3064     |
| AveragePolicyStd[4]  | 0.29027    |
| AveragePolicyStd[5]  | 0.27192    |
| AverageReturn        | 771.78     |
| MinReturn            | 10.339     |
| MaxReturn            | 871.53     |
| StdReturn            | 179.42     |
| AverageEpisodeLength | 932.46     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.91     |
| TotalNEpisodes       | 17749      |
| TotalNSamples        | 1.1351e+06 |
| ExplainedVariance    | 0.22994    |
-------------------------------------
[2018-01-21 13:08:15.260604 UTC] Saving snapshot
[2018-01-21 13:08:15.260962 UTC] Starting iteration 228
[2018-01-21 13:08:15.261239 UTC] Start collecting samples
[2018-01-21 13:08:20.302125 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:20.427850 UTC] Performing policy update
[2018-01-21 13:08:20.428774 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:20.557622 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:21.932142 UTC] Performing line search
[2018-01-21 13:08:22.119547 UTC] Updating baseline
[2018-01-21 13:08:24.316192 UTC] Computing logging information
-------------------------------------
| Iteration            | 228        |
| ExpectedImprovement  | 0.017126   |
| ActualImprovement    | 0.015219   |
| ImprovementRatio     | 0.88867    |
| MeanKL               | 0.0072232  |
| Entropy              | 1.1101     |
| Perplexity           | 3.0347     |
| AveragePolicyStd     | 0.29554    |
| AveragePolicyStd[0]  | 0.28049    |
| AveragePolicyStd[1]  | 0.40011    |
| AveragePolicyStd[2]  | 0.22607    |
| AveragePolicyStd[3]  | 0.30584    |
| AveragePolicyStd[4]  | 0.29056    |
| AveragePolicyStd[5]  | 0.27015    |
| AverageReturn        | 772.8      |
| MinReturn            | 10.339     |
| MaxReturn            | 871.53     |
| StdReturn            | 179.81     |
| AverageEpisodeLength | 932.46     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.91     |
| TotalNEpisodes       | 17752      |
| TotalNSamples        | 1.1381e+06 |
| ExplainedVariance    | -0.082266  |
-------------------------------------
[2018-01-21 13:08:24.957488 UTC] Saving snapshot
[2018-01-21 13:08:24.957749 UTC] Starting iteration 229
[2018-01-21 13:08:24.957930 UTC] Start collecting samples
[2018-01-21 13:08:29.969522 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:30.144884 UTC] Performing policy update
[2018-01-21 13:08:30.145611 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:30.266922 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:31.784007 UTC] Performing line search
[2018-01-21 13:08:31.975551 UTC] Updating baseline
[2018-01-21 13:08:34.273019 UTC] Computing logging information
-------------------------------------
| Iteration            | 229        |
| ExpectedImprovement  | 0.012349   |
| ActualImprovement    | 0.012015   |
| ImprovementRatio     | 0.97291    |
| MeanKL               | 0.0073877  |
| Entropy              | 1.0963     |
| Perplexity           | 2.9931     |
| AveragePolicyStd     | 0.29492    |
| AveragePolicyStd[0]  | 0.28108    |
| AveragePolicyStd[1]  | 0.39956    |
| AveragePolicyStd[2]  | 0.22441    |
| AveragePolicyStd[3]  | 0.30531    |
| AveragePolicyStd[4]  | 0.28915    |
| AveragePolicyStd[5]  | 0.27       |
| AverageReturn        | 774.19     |
| MinReturn            | 10.339     |
| MaxReturn            | 871.53     |
| StdReturn            | 180.22     |
| AverageEpisodeLength | 932.46     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.91     |
| TotalNEpisodes       | 17756      |
| TotalNSamples        | 1.1421e+06 |
| ExplainedVariance    | 0.15908    |
-------------------------------------
[2018-01-21 13:08:34.953654 UTC] Saving snapshot
[2018-01-21 13:08:34.953897 UTC] Starting iteration 230
[2018-01-21 13:08:34.954067 UTC] Start collecting samples
[2018-01-21 13:08:39.949459 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:40.094200 UTC] Performing policy update
[2018-01-21 13:08:40.094827 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:40.219809 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:41.652252 UTC] Performing line search
[2018-01-21 13:08:41.836625 UTC] Updating baseline
[2018-01-21 13:08:43.844450 UTC] Computing logging information
-------------------------------------
| Iteration            | 230        |
| ExpectedImprovement  | 0.014035   |
| ActualImprovement    | 0.013218   |
| ImprovementRatio     | 0.94174    |
| MeanKL               | 0.0073729  |
| Entropy              | 1.0969     |
| Perplexity           | 2.9948     |
| AveragePolicyStd     | 0.29494    |
| AveragePolicyStd[0]  | 0.2794     |
| AveragePolicyStd[1]  | 0.40019    |
| AveragePolicyStd[2]  | 0.22497    |
| AveragePolicyStd[3]  | 0.3039     |
| AveragePolicyStd[4]  | 0.29017    |
| AveragePolicyStd[5]  | 0.271      |
| AverageReturn        | 777.98     |
| MinReturn            | 10.339     |
| MaxReturn            | 871.53     |
| StdReturn            | 184.25     |
| AverageEpisodeLength | 933.57     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.7      |
| TotalNEpisodes       | 17765      |
| TotalNSamples        | 1.1502e+06 |
| ExplainedVariance    | 0.14137    |
-------------------------------------
[2018-01-21 13:08:44.511723 UTC] Saving snapshot
[2018-01-21 13:08:44.522002 UTC] Starting iteration 231
[2018-01-21 13:08:44.522234 UTC] Start collecting samples
[2018-01-21 13:08:49.371702 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:49.529305 UTC] Performing policy update
[2018-01-21 13:08:49.530246 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:49.666366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:08:51.087888 UTC] Performing line search
[2018-01-21 13:08:51.274564 UTC] Updating baseline
[2018-01-21 13:08:53.208721 UTC] Computing logging information
-------------------------------------
| Iteration            | 231        |
| ExpectedImprovement  | 0.01464    |
| ActualImprovement    | 0.014092   |
| ImprovementRatio     | 0.96258    |
| MeanKL               | 0.007203   |
| Entropy              | 1.0806     |
| Perplexity           | 2.9463     |
| AveragePolicyStd     | 0.29414    |
| AveragePolicyStd[0]  | 0.27995    |
| AveragePolicyStd[1]  | 0.39949    |
| AveragePolicyStd[2]  | 0.22406    |
| AveragePolicyStd[3]  | 0.30159    |
| AveragePolicyStd[4]  | 0.28892    |
| AveragePolicyStd[5]  | 0.27085    |
| AverageReturn        | 771.53     |
| MinReturn            | 10.339     |
| MaxReturn            | 877.33     |
| StdReturn            | 194.25     |
| AverageEpisodeLength | 923.71     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.04     |
| TotalNEpisodes       | 17770      |
| TotalNSamples        | 1.1542e+06 |
| ExplainedVariance    | 0.26708    |
-------------------------------------
[2018-01-21 13:08:53.877792 UTC] Saving snapshot
[2018-01-21 13:08:53.878023 UTC] Starting iteration 232
[2018-01-21 13:08:53.878167 UTC] Start collecting samples
[2018-01-21 13:08:58.849022 UTC] Computing input variables for policy optimization
[2018-01-21 13:08:58.986377 UTC] Performing policy update
[2018-01-21 13:08:58.987154 UTC] Computing gradient in Euclidean space
[2018-01-21 13:08:59.100505 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:00.535945 UTC] Performing line search
[2018-01-21 13:09:00.734835 UTC] Updating baseline
[2018-01-21 13:09:03.086452 UTC] Computing logging information
-------------------------------------
| Iteration            | 232        |
| ExpectedImprovement  | 0.013172   |
| ActualImprovement    | 0.012765   |
| ImprovementRatio     | 0.9691     |
| MeanKL               | 0.0070997  |
| Entropy              | 1.0651     |
| Perplexity           | 2.9012     |
| AveragePolicyStd     | 0.29327    |
| AveragePolicyStd[0]  | 0.28072    |
| AveragePolicyStd[1]  | 0.39685    |
| AveragePolicyStd[2]  | 0.22432    |
| AveragePolicyStd[3]  | 0.30094    |
| AveragePolicyStd[4]  | 0.28686    |
| AveragePolicyStd[5]  | 0.26992    |
| AverageReturn        | 758.8      |
| MinReturn            | 10.339     |
| MaxReturn            | 877.33     |
| StdReturn            | 213.17     |
| AverageEpisodeLength | 907.18     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 248.06     |
| TotalNEpisodes       | 17776      |
| TotalNSamples        | 1.1586e+06 |
| ExplainedVariance    | 0.31794    |
-------------------------------------
[2018-01-21 13:09:03.769054 UTC] Saving snapshot
[2018-01-21 13:09:03.769265 UTC] Starting iteration 233
[2018-01-21 13:09:03.769395 UTC] Start collecting samples
[2018-01-21 13:09:08.697284 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:08.862098 UTC] Performing policy update
[2018-01-21 13:09:08.862941 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:09.050460 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:10.550337 UTC] Performing line search
[2018-01-21 13:09:10.742878 UTC] Updating baseline
[2018-01-21 13:09:12.597324 UTC] Computing logging information
------------------------------------
| Iteration            | 233       |
| ExpectedImprovement  | 0.013429  |
| ActualImprovement    | 0.013232  |
| ImprovementRatio     | 0.98538   |
| MeanKL               | 0.0076959 |
| Entropy              | 1.067     |
| Perplexity           | 2.9066    |
| AveragePolicyStd     | 0.29332   |
| AveragePolicyStd[0]  | 0.281     |
| AveragePolicyStd[1]  | 0.39702   |
| AveragePolicyStd[2]  | 0.22508   |
| AveragePolicyStd[3]  | 0.29914   |
| AveragePolicyStd[4]  | 0.28828   |
| AveragePolicyStd[5]  | 0.26941   |
| AverageReturn        | 756.09    |
| MinReturn            | 10.339    |
| MaxReturn            | 889.83    |
| StdReturn            | 217.81    |
| AverageEpisodeLength | 901.24    |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 252.83    |
| TotalNEpisodes       | 17785     |
| TotalNSamples        | 1.167e+06 |
| ExplainedVariance    | -0.047334 |
------------------------------------
[2018-01-21 13:09:13.325825 UTC] Saving snapshot
[2018-01-21 13:09:13.326005 UTC] Starting iteration 234
[2018-01-21 13:09:13.326106 UTC] Start collecting samples
[2018-01-21 13:09:18.443544 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:18.579438 UTC] Performing policy update
[2018-01-21 13:09:18.580106 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:18.699754 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:20.189138 UTC] Performing line search
[2018-01-21 13:09:20.443098 UTC] Updating baseline
[2018-01-21 13:09:22.887649 UTC] Computing logging information
------------------------------------
| Iteration            | 234       |
| ExpectedImprovement  | 0.016046  |
| ActualImprovement    | 0.014668  |
| ImprovementRatio     | 0.91414   |
| MeanKL               | 0.0069523 |
| Entropy              | 1.077     |
| Perplexity           | 2.9358    |
| AveragePolicyStd     | 0.29392   |
| AveragePolicyStd[0]  | 0.28098   |
| AveragePolicyStd[1]  | 0.3991    |
| AveragePolicyStd[2]  | 0.2249    |
| AveragePolicyStd[3]  | 0.29982   |
| AveragePolicyStd[4]  | 0.29014   |
| AveragePolicyStd[5]  | 0.26859   |
| AverageReturn        | 762.34    |
| MinReturn            | 10.339    |
| MaxReturn            | 889.83    |
| StdReturn            | 209.67    |
| AverageEpisodeLength | 909.11    |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 243.36    |
| TotalNEpisodes       | 17787     |
| TotalNSamples        | 1.169e+06 |
| ExplainedVariance    | -0.024425 |
------------------------------------
[2018-01-21 13:09:23.523943 UTC] Saving snapshot
[2018-01-21 13:09:23.524261 UTC] Starting iteration 235
[2018-01-21 13:09:23.524484 UTC] Start collecting samples
[2018-01-21 13:09:28.739071 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:28.875661 UTC] Performing policy update
[2018-01-21 13:09:28.876397 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:29.010355 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:30.556115 UTC] Performing line search
[2018-01-21 13:09:30.804430 UTC] Updating baseline
[2018-01-21 13:09:33.024676 UTC] Computing logging information
------------------------------------
| Iteration            | 235       |
| ExpectedImprovement  | 0.01411   |
| ActualImprovement    | 0.01316   |
| ImprovementRatio     | 0.93261   |
| MeanKL               | 0.0074846 |
| Entropy              | 1.0744    |
| Perplexity           | 2.9282    |
| AveragePolicyStd     | 0.29381   |
| AveragePolicyStd[0]  | 0.28102   |
| AveragePolicyStd[1]  | 0.39882   |
| AveragePolicyStd[2]  | 0.22424   |
| AveragePolicyStd[3]  | 0.30127   |
| AveragePolicyStd[4]  | 0.28797   |
| AveragePolicyStd[5]  | 0.26956   |
| AverageReturn        | 762.93    |
| MinReturn            | 10.339    |
| MaxReturn            | 889.83    |
| StdReturn            | 209.9     |
| AverageEpisodeLength | 909.11    |
| MinEpisodeLength     | 20        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 243.36    |
| TotalNEpisodes       | 17791     |
| TotalNSamples        | 1.173e+06 |
| ExplainedVariance    | 0.039966  |
------------------------------------
[2018-01-21 13:09:33.711763 UTC] Saving snapshot
[2018-01-21 13:09:33.712123 UTC] Starting iteration 236
[2018-01-21 13:09:33.712358 UTC] Start collecting samples
[2018-01-21 13:09:38.913844 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:39.066398 UTC] Performing policy update
[2018-01-21 13:09:39.074728 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:39.194075 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:40.875305 UTC] Performing line search
[2018-01-21 13:09:41.116610 UTC] Updating baseline
[2018-01-21 13:09:43.353912 UTC] Computing logging information
-------------------------------------
| Iteration            | 236        |
| ExpectedImprovement  | 0.015397   |
| ActualImprovement    | 0.01516    |
| ImprovementRatio     | 0.98462    |
| MeanKL               | 0.0071843  |
| Entropy              | 1.0617     |
| Perplexity           | 2.8914     |
| AveragePolicyStd     | 0.29314    |
| AveragePolicyStd[0]  | 0.28108    |
| AveragePolicyStd[1]  | 0.39647    |
| AveragePolicyStd[2]  | 0.22385    |
| AveragePolicyStd[3]  | 0.30112    |
| AveragePolicyStd[4]  | 0.28865    |
| AveragePolicyStd[5]  | 0.26765    |
| AverageReturn        | 754.93     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 216.64     |
| AverageEpisodeLength | 900.39     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.17     |
| TotalNEpisodes       | 17799      |
| TotalNSamples        | 1.1801e+06 |
| ExplainedVariance    | 0.25658    |
-------------------------------------
[2018-01-21 13:09:43.998958 UTC] Saving snapshot
[2018-01-21 13:09:43.999193 UTC] Starting iteration 237
[2018-01-21 13:09:43.999387 UTC] Start collecting samples
[2018-01-21 13:09:49.187208 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:49.344141 UTC] Performing policy update
[2018-01-21 13:09:49.344916 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:49.462749 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:09:50.901667 UTC] Performing line search
[2018-01-21 13:09:51.106839 UTC] Updating baseline
[2018-01-21 13:09:53.206808 UTC] Computing logging information
-------------------------------------
| Iteration            | 237        |
| ExpectedImprovement  | 0.012104   |
| ActualImprovement    | 0.011728   |
| ImprovementRatio     | 0.96895    |
| MeanKL               | 0.007467   |
| Entropy              | 1.0623     |
| Perplexity           | 2.8929     |
| AveragePolicyStd     | 0.29305    |
| AveragePolicyStd[0]  | 0.28097    |
| AveragePolicyStd[1]  | 0.39379    |
| AveragePolicyStd[2]  | 0.22352    |
| AveragePolicyStd[3]  | 0.302      |
| AveragePolicyStd[4]  | 0.2896     |
| AveragePolicyStd[5]  | 0.26844    |
| AverageReturn        | 756.09     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 217.03     |
| AverageEpisodeLength | 900.39     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.17     |
| TotalNEpisodes       | 17803      |
| TotalNSamples        | 1.1841e+06 |
| ExplainedVariance    | 0.13059    |
-------------------------------------
[2018-01-21 13:09:53.829247 UTC] Saving snapshot
[2018-01-21 13:09:53.829454 UTC] Starting iteration 238
[2018-01-21 13:09:53.829637 UTC] Start collecting samples
[2018-01-21 13:09:58.859697 UTC] Computing input variables for policy optimization
[2018-01-21 13:09:58.988905 UTC] Performing policy update
[2018-01-21 13:09:58.989522 UTC] Computing gradient in Euclidean space
[2018-01-21 13:09:59.110768 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:00.588074 UTC] Performing line search
[2018-01-21 13:10:00.790262 UTC] Updating baseline
[2018-01-21 13:10:02.911460 UTC] Computing logging information
-------------------------------------
| Iteration            | 238        |
| ExpectedImprovement  | 0.018017   |
| ActualImprovement    | 0.01628    |
| ImprovementRatio     | 0.90362    |
| MeanKL               | 0.0067309  |
| Entropy              | 1.0464     |
| Perplexity           | 2.8474     |
| AveragePolicyStd     | 0.2923     |
| AveragePolicyStd[0]  | 0.28139    |
| AveragePolicyStd[1]  | 0.39258    |
| AveragePolicyStd[2]  | 0.22261    |
| AveragePolicyStd[3]  | 0.30175    |
| AveragePolicyStd[4]  | 0.28828    |
| AveragePolicyStd[5]  | 0.26716    |
| AverageReturn        | 753.86     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 221.33     |
| AverageEpisodeLength | 896.97     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 254.27     |
| TotalNEpisodes       | 17808      |
| TotalNSamples        | 1.1885e+06 |
| ExplainedVariance    | 0.30207    |
-------------------------------------
[2018-01-21 13:10:03.608975 UTC] Saving snapshot
[2018-01-21 13:10:03.609233 UTC] Starting iteration 239
[2018-01-21 13:10:03.609396 UTC] Start collecting samples
[2018-01-21 13:10:08.741676 UTC] Computing input variables for policy optimization
[2018-01-21 13:10:08.891545 UTC] Performing policy update
[2018-01-21 13:10:08.892159 UTC] Computing gradient in Euclidean space
[2018-01-21 13:10:09.018552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:10.411143 UTC] Performing line search
[2018-01-21 13:10:10.603155 UTC] Updating baseline
[2018-01-21 13:10:12.657343 UTC] Computing logging information
-------------------------------------
| Iteration            | 239        |
| ExpectedImprovement  | 0.015714   |
| ActualImprovement    | 0.014686   |
| ImprovementRatio     | 0.93456    |
| MeanKL               | 0.0073924  |
| Entropy              | 1.032      |
| Perplexity           | 2.8068     |
| AveragePolicyStd     | 0.29147    |
| AveragePolicyStd[0]  | 0.28129    |
| AveragePolicyStd[1]  | 0.38923    |
| AveragePolicyStd[2]  | 0.22259    |
| AveragePolicyStd[3]  | 0.30159    |
| AveragePolicyStd[4]  | 0.28733    |
| AveragePolicyStd[5]  | 0.26677    |
| AverageReturn        | 757        |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 218.11     |
| AverageEpisodeLength | 902.33     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.71     |
| TotalNEpisodes       | 17815      |
| TotalNSamples        | 1.1955e+06 |
| ExplainedVariance    | 0.065544   |
-------------------------------------
[2018-01-21 13:10:13.277020 UTC] Saving snapshot
[2018-01-21 13:10:13.277257 UTC] Starting iteration 240
[2018-01-21 13:10:13.277406 UTC] Start collecting samples
[2018-01-21 13:10:18.338620 UTC] Computing input variables for policy optimization
[2018-01-21 13:10:18.494961 UTC] Performing policy update
[2018-01-21 13:10:18.495638 UTC] Computing gradient in Euclidean space
[2018-01-21 13:10:18.626828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:20.055347 UTC] Performing line search
[2018-01-21 13:10:20.247701 UTC] Updating baseline
[2018-01-21 13:10:22.065897 UTC] Computing logging information
-------------------------------------
| Iteration            | 240        |
| ExpectedImprovement  | 0.01501    |
| ActualImprovement    | 0.01428    |
| ImprovementRatio     | 0.95134    |
| MeanKL               | 0.0073365  |
| Entropy              | 1.0154     |
| Perplexity           | 2.7605     |
| AveragePolicyStd     | 0.29052    |
| AveragePolicyStd[0]  | 0.28048    |
| AveragePolicyStd[1]  | 0.38608    |
| AveragePolicyStd[2]  | 0.22263    |
| AveragePolicyStd[3]  | 0.30039    |
| AveragePolicyStd[4]  | 0.28655    |
| AveragePolicyStd[5]  | 0.26701    |
| AverageReturn        | 755.28     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 216.97     |
| AverageEpisodeLength | 899.49     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.25     |
| TotalNEpisodes       | 17821      |
| TotalNSamples        | 1.2005e+06 |
| ExplainedVariance    | 0.3083     |
-------------------------------------
[2018-01-21 13:10:22.771607 UTC] Saving snapshot
[2018-01-21 13:10:22.781331 UTC] Starting iteration 241
[2018-01-21 13:10:22.781571 UTC] Start collecting samples
[2018-01-21 13:10:29.075103 UTC] Computing input variables for policy optimization
[2018-01-21 13:10:29.227570 UTC] Performing policy update
[2018-01-21 13:10:29.228575 UTC] Computing gradient in Euclidean space
[2018-01-21 13:10:29.378838 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:30.829555 UTC] Performing line search
[2018-01-21 13:10:31.019924 UTC] Updating baseline
[2018-01-21 13:10:33.465961 UTC] Computing logging information
-------------------------------------
| Iteration            | 241        |
| ExpectedImprovement  | 0.013064   |
| ActualImprovement    | 0.012624   |
| ImprovementRatio     | 0.96635    |
| MeanKL               | 0.0074054  |
| Entropy              | 1.003      |
| Perplexity           | 2.7265     |
| AveragePolicyStd     | 0.29002    |
| AveragePolicyStd[0]  | 0.27913    |
| AveragePolicyStd[1]  | 0.38721    |
| AveragePolicyStd[2]  | 0.22149    |
| AveragePolicyStd[3]  | 0.29873    |
| AveragePolicyStd[4]  | 0.28635    |
| AveragePolicyStd[5]  | 0.26723    |
| AverageReturn        | 755.12     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 216.93     |
| AverageEpisodeLength | 899.49     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.25     |
| TotalNEpisodes       | 17823      |
| TotalNSamples        | 1.2025e+06 |
| ExplainedVariance    | 0.023808   |
-------------------------------------
[2018-01-21 13:10:34.151044 UTC] Saving snapshot
[2018-01-21 13:10:34.151337 UTC] Starting iteration 242
[2018-01-21 13:10:34.151583 UTC] Start collecting samples
[2018-01-21 13:10:39.894854 UTC] Computing input variables for policy optimization
[2018-01-21 13:10:40.035065 UTC] Performing policy update
[2018-01-21 13:10:40.035982 UTC] Computing gradient in Euclidean space
[2018-01-21 13:10:40.154151 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:41.822346 UTC] Performing line search
[2018-01-21 13:10:42.032526 UTC] Updating baseline
[2018-01-21 13:10:44.396357 UTC] Computing logging information
-------------------------------------
| Iteration            | 242        |
| ExpectedImprovement  | 0.015077   |
| ActualImprovement    | 0.0142     |
| ImprovementRatio     | 0.94184    |
| MeanKL               | 0.0076544  |
| Entropy              | 1.0107     |
| Perplexity           | 2.7475     |
| AveragePolicyStd     | 0.29035    |
| AveragePolicyStd[0]  | 0.27872    |
| AveragePolicyStd[1]  | 0.38621    |
| AveragePolicyStd[2]  | 0.22119    |
| AveragePolicyStd[3]  | 0.29861    |
| AveragePolicyStd[4]  | 0.2885     |
| AveragePolicyStd[5]  | 0.26885    |
| AverageReturn        | 755.32     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 217.22     |
| AverageEpisodeLength | 899.49     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.25     |
| TotalNEpisodes       | 17829      |
| TotalNSamples        | 1.2085e+06 |
| ExplainedVariance    | 0.068809   |
-------------------------------------
[2018-01-21 13:10:45.043757 UTC] Saving snapshot
[2018-01-21 13:10:45.044093 UTC] Starting iteration 243
[2018-01-21 13:10:45.044363 UTC] Start collecting samples
[2018-01-21 13:10:50.637522 UTC] Computing input variables for policy optimization
[2018-01-21 13:10:50.789550 UTC] Performing policy update
[2018-01-21 13:10:50.791059 UTC] Computing gradient in Euclidean space
[2018-01-21 13:10:50.920993 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:10:52.337029 UTC] Performing line search
[2018-01-21 13:10:52.533242 UTC] Updating baseline
[2018-01-21 13:10:54.538066 UTC] Computing logging information
-------------------------------------
| Iteration            | 243        |
| ExpectedImprovement  | 0.014246   |
| ActualImprovement    | 0.013523   |
| ImprovementRatio     | 0.94922    |
| MeanKL               | 0.0071338  |
| Entropy              | 0.99704    |
| Perplexity           | 2.7102     |
| AveragePolicyStd     | 0.28965    |
| AveragePolicyStd[0]  | 0.27814    |
| AveragePolicyStd[1]  | 0.3845     |
| AveragePolicyStd[2]  | 0.22072    |
| AveragePolicyStd[3]  | 0.29759    |
| AveragePolicyStd[4]  | 0.28851    |
| AveragePolicyStd[5]  | 0.26842    |
| AverageReturn        | 756.97     |
| MinReturn            | 10.339     |
| MaxReturn            | 889.83     |
| StdReturn            | 217.98     |
| AverageEpisodeLength | 899.49     |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 249.25     |
| TotalNEpisodes       | 17836      |
| TotalNSamples        | 1.2155e+06 |
| ExplainedVariance    | -0.063833  |
-------------------------------------
[2018-01-21 13:10:55.241210 UTC] Saving snapshot
[2018-01-21 13:10:55.241496 UTC] Starting iteration 244
[2018-01-21 13:10:55.241675 UTC] Start collecting samples
[2018-01-21 13:11:00.403177 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:00.547223 UTC] Performing policy update
[2018-01-21 13:11:00.548411 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:00.684631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:02.208053 UTC] Performing line search
[2018-01-21 13:11:02.403620 UTC] Updating baseline
[2018-01-21 13:11:05.497382 UTC] Computing logging information
-------------------------------------
| Iteration            | 244        |
| ExpectedImprovement  | 0.015559   |
| ActualImprovement    | 0.014506   |
| ImprovementRatio     | 0.93234    |
| MeanKL               | 0.007033   |
| Entropy              | 1.0039     |
| Perplexity           | 2.7289     |
| AveragePolicyStd     | 0.29003    |
| AveragePolicyStd[0]  | 0.27792    |
| AveragePolicyStd[1]  | 0.3844     |
| AveragePolicyStd[2]  | 0.21917    |
| AveragePolicyStd[3]  | 0.29889    |
| AveragePolicyStd[4]  | 0.28847    |
| AveragePolicyStd[5]  | 0.27131    |
| AverageReturn        | 765.9      |
| MinReturn            | 101.7      |
| MaxReturn            | 889.83     |
| StdReturn            | 205.42     |
| AverageEpisodeLength | 909        |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.17     |
| TotalNEpisodes       | 17840      |
| TotalNSamples        | 1.2186e+06 |
| ExplainedVariance    | 0.063819   |
-------------------------------------
[2018-01-21 13:11:06.161828 UTC] Saving snapshot
[2018-01-21 13:11:06.162128 UTC] Starting iteration 245
[2018-01-21 13:11:06.162371 UTC] Start collecting samples
[2018-01-21 13:11:11.076052 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:11.214772 UTC] Performing policy update
[2018-01-21 13:11:11.215388 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:11.349475 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:12.885508 UTC] Performing line search
[2018-01-21 13:11:13.092187 UTC] Updating baseline
[2018-01-21 13:11:15.988527 UTC] Computing logging information
-------------------------------------
| Iteration            | 245        |
| ExpectedImprovement  | 0.014652   |
| ActualImprovement    | 0.013561   |
| ImprovementRatio     | 0.92553    |
| MeanKL               | 0.0072304  |
| Entropy              | 0.98631    |
| Perplexity           | 2.6813     |
| AveragePolicyStd     | 0.28912    |
| AveragePolicyStd[0]  | 0.27696    |
| AveragePolicyStd[1]  | 0.38224    |
| AveragePolicyStd[2]  | 0.2189     |
| AveragePolicyStd[3]  | 0.29829    |
| AveragePolicyStd[4]  | 0.28806    |
| AveragePolicyStd[5]  | 0.27027    |
| AverageReturn        | 765.1      |
| MinReturn            | 101.7      |
| MaxReturn            | 889.83     |
| StdReturn            | 205.62     |
| AverageEpisodeLength | 909        |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.17     |
| TotalNEpisodes       | 17843      |
| TotalNSamples        | 1.2216e+06 |
| ExplainedVariance    | 0.01016    |
-------------------------------------
[2018-01-21 13:11:16.629245 UTC] Saving snapshot
[2018-01-21 13:11:16.629486 UTC] Starting iteration 246
[2018-01-21 13:11:16.629706 UTC] Start collecting samples
[2018-01-21 13:11:21.655174 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:21.791787 UTC] Performing policy update
[2018-01-21 13:11:21.792580 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:21.914968 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:23.337720 UTC] Performing line search
[2018-01-21 13:11:23.531934 UTC] Updating baseline
[2018-01-21 13:11:25.499632 UTC] Computing logging information
-------------------------------------
| Iteration            | 246        |
| ExpectedImprovement  | 0.013926   |
| ActualImprovement    | 0.013058   |
| ImprovementRatio     | 0.9377     |
| MeanKL               | 0.0073923  |
| Entropy              | 0.97495    |
| Perplexity           | 2.651      |
| AveragePolicyStd     | 0.28855    |
| AveragePolicyStd[0]  | 0.27595    |
| AveragePolicyStd[1]  | 0.38055    |
| AveragePolicyStd[2]  | 0.21804    |
| AveragePolicyStd[3]  | 0.29762    |
| AveragePolicyStd[4]  | 0.28849    |
| AveragePolicyStd[5]  | 0.27064    |
| AverageReturn        | 781.68     |
| MinReturn            | 101.7      |
| MaxReturn            | 889.83     |
| StdReturn            | 190.96     |
| AverageEpisodeLength | 925.42     |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.81     |
| TotalNEpisodes       | 17851      |
| TotalNSamples        | 1.2296e+06 |
| ExplainedVariance    | 0.025788   |
-------------------------------------
[2018-01-21 13:11:26.153227 UTC] Saving snapshot
[2018-01-21 13:11:26.153483 UTC] Starting iteration 247
[2018-01-21 13:11:26.153683 UTC] Start collecting samples
[2018-01-21 13:11:31.200551 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:31.339795 UTC] Performing policy update
[2018-01-21 13:11:31.340437 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:31.465571 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:32.891935 UTC] Performing line search
[2018-01-21 13:11:33.095882 UTC] Updating baseline
[2018-01-21 13:11:35.248285 UTC] Computing logging information
-------------------------------------
| Iteration            | 247        |
| ExpectedImprovement  | 0.015752   |
| ActualImprovement    | 0.015356   |
| ImprovementRatio     | 0.9749     |
| MeanKL               | 0.0071687  |
| Entropy              | 0.98329    |
| Perplexity           | 2.6732     |
| AveragePolicyStd     | 0.28891    |
| AveragePolicyStd[0]  | 0.27636    |
| AveragePolicyStd[1]  | 0.38193    |
| AveragePolicyStd[2]  | 0.21979    |
| AveragePolicyStd[3]  | 0.29695    |
| AveragePolicyStd[4]  | 0.28744    |
| AveragePolicyStd[5]  | 0.27097    |
| AverageReturn        | 782        |
| MinReturn            | 101.7      |
| MaxReturn            | 889.83     |
| StdReturn            | 191.1      |
| AverageEpisodeLength | 925.42     |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.81     |
| TotalNEpisodes       | 17855      |
| TotalNSamples        | 1.2336e+06 |
| ExplainedVariance    | 0.049525   |
-------------------------------------
[2018-01-21 13:11:35.960745 UTC] Saving snapshot
[2018-01-21 13:11:35.961075 UTC] Starting iteration 248
[2018-01-21 13:11:35.961297 UTC] Start collecting samples
[2018-01-21 13:11:40.771056 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:40.911819 UTC] Performing policy update
[2018-01-21 13:11:40.912787 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:41.054029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:42.484086 UTC] Performing line search
[2018-01-21 13:11:42.679541 UTC] Updating baseline
[2018-01-21 13:11:44.580901 UTC] Computing logging information
-------------------------------------
| Iteration            | 248        |
| ExpectedImprovement  | 0.019208   |
| ActualImprovement    | 0.016919   |
| ImprovementRatio     | 0.88087    |
| MeanKL               | 0.0068003  |
| Entropy              | 0.99489    |
| Perplexity           | 2.7044     |
| AveragePolicyStd     | 0.28941    |
| AveragePolicyStd[0]  | 0.27599    |
| AveragePolicyStd[1]  | 0.38219    |
| AveragePolicyStd[2]  | 0.22118    |
| AveragePolicyStd[3]  | 0.29781    |
| AveragePolicyStd[4]  | 0.28794    |
| AveragePolicyStd[5]  | 0.27134    |
| AverageReturn        | 778.74     |
| MinReturn            | 101.7      |
| MaxReturn            | 889.83     |
| StdReturn            | 194.03     |
| AverageEpisodeLength | 920.98     |
| MinEpisodeLength     | 128        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.75     |
| TotalNEpisodes       | 17860      |
| TotalNSamples        | 1.2382e+06 |
| ExplainedVariance    | 0.04829    |
-------------------------------------
[2018-01-21 13:11:45.231300 UTC] Saving snapshot
[2018-01-21 13:11:45.231587 UTC] Starting iteration 249
[2018-01-21 13:11:45.231769 UTC] Start collecting samples
[2018-01-21 13:11:50.311367 UTC] Computing input variables for policy optimization
[2018-01-21 13:11:50.452816 UTC] Performing policy update
[2018-01-21 13:11:50.453687 UTC] Computing gradient in Euclidean space
[2018-01-21 13:11:50.587323 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:11:52.032170 UTC] Performing line search
[2018-01-21 13:11:52.228067 UTC] Updating baseline
[2018-01-21 13:11:54.865043 UTC] Computing logging information
-------------------------------------
| Iteration            | 249        |
| ExpectedImprovement  | 0.015275   |
| ActualImprovement    | 0.014902   |
| ImprovementRatio     | 0.97554    |
| MeanKL               | 0.0073829  |
| Entropy              | 0.98877    |
| Perplexity           | 2.6879     |
| AveragePolicyStd     | 0.2891     |
| AveragePolicyStd[0]  | 0.27645    |
| AveragePolicyStd[1]  | 0.38106    |
| AveragePolicyStd[2]  | 0.22058    |
| AveragePolicyStd[3]  | 0.29793    |
| AveragePolicyStd[4]  | 0.28785    |
| AveragePolicyStd[5]  | 0.27072    |
| AverageReturn        | 789.61     |
| MinReturn            | 113.23     |
| MaxReturn            | 895.14     |
| StdReturn            | 178.22     |
| AverageEpisodeLength | 933.25     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.22     |
| TotalNEpisodes       | 17868      |
| TotalNSamples        | 1.2455e+06 |
| ExplainedVariance    | 0.13345    |
-------------------------------------
[2018-01-21 13:11:55.498902 UTC] Saving snapshot
[2018-01-21 13:11:55.499651 UTC] Starting iteration 250
[2018-01-21 13:11:55.500234 UTC] Start collecting samples
[2018-01-21 13:12:00.389906 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:00.579562 UTC] Performing policy update
[2018-01-21 13:12:00.580682 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:00.702406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:02.293868 UTC] Performing line search
[2018-01-21 13:12:02.491516 UTC] Updating baseline
[2018-01-21 13:12:04.584942 UTC] Computing logging information
-------------------------------------
| Iteration            | 250        |
| ExpectedImprovement  | 0.015407   |
| ActualImprovement    | 0.014786   |
| ImprovementRatio     | 0.95973    |
| MeanKL               | 0.0074992  |
| Entropy              | 0.989      |
| Perplexity           | 2.6886     |
| AveragePolicyStd     | 0.28921    |
| AveragePolicyStd[0]  | 0.27711    |
| AveragePolicyStd[1]  | 0.38276    |
| AveragePolicyStd[2]  | 0.21975    |
| AveragePolicyStd[3]  | 0.29731    |
| AveragePolicyStd[4]  | 0.28666    |
| AveragePolicyStd[5]  | 0.27167    |
| AverageReturn        | 796.99     |
| MinReturn            | 118.39     |
| MaxReturn            | 895.14     |
| StdReturn            | 164.85     |
| AverageEpisodeLength | 941.85     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.76     |
| TotalNEpisodes       | 17872      |
| TotalNSamples        | 1.2495e+06 |
| ExplainedVariance    | 0.034574   |
-------------------------------------
[2018-01-21 13:12:05.261596 UTC] Saving snapshot
[2018-01-21 13:12:05.267898 UTC] Starting iteration 251
[2018-01-21 13:12:05.268070 UTC] Start collecting samples
[2018-01-21 13:12:10.427851 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:10.568856 UTC] Performing policy update
[2018-01-21 13:12:10.569522 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:10.687507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:12.195707 UTC] Performing line search
[2018-01-21 13:12:12.388532 UTC] Updating baseline
[2018-01-21 13:12:14.491281 UTC] Computing logging information
-------------------------------------
| Iteration            | 251        |
| ExpectedImprovement  | 0.016277   |
| ActualImprovement    | 0.015393   |
| ImprovementRatio     | 0.9457     |
| MeanKL               | 0.0069403  |
| Entropy              | 0.9889     |
| Perplexity           | 2.6883     |
| AveragePolicyStd     | 0.28925    |
| AveragePolicyStd[0]  | 0.27692    |
| AveragePolicyStd[1]  | 0.38378    |
| AveragePolicyStd[2]  | 0.21975    |
| AveragePolicyStd[3]  | 0.29664    |
| AveragePolicyStd[4]  | 0.28718    |
| AveragePolicyStd[5]  | 0.2712     |
| AverageReturn        | 804.71     |
| MinReturn            | 118.39     |
| MaxReturn            | 895.14     |
| StdReturn            | 153.06     |
| AverageEpisodeLength | 949.78     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.34     |
| TotalNEpisodes       | 17876      |
| TotalNSamples        | 1.2535e+06 |
| ExplainedVariance    | 0.16614    |
-------------------------------------
[2018-01-21 13:12:15.195924 UTC] Saving snapshot
[2018-01-21 13:12:15.196198 UTC] Starting iteration 252
[2018-01-21 13:12:15.196372 UTC] Start collecting samples
[2018-01-21 13:12:20.120992 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:20.280352 UTC] Performing policy update
[2018-01-21 13:12:20.280951 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:20.401091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:21.802563 UTC] Performing line search
[2018-01-21 13:12:21.992101 UTC] Updating baseline
[2018-01-21 13:12:24.337275 UTC] Computing logging information
-------------------------------------
| Iteration            | 252        |
| ExpectedImprovement  | 0.014568   |
| ActualImprovement    | 0.013959   |
| ImprovementRatio     | 0.95822    |
| MeanKL               | 0.0076661  |
| Entropy              | 0.97287    |
| Perplexity           | 2.6455     |
| AveragePolicyStd     | 0.28856    |
| AveragePolicyStd[0]  | 0.27834    |
| AveragePolicyStd[1]  | 0.38331    |
| AveragePolicyStd[2]  | 0.21829    |
| AveragePolicyStd[3]  | 0.29633    |
| AveragePolicyStd[4]  | 0.28707    |
| AveragePolicyStd[5]  | 0.26802    |
| AverageReturn        | 791.4      |
| MinReturn            | 118.39     |
| MaxReturn            | 895.14     |
| StdReturn            | 173.45     |
| AverageEpisodeLength | 934.2      |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.27     |
| TotalNEpisodes       | 17886      |
| TotalNSamples        | 1.2614e+06 |
| ExplainedVariance    | 0.39625    |
-------------------------------------
[2018-01-21 13:12:24.980296 UTC] Saving snapshot
[2018-01-21 13:12:24.980514 UTC] Starting iteration 253
[2018-01-21 13:12:24.980680 UTC] Start collecting samples
[2018-01-21 13:12:30.367366 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:30.506547 UTC] Performing policy update
[2018-01-21 13:12:30.507218 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:30.632755 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:32.082644 UTC] Performing line search
[2018-01-21 13:12:32.290463 UTC] Updating baseline
[2018-01-21 13:12:34.207114 UTC] Computing logging information
------------------------------------
| Iteration            | 253       |
| ExpectedImprovement  | 0.015251  |
| ActualImprovement    | 0.014402  |
| ImprovementRatio     | 0.94435   |
| MeanKL               | 0.0071337 |
| Entropy              | 0.96626   |
| Perplexity           | 2.6281    |
| AveragePolicyStd     | 0.28828   |
| AveragePolicyStd[0]  | 0.27714   |
| AveragePolicyStd[1]  | 0.38375   |
| AveragePolicyStd[2]  | 0.21797   |
| AveragePolicyStd[3]  | 0.29545   |
| AveragePolicyStd[4]  | 0.28691   |
| AveragePolicyStd[5]  | 0.26844   |
| AverageReturn        | 787.62    |
| MinReturn            | 118.39    |
| MaxReturn            | 895.14    |
| StdReturn            | 175.21    |
| AverageEpisodeLength | 930.45    |
| MinEpisodeLength     | 132       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 193.61    |
| TotalNEpisodes       | 17891     |
| TotalNSamples        | 1.266e+06 |
| ExplainedVariance    | 0.26993   |
------------------------------------
[2018-01-21 13:12:34.853864 UTC] Saving snapshot
[2018-01-21 13:12:34.854133 UTC] Starting iteration 254
[2018-01-21 13:12:34.854319 UTC] Start collecting samples
[2018-01-21 13:12:39.706746 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:39.855977 UTC] Performing policy update
[2018-01-21 13:12:39.856645 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:39.992627 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:41.393311 UTC] Performing line search
[2018-01-21 13:12:41.574769 UTC] Updating baseline
[2018-01-21 13:12:43.467193 UTC] Computing logging information
-------------------------------------
| Iteration            | 254        |
| ExpectedImprovement  | 0.017318   |
| ActualImprovement    | 0.017206   |
| ImprovementRatio     | 0.99354    |
| MeanKL               | 0.0071396  |
| Entropy              | 0.95082    |
| Perplexity           | 2.5878     |
| AveragePolicyStd     | 0.28757    |
| AveragePolicyStd[0]  | 0.27783    |
| AveragePolicyStd[1]  | 0.38183    |
| AveragePolicyStd[2]  | 0.21601    |
| AveragePolicyStd[3]  | 0.29529    |
| AveragePolicyStd[4]  | 0.2872     |
| AveragePolicyStd[5]  | 0.26728    |
| AverageReturn        | 765.94     |
| MinReturn            | 50.924     |
| MaxReturn            | 895.14     |
| StdReturn            | 209.78     |
| AverageEpisodeLength | 904.99     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.31     |
| TotalNEpisodes       | 17896      |
| TotalNSamples        | 1.2683e+06 |
| ExplainedVariance    | 0.39083    |
-------------------------------------
[2018-01-21 13:12:44.100703 UTC] Saving snapshot
[2018-01-21 13:12:44.101007 UTC] Starting iteration 255
[2018-01-21 13:12:44.101183 UTC] Start collecting samples
[2018-01-21 13:12:48.688836 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:48.823598 UTC] Performing policy update
[2018-01-21 13:12:48.824209 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:48.942213 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:12:50.339702 UTC] Performing line search
[2018-01-21 13:12:50.558107 UTC] Updating baseline
[2018-01-21 13:12:53.050729 UTC] Computing logging information
-------------------------------------
| Iteration            | 255        |
| ExpectedImprovement  | 0.014933   |
| ActualImprovement    | 0.013832   |
| ImprovementRatio     | 0.92623    |
| MeanKL               | 0.0076725  |
| Entropy              | 0.95094    |
| Perplexity           | 2.5881     |
| AveragePolicyStd     | 0.28755    |
| AveragePolicyStd[0]  | 0.27772    |
| AveragePolicyStd[1]  | 0.3811     |
| AveragePolicyStd[2]  | 0.21632    |
| AveragePolicyStd[3]  | 0.29699    |
| AveragePolicyStd[4]  | 0.28629    |
| AveragePolicyStd[5]  | 0.26685    |
| AverageReturn        | 774.4      |
| MinReturn            | 50.924     |
| MaxReturn            | 895.14     |
| StdReturn            | 203.12     |
| AverageEpisodeLength | 911.82     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.97     |
| TotalNEpisodes       | 17900      |
| TotalNSamples        | 1.2723e+06 |
| ExplainedVariance    | 0.17918    |
-------------------------------------
[2018-01-21 13:12:53.712331 UTC] Saving snapshot
[2018-01-21 13:12:53.712576 UTC] Starting iteration 256
[2018-01-21 13:12:53.712738 UTC] Start collecting samples
[2018-01-21 13:12:58.596179 UTC] Computing input variables for policy optimization
[2018-01-21 13:12:58.730196 UTC] Performing policy update
[2018-01-21 13:12:58.731429 UTC] Computing gradient in Euclidean space
[2018-01-21 13:12:58.867285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:00.328546 UTC] Performing line search
[2018-01-21 13:13:00.530474 UTC] Updating baseline
[2018-01-21 13:13:02.797294 UTC] Computing logging information
-------------------------------------
| Iteration            | 256        |
| ExpectedImprovement  | 0.017932   |
| ActualImprovement    | 0.016979   |
| ImprovementRatio     | 0.94687    |
| MeanKL               | 0.0072431  |
| Entropy              | 0.94516    |
| Perplexity           | 2.5732     |
| AveragePolicyStd     | 0.28742    |
| AveragePolicyStd[0]  | 0.27758    |
| AveragePolicyStd[1]  | 0.38318    |
| AveragePolicyStd[2]  | 0.21569    |
| AveragePolicyStd[3]  | 0.29839    |
| AveragePolicyStd[4]  | 0.2842     |
| AveragePolicyStd[5]  | 0.26547    |
| AverageReturn        | 773.96     |
| MinReturn            | 50.924     |
| MaxReturn            | 895.14     |
| StdReturn            | 205.09     |
| AverageEpisodeLength | 910.42     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.74     |
| TotalNEpisodes       | 17909      |
| TotalNSamples        | 1.2806e+06 |
| ExplainedVariance    | 0.1703     |
-------------------------------------
[2018-01-21 13:13:03.439011 UTC] Saving snapshot
[2018-01-21 13:13:03.439228 UTC] Starting iteration 257
[2018-01-21 13:13:03.439341 UTC] Start collecting samples
[2018-01-21 13:13:08.262768 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:08.413095 UTC] Performing policy update
[2018-01-21 13:13:08.414147 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:08.550173 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:09.937510 UTC] Performing line search
[2018-01-21 13:13:10.118650 UTC] Updating baseline
[2018-01-21 13:13:12.069572 UTC] Computing logging information
-------------------------------------
| Iteration            | 257        |
| ExpectedImprovement  | 0.020566   |
| ActualImprovement    | 0.019174   |
| ImprovementRatio     | 0.93229    |
| MeanKL               | 0.0068711  |
| Entropy              | 0.94447    |
| Perplexity           | 2.5715     |
| AveragePolicyStd     | 0.28739    |
| AveragePolicyStd[0]  | 0.27681    |
| AveragePolicyStd[1]  | 0.38267    |
| AveragePolicyStd[2]  | 0.21528    |
| AveragePolicyStd[3]  | 0.2988     |
| AveragePolicyStd[4]  | 0.28461    |
| AveragePolicyStd[5]  | 0.26614    |
| AverageReturn        | 775.39     |
| MinReturn            | 50.924     |
| MaxReturn            | 895.14     |
| StdReturn            | 204.88     |
| AverageEpisodeLength | 910.42     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.74     |
| TotalNEpisodes       | 17912      |
| TotalNSamples        | 1.2836e+06 |
| ExplainedVariance    | -0.059162  |
-------------------------------------
[2018-01-21 13:13:12.689233 UTC] Saving snapshot
[2018-01-21 13:13:12.689484 UTC] Starting iteration 258
[2018-01-21 13:13:12.689664 UTC] Start collecting samples
[2018-01-21 13:13:17.600008 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:17.733634 UTC] Performing policy update
[2018-01-21 13:13:17.734277 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:17.858944 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:19.386360 UTC] Performing line search
[2018-01-21 13:13:19.593761 UTC] Updating baseline
[2018-01-21 13:13:21.454858 UTC] Computing logging information
-------------------------------------
| Iteration            | 258        |
| ExpectedImprovement  | 0.014516   |
| ActualImprovement    | 0.013769   |
| ImprovementRatio     | 0.94854    |
| MeanKL               | 0.0068811  |
| Entropy              | 0.94114    |
| Perplexity           | 2.5629     |
| AveragePolicyStd     | 0.28716    |
| AveragePolicyStd[0]  | 0.27593    |
| AveragePolicyStd[1]  | 0.38304    |
| AveragePolicyStd[2]  | 0.217      |
| AveragePolicyStd[3]  | 0.29841    |
| AveragePolicyStd[4]  | 0.28287    |
| AveragePolicyStd[5]  | 0.26572    |
| AverageReturn        | 769.86     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 208.85     |
| AverageEpisodeLength | 902.52     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 236.33     |
| TotalNEpisodes       | 17917      |
| TotalNSamples        | 1.2878e+06 |
| ExplainedVariance    | 0.25969    |
-------------------------------------
[2018-01-21 13:13:22.163514 UTC] Saving snapshot
[2018-01-21 13:13:22.163815 UTC] Starting iteration 259
[2018-01-21 13:13:22.164028 UTC] Start collecting samples
[2018-01-21 13:13:26.921489 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:27.073792 UTC] Performing policy update
[2018-01-21 13:13:27.074423 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:27.200128 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:28.601559 UTC] Performing line search
[2018-01-21 13:13:28.793714 UTC] Updating baseline
[2018-01-21 13:13:30.577166 UTC] Computing logging information
-------------------------------------
| Iteration            | 259        |
| ExpectedImprovement  | 0.015143   |
| ActualImprovement    | 0.01479    |
| ImprovementRatio     | 0.9767     |
| MeanKL               | 0.0074905  |
| Entropy              | 0.92391    |
| Perplexity           | 2.5191     |
| AveragePolicyStd     | 0.28635    |
| AveragePolicyStd[0]  | 0.27575    |
| AveragePolicyStd[1]  | 0.38104    |
| AveragePolicyStd[2]  | 0.2155     |
| AveragePolicyStd[3]  | 0.29927    |
| AveragePolicyStd[4]  | 0.2813     |
| AveragePolicyStd[5]  | 0.26526    |
| AverageReturn        | 779.23     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 202.09     |
| AverageEpisodeLength | 909.45     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.94     |
| TotalNEpisodes       | 17926      |
| TotalNSamples        | 1.2964e+06 |
| ExplainedVariance    | 0.2808     |
-------------------------------------
[2018-01-21 13:13:31.230369 UTC] Saving snapshot
[2018-01-21 13:13:31.230716 UTC] Starting iteration 260
[2018-01-21 13:13:31.230881 UTC] Start collecting samples
[2018-01-21 13:13:35.995411 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:36.135097 UTC] Performing policy update
[2018-01-21 13:13:36.146994 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:36.285076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:37.726366 UTC] Performing line search
[2018-01-21 13:13:37.912504 UTC] Updating baseline
[2018-01-21 13:13:39.598647 UTC] Computing logging information
-------------------------------------
| Iteration            | 260        |
| ExpectedImprovement  | 0.013411   |
| ActualImprovement    | 0.013561   |
| ImprovementRatio     | 1.0112     |
| MeanKL               | 0.007036   |
| Entropy              | 0.92375    |
| Perplexity           | 2.5187     |
| AveragePolicyStd     | 0.28632    |
| AveragePolicyStd[0]  | 0.27565    |
| AveragePolicyStd[1]  | 0.38152    |
| AveragePolicyStd[2]  | 0.21592    |
| AveragePolicyStd[3]  | 0.29817    |
| AveragePolicyStd[4]  | 0.28022    |
| AveragePolicyStd[5]  | 0.26645    |
| AverageReturn        | 772.19     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 211.89     |
| AverageEpisodeLength | 900.97     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.83     |
| TotalNEpisodes       | 17929      |
| TotalNSamples        | 1.2986e+06 |
| ExplainedVariance    | 0.45732    |
-------------------------------------
[2018-01-21 13:13:40.251231 UTC] Saving snapshot
[2018-01-21 13:13:40.260950 UTC] Starting iteration 261
[2018-01-21 13:13:40.261188 UTC] Start collecting samples
[2018-01-21 13:13:44.978916 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:45.122018 UTC] Performing policy update
[2018-01-21 13:13:45.126733 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:45.262686 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:46.722814 UTC] Performing line search
[2018-01-21 13:13:46.907437 UTC] Updating baseline
[2018-01-21 13:13:48.976119 UTC] Computing logging information
-------------------------------------
| Iteration            | 261        |
| ExpectedImprovement  | 0.015298   |
| ActualImprovement    | 0.014555   |
| ImprovementRatio     | 0.95145    |
| MeanKL               | 0.0069895  |
| Entropy              | 0.92829    |
| Perplexity           | 2.5302     |
| AveragePolicyStd     | 0.28653    |
| AveragePolicyStd[0]  | 0.27548    |
| AveragePolicyStd[1]  | 0.38278    |
| AveragePolicyStd[2]  | 0.21729    |
| AveragePolicyStd[3]  | 0.29707    |
| AveragePolicyStd[4]  | 0.28163    |
| AveragePolicyStd[5]  | 0.26492    |
| AverageReturn        | 772.36     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 211.97     |
| AverageEpisodeLength | 900.97     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.83     |
| TotalNEpisodes       | 17934      |
| TotalNSamples        | 1.3036e+06 |
| ExplainedVariance    | 0.062682   |
-------------------------------------
[2018-01-21 13:13:49.685541 UTC] Saving snapshot
[2018-01-21 13:13:49.685789 UTC] Starting iteration 262
[2018-01-21 13:13:49.685942 UTC] Start collecting samples
[2018-01-21 13:13:55.103240 UTC] Computing input variables for policy optimization
[2018-01-21 13:13:55.258991 UTC] Performing policy update
[2018-01-21 13:13:55.259583 UTC] Computing gradient in Euclidean space
[2018-01-21 13:13:55.381618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:13:56.807874 UTC] Performing line search
[2018-01-21 13:13:57.013206 UTC] Updating baseline
[2018-01-21 13:13:58.820247 UTC] Computing logging information
-------------------------------------
| Iteration            | 262        |
| ExpectedImprovement  | 0.014065   |
| ActualImprovement    | 0.013652   |
| ImprovementRatio     | 0.97063    |
| MeanKL               | 0.0071867  |
| Entropy              | 0.91809    |
| Perplexity           | 2.5045     |
| AveragePolicyStd     | 0.28588    |
| AveragePolicyStd[0]  | 0.27602    |
| AveragePolicyStd[1]  | 0.37985    |
| AveragePolicyStd[2]  | 0.21825    |
| AveragePolicyStd[3]  | 0.29805    |
| AveragePolicyStd[4]  | 0.27805    |
| AveragePolicyStd[5]  | 0.26508    |
| AverageReturn        | 780.78     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 202.16     |
| AverageEpisodeLength | 909.65     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.27     |
| TotalNEpisodes       | 17942      |
| TotalNSamples        | 1.3116e+06 |
| ExplainedVariance    | 0.13083    |
-------------------------------------
[2018-01-21 13:13:59.471382 UTC] Saving snapshot
[2018-01-21 13:13:59.471564 UTC] Starting iteration 263
[2018-01-21 13:13:59.471717 UTC] Start collecting samples
[2018-01-21 13:14:04.211649 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:04.340998 UTC] Performing policy update
[2018-01-21 13:14:04.342082 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:04.459617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:05.860735 UTC] Performing line search
[2018-01-21 13:14:06.056096 UTC] Updating baseline
[2018-01-21 13:14:07.914404 UTC] Computing logging information
------------------------------------
| Iteration            | 263       |
| ExpectedImprovement  | 0.01327   |
| ActualImprovement    | 0.012894  |
| ImprovementRatio     | 0.97173   |
| MeanKL               | 0.007487  |
| Entropy              | 0.91162   |
| Perplexity           | 2.4883    |
| AveragePolicyStd     | 0.2856    |
| AveragePolicyStd[0]  | 0.27471   |
| AveragePolicyStd[1]  | 0.37924   |
| AveragePolicyStd[2]  | 0.21793   |
| AveragePolicyStd[3]  | 0.29987   |
| AveragePolicyStd[4]  | 0.27779   |
| AveragePolicyStd[5]  | 0.26408   |
| AverageReturn        | 769.39    |
| MinReturn            | 50.924    |
| MaxReturn            | 913.79    |
| StdReturn            | 220.58    |
| AverageEpisodeLength | 893.44    |
| MinEpisodeLength     | 89        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 249.16    |
| TotalNEpisodes       | 17947     |
| TotalNSamples        | 1.315e+06 |
| ExplainedVariance    | 0.31484   |
------------------------------------
[2018-01-21 13:14:08.618132 UTC] Saving snapshot
[2018-01-21 13:14:08.618367 UTC] Starting iteration 264
[2018-01-21 13:14:08.618539 UTC] Start collecting samples
[2018-01-21 13:14:13.436705 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:13.576728 UTC] Performing policy update
[2018-01-21 13:14:13.577459 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:13.702057 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:15.113537 UTC] Performing line search
[2018-01-21 13:14:15.368079 UTC] Updating baseline
[2018-01-21 13:14:17.756586 UTC] Computing logging information
-------------------------------------
| Iteration            | 264        |
| ExpectedImprovement  | 0.014425   |
| ActualImprovement    | 0.013793   |
| ImprovementRatio     | 0.95618    |
| MeanKL               | 0.0075823  |
| Entropy              | 0.92723    |
| Perplexity           | 2.5275     |
| AveragePolicyStd     | 0.28639    |
| AveragePolicyStd[0]  | 0.27788    |
| AveragePolicyStd[1]  | 0.38034    |
| AveragePolicyStd[2]  | 0.21756    |
| AveragePolicyStd[3]  | 0.30018    |
| AveragePolicyStd[4]  | 0.27771    |
| AveragePolicyStd[5]  | 0.26467    |
| AverageReturn        | 762.51     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 228.88     |
| AverageEpisodeLength | 885.43     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 258.31     |
| TotalNEpisodes       | 17951      |
| TotalNSamples        | 1.3182e+06 |
| ExplainedVariance    | 0.22025    |
-------------------------------------
[2018-01-21 13:14:18.396214 UTC] Saving snapshot
[2018-01-21 13:14:18.396493 UTC] Starting iteration 265
[2018-01-21 13:14:18.396669 UTC] Start collecting samples
[2018-01-21 13:14:23.209898 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:23.346181 UTC] Performing policy update
[2018-01-21 13:14:23.346828 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:23.463580 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:24.960367 UTC] Performing line search
[2018-01-21 13:14:25.155411 UTC] Updating baseline
[2018-01-21 13:14:27.228835 UTC] Computing logging information
-------------------------------------
| Iteration            | 265        |
| ExpectedImprovement  | 0.014295   |
| ActualImprovement    | 0.013756   |
| ImprovementRatio     | 0.96229    |
| MeanKL               | 0.007061   |
| Entropy              | 0.92534    |
| Perplexity           | 2.5227     |
| AveragePolicyStd     | 0.28623    |
| AveragePolicyStd[0]  | 0.27767    |
| AveragePolicyStd[1]  | 0.3781     |
| AveragePolicyStd[2]  | 0.21695    |
| AveragePolicyStd[3]  | 0.30058    |
| AveragePolicyStd[4]  | 0.2791     |
| AveragePolicyStd[5]  | 0.265      |
| AverageReturn        | 768.24     |
| MinReturn            | 50.924     |
| MaxReturn            | 913.79     |
| StdReturn            | 227.7      |
| AverageEpisodeLength | 889.87     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.42     |
| TotalNEpisodes       | 17959      |
| TotalNSamples        | 1.3262e+06 |
| ExplainedVariance    | 0.04905    |
-------------------------------------
[2018-01-21 13:14:27.910664 UTC] Saving snapshot
[2018-01-21 13:14:27.910917 UTC] Starting iteration 266
[2018-01-21 13:14:27.911071 UTC] Start collecting samples
[2018-01-21 13:14:32.813007 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:32.940959 UTC] Performing policy update
[2018-01-21 13:14:32.941572 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:33.081209 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:34.526580 UTC] Performing line search
[2018-01-21 13:14:34.731114 UTC] Updating baseline
[2018-01-21 13:14:36.814855 UTC] Computing logging information
-------------------------------------
| Iteration            | 266        |
| ExpectedImprovement  | 0.014953   |
| ActualImprovement    | 0.014694   |
| ImprovementRatio     | 0.98269    |
| MeanKL               | 0.0076124  |
| Entropy              | 0.93484    |
| Perplexity           | 2.5468     |
| AveragePolicyStd     | 0.28673    |
| AveragePolicyStd[0]  | 0.27827    |
| AveragePolicyStd[1]  | 0.37967    |
| AveragePolicyStd[2]  | 0.2169     |
| AveragePolicyStd[3]  | 0.29982    |
| AveragePolicyStd[4]  | 0.28018    |
| AveragePolicyStd[5]  | 0.26555    |
| AverageReturn        | 767.16     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 227        |
| AverageEpisodeLength | 885.84     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.68     |
| TotalNEpisodes       | 17965      |
| TotalNSamples        | 1.3311e+06 |
| ExplainedVariance    | 0.33984    |
-------------------------------------
[2018-01-21 13:14:37.509108 UTC] Saving snapshot
[2018-01-21 13:14:37.509425 UTC] Starting iteration 267
[2018-01-21 13:14:37.509647 UTC] Start collecting samples
[2018-01-21 13:14:42.245131 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:42.400326 UTC] Performing policy update
[2018-01-21 13:14:42.403306 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:42.524441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:43.937298 UTC] Performing line search
[2018-01-21 13:14:44.125210 UTC] Updating baseline
[2018-01-21 13:14:46.336517 UTC] Computing logging information
-------------------------------------
| Iteration            | 267        |
| ExpectedImprovement  | 0.018369   |
| ActualImprovement    | 0.015952   |
| ImprovementRatio     | 0.8684     |
| MeanKL               | 0.0069606  |
| Entropy              | 0.92752    |
| Perplexity           | 2.5282     |
| AveragePolicyStd     | 0.28657    |
| AveragePolicyStd[0]  | 0.27796    |
| AveragePolicyStd[1]  | 0.38242    |
| AveragePolicyStd[2]  | 0.21565    |
| AveragePolicyStd[3]  | 0.29925    |
| AveragePolicyStd[4]  | 0.27901    |
| AveragePolicyStd[5]  | 0.26513    |
| AverageReturn        | 767.02     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 226.93     |
| AverageEpisodeLength | 885.84     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.68     |
| TotalNEpisodes       | 17967      |
| TotalNSamples        | 1.3331e+06 |
| ExplainedVariance    | 0.040852   |
-------------------------------------
[2018-01-21 13:14:47.020223 UTC] Saving snapshot
[2018-01-21 13:14:47.020470 UTC] Starting iteration 268
[2018-01-21 13:14:47.020627 UTC] Start collecting samples
[2018-01-21 13:14:51.850272 UTC] Computing input variables for policy optimization
[2018-01-21 13:14:51.978201 UTC] Performing policy update
[2018-01-21 13:14:51.978810 UTC] Computing gradient in Euclidean space
[2018-01-21 13:14:52.095340 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:14:53.509983 UTC] Performing line search
[2018-01-21 13:14:53.696198 UTC] Updating baseline
[2018-01-21 13:14:56.003375 UTC] Computing logging information
-------------------------------------
| Iteration            | 268        |
| ExpectedImprovement  | 0.015088   |
| ActualImprovement    | 0.014476   |
| ImprovementRatio     | 0.95944    |
| MeanKL               | 0.00732    |
| Entropy              | 0.92294    |
| Perplexity           | 2.5167     |
| AveragePolicyStd     | 0.28631    |
| AveragePolicyStd[0]  | 0.27826    |
| AveragePolicyStd[1]  | 0.38149    |
| AveragePolicyStd[2]  | 0.216      |
| AveragePolicyStd[3]  | 0.2987     |
| AveragePolicyStd[4]  | 0.28023    |
| AveragePolicyStd[5]  | 0.2632     |
| AverageReturn        | 768.42     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 227.58     |
| AverageEpisodeLength | 885.84     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 255.68     |
| TotalNEpisodes       | 17973      |
| TotalNSamples        | 1.3391e+06 |
| ExplainedVariance    | 0.071461   |
-------------------------------------
[2018-01-21 13:14:56.612345 UTC] Saving snapshot
[2018-01-21 13:14:56.612606 UTC] Starting iteration 269
[2018-01-21 13:14:56.612776 UTC] Start collecting samples
[2018-01-21 13:15:01.291850 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:01.430437 UTC] Performing policy update
[2018-01-21 13:15:01.431622 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:01.555238 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:02.991363 UTC] Performing line search
[2018-01-21 13:15:03.189873 UTC] Updating baseline
[2018-01-21 13:15:06.078006 UTC] Computing logging information
-------------------------------------
| Iteration            | 269        |
| ExpectedImprovement  | 0.015913   |
| ActualImprovement    | 0.014343   |
| ImprovementRatio     | 0.90137    |
| MeanKL               | 0.0072493  |
| Entropy              | 0.89092    |
| Perplexity           | 2.4374     |
| AveragePolicyStd     | 0.28478    |
| AveragePolicyStd[0]  | 0.27748    |
| AveragePolicyStd[1]  | 0.37898    |
| AveragePolicyStd[2]  | 0.2142     |
| AveragePolicyStd[3]  | 0.29697    |
| AveragePolicyStd[4]  | 0.27748    |
| AveragePolicyStd[5]  | 0.26356    |
| AverageReturn        | 775.24     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 223.69     |
| AverageEpisodeLength | 892.48     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247.93     |
| TotalNEpisodes       | 17979      |
| TotalNSamples        | 1.3447e+06 |
| ExplainedVariance    | 0.060496   |
-------------------------------------
[2018-01-21 13:15:06.736541 UTC] Saving snapshot
[2018-01-21 13:15:06.736840 UTC] Starting iteration 270
[2018-01-21 13:15:06.737060 UTC] Start collecting samples
[2018-01-21 13:15:11.357610 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:11.492132 UTC] Performing policy update
[2018-01-21 13:15:11.492880 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:11.622494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:13.066873 UTC] Performing line search
[2018-01-21 13:15:13.257395 UTC] Updating baseline
[2018-01-21 13:15:15.925083 UTC] Computing logging information
-------------------------------------
| Iteration            | 270        |
| ExpectedImprovement  | 0.01384    |
| ActualImprovement    | 0.013386   |
| ImprovementRatio     | 0.96717    |
| MeanKL               | 0.0080955  |
| Entropy              | 0.88428    |
| Perplexity           | 2.4212     |
| AveragePolicyStd     | 0.28443    |
| AveragePolicyStd[0]  | 0.27658    |
| AveragePolicyStd[1]  | 0.37805    |
| AveragePolicyStd[2]  | 0.214      |
| AveragePolicyStd[3]  | 0.29641    |
| AveragePolicyStd[4]  | 0.27751    |
| AveragePolicyStd[5]  | 0.26403    |
| AverageReturn        | 775.92     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 224.03     |
| AverageEpisodeLength | 892.48     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247.93     |
| TotalNEpisodes       | 17983      |
| TotalNSamples        | 1.3487e+06 |
| ExplainedVariance    | 0.023822   |
-------------------------------------
[2018-01-21 13:15:16.549520 UTC] Saving snapshot
[2018-01-21 13:15:16.559538 UTC] Starting iteration 271
[2018-01-21 13:15:16.559785 UTC] Start collecting samples
[2018-01-21 13:15:21.360498 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:21.530751 UTC] Performing policy update
[2018-01-21 13:15:21.531390 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:21.649107 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:23.154562 UTC] Performing line search
[2018-01-21 13:15:23.341747 UTC] Updating baseline
[2018-01-21 13:15:25.477362 UTC] Computing logging information
-------------------------------------
| Iteration            | 271        |
| ExpectedImprovement  | 0.013895   |
| ActualImprovement    | 0.014312   |
| ImprovementRatio     | 1.03       |
| MeanKL               | 0.0072787  |
| Entropy              | 0.86116    |
| Perplexity           | 2.3659     |
| AveragePolicyStd     | 0.28338    |
| AveragePolicyStd[0]  | 0.27607    |
| AveragePolicyStd[1]  | 0.37654    |
| AveragePolicyStd[2]  | 0.21213    |
| AveragePolicyStd[3]  | 0.29511    |
| AveragePolicyStd[4]  | 0.2765     |
| AveragePolicyStd[5]  | 0.26391    |
| AverageReturn        | 786.32     |
| MinReturn            | 50.924     |
| MaxReturn            | 916.46     |
| StdReturn            | 214.93     |
| AverageEpisodeLength | 903.37     |
| MinEpisodeLength     | 89         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.9      |
| TotalNEpisodes       | 17986      |
| TotalNSamples        | 1.3517e+06 |
| ExplainedVariance    | -0.05623   |
-------------------------------------
[2018-01-21 13:15:26.163317 UTC] Saving snapshot
[2018-01-21 13:15:26.163561 UTC] Starting iteration 272
[2018-01-21 13:15:26.163731 UTC] Start collecting samples
[2018-01-21 13:15:31.025877 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:31.171701 UTC] Performing policy update
[2018-01-21 13:15:31.173179 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:31.302589 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:32.789544 UTC] Performing line search
[2018-01-21 13:15:32.981225 UTC] Updating baseline
[2018-01-21 13:15:35.054566 UTC] Computing logging information
-------------------------------------
| Iteration            | 272        |
| ExpectedImprovement  | 0.01538    |
| ActualImprovement    | 0.014248   |
| ImprovementRatio     | 0.92642    |
| MeanKL               | 0.0073915  |
| Entropy              | 0.86175    |
| Perplexity           | 2.3673     |
| AveragePolicyStd     | 0.28355    |
| AveragePolicyStd[0]  | 0.27484    |
| AveragePolicyStd[1]  | 0.37874    |
| AveragePolicyStd[2]  | 0.2113     |
| AveragePolicyStd[3]  | 0.29626    |
| AveragePolicyStd[4]  | 0.2757     |
| AveragePolicyStd[5]  | 0.2645     |
| AverageReturn        | 811.67     |
| MinReturn            | 125.05     |
| MaxReturn            | 923.25     |
| StdReturn            | 190        |
| AverageEpisodeLength | 926.3      |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.2      |
| TotalNEpisodes       | 17996      |
| TotalNSamples        | 1.3609e+06 |
| ExplainedVariance    | 0.10889    |
-------------------------------------
[2018-01-21 13:15:35.762146 UTC] Saving snapshot
[2018-01-21 13:15:35.762394 UTC] Starting iteration 273
[2018-01-21 13:15:35.762557 UTC] Start collecting samples
[2018-01-21 13:15:40.607791 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:40.744385 UTC] Performing policy update
[2018-01-21 13:15:40.745101 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:40.868317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:42.286092 UTC] Performing line search
[2018-01-21 13:15:42.471493 UTC] Updating baseline
[2018-01-21 13:15:44.844685 UTC] Computing logging information
-------------------------------------
| Iteration            | 273        |
| ExpectedImprovement  | 0.013363   |
| ActualImprovement    | 0.012683   |
| ImprovementRatio     | 0.94906    |
| MeanKL               | 0.0072594  |
| Entropy              | 0.87954    |
| Perplexity           | 2.4098     |
| AveragePolicyStd     | 0.28435    |
| AveragePolicyStd[0]  | 0.27715    |
| AveragePolicyStd[1]  | 0.37912    |
| AveragePolicyStd[2]  | 0.2121     |
| AveragePolicyStd[3]  | 0.29677    |
| AveragePolicyStd[4]  | 0.27571    |
| AveragePolicyStd[5]  | 0.26525    |
| AverageReturn        | 813.11     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 190.62     |
| AverageEpisodeLength | 926.3      |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.2      |
| TotalNEpisodes       | 18000      |
| TotalNSamples        | 1.3649e+06 |
| ExplainedVariance    | 0.057253   |
-------------------------------------
[2018-01-21 13:15:45.484458 UTC] Saving snapshot
[2018-01-21 13:15:45.484772 UTC] Starting iteration 274
[2018-01-21 13:15:45.484941 UTC] Start collecting samples
[2018-01-21 13:15:50.077768 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:50.216305 UTC] Performing policy update
[2018-01-21 13:15:50.216923 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:50.339910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:15:51.814408 UTC] Performing line search
[2018-01-21 13:15:52.022672 UTC] Updating baseline
[2018-01-21 13:15:53.956541 UTC] Computing logging information
-------------------------------------
| Iteration            | 274        |
| ExpectedImprovement  | 0.01698    |
| ActualImprovement    | 0.016842   |
| ImprovementRatio     | 0.99187    |
| MeanKL               | 0.0070192  |
| Entropy              | 0.87503    |
| Perplexity           | 2.3989     |
| AveragePolicyStd     | 0.28417    |
| AveragePolicyStd[0]  | 0.27593    |
| AveragePolicyStd[1]  | 0.37917    |
| AveragePolicyStd[2]  | 0.21142    |
| AveragePolicyStd[3]  | 0.29648    |
| AveragePolicyStd[4]  | 0.27547    |
| AveragePolicyStd[5]  | 0.26653    |
| AverageReturn        | 814.78     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 190.99     |
| AverageEpisodeLength | 925.88     |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.13     |
| TotalNEpisodes       | 18003      |
| TotalNSamples        | 1.3679e+06 |
| ExplainedVariance    | 0.2738     |
-------------------------------------
[2018-01-21 13:15:54.657682 UTC] Saving snapshot
[2018-01-21 13:15:54.657929 UTC] Starting iteration 275
[2018-01-21 13:15:54.658091 UTC] Start collecting samples
[2018-01-21 13:15:59.519654 UTC] Computing input variables for policy optimization
[2018-01-21 13:15:59.671917 UTC] Performing policy update
[2018-01-21 13:15:59.672551 UTC] Computing gradient in Euclidean space
[2018-01-21 13:15:59.794654 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:01.199061 UTC] Performing line search
[2018-01-21 13:16:01.383239 UTC] Updating baseline
[2018-01-21 13:16:03.304696 UTC] Computing logging information
-------------------------------------
| Iteration            | 275        |
| ExpectedImprovement  | 0.013771   |
| ActualImprovement    | 0.013042   |
| ImprovementRatio     | 0.94706    |
| MeanKL               | 0.006871   |
| Entropy              | 0.87064    |
| Perplexity           | 2.3884     |
| AveragePolicyStd     | 0.28403    |
| AveragePolicyStd[0]  | 0.27594    |
| AveragePolicyStd[1]  | 0.37977    |
| AveragePolicyStd[2]  | 0.21066    |
| AveragePolicyStd[3]  | 0.29667    |
| AveragePolicyStd[4]  | 0.27479    |
| AveragePolicyStd[5]  | 0.26638    |
| AverageReturn        | 824.88     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 183.15     |
| AverageEpisodeLength | 932.83     |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.67     |
| TotalNEpisodes       | 18010      |
| TotalNSamples        | 1.3749e+06 |
| ExplainedVariance    | 0.00094183 |
-------------------------------------
[2018-01-21 13:16:03.992926 UTC] Saving snapshot
[2018-01-21 13:16:03.993194 UTC] Starting iteration 276
[2018-01-21 13:16:03.993381 UTC] Start collecting samples
[2018-01-21 13:16:08.586839 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:08.716915 UTC] Performing policy update
[2018-01-21 13:16:08.717516 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:08.836789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:10.281568 UTC] Performing line search
[2018-01-21 13:16:10.484383 UTC] Updating baseline
[2018-01-21 13:16:12.992382 UTC] Computing logging information
-------------------------------------
| Iteration            | 276        |
| ExpectedImprovement  | 0.013622   |
| ActualImprovement    | 0.013109   |
| ImprovementRatio     | 0.96235    |
| MeanKL               | 0.0074219  |
| Entropy              | 0.87644    |
| Perplexity           | 2.4023     |
| AveragePolicyStd     | 0.28435    |
| AveragePolicyStd[0]  | 0.2745     |
| AveragePolicyStd[1]  | 0.37963    |
| AveragePolicyStd[2]  | 0.21013    |
| AveragePolicyStd[3]  | 0.29887    |
| AveragePolicyStd[4]  | 0.27641    |
| AveragePolicyStd[5]  | 0.26655    |
| AverageReturn        | 828.14     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 182.78     |
| AverageEpisodeLength | 935.19     |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.05     |
| TotalNEpisodes       | 18015      |
| TotalNSamples        | 1.3799e+06 |
| ExplainedVariance    | 0.052361   |
-------------------------------------
[2018-01-21 13:16:13.710502 UTC] Saving snapshot
[2018-01-21 13:16:13.710746 UTC] Starting iteration 277
[2018-01-21 13:16:13.710954 UTC] Start collecting samples
[2018-01-21 13:16:18.536005 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:18.681223 UTC] Performing policy update
[2018-01-21 13:16:18.681881 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:18.798820 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:20.232916 UTC] Performing line search
[2018-01-21 13:16:20.440877 UTC] Updating baseline
[2018-01-21 13:16:22.495513 UTC] Computing logging information
-------------------------------------
| Iteration            | 277        |
| ExpectedImprovement  | 0.015177   |
| ActualImprovement    | 0.014202   |
| ImprovementRatio     | 0.9358     |
| MeanKL               | 0.0075744  |
| Entropy              | 0.8724     |
| Perplexity           | 2.3927     |
| AveragePolicyStd     | 0.28418    |
| AveragePolicyStd[0]  | 0.27476    |
| AveragePolicyStd[1]  | 0.38083    |
| AveragePolicyStd[2]  | 0.21005    |
| AveragePolicyStd[3]  | 0.29593    |
| AveragePolicyStd[4]  | 0.27604    |
| AveragePolicyStd[5]  | 0.26748    |
| AverageReturn        | 834.24     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 177.88     |
| AverageEpisodeLength | 940.73     |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.94     |
| TotalNEpisodes       | 18019      |
| TotalNSamples        | 1.3839e+06 |
| ExplainedVariance    | 0.013049   |
-------------------------------------
[2018-01-21 13:16:23.147190 UTC] Saving snapshot
[2018-01-21 13:16:23.147478 UTC] Starting iteration 278
[2018-01-21 13:16:23.147662 UTC] Start collecting samples
[2018-01-21 13:16:27.879661 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:28.011720 UTC] Performing policy update
[2018-01-21 13:16:28.012342 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:28.131983 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:29.547097 UTC] Performing line search
[2018-01-21 13:16:29.730544 UTC] Updating baseline
[2018-01-21 13:16:32.366812 UTC] Computing logging information
-------------------------------------
| Iteration            | 278        |
| ExpectedImprovement  | 0.015394   |
| ActualImprovement    | 0.014262   |
| ImprovementRatio     | 0.92645    |
| MeanKL               | 0.007574   |
| Entropy              | 0.87087    |
| Perplexity           | 2.389      |
| AveragePolicyStd     | 0.28422    |
| AveragePolicyStd[0]  | 0.27518    |
| AveragePolicyStd[1]  | 0.3815     |
| AveragePolicyStd[2]  | 0.20893    |
| AveragePolicyStd[3]  | 0.29696    |
| AveragePolicyStd[4]  | 0.27609    |
| AveragePolicyStd[5]  | 0.26663    |
| AverageReturn        | 838.59     |
| MinReturn            | 125.05     |
| MaxReturn            | 937.99     |
| StdReturn            | 177.16     |
| AverageEpisodeLength | 943.4      |
| MinEpisodeLength     | 152        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.89     |
| TotalNEpisodes       | 18024      |
| TotalNSamples        | 1.3889e+06 |
| ExplainedVariance    | -0.028903  |
-------------------------------------
[2018-01-21 13:16:33.025672 UTC] Saving snapshot
[2018-01-21 13:16:33.025925 UTC] Starting iteration 279
[2018-01-21 13:16:33.026086 UTC] Start collecting samples
[2018-01-21 13:16:38.034081 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:38.154941 UTC] Performing policy update
[2018-01-21 13:16:38.155925 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:38.269743 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:39.744128 UTC] Performing line search
[2018-01-21 13:16:39.955874 UTC] Updating baseline
[2018-01-21 13:16:42.081376 UTC] Computing logging information
-------------------------------------
| Iteration            | 279        |
| ExpectedImprovement  | 0.014764   |
| ActualImprovement    | 0.014117   |
| ImprovementRatio     | 0.95616    |
| MeanKL               | 0.0073769  |
| Entropy              | 0.85324    |
| Perplexity           | 2.3472     |
| AveragePolicyStd     | 0.28331    |
| AveragePolicyStd[0]  | 0.27659    |
| AveragePolicyStd[1]  | 0.37903    |
| AveragePolicyStd[2]  | 0.20851    |
| AveragePolicyStd[3]  | 0.29601    |
| AveragePolicyStd[4]  | 0.27393    |
| AveragePolicyStd[5]  | 0.2658     |
| AverageReturn        | 848.81     |
| MinReturn            | 125.05     |
| MaxReturn            | 945.72     |
| StdReturn            | 163.25     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 155        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.64     |
| TotalNEpisodes       | 18031      |
| TotalNSamples        | 1.3959e+06 |
| ExplainedVariance    | 0.037815   |
-------------------------------------
[2018-01-21 13:16:42.710857 UTC] Saving snapshot
[2018-01-21 13:16:42.711114 UTC] Starting iteration 280
[2018-01-21 13:16:42.711298 UTC] Start collecting samples
[2018-01-21 13:16:47.516877 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:47.667912 UTC] Performing policy update
[2018-01-21 13:16:47.668562 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:47.790054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:49.259202 UTC] Performing line search
[2018-01-21 13:16:49.448899 UTC] Updating baseline
[2018-01-21 13:16:51.403268 UTC] Computing logging information
-------------------------------------
| Iteration            | 280        |
| ExpectedImprovement  | 0.016585   |
| ActualImprovement    | 0.015762   |
| ImprovementRatio     | 0.95038    |
| MeanKL               | 0.006999   |
| Entropy              | 0.83839    |
| Perplexity           | 2.3127     |
| AveragePolicyStd     | 0.28257    |
| AveragePolicyStd[0]  | 0.27693    |
| AveragePolicyStd[1]  | 0.3779     |
| AveragePolicyStd[2]  | 0.20823    |
| AveragePolicyStd[3]  | 0.29371    |
| AveragePolicyStd[4]  | 0.27385    |
| AveragePolicyStd[5]  | 0.26483    |
| AverageReturn        | 849.95     |
| MinReturn            | 125.05     |
| MaxReturn            | 945.72     |
| StdReturn            | 163.63     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 155        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.64     |
| TotalNEpisodes       | 18034      |
| TotalNSamples        | 1.3989e+06 |
| ExplainedVariance    | 0.21363    |
-------------------------------------
[2018-01-21 13:16:52.091062 UTC] Saving snapshot
[2018-01-21 13:16:52.101654 UTC] Starting iteration 281
[2018-01-21 13:16:52.101886 UTC] Start collecting samples
[2018-01-21 13:16:57.029198 UTC] Computing input variables for policy optimization
[2018-01-21 13:16:57.173133 UTC] Performing policy update
[2018-01-21 13:16:57.173996 UTC] Computing gradient in Euclidean space
[2018-01-21 13:16:57.292856 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:16:58.722298 UTC] Performing line search
[2018-01-21 13:16:58.915123 UTC] Updating baseline
[2018-01-21 13:17:00.979647 UTC] Computing logging information
-------------------------------------
| Iteration            | 281        |
| ExpectedImprovement  | 0.015147   |
| ActualImprovement    | 0.014512   |
| ImprovementRatio     | 0.95811    |
| MeanKL               | 0.0079998  |
| Entropy              | 0.83023    |
| Perplexity           | 2.2938     |
| AveragePolicyStd     | 0.28211    |
| AveragePolicyStd[0]  | 0.27513    |
| AveragePolicyStd[1]  | 0.37614    |
| AveragePolicyStd[2]  | 0.20845    |
| AveragePolicyStd[3]  | 0.29351    |
| AveragePolicyStd[4]  | 0.27463    |
| AveragePolicyStd[5]  | 0.26479    |
| AverageReturn        | 851.83     |
| MinReturn            | 125.05     |
| MaxReturn            | 945.72     |
| StdReturn            | 164.17     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 155        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.64     |
| TotalNEpisodes       | 18039      |
| TotalNSamples        | 1.4039e+06 |
| ExplainedVariance    | 0.025026   |
-------------------------------------
[2018-01-21 13:17:01.680325 UTC] Saving snapshot
[2018-01-21 13:17:01.680601 UTC] Starting iteration 282
[2018-01-21 13:17:01.680775 UTC] Start collecting samples
[2018-01-21 13:17:06.495967 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:06.646259 UTC] Performing policy update
[2018-01-21 13:17:06.647038 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:06.765364 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:08.216270 UTC] Performing line search
[2018-01-21 13:17:08.407088 UTC] Updating baseline
[2018-01-21 13:17:10.629915 UTC] Computing logging information
-------------------------------------
| Iteration            | 282        |
| ExpectedImprovement  | 0.015996   |
| ActualImprovement    | 0.015211   |
| ImprovementRatio     | 0.95092    |
| MeanKL               | 0.0075286  |
| Entropy              | 0.82372    |
| Perplexity           | 2.279      |
| AveragePolicyStd     | 0.28172    |
| AveragePolicyStd[0]  | 0.27543    |
| AveragePolicyStd[1]  | 0.3741     |
| AveragePolicyStd[2]  | 0.20829    |
| AveragePolicyStd[3]  | 0.29277    |
| AveragePolicyStd[4]  | 0.27461    |
| AveragePolicyStd[5]  | 0.26509    |
| AverageReturn        | 862.75     |
| MinReturn            | 152.16     |
| MaxReturn            | 945.72     |
| StdReturn            | 132.05     |
| AverageEpisodeLength | 968.87     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.13     |
| TotalNEpisodes       | 18047      |
| TotalNSamples        | 1.4119e+06 |
| ExplainedVariance    | 0.10661    |
-------------------------------------
[2018-01-21 13:17:11.309375 UTC] Saving snapshot
[2018-01-21 13:17:11.309604 UTC] Starting iteration 283
[2018-01-21 13:17:11.309780 UTC] Start collecting samples
[2018-01-21 13:17:15.825240 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:15.954396 UTC] Performing policy update
[2018-01-21 13:17:15.955091 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:16.076611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:17.474643 UTC] Performing line search
[2018-01-21 13:17:17.663231 UTC] Updating baseline
[2018-01-21 13:17:20.301199 UTC] Computing logging information
-------------------------------------
| Iteration            | 283        |
| ExpectedImprovement  | 0.015431   |
| ActualImprovement    | 0.014996   |
| ImprovementRatio     | 0.97181    |
| MeanKL               | 0.0074561  |
| Entropy              | 0.81482    |
| Perplexity           | 2.2588     |
| AveragePolicyStd     | 0.28131    |
| AveragePolicyStd[0]  | 0.27628    |
| AveragePolicyStd[1]  | 0.37288    |
| AveragePolicyStd[2]  | 0.20745    |
| AveragePolicyStd[3]  | 0.29307    |
| AveragePolicyStd[4]  | 0.27364    |
| AveragePolicyStd[5]  | 0.26454    |
| AverageReturn        | 870.01     |
| MinReturn            | 152.4      |
| MaxReturn            | 945.72     |
| StdReturn            | 111.07     |
| AverageEpisodeLength | 976.88     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 110.81     |
| TotalNEpisodes       | 18048      |
| TotalNSamples        | 1.4129e+06 |
| ExplainedVariance    | 0.0021384  |
-------------------------------------
[2018-01-21 13:17:20.998148 UTC] Saving snapshot
[2018-01-21 13:17:20.998464 UTC] Starting iteration 284
[2018-01-21 13:17:20.998638 UTC] Start collecting samples
[2018-01-21 13:17:25.498904 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:25.635189 UTC] Performing policy update
[2018-01-21 13:17:25.635841 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:25.760304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:27.246231 UTC] Performing line search
[2018-01-21 13:17:27.435535 UTC] Updating baseline
[2018-01-21 13:17:29.738627 UTC] Computing logging information
-------------------------------------
| Iteration            | 284        |
| ExpectedImprovement  | 0.015639   |
| ActualImprovement    | 0.014115   |
| ImprovementRatio     | 0.90257    |
| MeanKL               | 0.0072073  |
| Entropy              | 0.80706    |
| Perplexity           | 2.2413     |
| AveragePolicyStd     | 0.28084    |
| AveragePolicyStd[0]  | 0.27754    |
| AveragePolicyStd[1]  | 0.37128    |
| AveragePolicyStd[2]  | 0.20811    |
| AveragePolicyStd[3]  | 0.29136    |
| AveragePolicyStd[4]  | 0.27286    |
| AveragePolicyStd[5]  | 0.26388    |
| AverageReturn        | 870.09     |
| MinReturn            | 152.4      |
| MaxReturn            | 945.72     |
| StdReturn            | 112.38     |
| AverageEpisodeLength | 975.01     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.97     |
| TotalNEpisodes       | 18054      |
| TotalNSamples        | 1.4187e+06 |
| ExplainedVariance    | -0.033948  |
-------------------------------------
[2018-01-21 13:17:30.382062 UTC] Saving snapshot
[2018-01-21 13:17:30.382336 UTC] Starting iteration 285
[2018-01-21 13:17:30.382544 UTC] Start collecting samples
[2018-01-21 13:17:35.442944 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:35.593692 UTC] Performing policy update
[2018-01-21 13:17:35.594602 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:35.678528 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:36.943225 UTC] Performing line search
[2018-01-21 13:17:37.157745 UTC] Updating baseline
[2018-01-21 13:17:39.511877 UTC] Computing logging information
-------------------------------------
| Iteration            | 285        |
| ExpectedImprovement  | 0.017288   |
| ActualImprovement    | 0.015901   |
| ImprovementRatio     | 0.91979    |
| MeanKL               | 0.0070558  |
| Entropy              | 0.79724    |
| Perplexity           | 2.2194     |
| AveragePolicyStd     | 0.28044    |
| AveragePolicyStd[0]  | 0.27653    |
| AveragePolicyStd[1]  | 0.37175    |
| AveragePolicyStd[2]  | 0.20743    |
| AveragePolicyStd[3]  | 0.29082    |
| AveragePolicyStd[4]  | 0.2716     |
| AveragePolicyStd[5]  | 0.26449    |
| AverageReturn        | 865.96     |
| MinReturn            | 152.4      |
| MaxReturn            | 955.14     |
| StdReturn            | 128.26     |
| AverageEpisodeLength | 967.91     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.84     |
| TotalNEpisodes       | 18061      |
| TotalNSamples        | 1.4248e+06 |
| ExplainedVariance    | 0.43358    |
-------------------------------------
[2018-01-21 13:17:40.206899 UTC] Saving snapshot
[2018-01-21 13:17:40.207106 UTC] Starting iteration 286
[2018-01-21 13:17:40.207292 UTC] Start collecting samples
[2018-01-21 13:17:45.201673 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:45.327673 UTC] Performing policy update
[2018-01-21 13:17:45.328279 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:45.446554 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:46.852727 UTC] Performing line search
[2018-01-21 13:17:47.044040 UTC] Updating baseline
[2018-01-21 13:17:49.123269 UTC] Computing logging information
-------------------------------------
| Iteration            | 286        |
| ExpectedImprovement  | 0.0142     |
| ActualImprovement    | 0.01393    |
| ImprovementRatio     | 0.98094    |
| MeanKL               | 0.0073674  |
| Entropy              | 0.79158    |
| Perplexity           | 2.2069     |
| AveragePolicyStd     | 0.28011    |
| AveragePolicyStd[0]  | 0.27508    |
| AveragePolicyStd[1]  | 0.37101    |
| AveragePolicyStd[2]  | 0.20767    |
| AveragePolicyStd[3]  | 0.28959    |
| AveragePolicyStd[4]  | 0.27096    |
| AveragePolicyStd[5]  | 0.26635    |
| AverageReturn        | 858.37     |
| MinReturn            | 152.4      |
| MaxReturn            | 955.14     |
| StdReturn            | 150.59     |
| AverageEpisodeLength | 958.05     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.16     |
| TotalNEpisodes       | 18068      |
| TotalNSamples        | 1.4299e+06 |
| ExplainedVariance    | 0.31643    |
-------------------------------------
[2018-01-21 13:17:49.750194 UTC] Saving snapshot
[2018-01-21 13:17:49.750473 UTC] Starting iteration 287
[2018-01-21 13:17:49.750647 UTC] Start collecting samples
[2018-01-21 13:17:54.525985 UTC] Computing input variables for policy optimization
[2018-01-21 13:17:54.662994 UTC] Performing policy update
[2018-01-21 13:17:54.663594 UTC] Computing gradient in Euclidean space
[2018-01-21 13:17:54.791227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:17:56.275980 UTC] Performing line search
[2018-01-21 13:17:56.478266 UTC] Updating baseline
[2018-01-21 13:17:58.813680 UTC] Computing logging information
-------------------------------------
| Iteration            | 287        |
| ExpectedImprovement  | 0.017173   |
| ActualImprovement    | 0.015527   |
| ImprovementRatio     | 0.90412    |
| MeanKL               | 0.007587   |
| Entropy              | 0.78889    |
| Perplexity           | 2.2009     |
| AveragePolicyStd     | 0.2801     |
| AveragePolicyStd[0]  | 0.27548    |
| AveragePolicyStd[1]  | 0.37143    |
| AveragePolicyStd[2]  | 0.20614    |
| AveragePolicyStd[3]  | 0.29085    |
| AveragePolicyStd[4]  | 0.27073    |
| AveragePolicyStd[5]  | 0.26599    |
| AverageReturn        | 850.14     |
| MinReturn            | 132.94     |
| MaxReturn            | 955.14     |
| StdReturn            | 168.4      |
| AverageEpisodeLength | 946.87     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.94     |
| TotalNEpisodes       | 18074      |
| TotalNSamples        | 1.4348e+06 |
| ExplainedVariance    | 0.29841    |
-------------------------------------
[2018-01-21 13:17:59.556708 UTC] Saving snapshot
[2018-01-21 13:17:59.556955 UTC] Starting iteration 288
[2018-01-21 13:17:59.557131 UTC] Start collecting samples
[2018-01-21 13:18:04.312867 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:04.465535 UTC] Performing policy update
[2018-01-21 13:18:04.466694 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:04.594042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:06.043204 UTC] Performing line search
[2018-01-21 13:18:06.234679 UTC] Updating baseline
[2018-01-21 13:18:08.462033 UTC] Computing logging information
-------------------------------------
| Iteration            | 288        |
| ExpectedImprovement  | 0.015545   |
| ActualImprovement    | 0.014747   |
| ImprovementRatio     | 0.94863    |
| MeanKL               | 0.0075197  |
| Entropy              | 0.77398    |
| Perplexity           | 2.1684     |
| AveragePolicyStd     | 0.27942    |
| AveragePolicyStd[0]  | 0.27529    |
| AveragePolicyStd[1]  | 0.36982    |
| AveragePolicyStd[2]  | 0.20484    |
| AveragePolicyStd[3]  | 0.29074    |
| AveragePolicyStd[4]  | 0.26972    |
| AveragePolicyStd[5]  | 0.26613    |
| AverageReturn        | 853.96     |
| MinReturn            | 132.94     |
| MaxReturn            | 955.14     |
| StdReturn            | 163.53     |
| AverageEpisodeLength | 947.97     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.13     |
| TotalNEpisodes       | 18079      |
| TotalNSamples        | 1.4395e+06 |
| ExplainedVariance    | 0.24071    |
-------------------------------------
[2018-01-21 13:18:09.176429 UTC] Saving snapshot
[2018-01-21 13:18:09.176673 UTC] Starting iteration 289
[2018-01-21 13:18:09.176855 UTC] Start collecting samples
[2018-01-21 13:18:14.245045 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:14.384571 UTC] Performing policy update
[2018-01-21 13:18:14.385178 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:14.523574 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:16.050424 UTC] Performing line search
[2018-01-21 13:18:16.247062 UTC] Updating baseline
[2018-01-21 13:18:18.224444 UTC] Computing logging information
-------------------------------------
| Iteration            | 289        |
| ExpectedImprovement  | 0.014192   |
| ActualImprovement    | 0.013253   |
| ImprovementRatio     | 0.93382    |
| MeanKL               | 0.0077301  |
| Entropy              | 0.78268    |
| Perplexity           | 2.1873     |
| AveragePolicyStd     | 0.27981    |
| AveragePolicyStd[0]  | 0.27512    |
| AveragePolicyStd[1]  | 0.37047    |
| AveragePolicyStd[2]  | 0.20579    |
| AveragePolicyStd[3]  | 0.29183    |
| AveragePolicyStd[4]  | 0.26944    |
| AveragePolicyStd[5]  | 0.26619    |
| AverageReturn        | 849.13     |
| MinReturn            | 132.56     |
| MaxReturn            | 955.14     |
| StdReturn            | 179.42     |
| AverageEpisodeLength | 939.35     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.6      |
| TotalNEpisodes       | 18086      |
| TotalNSamples        | 1.4457e+06 |
| ExplainedVariance    | 0.01016    |
-------------------------------------
[2018-01-21 13:18:18.850171 UTC] Saving snapshot
[2018-01-21 13:18:18.850377 UTC] Starting iteration 290
[2018-01-21 13:18:18.850582 UTC] Start collecting samples
[2018-01-21 13:18:24.004127 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:24.134332 UTC] Performing policy update
[2018-01-21 13:18:24.135429 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:24.261792 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:25.759491 UTC] Performing line search
[2018-01-21 13:18:25.963006 UTC] Updating baseline
[2018-01-21 13:18:28.010250 UTC] Computing logging information
-------------------------------------
| Iteration            | 290        |
| ExpectedImprovement  | 0.016029   |
| ActualImprovement    | 0.01534    |
| ImprovementRatio     | 0.95702    |
| MeanKL               | 0.0075553  |
| Entropy              | 0.77548    |
| Perplexity           | 2.1716     |
| AveragePolicyStd     | 0.27943    |
| AveragePolicyStd[0]  | 0.27472    |
| AveragePolicyStd[1]  | 0.36905    |
| AveragePolicyStd[2]  | 0.20572    |
| AveragePolicyStd[3]  | 0.2922     |
| AveragePolicyStd[4]  | 0.26913    |
| AveragePolicyStd[5]  | 0.26574    |
| AverageReturn        | 850.12     |
| MinReturn            | 132.56     |
| MaxReturn            | 959.53     |
| StdReturn            | 179.96     |
| AverageEpisodeLength | 939.35     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.6      |
| TotalNEpisodes       | 18090      |
| TotalNSamples        | 1.4497e+06 |
| ExplainedVariance    | -0.044319  |
-------------------------------------
[2018-01-21 13:18:28.764381 UTC] Saving snapshot
[2018-01-21 13:18:28.774703 UTC] Starting iteration 291
[2018-01-21 13:18:28.774944 UTC] Start collecting samples
[2018-01-21 13:18:33.447827 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:33.574627 UTC] Performing policy update
[2018-01-21 13:18:33.575347 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:33.705480 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:35.227405 UTC] Performing line search
[2018-01-21 13:18:35.428443 UTC] Updating baseline
[2018-01-21 13:18:37.541254 UTC] Computing logging information
-------------------------------------
| Iteration            | 291        |
| ExpectedImprovement  | 0.016238   |
| ActualImprovement    | 0.015347   |
| ImprovementRatio     | 0.94512    |
| MeanKL               | 0.0072239  |
| Entropy              | 0.76319    |
| Perplexity           | 2.1451     |
| AveragePolicyStd     | 0.27886    |
| AveragePolicyStd[0]  | 0.27403    |
| AveragePolicyStd[1]  | 0.36843    |
| AveragePolicyStd[2]  | 0.20513    |
| AveragePolicyStd[3]  | 0.2906     |
| AveragePolicyStd[4]  | 0.26874    |
| AveragePolicyStd[5]  | 0.2662     |
| AverageReturn        | 850.59     |
| MinReturn            | 132.56     |
| MaxReturn            | 959.53     |
| StdReturn            | 180.16     |
| AverageEpisodeLength | 939.35     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.6      |
| TotalNEpisodes       | 18092      |
| TotalNSamples        | 1.4517e+06 |
| ExplainedVariance    | 0.025759   |
-------------------------------------
[2018-01-21 13:18:38.225888 UTC] Saving snapshot
[2018-01-21 13:18:38.226162 UTC] Starting iteration 292
[2018-01-21 13:18:38.226337 UTC] Start collecting samples
[2018-01-21 13:18:43.246386 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:43.391974 UTC] Performing policy update
[2018-01-21 13:18:43.392610 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:43.510336 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:45.022445 UTC] Performing line search
[2018-01-21 13:18:45.207668 UTC] Updating baseline
[2018-01-21 13:18:47.301584 UTC] Computing logging information
-------------------------------------
| Iteration            | 292        |
| ExpectedImprovement  | 0.014279   |
| ActualImprovement    | 0.01318    |
| ImprovementRatio     | 0.92302    |
| MeanKL               | 0.0073667  |
| Entropy              | 0.74754    |
| Perplexity           | 2.1118     |
| AveragePolicyStd     | 0.27805    |
| AveragePolicyStd[0]  | 0.27288    |
| AveragePolicyStd[1]  | 0.36709    |
| AveragePolicyStd[2]  | 0.20559    |
| AveragePolicyStd[3]  | 0.28909    |
| AveragePolicyStd[4]  | 0.26926    |
| AveragePolicyStd[5]  | 0.2644     |
| AverageReturn        | 854.17     |
| MinReturn            | 132.56     |
| MaxReturn            | 976.56     |
| StdReturn            | 181.78     |
| AverageEpisodeLength | 939.92     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.89     |
| TotalNEpisodes       | 18102      |
| TotalNSamples        | 1.4608e+06 |
| ExplainedVariance    | 0.063156   |
-------------------------------------
[2018-01-21 13:18:47.960364 UTC] Saving snapshot
[2018-01-21 13:18:47.960598 UTC] Starting iteration 293
[2018-01-21 13:18:47.960744 UTC] Start collecting samples
[2018-01-21 13:18:52.873942 UTC] Computing input variables for policy optimization
[2018-01-21 13:18:53.005354 UTC] Performing policy update
[2018-01-21 13:18:53.006259 UTC] Computing gradient in Euclidean space
[2018-01-21 13:18:53.130199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:18:54.550748 UTC] Performing line search
[2018-01-21 13:18:54.747605 UTC] Updating baseline
[2018-01-21 13:18:57.135655 UTC] Computing logging information
-------------------------------------
| Iteration            | 293        |
| ExpectedImprovement  | 0.018273   |
| ActualImprovement    | 0.01665    |
| ImprovementRatio     | 0.91119    |
| MeanKL               | 0.0069913  |
| Entropy              | 0.74019    |
| Perplexity           | 2.0963     |
| AveragePolicyStd     | 0.27765    |
| AveragePolicyStd[0]  | 0.27161    |
| AveragePolicyStd[1]  | 0.36516    |
| AveragePolicyStd[2]  | 0.20562    |
| AveragePolicyStd[3]  | 0.29003    |
| AveragePolicyStd[4]  | 0.27002    |
| AveragePolicyStd[5]  | 0.26345    |
| AverageReturn        | 854.21     |
| MinReturn            | 132.56     |
| MaxReturn            | 976.56     |
| StdReturn            | 181.93     |
| AverageEpisodeLength | 939.92     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.89     |
| TotalNEpisodes       | 18106      |
| TotalNSamples        | 1.4648e+06 |
| ExplainedVariance    | -0.014857  |
-------------------------------------
[2018-01-21 13:18:57.782054 UTC] Saving snapshot
[2018-01-21 13:18:57.782288 UTC] Starting iteration 294
[2018-01-21 13:18:57.782450 UTC] Start collecting samples
[2018-01-21 13:19:02.532897 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:02.663605 UTC] Performing policy update
[2018-01-21 13:19:02.664232 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:02.784535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:04.229131 UTC] Performing line search
[2018-01-21 13:19:04.431685 UTC] Updating baseline
[2018-01-21 13:19:06.214947 UTC] Computing logging information
-------------------------------------
| Iteration            | 294        |
| ExpectedImprovement  | 0.015411   |
| ActualImprovement    | 0.014731   |
| ImprovementRatio     | 0.95586    |
| MeanKL               | 0.0075718  |
| Entropy              | 0.72611    |
| Perplexity           | 2.067      |
| AveragePolicyStd     | 0.27695    |
| AveragePolicyStd[0]  | 0.27113    |
| AveragePolicyStd[1]  | 0.36346    |
| AveragePolicyStd[2]  | 0.2053     |
| AveragePolicyStd[3]  | 0.28933    |
| AveragePolicyStd[4]  | 0.26997    |
| AveragePolicyStd[5]  | 0.26253    |
| AverageReturn        | 854.9      |
| MinReturn            | 132.56     |
| MaxReturn            | 976.56     |
| StdReturn            | 182.15     |
| AverageEpisodeLength | 939.92     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.89     |
| TotalNEpisodes       | 18108      |
| TotalNSamples        | 1.4668e+06 |
| ExplainedVariance    | -0.2406    |
-------------------------------------
[2018-01-21 13:19:06.872722 UTC] Saving snapshot
[2018-01-21 13:19:06.873046 UTC] Starting iteration 295
[2018-01-21 13:19:06.873277 UTC] Start collecting samples
[2018-01-21 13:19:11.888313 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:12.038159 UTC] Performing policy update
[2018-01-21 13:19:12.038775 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:12.152636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:13.609699 UTC] Performing line search
[2018-01-21 13:19:13.807755 UTC] Updating baseline
[2018-01-21 13:19:15.523424 UTC] Computing logging information
-------------------------------------
| Iteration            | 295        |
| ExpectedImprovement  | 0.015043   |
| ActualImprovement    | 0.014222   |
| ImprovementRatio     | 0.94544    |
| MeanKL               | 0.0073153  |
| Entropy              | 0.70971    |
| Perplexity           | 2.0334     |
| AveragePolicyStd     | 0.27622    |
| AveragePolicyStd[0]  | 0.27049    |
| AveragePolicyStd[1]  | 0.36208    |
| AveragePolicyStd[2]  | 0.20405    |
| AveragePolicyStd[3]  | 0.28896    |
| AveragePolicyStd[4]  | 0.27021    |
| AveragePolicyStd[5]  | 0.26156    |
| AverageReturn        | 855.13     |
| MinReturn            | 132.56     |
| MaxReturn            | 980.83     |
| StdReturn            | 188.67     |
| AverageEpisodeLength | 934.64     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.29     |
| TotalNEpisodes       | 18118      |
| TotalNSamples        | 1.4763e+06 |
| ExplainedVariance    | 0.054608   |
-------------------------------------
[2018-01-21 13:19:16.196152 UTC] Saving snapshot
[2018-01-21 13:19:16.196351 UTC] Starting iteration 296
[2018-01-21 13:19:16.196498 UTC] Start collecting samples
[2018-01-21 13:19:21.100630 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:21.232019 UTC] Performing policy update
[2018-01-21 13:19:21.232618 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:21.351209 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:22.803864 UTC] Performing line search
[2018-01-21 13:19:23.018756 UTC] Updating baseline
[2018-01-21 13:19:24.950984 UTC] Computing logging information
-------------------------------------
| Iteration            | 296        |
| ExpectedImprovement  | 0.014351   |
| ActualImprovement    | 0.013691   |
| ImprovementRatio     | 0.95395    |
| MeanKL               | 0.0076819  |
| Entropy              | 0.7126     |
| Perplexity           | 2.0393     |
| AveragePolicyStd     | 0.27642    |
| AveragePolicyStd[0]  | 0.27062    |
| AveragePolicyStd[1]  | 0.36328    |
| AveragePolicyStd[2]  | 0.20392    |
| AveragePolicyStd[3]  | 0.28932    |
| AveragePolicyStd[4]  | 0.27059    |
| AveragePolicyStd[5]  | 0.26079    |
| AverageReturn        | 847.11     |
| MinReturn            | 132.56     |
| MaxReturn            | 980.83     |
| StdReturn            | 201.01     |
| AverageEpisodeLength | 926.36     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.12     |
| TotalNEpisodes       | 18123      |
| TotalNSamples        | 1.4805e+06 |
| ExplainedVariance    | 0.16164    |
-------------------------------------
[2018-01-21 13:19:25.612765 UTC] Saving snapshot
[2018-01-21 13:19:25.613002 UTC] Starting iteration 297
[2018-01-21 13:19:25.613149 UTC] Start collecting samples
[2018-01-21 13:19:30.341248 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:30.473046 UTC] Performing policy update
[2018-01-21 13:19:30.474373 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:30.604629 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:32.070421 UTC] Performing line search
[2018-01-21 13:19:32.255925 UTC] Updating baseline
[2018-01-21 13:19:34.049336 UTC] Computing logging information
------------------------------------
| Iteration            | 297       |
| ExpectedImprovement  | 0.01426   |
| ActualImprovement    | 0.013778  |
| ImprovementRatio     | 0.96619   |
| MeanKL               | 0.0070819 |
| Entropy              | 0.70524   |
| Perplexity           | 2.0243    |
| AveragePolicyStd     | 0.27616   |
| AveragePolicyStd[0]  | 0.27049   |
| AveragePolicyStd[1]  | 0.36441   |
| AveragePolicyStd[2]  | 0.20337   |
| AveragePolicyStd[3]  | 0.28819   |
| AveragePolicyStd[4]  | 0.27036   |
| AveragePolicyStd[5]  | 0.26015   |
| AverageReturn        | 843.28    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 204.83    |
| AverageEpisodeLength | 921.23    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.42    |
| TotalNEpisodes       | 18126     |
| TotalNSamples        | 1.483e+06 |
| ExplainedVariance    | 0.18461   |
------------------------------------
[2018-01-21 13:19:34.705466 UTC] Saving snapshot
[2018-01-21 13:19:34.705959 UTC] Starting iteration 298
[2018-01-21 13:19:34.706137 UTC] Start collecting samples
[2018-01-21 13:19:39.656221 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:39.810390 UTC] Performing policy update
[2018-01-21 13:19:39.811011 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:39.931750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:41.371103 UTC] Performing line search
[2018-01-21 13:19:41.562499 UTC] Updating baseline
[2018-01-21 13:19:43.913036 UTC] Computing logging information
------------------------------------
| Iteration            | 298       |
| ExpectedImprovement  | 0.01599   |
| ActualImprovement    | 0.015457  |
| ImprovementRatio     | 0.96665   |
| MeanKL               | 0.0075987 |
| Entropy              | 0.70687   |
| Perplexity           | 2.0276    |
| AveragePolicyStd     | 0.27628   |
| AveragePolicyStd[0]  | 0.27043   |
| AveragePolicyStd[1]  | 0.36639   |
| AveragePolicyStd[2]  | 0.20374   |
| AveragePolicyStd[3]  | 0.28735   |
| AveragePolicyStd[4]  | 0.26783   |
| AveragePolicyStd[5]  | 0.26195   |
| AverageReturn        | 844.33    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 204.91    |
| AverageEpisodeLength | 921.23    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.42    |
| TotalNEpisodes       | 18135     |
| TotalNSamples        | 1.492e+06 |
| ExplainedVariance    | 0.026842  |
------------------------------------
[2018-01-21 13:19:44.524186 UTC] Saving snapshot
[2018-01-21 13:19:44.524365 UTC] Starting iteration 299
[2018-01-21 13:19:44.524468 UTC] Start collecting samples
[2018-01-21 13:19:49.209661 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:49.325030 UTC] Performing policy update
[2018-01-21 13:19:49.325662 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:49.452227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:19:50.875638 UTC] Performing line search
[2018-01-21 13:19:51.075493 UTC] Updating baseline
[2018-01-21 13:19:53.527541 UTC] Computing logging information
------------------------------------
| Iteration            | 299       |
| ExpectedImprovement  | 0.02024   |
| ActualImprovement    | 0.020195  |
| ImprovementRatio     | 0.99777   |
| MeanKL               | 0.0069348 |
| Entropy              | 0.71352   |
| Perplexity           | 2.0412    |
| AveragePolicyStd     | 0.27667   |
| AveragePolicyStd[0]  | 0.27002   |
| AveragePolicyStd[1]  | 0.36727   |
| AveragePolicyStd[2]  | 0.20277   |
| AveragePolicyStd[3]  | 0.28835   |
| AveragePolicyStd[4]  | 0.26719   |
| AveragePolicyStd[5]  | 0.26444   |
| AverageReturn        | 842.03    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 205.3     |
| AverageEpisodeLength | 921.23    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.42    |
| TotalNEpisodes       | 18136     |
| TotalNSamples        | 1.493e+06 |
| ExplainedVariance    | -0.035892 |
------------------------------------
[2018-01-21 13:19:54.228503 UTC] Saving snapshot
[2018-01-21 13:19:54.228742 UTC] Starting iteration 300
[2018-01-21 13:19:54.228890 UTC] Start collecting samples
[2018-01-21 13:19:58.940665 UTC] Computing input variables for policy optimization
[2018-01-21 13:19:59.064705 UTC] Performing policy update
[2018-01-21 13:19:59.065339 UTC] Computing gradient in Euclidean space
[2018-01-21 13:19:59.186068 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:00.612260 UTC] Performing line search
[2018-01-21 13:20:00.806283 UTC] Updating baseline
[2018-01-21 13:20:02.798710 UTC] Computing logging information
------------------------------------
| Iteration            | 300       |
| ExpectedImprovement  | 0.014387  |
| ActualImprovement    | 0.013765  |
| ImprovementRatio     | 0.95681   |
| MeanKL               | 0.0072093 |
| Entropy              | 0.71073   |
| Perplexity           | 2.0355    |
| AveragePolicyStd     | 0.27651   |
| AveragePolicyStd[0]  | 0.27062   |
| AveragePolicyStd[1]  | 0.36726   |
| AveragePolicyStd[2]  | 0.20331   |
| AveragePolicyStd[3]  | 0.2874    |
| AveragePolicyStd[4]  | 0.26732   |
| AveragePolicyStd[5]  | 0.26318   |
| AverageReturn        | 845.83    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 205.72    |
| AverageEpisodeLength | 921.23    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.42    |
| TotalNEpisodes       | 18142     |
| TotalNSamples        | 1.499e+06 |
| ExplainedVariance    | -0.026515 |
------------------------------------
[2018-01-21 13:20:03.472560 UTC] Saving snapshot
[2018-01-21 13:20:03.482305 UTC] Starting iteration 301
[2018-01-21 13:20:03.482563 UTC] Start collecting samples
[2018-01-21 13:20:08.348994 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:08.505020 UTC] Performing policy update
[2018-01-21 13:20:08.505914 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:08.627968 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:10.116853 UTC] Performing line search
[2018-01-21 13:20:10.313923 UTC] Updating baseline
[2018-01-21 13:20:12.598189 UTC] Computing logging information
------------------------------------
| Iteration            | 301       |
| ExpectedImprovement  | 0.016297  |
| ActualImprovement    | 0.015559  |
| ImprovementRatio     | 0.95467   |
| MeanKL               | 0.0073963 |
| Entropy              | 0.71848   |
| Perplexity           | 2.0513    |
| AveragePolicyStd     | 0.27697   |
| AveragePolicyStd[0]  | 0.27139   |
| AveragePolicyStd[1]  | 0.36893   |
| AveragePolicyStd[2]  | 0.20278   |
| AveragePolicyStd[3]  | 0.28784   |
| AveragePolicyStd[4]  | 0.26815   |
| AveragePolicyStd[5]  | 0.26274   |
| AverageReturn        | 849.08    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 205.91    |
| AverageEpisodeLength | 921.23    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.42    |
| TotalNEpisodes       | 18148     |
| TotalNSamples        | 1.505e+06 |
| ExplainedVariance    | -0.013946 |
------------------------------------
[2018-01-21 13:20:13.247794 UTC] Saving snapshot
[2018-01-21 13:20:13.248130 UTC] Starting iteration 302
[2018-01-21 13:20:13.248362 UTC] Start collecting samples
[2018-01-21 13:20:17.938970 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:18.070904 UTC] Performing policy update
[2018-01-21 13:20:18.071535 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:18.190477 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:19.616444 UTC] Performing line search
[2018-01-21 13:20:19.806102 UTC] Updating baseline
[2018-01-21 13:20:22.438555 UTC] Computing logging information
------------------------------------
| Iteration            | 302       |
| ExpectedImprovement  | 0.018615  |
| ActualImprovement    | 0.016818  |
| ImprovementRatio     | 0.90347   |
| MeanKL               | 0.0069791 |
| Entropy              | 0.72344   |
| Perplexity           | 2.0615    |
| AveragePolicyStd     | 0.2772    |
| AveragePolicyStd[0]  | 0.27248   |
| AveragePolicyStd[1]  | 0.36923   |
| AveragePolicyStd[2]  | 0.20276   |
| AveragePolicyStd[3]  | 0.28697   |
| AveragePolicyStd[4]  | 0.26908   |
| AveragePolicyStd[5]  | 0.26267   |
| AverageReturn        | 851.02    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 205.71    |
| AverageEpisodeLength | 923.1     |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 217.29    |
| TotalNEpisodes       | 18152     |
| TotalNSamples        | 1.509e+06 |
| ExplainedVariance    | 0.056617  |
------------------------------------
[2018-01-21 13:20:23.097589 UTC] Saving snapshot
[2018-01-21 13:20:23.097830 UTC] Starting iteration 303
[2018-01-21 13:20:23.098003 UTC] Start collecting samples
[2018-01-21 13:20:27.908949 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:28.056072 UTC] Performing policy update
[2018-01-21 13:20:28.057939 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:28.184107 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:29.648929 UTC] Performing line search
[2018-01-21 13:20:29.873639 UTC] Updating baseline
[2018-01-21 13:20:32.374145 UTC] Computing logging information
-------------------------------------
| Iteration            | 303        |
| ExpectedImprovement  | 0.017454   |
| ActualImprovement    | 0.016852   |
| ImprovementRatio     | 0.9655     |
| MeanKL               | 0.0069914  |
| Entropy              | 0.70675    |
| Perplexity           | 2.0274     |
| AveragePolicyStd     | 0.27629    |
| AveragePolicyStd[0]  | 0.27222    |
| AveragePolicyStd[1]  | 0.36627    |
| AveragePolicyStd[2]  | 0.2031     |
| AveragePolicyStd[3]  | 0.28557    |
| AveragePolicyStd[4]  | 0.26789    |
| AveragePolicyStd[5]  | 0.26269    |
| AverageReturn        | 852.86     |
| MinReturn            | 132.56     |
| MaxReturn            | 980.83     |
| StdReturn            | 204.53     |
| AverageEpisodeLength | 924.78     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.34     |
| TotalNEpisodes       | 18159      |
| TotalNSamples        | 1.5153e+06 |
| ExplainedVariance    | -0.04563   |
-------------------------------------
[2018-01-21 13:20:33.025930 UTC] Saving snapshot
[2018-01-21 13:20:33.026324 UTC] Starting iteration 304
[2018-01-21 13:20:33.026545 UTC] Start collecting samples
[2018-01-21 13:20:37.828546 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:37.979207 UTC] Performing policy update
[2018-01-21 13:20:37.979800 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:38.112664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:39.601879 UTC] Performing line search
[2018-01-21 13:20:39.798653 UTC] Updating baseline
[2018-01-21 13:20:41.831177 UTC] Computing logging information
------------------------------------
| Iteration            | 304       |
| ExpectedImprovement  | 0.015945  |
| ActualImprovement    | 0.015145  |
| ImprovementRatio     | 0.94979   |
| MeanKL               | 0.0068331 |
| Entropy              | 0.70074   |
| Perplexity           | 2.0152    |
| AveragePolicyStd     | 0.276     |
| AveragePolicyStd[0]  | 0.27256   |
| AveragePolicyStd[1]  | 0.36548   |
| AveragePolicyStd[2]  | 0.20269   |
| AveragePolicyStd[3]  | 0.28488   |
| AveragePolicyStd[4]  | 0.26827   |
| AveragePolicyStd[5]  | 0.26213   |
| AverageReturn        | 854.12    |
| MinReturn            | 132.56    |
| MaxReturn            | 980.83    |
| StdReturn            | 198.56    |
| AverageEpisodeLength | 925.75    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 210.12    |
| TotalNEpisodes       | 18166     |
| TotalNSamples        | 1.521e+06 |
| ExplainedVariance    | 0.32972   |
------------------------------------
[2018-01-21 13:20:42.509764 UTC] Saving snapshot
[2018-01-21 13:20:42.510015 UTC] Starting iteration 305
[2018-01-21 13:20:42.510174 UTC] Start collecting samples
[2018-01-21 13:20:47.290796 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:47.443340 UTC] Performing policy update
[2018-01-21 13:20:47.443974 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:47.560859 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:49.064089 UTC] Performing line search
[2018-01-21 13:20:49.260217 UTC] Updating baseline
[2018-01-21 13:20:50.889644 UTC] Computing logging information
-------------------------------------
| Iteration            | 305        |
| ExpectedImprovement  | 0.016778   |
| ActualImprovement    | 0.015418   |
| ImprovementRatio     | 0.91892    |
| MeanKL               | 0.0070237  |
| Entropy              | 0.69593    |
| Perplexity           | 2.0056     |
| AveragePolicyStd     | 0.27579    |
| AveragePolicyStd[0]  | 0.27192    |
| AveragePolicyStd[1]  | 0.36481    |
| AveragePolicyStd[2]  | 0.20248    |
| AveragePolicyStd[3]  | 0.28638    |
| AveragePolicyStd[4]  | 0.26785    |
| AveragePolicyStd[5]  | 0.26127    |
| AverageReturn        | 848.08     |
| MinReturn            | 132.56     |
| MaxReturn            | 980.83     |
| StdReturn            | 209.96     |
| AverageEpisodeLength | 918.96     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.58     |
| TotalNEpisodes       | 18172      |
| TotalNSamples        | 1.5256e+06 |
| ExplainedVariance    | 0.38591    |
-------------------------------------
[2018-01-21 13:20:51.572081 UTC] Saving snapshot
[2018-01-21 13:20:51.572352 UTC] Starting iteration 306
[2018-01-21 13:20:51.572566 UTC] Start collecting samples
[2018-01-21 13:20:56.245137 UTC] Computing input variables for policy optimization
[2018-01-21 13:20:56.373825 UTC] Performing policy update
[2018-01-21 13:20:56.374425 UTC] Computing gradient in Euclidean space
[2018-01-21 13:20:56.503833 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:20:57.910108 UTC] Performing line search
[2018-01-21 13:20:58.106386 UTC] Updating baseline
[2018-01-21 13:21:00.096486 UTC] Computing logging information
-------------------------------------
| Iteration            | 306        |
| ExpectedImprovement  | 0.015798   |
| ActualImprovement    | 0.014873   |
| ImprovementRatio     | 0.94145    |
| MeanKL               | 0.0075438  |
| Entropy              | 0.69163    |
| Perplexity           | 1.997      |
| AveragePolicyStd     | 0.27557    |
| AveragePolicyStd[0]  | 0.27194    |
| AveragePolicyStd[1]  | 0.36326    |
| AveragePolicyStd[2]  | 0.20199    |
| AveragePolicyStd[3]  | 0.28757    |
| AveragePolicyStd[4]  | 0.26836    |
| AveragePolicyStd[5]  | 0.2603     |
| AverageReturn        | 853.34     |
| MinReturn            | 132.56     |
| MaxReturn            | 980.83     |
| StdReturn            | 198.69     |
| AverageEpisodeLength | 924.45     |
| MinEpisodeLength     | 138        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.78     |
| TotalNEpisodes       | 18176      |
| TotalNSamples        | 1.5293e+06 |
| ExplainedVariance    | 0.23672    |
-------------------------------------
[2018-01-21 13:21:00.735300 UTC] Saving snapshot
[2018-01-21 13:21:00.735535 UTC] Starting iteration 307
[2018-01-21 13:21:00.735683 UTC] Start collecting samples
[2018-01-21 13:21:05.410752 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:05.571274 UTC] Performing policy update
[2018-01-21 13:21:05.571963 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:05.699667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:07.145990 UTC] Performing line search
[2018-01-21 13:21:07.331265 UTC] Updating baseline
[2018-01-21 13:21:09.265638 UTC] Computing logging information
-------------------------------------
| Iteration            | 307        |
| ExpectedImprovement  | 0.016507   |
| ActualImprovement    | 0.015613   |
| ImprovementRatio     | 0.94588    |
| MeanKL               | 0.007222   |
| Entropy              | 0.69158    |
| Perplexity           | 1.9969     |
| AveragePolicyStd     | 0.27557    |
| AveragePolicyStd[0]  | 0.27343    |
| AveragePolicyStd[1]  | 0.36218    |
| AveragePolicyStd[2]  | 0.2012     |
| AveragePolicyStd[3]  | 0.28754    |
| AveragePolicyStd[4]  | 0.26833    |
| AveragePolicyStd[5]  | 0.2607     |
| AverageReturn        | 850.02     |
| MinReturn            | 141.82     |
| MaxReturn            | 980.83     |
| StdReturn            | 195.4      |
| AverageEpisodeLength | 921.65     |
| MinEpisodeLength     | 172        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.9      |
| TotalNEpisodes       | 18183      |
| TotalNSamples        | 1.5348e+06 |
| ExplainedVariance    | 0.58207    |
-------------------------------------
[2018-01-21 13:21:09.909106 UTC] Saving snapshot
[2018-01-21 13:21:09.909344 UTC] Starting iteration 308
[2018-01-21 13:21:09.909505 UTC] Start collecting samples
[2018-01-21 13:21:14.808802 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:14.965865 UTC] Performing policy update
[2018-01-21 13:21:14.966474 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:15.085678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:16.546230 UTC] Performing line search
[2018-01-21 13:21:16.735022 UTC] Updating baseline
[2018-01-21 13:21:18.627950 UTC] Computing logging information
-------------------------------------
| Iteration            | 308        |
| ExpectedImprovement  | 0.015699   |
| ActualImprovement    | 0.014431   |
| ImprovementRatio     | 0.91924    |
| MeanKL               | 0.0077665  |
| Entropy              | 0.67995    |
| Perplexity           | 1.9738     |
| AveragePolicyStd     | 0.2751     |
| AveragePolicyStd[0]  | 0.2736     |
| AveragePolicyStd[1]  | 0.36166    |
| AveragePolicyStd[2]  | 0.19984    |
| AveragePolicyStd[3]  | 0.288      |
| AveragePolicyStd[4]  | 0.26658    |
| AveragePolicyStd[5]  | 0.26095    |
| AverageReturn        | 834.36     |
| MinReturn            | 112.28     |
| MaxReturn            | 980.83     |
| StdReturn            | 217.74     |
| AverageEpisodeLength | 905.28     |
| MinEpisodeLength     | 124        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.38     |
| TotalNEpisodes       | 18190      |
| TotalNSamples        | 1.5402e+06 |
| ExplainedVariance    | 0.38342    |
-------------------------------------
[2018-01-21 13:21:19.287227 UTC] Saving snapshot
[2018-01-21 13:21:19.287425 UTC] Starting iteration 309
[2018-01-21 13:21:19.287587 UTC] Start collecting samples
[2018-01-21 13:21:24.088624 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:24.215027 UTC] Performing policy update
[2018-01-21 13:21:24.215681 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:24.332159 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:25.754227 UTC] Performing line search
[2018-01-21 13:21:25.947195 UTC] Updating baseline
[2018-01-21 13:21:28.123739 UTC] Computing logging information
------------------------------------
| Iteration            | 309       |
| ExpectedImprovement  | 0.013748  |
| ActualImprovement    | 0.012991  |
| ImprovementRatio     | 0.94498   |
| MeanKL               | 0.0076508 |
| Entropy              | 0.66378   |
| Perplexity           | 1.9421    |
| AveragePolicyStd     | 0.27436   |
| AveragePolicyStd[0]  | 0.27264   |
| AveragePolicyStd[1]  | 0.35941   |
| AveragePolicyStd[2]  | 0.19888   |
| AveragePolicyStd[3]  | 0.28902   |
| AveragePolicyStd[4]  | 0.26652   |
| AveragePolicyStd[5]  | 0.25968   |
| AverageReturn        | 820.71    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 229.06    |
| AverageEpisodeLength | 893.76    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 240.38    |
| TotalNEpisodes       | 18195     |
| TotalNSamples        | 1.544e+06 |
| ExplainedVariance    | 0.41198   |
------------------------------------
[2018-01-21 13:21:28.831241 UTC] Saving snapshot
[2018-01-21 13:21:28.831545 UTC] Starting iteration 310
[2018-01-21 13:21:28.831778 UTC] Start collecting samples
[2018-01-21 13:21:33.638341 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:33.766639 UTC] Performing policy update
[2018-01-21 13:21:33.767683 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:33.895342 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:35.355953 UTC] Performing line search
[2018-01-21 13:21:35.560064 UTC] Updating baseline
[2018-01-21 13:21:37.850020 UTC] Computing logging information
------------------------------------
| Iteration            | 310       |
| ExpectedImprovement  | 0.015614  |
| ActualImprovement    | 0.015005  |
| ImprovementRatio     | 0.96096   |
| MeanKL               | 0.0077005 |
| Entropy              | 0.65703   |
| Perplexity           | 1.929     |
| AveragePolicyStd     | 0.27406   |
| AveragePolicyStd[0]  | 0.27157   |
| AveragePolicyStd[1]  | 0.35962   |
| AveragePolicyStd[2]  | 0.19867   |
| AveragePolicyStd[3]  | 0.28852   |
| AveragePolicyStd[4]  | 0.26542   |
| AveragePolicyStd[5]  | 0.26058   |
| AverageReturn        | 828.84    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 219.01    |
| AverageEpisodeLength | 901.98    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 229.58    |
| TotalNEpisodes       | 18201     |
| TotalNSamples        | 1.55e+06  |
| ExplainedVariance    | 0.01651   |
------------------------------------
[2018-01-21 13:21:38.505003 UTC] Saving snapshot
[2018-01-21 13:21:38.513265 UTC] Starting iteration 311
[2018-01-21 13:21:38.513444 UTC] Start collecting samples
[2018-01-21 13:21:43.221063 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:43.373530 UTC] Performing policy update
[2018-01-21 13:21:43.374141 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:43.494721 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:44.937051 UTC] Performing line search
[2018-01-21 13:21:45.134952 UTC] Updating baseline
[2018-01-21 13:21:47.168466 UTC] Computing logging information
------------------------------------
| Iteration            | 311       |
| ExpectedImprovement  | 0.015299  |
| ActualImprovement    | 0.014044  |
| ImprovementRatio     | 0.91801   |
| MeanKL               | 0.0073083 |
| Entropy              | 0.66067   |
| Perplexity           | 1.9361    |
| AveragePolicyStd     | 0.27422   |
| AveragePolicyStd[0]  | 0.27138   |
| AveragePolicyStd[1]  | 0.35844   |
| AveragePolicyStd[2]  | 0.19812   |
| AveragePolicyStd[3]  | 0.28981   |
| AveragePolicyStd[4]  | 0.26621   |
| AveragePolicyStd[5]  | 0.26137   |
| AverageReturn        | 828.03    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 218.6     |
| AverageEpisodeLength | 901.98    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 229.58    |
| TotalNEpisodes       | 18205     |
| TotalNSamples        | 1.554e+06 |
| ExplainedVariance    | 0.026734  |
------------------------------------
[2018-01-21 13:21:47.889573 UTC] Saving snapshot
[2018-01-21 13:21:47.890039 UTC] Starting iteration 312
[2018-01-21 13:21:47.890758 UTC] Start collecting samples
[2018-01-21 13:21:53.319402 UTC] Computing input variables for policy optimization
[2018-01-21 13:21:53.390852 UTC] Performing policy update
[2018-01-21 13:21:53.391368 UTC] Computing gradient in Euclidean space
[2018-01-21 13:21:53.464337 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:21:54.895955 UTC] Performing line search
[2018-01-21 13:21:55.086797 UTC] Updating baseline
[2018-01-21 13:21:56.927810 UTC] Computing logging information
------------------------------------
| Iteration            | 312       |
| ExpectedImprovement  | 0.013981  |
| ActualImprovement    | 0.013431  |
| ImprovementRatio     | 0.96063   |
| MeanKL               | 0.0078308 |
| Entropy              | 0.66374   |
| Perplexity           | 1.942     |
| AveragePolicyStd     | 0.27441   |
| AveragePolicyStd[0]  | 0.27207   |
| AveragePolicyStd[1]  | 0.35949   |
| AveragePolicyStd[2]  | 0.19784   |
| AveragePolicyStd[3]  | 0.28883   |
| AveragePolicyStd[4]  | 0.26753   |
| AveragePolicyStd[5]  | 0.2607    |
| AverageReturn        | 826.52    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 217.9     |
| AverageEpisodeLength | 901.98    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 229.58    |
| TotalNEpisodes       | 18211     |
| TotalNSamples        | 1.56e+06  |
| ExplainedVariance    | 0.017076  |
------------------------------------
[2018-01-21 13:21:57.583759 UTC] Saving snapshot
[2018-01-21 13:21:57.583967 UTC] Starting iteration 313
[2018-01-21 13:21:57.584088 UTC] Start collecting samples
[2018-01-21 13:22:02.164972 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:02.304548 UTC] Performing policy update
[2018-01-21 13:22:02.305169 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:02.426410 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:03.826086 UTC] Performing line search
[2018-01-21 13:22:04.016587 UTC] Updating baseline
[2018-01-21 13:22:07.064577 UTC] Computing logging information
------------------------------------
| Iteration            | 313       |
| ExpectedImprovement  | 0.016283  |
| ActualImprovement    | 0.015236  |
| ImprovementRatio     | 0.93566   |
| MeanKL               | 0.0074213 |
| Entropy              | 0.6644    |
| Perplexity           | 1.9433    |
| AveragePolicyStd     | 0.27444   |
| AveragePolicyStd[0]  | 0.27185   |
| AveragePolicyStd[1]  | 0.35823   |
| AveragePolicyStd[2]  | 0.19693   |
| AveragePolicyStd[3]  | 0.28967   |
| AveragePolicyStd[4]  | 0.26796   |
| AveragePolicyStd[5]  | 0.26202   |
| AverageReturn        | 831.88    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 214.7     |
| AverageEpisodeLength | 907.26    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 225.67    |
| TotalNEpisodes       | 18215     |
| TotalNSamples        | 1.564e+06 |
| ExplainedVariance    | 0.042645  |
------------------------------------
[2018-01-21 13:22:07.710726 UTC] Saving snapshot
[2018-01-21 13:22:07.710949 UTC] Starting iteration 314
[2018-01-21 13:22:07.711125 UTC] Start collecting samples
[2018-01-21 13:22:12.260973 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:12.391502 UTC] Performing policy update
[2018-01-21 13:22:12.392143 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:12.510030 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:13.882716 UTC] Performing line search
[2018-01-21 13:22:14.072790 UTC] Updating baseline
[2018-01-21 13:22:16.498721 UTC] Computing logging information
------------------------------------
| Iteration            | 314       |
| ExpectedImprovement  | 0.013236  |
| ActualImprovement    | 0.012667  |
| ImprovementRatio     | 0.95699   |
| MeanKL               | 0.0080525 |
| Entropy              | 0.66494   |
| Perplexity           | 1.9444    |
| AveragePolicyStd     | 0.2744    |
| AveragePolicyStd[0]  | 0.27049   |
| AveragePolicyStd[1]  | 0.35943   |
| AveragePolicyStd[2]  | 0.19881   |
| AveragePolicyStd[3]  | 0.28883   |
| AveragePolicyStd[4]  | 0.26841   |
| AveragePolicyStd[5]  | 0.26045   |
| AverageReturn        | 832.19    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 214.77    |
| AverageEpisodeLength | 907.26    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 225.67    |
| TotalNEpisodes       | 18221     |
| TotalNSamples        | 1.57e+06  |
| ExplainedVariance    | -0.050392 |
------------------------------------
[2018-01-21 13:22:17.123245 UTC] Saving snapshot
[2018-01-21 13:22:17.123493 UTC] Starting iteration 315
[2018-01-21 13:22:17.123674 UTC] Start collecting samples
[2018-01-21 13:22:21.666335 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:21.807795 UTC] Performing policy update
[2018-01-21 13:22:21.808498 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:21.932582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:23.327327 UTC] Performing line search
[2018-01-21 13:22:23.507248 UTC] Updating baseline
[2018-01-21 13:22:25.370923 UTC] Computing logging information
------------------------------------
| Iteration            | 315       |
| ExpectedImprovement  | 0.01385   |
| ActualImprovement    | 0.013035  |
| ImprovementRatio     | 0.94112   |
| MeanKL               | 0.0070538 |
| Entropy              | 0.66796   |
| Perplexity           | 1.9503    |
| AveragePolicyStd     | 0.27458   |
| AveragePolicyStd[0]  | 0.27076   |
| AveragePolicyStd[1]  | 0.36144   |
| AveragePolicyStd[2]  | 0.19959   |
| AveragePolicyStd[3]  | 0.28878   |
| AveragePolicyStd[4]  | 0.26769   |
| AveragePolicyStd[5]  | 0.25924   |
| AverageReturn        | 845.34    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 200.59    |
| AverageEpisodeLength | 920.67    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 209.16    |
| TotalNEpisodes       | 18225     |
| TotalNSamples        | 1.574e+06 |
| ExplainedVariance    | 0.0091828 |
------------------------------------
[2018-01-21 13:22:26.017760 UTC] Saving snapshot
[2018-01-21 13:22:26.017997 UTC] Starting iteration 316
[2018-01-21 13:22:26.018156 UTC] Start collecting samples
[2018-01-21 13:22:30.559908 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:30.696768 UTC] Performing policy update
[2018-01-21 13:22:30.697392 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:30.827387 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:32.216805 UTC] Performing line search
[2018-01-21 13:22:32.398907 UTC] Updating baseline
[2018-01-21 13:22:34.304587 UTC] Computing logging information
------------------------------------
| Iteration            | 316       |
| ExpectedImprovement  | 0.016557  |
| ActualImprovement    | 0.015103  |
| ImprovementRatio     | 0.91215   |
| MeanKL               | 0.007052  |
| Entropy              | 0.672     |
| Perplexity           | 1.9582    |
| AveragePolicyStd     | 0.27477   |
| AveragePolicyStd[0]  | 0.27152   |
| AveragePolicyStd[1]  | 0.36098   |
| AveragePolicyStd[2]  | 0.19932   |
| AveragePolicyStd[3]  | 0.2897    |
| AveragePolicyStd[4]  | 0.26747   |
| AveragePolicyStd[5]  | 0.25963   |
| AverageReturn        | 846.2     |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 200.93    |
| AverageEpisodeLength | 920.67    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 209.16    |
| TotalNEpisodes       | 18229     |
| TotalNSamples        | 1.578e+06 |
| ExplainedVariance    | 0.018274  |
------------------------------------
[2018-01-21 13:22:34.931599 UTC] Saving snapshot
[2018-01-21 13:22:34.931831 UTC] Starting iteration 317
[2018-01-21 13:22:34.932004 UTC] Start collecting samples
[2018-01-21 13:22:39.705982 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:39.837963 UTC] Performing policy update
[2018-01-21 13:22:39.838578 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:39.957121 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:41.368087 UTC] Performing line search
[2018-01-21 13:22:41.557097 UTC] Updating baseline
[2018-01-21 13:22:44.214573 UTC] Computing logging information
------------------------------------
| Iteration            | 317       |
| ExpectedImprovement  | 0.014759  |
| ActualImprovement    | 0.014236  |
| ImprovementRatio     | 0.96451   |
| MeanKL               | 0.0072118 |
| Entropy              | 0.63964   |
| Perplexity           | 1.8958    |
| AveragePolicyStd     | 0.27333   |
| AveragePolicyStd[0]  | 0.2707    |
| AveragePolicyStd[1]  | 0.36072   |
| AveragePolicyStd[2]  | 0.19844   |
| AveragePolicyStd[3]  | 0.28605   |
| AveragePolicyStd[4]  | 0.26531   |
| AveragePolicyStd[5]  | 0.25877   |
| AverageReturn        | 849.48    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 200.9     |
| AverageEpisodeLength | 920.67    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 209.16    |
| TotalNEpisodes       | 18237     |
| TotalNSamples        | 1.586e+06 |
| ExplainedVariance    | 0.012055  |
------------------------------------
[2018-01-21 13:22:44.904936 UTC] Saving snapshot
[2018-01-21 13:22:44.905483 UTC] Starting iteration 318
[2018-01-21 13:22:44.905865 UTC] Start collecting samples
[2018-01-21 13:22:49.453375 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:49.599347 UTC] Performing policy update
[2018-01-21 13:22:49.599980 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:49.724319 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:22:51.104061 UTC] Performing line search
[2018-01-21 13:22:51.295729 UTC] Updating baseline
[2018-01-21 13:22:53.168792 UTC] Computing logging information
-------------------------------------
| Iteration            | 318        |
| ExpectedImprovement  | 0.015258   |
| ActualImprovement    | 0.014632   |
| ImprovementRatio     | 0.95894    |
| MeanKL               | 0.0072041  |
| Entropy              | 0.63654    |
| Perplexity           | 1.8899     |
| AveragePolicyStd     | 0.27327    |
| AveragePolicyStd[0]  | 0.27048    |
| AveragePolicyStd[1]  | 0.36217    |
| AveragePolicyStd[2]  | 0.1982     |
| AveragePolicyStd[3]  | 0.28583    |
| AveragePolicyStd[4]  | 0.26466    |
| AveragePolicyStd[5]  | 0.25827    |
| AverageReturn        | 837.9      |
| MinReturn            | 112.28     |
| MaxReturn            | 986.54     |
| StdReturn            | 216.01     |
| AverageEpisodeLength | 907.98     |
| MinEpisodeLength     | 124        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.73     |
| TotalNEpisodes       | 18243      |
| TotalNSamples        | 1.5908e+06 |
| ExplainedVariance    | 0.17552    |
-------------------------------------
[2018-01-21 13:22:53.854661 UTC] Saving snapshot
[2018-01-21 13:22:53.854963 UTC] Starting iteration 319
[2018-01-21 13:22:53.855176 UTC] Start collecting samples
[2018-01-21 13:22:58.519413 UTC] Computing input variables for policy optimization
[2018-01-21 13:22:58.645326 UTC] Performing policy update
[2018-01-21 13:22:58.650753 UTC] Computing gradient in Euclidean space
[2018-01-21 13:22:58.776152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:00.157504 UTC] Performing line search
[2018-01-21 13:23:00.348213 UTC] Updating baseline
[2018-01-21 13:23:02.260679 UTC] Computing logging information
-------------------------------------
| Iteration            | 319        |
| ExpectedImprovement  | 0.016181   |
| ActualImprovement    | 0.015267   |
| ImprovementRatio     | 0.94351    |
| MeanKL               | 0.0070298  |
| Entropy              | 0.63675    |
| Perplexity           | 1.8903     |
| AveragePolicyStd     | 0.27332    |
| AveragePolicyStd[0]  | 0.27085    |
| AveragePolicyStd[1]  | 0.36261    |
| AveragePolicyStd[2]  | 0.1978     |
| AveragePolicyStd[3]  | 0.28558    |
| AveragePolicyStd[4]  | 0.26463    |
| AveragePolicyStd[5]  | 0.25843    |
| AverageReturn        | 823.19     |
| MinReturn            | 112.28     |
| MaxReturn            | 986.54     |
| StdReturn            | 229.31     |
| AverageEpisodeLength | 892.86     |
| MinEpisodeLength     | 124        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.66     |
| TotalNEpisodes       | 18249      |
| TotalNSamples        | 1.5953e+06 |
| ExplainedVariance    | 0.55107    |
-------------------------------------
[2018-01-21 13:23:02.926472 UTC] Saving snapshot
[2018-01-21 13:23:02.926735 UTC] Starting iteration 320
[2018-01-21 13:23:02.926920 UTC] Start collecting samples
[2018-01-21 13:23:07.400496 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:07.537778 UTC] Performing policy update
[2018-01-21 13:23:07.538474 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:07.655888 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:09.106718 UTC] Performing line search
[2018-01-21 13:23:09.297384 UTC] Updating baseline
[2018-01-21 13:23:11.054097 UTC] Computing logging information
-------------------------------------
| Iteration            | 320        |
| ExpectedImprovement  | 0.014383   |
| ActualImprovement    | 0.014065   |
| ImprovementRatio     | 0.97787    |
| MeanKL               | 0.0083401  |
| Entropy              | 0.61834    |
| Perplexity           | 1.8558     |
| AveragePolicyStd     | 0.27243    |
| AveragePolicyStd[0]  | 0.27145    |
| AveragePolicyStd[1]  | 0.36047    |
| AveragePolicyStd[2]  | 0.19714    |
| AveragePolicyStd[3]  | 0.28369    |
| AveragePolicyStd[4]  | 0.26332    |
| AveragePolicyStd[5]  | 0.2585     |
| AverageReturn        | 815.99     |
| MinReturn            | 112.28     |
| MaxReturn            | 986.54     |
| StdReturn            | 239.47     |
| AverageEpisodeLength | 883.56     |
| MinEpisodeLength     | 124        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 247.94     |
| TotalNEpisodes       | 18258      |
| TotalNSamples        | 1.6027e+06 |
| ExplainedVariance    | 0.34343    |
-------------------------------------
[2018-01-21 13:23:11.712080 UTC] Saving snapshot
[2018-01-21 13:23:11.718159 UTC] Starting iteration 321
[2018-01-21 13:23:11.718349 UTC] Start collecting samples
[2018-01-21 13:23:16.296203 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:16.434820 UTC] Performing policy update
[2018-01-21 13:23:16.435580 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:16.556718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:17.929870 UTC] Performing line search
[2018-01-21 13:23:18.116675 UTC] Updating baseline
[2018-01-21 13:23:19.737480 UTC] Computing logging information
-------------------------------------
| Iteration            | 321        |
| ExpectedImprovement  | 0.015222   |
| ActualImprovement    | 0.014165   |
| ImprovementRatio     | 0.9306     |
| MeanKL               | 0.0070519  |
| Entropy              | 0.62574    |
| Perplexity           | 1.8696     |
| AveragePolicyStd     | 0.27281    |
| AveragePolicyStd[0]  | 0.27193    |
| AveragePolicyStd[1]  | 0.36048    |
| AveragePolicyStd[2]  | 0.19635    |
| AveragePolicyStd[3]  | 0.28495    |
| AveragePolicyStd[4]  | 0.26372    |
| AveragePolicyStd[5]  | 0.25944    |
| AverageReturn        | 822.72     |
| MinReturn            | 112.28     |
| MaxReturn            | 986.54     |
| StdReturn            | 234.47     |
| AverageEpisodeLength | 890.13     |
| MinEpisodeLength     | 124        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 242.16     |
| TotalNEpisodes       | 18260      |
| TotalNSamples        | 1.6047e+06 |
| ExplainedVariance    | -0.086441  |
-------------------------------------
[2018-01-21 13:23:20.364098 UTC] Saving snapshot
[2018-01-21 13:23:20.364360 UTC] Starting iteration 322
[2018-01-21 13:23:20.364544 UTC] Start collecting samples
[2018-01-21 13:23:24.906134 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:25.031405 UTC] Performing policy update
[2018-01-21 13:23:25.031990 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:25.149219 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:26.545604 UTC] Performing line search
[2018-01-21 13:23:26.732814 UTC] Updating baseline
[2018-01-21 13:23:29.051488 UTC] Computing logging information
------------------------------------
| Iteration            | 322       |
| ExpectedImprovement  | 0.016293  |
| ActualImprovement    | 0.015409  |
| ImprovementRatio     | 0.94577   |
| MeanKL               | 0.0077541 |
| Entropy              | 0.623     |
| Perplexity           | 1.8645    |
| AveragePolicyStd     | 0.2727    |
| AveragePolicyStd[0]  | 0.2726    |
| AveragePolicyStd[1]  | 0.36054   |
| AveragePolicyStd[2]  | 0.19671   |
| AveragePolicyStd[3]  | 0.28592   |
| AveragePolicyStd[4]  | 0.26347   |
| AveragePolicyStd[5]  | 0.25696   |
| AverageReturn        | 822.99    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 238.17    |
| AverageEpisodeLength | 889.96    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 244.57    |
| TotalNEpisodes       | 18265     |
| TotalNSamples        | 1.609e+06 |
| ExplainedVariance    | 0.26291   |
------------------------------------
[2018-01-21 13:23:29.697323 UTC] Saving snapshot
[2018-01-21 13:23:29.697598 UTC] Starting iteration 323
[2018-01-21 13:23:29.697794 UTC] Start collecting samples
[2018-01-21 13:23:34.418291 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:34.556693 UTC] Performing policy update
[2018-01-21 13:23:34.557315 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:34.677877 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:36.059760 UTC] Performing line search
[2018-01-21 13:23:36.250307 UTC] Updating baseline
[2018-01-21 13:23:38.007531 UTC] Computing logging information
------------------------------------
| Iteration            | 323       |
| ExpectedImprovement  | 0.01368   |
| ActualImprovement    | 0.013393  |
| ImprovementRatio     | 0.97899   |
| MeanKL               | 0.0076142 |
| Entropy              | 0.62099   |
| Perplexity           | 1.8608    |
| AveragePolicyStd     | 0.2726    |
| AveragePolicyStd[0]  | 0.2725    |
| AveragePolicyStd[1]  | 0.36042   |
| AveragePolicyStd[2]  | 0.19683   |
| AveragePolicyStd[3]  | 0.28619   |
| AveragePolicyStd[4]  | 0.26307   |
| AveragePolicyStd[5]  | 0.25663   |
| AverageReturn        | 839.61    |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 226.63    |
| AverageEpisodeLength | 904.5     |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 230.3     |
| TotalNEpisodes       | 18273     |
| TotalNSamples        | 1.617e+06 |
| ExplainedVariance    | 0.058359  |
------------------------------------
[2018-01-21 13:23:38.643635 UTC] Saving snapshot
[2018-01-21 13:23:38.643880 UTC] Starting iteration 324
[2018-01-21 13:23:38.644089 UTC] Start collecting samples
[2018-01-21 13:23:43.129559 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:43.249382 UTC] Performing policy update
[2018-01-21 13:23:43.250189 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:43.366704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:44.730824 UTC] Performing line search
[2018-01-21 13:23:44.928181 UTC] Updating baseline
[2018-01-21 13:23:46.827880 UTC] Computing logging information
------------------------------------
| Iteration            | 324       |
| ExpectedImprovement  | 0.016751  |
| ActualImprovement    | 0.015638  |
| ImprovementRatio     | 0.93359   |
| MeanKL               | 0.0072828 |
| Entropy              | 0.6366    |
| Perplexity           | 1.89      |
| AveragePolicyStd     | 0.27337   |
| AveragePolicyStd[0]  | 0.27287   |
| AveragePolicyStd[1]  | 0.36279   |
| AveragePolicyStd[2]  | 0.19765   |
| AveragePolicyStd[3]  | 0.28767   |
| AveragePolicyStd[4]  | 0.26153   |
| AveragePolicyStd[5]  | 0.2577    |
| AverageReturn        | 833.7     |
| MinReturn            | 112.28    |
| MaxReturn            | 986.54    |
| StdReturn            | 230.98    |
| AverageEpisodeLength | 897.74    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 235.82    |
| TotalNEpisodes       | 18277     |
| TotalNSamples        | 1.62e+06  |
| ExplainedVariance    | 0.50231   |
------------------------------------
[2018-01-21 13:23:47.452365 UTC] Saving snapshot
[2018-01-21 13:23:47.452600 UTC] Starting iteration 325
[2018-01-21 13:23:47.452782 UTC] Start collecting samples
[2018-01-21 13:23:51.832818 UTC] Computing input variables for policy optimization
[2018-01-21 13:23:51.959513 UTC] Performing policy update
[2018-01-21 13:23:51.960585 UTC] Computing gradient in Euclidean space
[2018-01-21 13:23:52.076918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:23:53.448194 UTC] Performing line search
[2018-01-21 13:23:53.633222 UTC] Updating baseline
[2018-01-21 13:23:55.582250 UTC] Computing logging information
------------------------------------
| Iteration            | 325       |
| ExpectedImprovement  | 0.015709  |
| ActualImprovement    | 0.014706  |
| ImprovementRatio     | 0.93612   |
| MeanKL               | 0.0075946 |
| Entropy              | 0.6261    |
| Perplexity           | 1.8703    |
| AveragePolicyStd     | 0.27282   |
| AveragePolicyStd[0]  | 0.27259   |
| AveragePolicyStd[1]  | 0.36072   |
| AveragePolicyStd[2]  | 0.19748   |
| AveragePolicyStd[3]  | 0.28704   |
| AveragePolicyStd[4]  | 0.26216   |
| AveragePolicyStd[5]  | 0.25691   |
| AverageReturn        | 845.71    |
| MinReturn            | 112.28    |
| MaxReturn            | 1008.9    |
| StdReturn            | 222.74    |
| AverageEpisodeLength | 908.92    |
| MinEpisodeLength     | 124       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 227.16    |
| TotalNEpisodes       | 18280     |
| TotalNSamples        | 1.623e+06 |
| ExplainedVariance    | 0.0062313 |
------------------------------------
[2018-01-21 13:23:56.228216 UTC] Saving snapshot
[2018-01-21 13:23:56.228551 UTC] Starting iteration 326
[2018-01-21 13:23:56.228799 UTC] Start collecting samples
[2018-01-21 13:24:00.934867 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:01.069616 UTC] Performing policy update
[2018-01-21 13:24:01.070268 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:01.196014 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:02.604883 UTC] Performing line search
[2018-01-21 13:24:02.790664 UTC] Updating baseline
[2018-01-21 13:24:04.541631 UTC] Computing logging information
-------------------------------------
| Iteration            | 326        |
| ExpectedImprovement  | 0.013617   |
| ActualImprovement    | 0.012455   |
| ImprovementRatio     | 0.91467    |
| MeanKL               | 0.0078163  |
| Entropy              | 0.61609    |
| Perplexity           | 1.8517     |
| AveragePolicyStd     | 0.27237    |
| AveragePolicyStd[0]  | 0.27168    |
| AveragePolicyStd[1]  | 0.36019    |
| AveragePolicyStd[2]  | 0.19726    |
| AveragePolicyStd[3]  | 0.2877     |
| AveragePolicyStd[4]  | 0.26153    |
| AveragePolicyStd[5]  | 0.25588    |
| AverageReturn        | 859.55     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 211.4      |
| AverageEpisodeLength | 920.81     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.19     |
| TotalNEpisodes       | 18287      |
| TotalNSamples        | 1.63e+06   |
| ExplainedVariance    | -0.0011686 |
-------------------------------------
[2018-01-21 13:24:05.251294 UTC] Saving snapshot
[2018-01-21 13:24:05.251533 UTC] Starting iteration 327
[2018-01-21 13:24:05.251748 UTC] Start collecting samples
[2018-01-21 13:24:09.803971 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:09.958952 UTC] Performing policy update
[2018-01-21 13:24:09.959586 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:10.074999 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:11.536117 UTC] Performing line search
[2018-01-21 13:24:11.732404 UTC] Updating baseline
[2018-01-21 13:24:13.930052 UTC] Computing logging information
------------------------------------
| Iteration            | 327       |
| ExpectedImprovement  | 0.015089  |
| ActualImprovement    | 0.014367  |
| ImprovementRatio     | 0.95215   |
| MeanKL               | 0.0074104 |
| Entropy              | 0.61735   |
| Perplexity           | 1.854     |
| AveragePolicyStd     | 0.27233   |
| AveragePolicyStd[0]  | 0.27235   |
| AveragePolicyStd[1]  | 0.35864   |
| AveragePolicyStd[2]  | 0.19792   |
| AveragePolicyStd[3]  | 0.28783   |
| AveragePolicyStd[4]  | 0.26086   |
| AveragePolicyStd[5]  | 0.25637   |
| AverageReturn        | 883.4     |
| MinReturn            | 130.36    |
| MaxReturn            | 1008.9    |
| StdReturn            | 185.39    |
| AverageEpisodeLength | 939.94    |
| MinEpisodeLength     | 156       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.65    |
| TotalNEpisodes       | 18293     |
| TotalNSamples        | 1.636e+06 |
| ExplainedVariance    | 0.050294  |
------------------------------------
[2018-01-21 13:24:14.609193 UTC] Saving snapshot
[2018-01-21 13:24:14.609419 UTC] Starting iteration 328
[2018-01-21 13:24:14.609526 UTC] Start collecting samples
[2018-01-21 13:24:19.340173 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:19.482031 UTC] Performing policy update
[2018-01-21 13:24:19.482682 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:19.598555 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:20.980613 UTC] Performing line search
[2018-01-21 13:24:21.169655 UTC] Updating baseline
[2018-01-21 13:24:23.007544 UTC] Computing logging information
-------------------------------------
| Iteration            | 328        |
| ExpectedImprovement  | 0.024346   |
| ActualImprovement    | 0.023097   |
| ImprovementRatio     | 0.94872    |
| MeanKL               | 0.0067603  |
| Entropy              | 0.61912    |
| Perplexity           | 1.8573     |
| AveragePolicyStd     | 0.27234    |
| AveragePolicyStd[0]  | 0.27191    |
| AveragePolicyStd[1]  | 0.35811    |
| AveragePolicyStd[2]  | 0.1984     |
| AveragePolicyStd[3]  | 0.287      |
| AveragePolicyStd[4]  | 0.26206    |
| AveragePolicyStd[5]  | 0.25656    |
| AverageReturn        | 870.34     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 205.73     |
| AverageEpisodeLength | 926.49     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.21     |
| TotalNEpisodes       | 18298      |
| TotalNSamples        | 1.6397e+06 |
| ExplainedVariance    | 0.17385    |
-------------------------------------
[2018-01-21 13:24:23.657626 UTC] Saving snapshot
[2018-01-21 13:24:23.657864 UTC] Starting iteration 329
[2018-01-21 13:24:23.658064 UTC] Start collecting samples
[2018-01-21 13:24:28.288197 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:28.429184 UTC] Performing policy update
[2018-01-21 13:24:28.429849 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:28.557700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:29.972850 UTC] Performing line search
[2018-01-21 13:24:30.165313 UTC] Updating baseline
[2018-01-21 13:24:32.275668 UTC] Computing logging information
-------------------------------------
| Iteration            | 329        |
| ExpectedImprovement  | 0.014718   |
| ActualImprovement    | 0.013238   |
| ImprovementRatio     | 0.89941    |
| MeanKL               | 0.007947   |
| Entropy              | 0.61372    |
| Perplexity           | 1.8473     |
| AveragePolicyStd     | 0.27211    |
| AveragePolicyStd[0]  | 0.27141    |
| AveragePolicyStd[1]  | 0.35851    |
| AveragePolicyStd[2]  | 0.19839    |
| AveragePolicyStd[3]  | 0.28596    |
| AveragePolicyStd[4]  | 0.26194    |
| AveragePolicyStd[5]  | 0.25642    |
| AverageReturn        | 868.29     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 206.07     |
| AverageEpisodeLength | 925.73     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.08     |
| TotalNEpisodes       | 18303      |
| TotalNSamples        | 1.6446e+06 |
| ExplainedVariance    | 0.32957    |
-------------------------------------
[2018-01-21 13:24:33.002771 UTC] Saving snapshot
[2018-01-21 13:24:33.003269 UTC] Starting iteration 330
[2018-01-21 13:24:33.003657 UTC] Start collecting samples
[2018-01-21 13:24:37.571606 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:37.700556 UTC] Performing policy update
[2018-01-21 13:24:37.701291 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:37.832951 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:39.235675 UTC] Performing line search
[2018-01-21 13:24:39.423575 UTC] Updating baseline
[2018-01-21 13:24:42.672646 UTC] Computing logging information
-------------------------------------
| Iteration            | 330        |
| ExpectedImprovement  | 0.014652   |
| ActualImprovement    | 0.014081   |
| ImprovementRatio     | 0.96107    |
| MeanKL               | 0.0079189  |
| Entropy              | 0.60448    |
| Perplexity           | 1.8303     |
| AveragePolicyStd     | 0.27158    |
| AveragePolicyStd[0]  | 0.27042    |
| AveragePolicyStd[1]  | 0.35729    |
| AveragePolicyStd[2]  | 0.19943    |
| AveragePolicyStd[3]  | 0.28479    |
| AveragePolicyStd[4]  | 0.26216    |
| AveragePolicyStd[5]  | 0.25537    |
| AverageReturn        | 870.62     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 206.71     |
| AverageEpisodeLength | 925.73     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.08     |
| TotalNEpisodes       | 18308      |
| TotalNSamples        | 1.6496e+06 |
| ExplainedVariance    | -0.013686  |
-------------------------------------
[2018-01-21 13:24:43.357512 UTC] Saving snapshot
[2018-01-21 13:24:43.367281 UTC] Starting iteration 331
[2018-01-21 13:24:43.367525 UTC] Start collecting samples
[2018-01-21 13:24:47.790914 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:47.921346 UTC] Performing policy update
[2018-01-21 13:24:47.922485 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:48.051811 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:49.455433 UTC] Performing line search
[2018-01-21 13:24:49.646279 UTC] Updating baseline
[2018-01-21 13:24:52.172540 UTC] Computing logging information
-------------------------------------
| Iteration            | 331        |
| ExpectedImprovement  | 0.013792   |
| ActualImprovement    | 0.01304    |
| ImprovementRatio     | 0.94543    |
| MeanKL               | 0.0079128  |
| Entropy              | 0.59856    |
| Perplexity           | 1.8195     |
| AveragePolicyStd     | 0.27129    |
| AveragePolicyStd[0]  | 0.27022    |
| AveragePolicyStd[1]  | 0.35617    |
| AveragePolicyStd[2]  | 0.19925    |
| AveragePolicyStd[3]  | 0.28553    |
| AveragePolicyStd[4]  | 0.26122    |
| AveragePolicyStd[5]  | 0.25533    |
| AverageReturn        | 872.32     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 207.29     |
| AverageEpisodeLength | 925.73     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.08     |
| TotalNEpisodes       | 18313      |
| TotalNSamples        | 1.6546e+06 |
| ExplainedVariance    | 0.0096266  |
-------------------------------------
[2018-01-21 13:24:52.818179 UTC] Saving snapshot
[2018-01-21 13:24:52.818477 UTC] Starting iteration 332
[2018-01-21 13:24:52.818679 UTC] Start collecting samples
[2018-01-21 13:24:57.293110 UTC] Computing input variables for policy optimization
[2018-01-21 13:24:57.423300 UTC] Performing policy update
[2018-01-21 13:24:57.423925 UTC] Computing gradient in Euclidean space
[2018-01-21 13:24:57.551528 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:24:58.932316 UTC] Performing line search
[2018-01-21 13:24:59.120732 UTC] Updating baseline
[2018-01-21 13:25:01.371412 UTC] Computing logging information
-------------------------------------
| Iteration            | 332        |
| ExpectedImprovement  | 0.013492   |
| ActualImprovement    | 0.012635   |
| ImprovementRatio     | 0.93649    |
| MeanKL               | 0.0071981  |
| Entropy              | 0.59569    |
| Perplexity           | 1.8143     |
| AveragePolicyStd     | 0.27117    |
| AveragePolicyStd[0]  | 0.27042    |
| AveragePolicyStd[1]  | 0.35664    |
| AveragePolicyStd[2]  | 0.19947    |
| AveragePolicyStd[3]  | 0.28501    |
| AveragePolicyStd[4]  | 0.26065    |
| AveragePolicyStd[5]  | 0.25481    |
| AverageReturn        | 872.35     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 207.28     |
| AverageEpisodeLength | 925.73     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.08     |
| TotalNEpisodes       | 18316      |
| TotalNSamples        | 1.6576e+06 |
| ExplainedVariance    | -0.012194  |
-------------------------------------
[2018-01-21 13:25:02.007972 UTC] Saving snapshot
[2018-01-21 13:25:02.008463 UTC] Starting iteration 333
[2018-01-21 13:25:02.008873 UTC] Start collecting samples
[2018-01-21 13:25:06.837269 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:06.977620 UTC] Performing policy update
[2018-01-21 13:25:06.978223 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:07.089024 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:08.525146 UTC] Performing line search
[2018-01-21 13:25:08.720493 UTC] Updating baseline
[2018-01-21 13:25:10.516127 UTC] Computing logging information
-------------------------------------
| Iteration            | 333        |
| ExpectedImprovement  | 0.015357   |
| ActualImprovement    | 0.014802   |
| ImprovementRatio     | 0.96386    |
| MeanKL               | 0.0072024  |
| Entropy              | 0.5909     |
| Perplexity           | 1.8056     |
| AveragePolicyStd     | 0.2709     |
| AveragePolicyStd[0]  | 0.27154    |
| AveragePolicyStd[1]  | 0.35588    |
| AveragePolicyStd[2]  | 0.1999     |
| AveragePolicyStd[3]  | 0.28394    |
| AveragePolicyStd[4]  | 0.25951    |
| AveragePolicyStd[5]  | 0.25461    |
| AverageReturn        | 863.19     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 221.17     |
| AverageEpisodeLength | 913.96     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.31     |
| TotalNEpisodes       | 18326      |
| TotalNSamples        | 1.6664e+06 |
| ExplainedVariance    | 0.18696    |
-------------------------------------
[2018-01-21 13:25:11.196350 UTC] Saving snapshot
[2018-01-21 13:25:11.196597 UTC] Starting iteration 334
[2018-01-21 13:25:11.196810 UTC] Start collecting samples
[2018-01-21 13:25:15.881308 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:16.019629 UTC] Performing policy update
[2018-01-21 13:25:16.020246 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:16.139451 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:17.561371 UTC] Performing line search
[2018-01-21 13:25:17.766760 UTC] Updating baseline
[2018-01-21 13:25:19.581701 UTC] Computing logging information
-------------------------------------
| Iteration            | 334        |
| ExpectedImprovement  | 0.015055   |
| ActualImprovement    | 0.015418   |
| ImprovementRatio     | 1.0241     |
| MeanKL               | 0.0070334  |
| Entropy              | 0.59337    |
| Perplexity           | 1.8101     |
| AveragePolicyStd     | 0.27091    |
| AveragePolicyStd[0]  | 0.27269    |
| AveragePolicyStd[1]  | 0.35432    |
| AveragePolicyStd[2]  | 0.20003    |
| AveragePolicyStd[3]  | 0.28254    |
| AveragePolicyStd[4]  | 0.26005    |
| AveragePolicyStd[5]  | 0.25585    |
| AverageReturn        | 856.86     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 228.78     |
| AverageEpisodeLength | 907.61     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.82     |
| TotalNEpisodes       | 18331      |
| TotalNSamples        | 1.6708e+06 |
| ExplainedVariance    | 0.2746     |
-------------------------------------
[2018-01-21 13:25:20.225773 UTC] Saving snapshot
[2018-01-21 13:25:20.226010 UTC] Starting iteration 335
[2018-01-21 13:25:20.226158 UTC] Start collecting samples
[2018-01-21 13:25:24.877044 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:25.010281 UTC] Performing policy update
[2018-01-21 13:25:25.011367 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:25.135105 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:26.545974 UTC] Performing line search
[2018-01-21 13:25:26.730592 UTC] Updating baseline
[2018-01-21 13:25:28.865855 UTC] Computing logging information
-------------------------------------
| Iteration            | 335        |
| ExpectedImprovement  | 0.014389   |
| ActualImprovement    | 0.013616   |
| ImprovementRatio     | 0.94625    |
| MeanKL               | 0.0077498  |
| Entropy              | 0.61343    |
| Perplexity           | 1.8467     |
| AveragePolicyStd     | 0.27181    |
| AveragePolicyStd[0]  | 0.27354    |
| AveragePolicyStd[1]  | 0.35485    |
| AveragePolicyStd[2]  | 0.20067    |
| AveragePolicyStd[3]  | 0.28478    |
| AveragePolicyStd[4]  | 0.25976    |
| AveragePolicyStd[5]  | 0.25724    |
| AverageReturn        | 850.33     |
| MinReturn            | 130.36     |
| MaxReturn            | 1008.9     |
| StdReturn            | 238.57     |
| AverageEpisodeLength | 900.11     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.9      |
| TotalNEpisodes       | 18335      |
| TotalNSamples        | 1.6741e+06 |
| ExplainedVariance    | 0.30869    |
-------------------------------------
[2018-01-21 13:25:29.549815 UTC] Saving snapshot
[2018-01-21 13:25:29.550056 UTC] Starting iteration 336
[2018-01-21 13:25:29.550271 UTC] Start collecting samples
[2018-01-21 13:25:34.183809 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:34.342078 UTC] Performing policy update
[2018-01-21 13:25:34.342652 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:34.460876 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:35.818360 UTC] Performing line search
[2018-01-21 13:25:36.010906 UTC] Updating baseline
[2018-01-21 13:25:37.952645 UTC] Computing logging information
-------------------------------------
| Iteration            | 336        |
| ExpectedImprovement  | 0.014814   |
| ActualImprovement    | 0.014473   |
| ImprovementRatio     | 0.97699    |
| MeanKL               | 0.0070964  |
| Entropy              | 0.59904    |
| Perplexity           | 1.8204     |
| AveragePolicyStd     | 0.27105    |
| AveragePolicyStd[0]  | 0.27397    |
| AveragePolicyStd[1]  | 0.35354    |
| AveragePolicyStd[2]  | 0.20177    |
| AveragePolicyStd[3]  | 0.28262    |
| AveragePolicyStd[4]  | 0.2583     |
| AveragePolicyStd[5]  | 0.2561     |
| AverageReturn        | 866.06     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 225.66     |
| AverageEpisodeLength | 912.8      |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.51     |
| TotalNEpisodes       | 18343      |
| TotalNSamples        | 1.6821e+06 |
| ExplainedVariance    | 0.068342   |
-------------------------------------
[2018-01-21 13:25:38.580165 UTC] Saving snapshot
[2018-01-21 13:25:38.580436 UTC] Starting iteration 337
[2018-01-21 13:25:38.580627 UTC] Start collecting samples
[2018-01-21 13:25:43.072968 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:43.199971 UTC] Performing policy update
[2018-01-21 13:25:43.200574 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:43.312360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:44.734134 UTC] Performing line search
[2018-01-21 13:25:44.919342 UTC] Updating baseline
[2018-01-21 13:25:46.908615 UTC] Computing logging information
-------------------------------------
| Iteration            | 337        |
| ExpectedImprovement  | 0.014545   |
| ActualImprovement    | 0.013874   |
| ImprovementRatio     | 0.95391    |
| MeanKL               | 0.0074162  |
| Entropy              | 0.60752    |
| Perplexity           | 1.8359     |
| AveragePolicyStd     | 0.27141    |
| AveragePolicyStd[0]  | 0.27372    |
| AveragePolicyStd[1]  | 0.35441    |
| AveragePolicyStd[2]  | 0.20282    |
| AveragePolicyStd[3]  | 0.28307    |
| AveragePolicyStd[4]  | 0.25719    |
| AveragePolicyStd[5]  | 0.25725    |
| AverageReturn        | 866.8      |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 225.82     |
| AverageEpisodeLength | 912.61     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.02     |
| TotalNEpisodes       | 18347      |
| TotalNSamples        | 1.6854e+06 |
| ExplainedVariance    | 0.35162    |
-------------------------------------
[2018-01-21 13:25:47.548155 UTC] Saving snapshot
[2018-01-21 13:25:47.548389 UTC] Starting iteration 338
[2018-01-21 13:25:47.548593 UTC] Start collecting samples
[2018-01-21 13:25:51.998083 UTC] Computing input variables for policy optimization
[2018-01-21 13:25:52.136543 UTC] Performing policy update
[2018-01-21 13:25:52.137162 UTC] Computing gradient in Euclidean space
[2018-01-21 13:25:52.251387 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:25:53.620024 UTC] Performing line search
[2018-01-21 13:25:53.808517 UTC] Updating baseline
[2018-01-21 13:25:55.547679 UTC] Computing logging information
-------------------------------------
| Iteration            | 338        |
| ExpectedImprovement  | 0.01601    |
| ActualImprovement    | 0.01522    |
| ImprovementRatio     | 0.95065    |
| MeanKL               | 0.0070739  |
| Entropy              | 0.6076     |
| Perplexity           | 1.836      |
| AveragePolicyStd     | 0.27144    |
| AveragePolicyStd[0]  | 0.27464    |
| AveragePolicyStd[1]  | 0.355      |
| AveragePolicyStd[2]  | 0.20323    |
| AveragePolicyStd[3]  | 0.28342    |
| AveragePolicyStd[4]  | 0.25642    |
| AveragePolicyStd[5]  | 0.25591    |
| AverageReturn        | 878.81     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 211.96     |
| AverageEpisodeLength | 926.93     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.59     |
| TotalNEpisodes       | 18353      |
| TotalNSamples        | 1.6911e+06 |
| ExplainedVariance    | 0.22264    |
-------------------------------------
[2018-01-21 13:25:56.179577 UTC] Saving snapshot
[2018-01-21 13:25:56.179812 UTC] Starting iteration 339
[2018-01-21 13:25:56.179992 UTC] Start collecting samples
[2018-01-21 13:26:00.692731 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:00.827194 UTC] Performing policy update
[2018-01-21 13:26:00.830447 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:00.951906 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:02.298686 UTC] Performing line search
[2018-01-21 13:26:02.481393 UTC] Updating baseline
[2018-01-21 13:26:04.820088 UTC] Computing logging information
-------------------------------------
| Iteration            | 339        |
| ExpectedImprovement  | 0.014651   |
| ActualImprovement    | 0.014501   |
| ImprovementRatio     | 0.98978    |
| MeanKL               | 0.0071322  |
| Entropy              | 0.60061    |
| Perplexity           | 1.8232     |
| AveragePolicyStd     | 0.27116    |
| AveragePolicyStd[0]  | 0.27524    |
| AveragePolicyStd[1]  | 0.35518    |
| AveragePolicyStd[2]  | 0.20259    |
| AveragePolicyStd[3]  | 0.2819     |
| AveragePolicyStd[4]  | 0.2556     |
| AveragePolicyStd[5]  | 0.25643    |
| AverageReturn        | 888.35     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 203.25     |
| AverageEpisodeLength | 934.66     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.2      |
| TotalNEpisodes       | 18356      |
| TotalNSamples        | 1.6941e+06 |
| ExplainedVariance    | -0.024994  |
-------------------------------------
[2018-01-21 13:26:05.435444 UTC] Saving snapshot
[2018-01-21 13:26:05.435723 UTC] Starting iteration 340
[2018-01-21 13:26:05.435903 UTC] Start collecting samples
[2018-01-21 13:26:10.022871 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:10.150176 UTC] Performing policy update
[2018-01-21 13:26:10.150965 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:10.263380 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:11.615434 UTC] Performing line search
[2018-01-21 13:26:11.802883 UTC] Updating baseline
[2018-01-21 13:26:13.687851 UTC] Computing logging information
-------------------------------------
| Iteration            | 340        |
| ExpectedImprovement  | 0.01432    |
| ActualImprovement    | 0.013587   |
| ImprovementRatio     | 0.94876    |
| MeanKL               | 0.0076863  |
| Entropy              | 0.59437    |
| Perplexity           | 1.8119     |
| AveragePolicyStd     | 0.27087    |
| AveragePolicyStd[0]  | 0.27444    |
| AveragePolicyStd[1]  | 0.35558    |
| AveragePolicyStd[2]  | 0.20308    |
| AveragePolicyStd[3]  | 0.28153    |
| AveragePolicyStd[4]  | 0.25538    |
| AveragePolicyStd[5]  | 0.25525    |
| AverageReturn        | 889.38     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 203.64     |
| AverageEpisodeLength | 934.66     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.2      |
| TotalNEpisodes       | 18362      |
| TotalNSamples        | 1.7001e+06 |
| ExplainedVariance    | -0.0059264 |
-------------------------------------
[2018-01-21 13:26:14.400203 UTC] Saving snapshot
[2018-01-21 13:26:14.412490 UTC] Starting iteration 341
[2018-01-21 13:26:14.412720 UTC] Start collecting samples
[2018-01-21 13:26:18.983509 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:19.143230 UTC] Performing policy update
[2018-01-21 13:26:19.144021 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:19.266563 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:20.646273 UTC] Performing line search
[2018-01-21 13:26:20.833073 UTC] Updating baseline
[2018-01-21 13:26:23.204037 UTC] Computing logging information
-------------------------------------
| Iteration            | 341        |
| ExpectedImprovement  | 0.012907   |
| ActualImprovement    | 0.012535   |
| ImprovementRatio     | 0.9712     |
| MeanKL               | 0.0079106  |
| Entropy              | 0.5951     |
| Perplexity           | 1.8132     |
| AveragePolicyStd     | 0.27101    |
| AveragePolicyStd[0]  | 0.27492    |
| AveragePolicyStd[1]  | 0.35731    |
| AveragePolicyStd[2]  | 0.20291    |
| AveragePolicyStd[3]  | 0.2823     |
| AveragePolicyStd[4]  | 0.2558     |
| AveragePolicyStd[5]  | 0.25284    |
| AverageReturn        | 897.23     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 194.56     |
| AverageEpisodeLength | 941.23     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.86     |
| TotalNEpisodes       | 18366      |
| TotalNSamples        | 1.7041e+06 |
| ExplainedVariance    | 0.018681   |
-------------------------------------
[2018-01-21 13:26:23.839479 UTC] Saving snapshot
[2018-01-21 13:26:23.839709 UTC] Starting iteration 342
[2018-01-21 13:26:23.839877 UTC] Start collecting samples
[2018-01-21 13:26:28.427789 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:28.557892 UTC] Performing policy update
[2018-01-21 13:26:28.558859 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:28.673431 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:30.085038 UTC] Performing line search
[2018-01-21 13:26:30.270664 UTC] Updating baseline
[2018-01-21 13:26:32.029846 UTC] Computing logging information
-------------------------------------
| Iteration            | 342        |
| ExpectedImprovement  | 0.015891   |
| ActualImprovement    | 0.015005   |
| ImprovementRatio     | 0.94424    |
| MeanKL               | 0.0073618  |
| Entropy              | 0.57605    |
| Perplexity           | 1.779      |
| AveragePolicyStd     | 0.27007    |
| AveragePolicyStd[0]  | 0.27586    |
| AveragePolicyStd[1]  | 0.35453    |
| AveragePolicyStd[2]  | 0.20285    |
| AveragePolicyStd[3]  | 0.2814     |
| AveragePolicyStd[4]  | 0.25351    |
| AveragePolicyStd[5]  | 0.25229    |
| AverageReturn        | 898.04     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 194.88     |
| AverageEpisodeLength | 941.23     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.86     |
| TotalNEpisodes       | 18371      |
| TotalNSamples        | 1.7091e+06 |
| ExplainedVariance    | 0.018173   |
-------------------------------------
[2018-01-21 13:26:32.709986 UTC] Saving snapshot
[2018-01-21 13:26:32.710276 UTC] Starting iteration 343
[2018-01-21 13:26:32.710494 UTC] Start collecting samples
[2018-01-21 13:26:37.402855 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:37.522158 UTC] Performing policy update
[2018-01-21 13:26:37.522784 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:37.640470 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:39.022648 UTC] Performing line search
[2018-01-21 13:26:39.206818 UTC] Updating baseline
[2018-01-21 13:26:41.211168 UTC] Computing logging information
-------------------------------------
| Iteration            | 343        |
| ExpectedImprovement  | 0.014397   |
| ActualImprovement    | 0.013646   |
| ImprovementRatio     | 0.94782    |
| MeanKL               | 0.007359   |
| Entropy              | 0.57674    |
| Perplexity           | 1.7802     |
| AveragePolicyStd     | 0.26996    |
| AveragePolicyStd[0]  | 0.27585    |
| AveragePolicyStd[1]  | 0.35328    |
| AveragePolicyStd[2]  | 0.20464    |
| AveragePolicyStd[3]  | 0.28075    |
| AveragePolicyStd[4]  | 0.25384    |
| AveragePolicyStd[5]  | 0.25141    |
| AverageReturn        | 904.82     |
| MinReturn            | 154.46     |
| MaxReturn            | 1008.9     |
| StdReturn            | 191.88     |
| AverageEpisodeLength | 945.7      |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.8      |
| TotalNEpisodes       | 18378      |
| TotalNSamples        | 1.7156e+06 |
| ExplainedVariance    | 0.2332     |
-------------------------------------
[2018-01-21 13:26:41.839364 UTC] Saving snapshot
[2018-01-21 13:26:41.839605 UTC] Starting iteration 344
[2018-01-21 13:26:41.839788 UTC] Start collecting samples
[2018-01-21 13:26:46.440718 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:46.567247 UTC] Performing policy update
[2018-01-21 13:26:46.568308 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:46.686781 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:48.111754 UTC] Performing line search
[2018-01-21 13:26:48.332256 UTC] Updating baseline
[2018-01-21 13:26:50.438726 UTC] Computing logging information
------------------------------------
| Iteration            | 344       |
| ExpectedImprovement  | 0.017104  |
| ActualImprovement    | 0.015853  |
| ImprovementRatio     | 0.92687   |
| MeanKL               | 0.0069911 |
| Entropy              | 0.57997   |
| Perplexity           | 1.786     |
| AveragePolicyStd     | 0.27002   |
| AveragePolicyStd[0]  | 0.2768    |
| AveragePolicyStd[1]  | 0.35216   |
| AveragePolicyStd[2]  | 0.20576   |
| AveragePolicyStd[3]  | 0.28086   |
| AveragePolicyStd[4]  | 0.25448   |
| AveragePolicyStd[5]  | 0.25005   |
| AverageReturn        | 898.56    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 199.19    |
| AverageEpisodeLength | 939.4     |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.65    |
| TotalNEpisodes       | 18382     |
| TotalNSamples        | 1.719e+06 |
| ExplainedVariance    | 0.20118   |
------------------------------------
[2018-01-21 13:26:51.090285 UTC] Saving snapshot
[2018-01-21 13:26:51.090585 UTC] Starting iteration 345
[2018-01-21 13:26:51.090780 UTC] Start collecting samples
[2018-01-21 13:26:55.637009 UTC] Computing input variables for policy optimization
[2018-01-21 13:26:55.771746 UTC] Performing policy update
[2018-01-21 13:26:55.772396 UTC] Computing gradient in Euclidean space
[2018-01-21 13:26:55.893981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:26:57.317588 UTC] Performing line search
[2018-01-21 13:26:57.496932 UTC] Updating baseline
[2018-01-21 13:27:00.196474 UTC] Computing logging information
------------------------------------
| Iteration            | 345       |
| ExpectedImprovement  | 0.017727  |
| ActualImprovement    | 0.016305  |
| ImprovementRatio     | 0.91977   |
| MeanKL               | 0.007068  |
| Entropy              | 0.58098   |
| Perplexity           | 1.7878    |
| AveragePolicyStd     | 0.27008   |
| AveragePolicyStd[0]  | 0.27636   |
| AveragePolicyStd[1]  | 0.35161   |
| AveragePolicyStd[2]  | 0.2047    |
| AveragePolicyStd[3]  | 0.28163   |
| AveragePolicyStd[4]  | 0.25532   |
| AveragePolicyStd[5]  | 0.25088   |
| AverageReturn        | 889.34    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 207.24    |
| AverageEpisodeLength | 929.84    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 196.92    |
| TotalNEpisodes       | 18388     |
| TotalNSamples        | 1.724e+06 |
| ExplainedVariance    | 0.29517   |
------------------------------------
[2018-01-21 13:27:00.900994 UTC] Saving snapshot
[2018-01-21 13:27:00.901239 UTC] Starting iteration 346
[2018-01-21 13:27:00.901404 UTC] Start collecting samples
[2018-01-21 13:27:05.561823 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:05.683715 UTC] Performing policy update
[2018-01-21 13:27:05.684400 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:05.793461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:07.208072 UTC] Performing line search
[2018-01-21 13:27:07.392874 UTC] Updating baseline
[2018-01-21 13:27:09.480052 UTC] Computing logging information
------------------------------------
| Iteration            | 346       |
| ExpectedImprovement  | 0.016789  |
| ActualImprovement    | 0.016088  |
| ImprovementRatio     | 0.95827   |
| MeanKL               | 0.007439  |
| Entropy              | 0.56929   |
| Perplexity           | 1.767     |
| AveragePolicyStd     | 0.26965   |
| AveragePolicyStd[0]  | 0.27476   |
| AveragePolicyStd[1]  | 0.3529    |
| AveragePolicyStd[2]  | 0.20425   |
| AveragePolicyStd[3]  | 0.28165   |
| AveragePolicyStd[4]  | 0.25477   |
| AveragePolicyStd[5]  | 0.24956   |
| AverageReturn        | 898.15    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 196.57    |
| AverageEpisodeLength | 936.92    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.31    |
| TotalNEpisodes       | 18394     |
| TotalNSamples        | 1.73e+06  |
| ExplainedVariance    | -0.024285 |
------------------------------------
[2018-01-21 13:27:10.197083 UTC] Saving snapshot
[2018-01-21 13:27:10.197366 UTC] Starting iteration 347
[2018-01-21 13:27:10.197581 UTC] Start collecting samples
[2018-01-21 13:27:14.595679 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:14.718175 UTC] Performing policy update
[2018-01-21 13:27:14.718866 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:14.832953 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:16.297012 UTC] Performing line search
[2018-01-21 13:27:16.498214 UTC] Updating baseline
[2018-01-21 13:27:19.065868 UTC] Computing logging information
------------------------------------
| Iteration            | 347       |
| ExpectedImprovement  | 0.015864  |
| ActualImprovement    | 0.015079  |
| ImprovementRatio     | 0.95053   |
| MeanKL               | 0.0073645 |
| Entropy              | 0.56236   |
| Perplexity           | 1.7548    |
| AveragePolicyStd     | 0.26935   |
| AveragePolicyStd[0]  | 0.27513   |
| AveragePolicyStd[1]  | 0.35406   |
| AveragePolicyStd[2]  | 0.20532   |
| AveragePolicyStd[3]  | 0.27973   |
| AveragePolicyStd[4]  | 0.25423   |
| AveragePolicyStd[5]  | 0.24763   |
| AverageReturn        | 905.55    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 186.71    |
| AverageEpisodeLength | 943.29    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 177.24    |
| TotalNEpisodes       | 18398     |
| TotalNSamples        | 1.734e+06 |
| ExplainedVariance    | 0.022518  |
------------------------------------
[2018-01-21 13:27:19.687262 UTC] Saving snapshot
[2018-01-21 13:27:19.687440 UTC] Starting iteration 348
[2018-01-21 13:27:19.687546 UTC] Start collecting samples
[2018-01-21 13:27:24.114858 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:24.261499 UTC] Performing policy update
[2018-01-21 13:27:24.262121 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:24.382612 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:25.838288 UTC] Performing line search
[2018-01-21 13:27:26.042950 UTC] Updating baseline
[2018-01-21 13:27:28.202394 UTC] Computing logging information
------------------------------------
| Iteration            | 348       |
| ExpectedImprovement  | 0.012826  |
| ActualImprovement    | 0.012422  |
| ImprovementRatio     | 0.96851   |
| MeanKL               | 0.0081941 |
| Entropy              | 0.5399    |
| Perplexity           | 1.7158    |
| AveragePolicyStd     | 0.26824   |
| AveragePolicyStd[0]  | 0.27462   |
| AveragePolicyStd[1]  | 0.35149   |
| AveragePolicyStd[2]  | 0.20563   |
| AveragePolicyStd[3]  | 0.27779   |
| AveragePolicyStd[4]  | 0.25403   |
| AveragePolicyStd[5]  | 0.24588   |
| AverageReturn        | 909.39    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 186.44    |
| AverageEpisodeLength | 944.05    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 177.32    |
| TotalNEpisodes       | 18404     |
| TotalNSamples        | 1.74e+06  |
| ExplainedVariance    | 0.13623   |
------------------------------------
[2018-01-21 13:27:28.927770 UTC] Saving snapshot
[2018-01-21 13:27:28.928008 UTC] Starting iteration 349
[2018-01-21 13:27:28.928155 UTC] Start collecting samples
[2018-01-21 13:27:33.518452 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:33.665962 UTC] Performing policy update
[2018-01-21 13:27:33.666565 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:33.794409 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:35.205804 UTC] Performing line search
[2018-01-21 13:27:35.395671 UTC] Updating baseline
[2018-01-21 13:27:37.232469 UTC] Computing logging information
------------------------------------
| Iteration            | 349       |
| ExpectedImprovement  | 0.013146  |
| ActualImprovement    | 0.012499  |
| ImprovementRatio     | 0.95081   |
| MeanKL               | 0.0076019 |
| Entropy              | 0.52822   |
| Perplexity           | 1.6959    |
| AveragePolicyStd     | 0.26771   |
| AveragePolicyStd[0]  | 0.2738    |
| AveragePolicyStd[1]  | 0.35048   |
| AveragePolicyStd[2]  | 0.20538   |
| AveragePolicyStd[3]  | 0.27828   |
| AveragePolicyStd[4]  | 0.25211   |
| AveragePolicyStd[5]  | 0.24618   |
| AverageReturn        | 910.48    |
| MinReturn            | 154.46    |
| MaxReturn            | 1005.9    |
| StdReturn            | 186.85    |
| AverageEpisodeLength | 944.05    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 177.32    |
| TotalNEpisodes       | 18409     |
| TotalNSamples        | 1.745e+06 |
| ExplainedVariance    | 0.11289   |
------------------------------------
[2018-01-21 13:27:37.873482 UTC] Saving snapshot
[2018-01-21 13:27:37.873691 UTC] Starting iteration 350
[2018-01-21 13:27:37.873854 UTC] Start collecting samples
[2018-01-21 13:27:42.318557 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:42.452981 UTC] Performing policy update
[2018-01-21 13:27:42.453686 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:42.567152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:43.994172 UTC] Performing line search
[2018-01-21 13:27:44.185363 UTC] Updating baseline
[2018-01-21 13:27:46.350830 UTC] Computing logging information
-------------------------------------
| Iteration            | 350        |
| ExpectedImprovement  | 0.016104   |
| ActualImprovement    | 0.014843   |
| ImprovementRatio     | 0.92171    |
| MeanKL               | 0.0074293  |
| Entropy              | 0.52246    |
| Perplexity           | 1.6862     |
| AveragePolicyStd     | 0.26744    |
| AveragePolicyStd[0]  | 0.27538    |
| AveragePolicyStd[1]  | 0.34775    |
| AveragePolicyStd[2]  | 0.20392    |
| AveragePolicyStd[3]  | 0.27964    |
| AveragePolicyStd[4]  | 0.25235    |
| AveragePolicyStd[5]  | 0.24559    |
| AverageReturn        | 912.99     |
| MinReturn            | 154.46     |
| MaxReturn            | 1017.9     |
| StdReturn            | 187.67     |
| AverageEpisodeLength | 944.05     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.32     |
| TotalNEpisodes       | 18414      |
| TotalNSamples        | 1.75e+06   |
| ExplainedVariance    | -0.0091418 |
-------------------------------------
[2018-01-21 13:27:46.999387 UTC] Saving snapshot
[2018-01-21 13:27:47.008879 UTC] Starting iteration 351
[2018-01-21 13:27:47.009122 UTC] Start collecting samples
[2018-01-21 13:27:51.588657 UTC] Computing input variables for policy optimization
[2018-01-21 13:27:51.714179 UTC] Performing policy update
[2018-01-21 13:27:51.714791 UTC] Computing gradient in Euclidean space
[2018-01-21 13:27:51.834154 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:27:53.229041 UTC] Performing line search
[2018-01-21 13:27:53.415931 UTC] Updating baseline
[2018-01-21 13:27:55.573104 UTC] Computing logging information
------------------------------------
| Iteration            | 351       |
| ExpectedImprovement  | 0.015093  |
| ActualImprovement    | 0.014375  |
| ImprovementRatio     | 0.95243   |
| MeanKL               | 0.0073525 |
| Entropy              | 0.50882   |
| Perplexity           | 1.6633    |
| AveragePolicyStd     | 0.26678   |
| AveragePolicyStd[0]  | 0.27461   |
| AveragePolicyStd[1]  | 0.34683   |
| AveragePolicyStd[2]  | 0.20421   |
| AveragePolicyStd[3]  | 0.27836   |
| AveragePolicyStd[4]  | 0.25029   |
| AveragePolicyStd[5]  | 0.24637   |
| AverageReturn        | 916.93    |
| MinReturn            | 154.46    |
| MaxReturn            | 1032.1    |
| StdReturn            | 189.09    |
| AverageEpisodeLength | 944.05    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 177.32    |
| TotalNEpisodes       | 18420     |
| TotalNSamples        | 1.756e+06 |
| ExplainedVariance    | 0.015803  |
------------------------------------
[2018-01-21 13:27:56.262177 UTC] Saving snapshot
[2018-01-21 13:27:56.262411 UTC] Starting iteration 352
[2018-01-21 13:27:56.262575 UTC] Start collecting samples
[2018-01-21 13:28:00.816856 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:00.955233 UTC] Performing policy update
[2018-01-21 13:28:00.956302 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:01.071080 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:02.477027 UTC] Performing line search
[2018-01-21 13:28:02.661711 UTC] Updating baseline
[2018-01-21 13:28:04.745957 UTC] Computing logging information
-------------------------------------
| Iteration            | 352        |
| ExpectedImprovement  | 0.015944   |
| ActualImprovement    | 0.015095   |
| ImprovementRatio     | 0.94672    |
| MeanKL               | 0.0068579  |
| Entropy              | 0.4989     |
| Perplexity           | 1.6469     |
| AveragePolicyStd     | 0.26627    |
| AveragePolicyStd[0]  | 0.27551    |
| AveragePolicyStd[1]  | 0.34497    |
| AveragePolicyStd[2]  | 0.20451    |
| AveragePolicyStd[3]  | 0.27746    |
| AveragePolicyStd[4]  | 0.24934    |
| AveragePolicyStd[5]  | 0.24583    |
| AverageReturn        | 922.52     |
| MinReturn            | 185.87     |
| MaxReturn            | 1032.1     |
| StdReturn            | 177.62     |
| AverageEpisodeLength | 947.9      |
| MinEpisodeLength     | 250        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.71     |
| TotalNEpisodes       | 18427      |
| TotalNSamples        | 1.7622e+06 |
| ExplainedVariance    | 0.16035    |
-------------------------------------
[2018-01-21 13:28:05.396328 UTC] Saving snapshot
[2018-01-21 13:28:05.396591 UTC] Starting iteration 353
[2018-01-21 13:28:05.396779 UTC] Start collecting samples
[2018-01-21 13:28:09.848204 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:09.991217 UTC] Performing policy update
[2018-01-21 13:28:09.992032 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:10.116000 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:11.516201 UTC] Performing line search
[2018-01-21 13:28:11.709404 UTC] Updating baseline
[2018-01-21 13:28:13.576163 UTC] Computing logging information
------------------------------------
| Iteration            | 353       |
| ExpectedImprovement  | 0.016186  |
| ActualImprovement    | 0.015729  |
| ImprovementRatio     | 0.97176   |
| MeanKL               | 0.0070616 |
| Entropy              | 0.49717   |
| Perplexity           | 1.6441    |
| AveragePolicyStd     | 0.26623   |
| AveragePolicyStd[0]  | 0.27576   |
| AveragePolicyStd[1]  | 0.34585   |
| AveragePolicyStd[2]  | 0.20435   |
| AveragePolicyStd[3]  | 0.27669   |
| AveragePolicyStd[4]  | 0.24883   |
| AveragePolicyStd[5]  | 0.24592   |
| AverageReturn        | 928.79    |
| MinReturn            | 185.87    |
| MaxReturn            | 1032.1    |
| StdReturn            | 166.27    |
| AverageEpisodeLength | 952.28    |
| MinEpisodeLength     | 250       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 154.66    |
| TotalNEpisodes       | 18430     |
| TotalNSamples        | 1.765e+06 |
| ExplainedVariance    | 0.23607   |
------------------------------------
[2018-01-21 13:28:14.197688 UTC] Saving snapshot
[2018-01-21 13:28:14.197953 UTC] Starting iteration 354
[2018-01-21 13:28:14.198157 UTC] Start collecting samples
[2018-01-21 13:28:18.743954 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:18.865812 UTC] Performing policy update
[2018-01-21 13:28:18.866926 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:18.989992 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:20.385786 UTC] Performing line search
[2018-01-21 13:28:20.577512 UTC] Updating baseline
[2018-01-21 13:28:22.733798 UTC] Computing logging information
------------------------------------
| Iteration            | 354       |
| ExpectedImprovement  | 0.01535   |
| ActualImprovement    | 0.014705  |
| ImprovementRatio     | 0.958     |
| MeanKL               | 0.0077321 |
| Entropy              | 0.50228   |
| Perplexity           | 1.6525    |
| AveragePolicyStd     | 0.26643   |
| AveragePolicyStd[0]  | 0.27645   |
| AveragePolicyStd[1]  | 0.34531   |
| AveragePolicyStd[2]  | 0.20484   |
| AveragePolicyStd[3]  | 0.27785   |
| AveragePolicyStd[4]  | 0.24797   |
| AveragePolicyStd[5]  | 0.24618   |
| AverageReturn        | 939.23    |
| MinReturn            | 278.36    |
| MaxReturn            | 1046.7    |
| StdReturn            | 149.41    |
| AverageEpisodeLength | 959.78    |
| MinEpisodeLength     | 302       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 137.68    |
| TotalNEpisodes       | 18436     |
| TotalNSamples        | 1.771e+06 |
| ExplainedVariance    | 0.029965  |
------------------------------------
[2018-01-21 13:28:23.362511 UTC] Saving snapshot
[2018-01-21 13:28:23.362812 UTC] Starting iteration 355
[2018-01-21 13:28:23.362984 UTC] Start collecting samples
[2018-01-21 13:28:28.037253 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:28.186189 UTC] Performing policy update
[2018-01-21 13:28:28.187291 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:28.312777 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:29.724525 UTC] Performing line search
[2018-01-21 13:28:29.916640 UTC] Updating baseline
[2018-01-21 13:28:31.938299 UTC] Computing logging information
------------------------------------
| Iteration            | 355       |
| ExpectedImprovement  | 0.013744  |
| ActualImprovement    | 0.013041  |
| ImprovementRatio     | 0.94891   |
| MeanKL               | 0.0079104 |
| Entropy              | 0.50621   |
| Perplexity           | 1.659     |
| AveragePolicyStd     | 0.26664   |
| AveragePolicyStd[0]  | 0.27646   |
| AveragePolicyStd[1]  | 0.34565   |
| AveragePolicyStd[2]  | 0.20477   |
| AveragePolicyStd[3]  | 0.27893   |
| AveragePolicyStd[4]  | 0.24803   |
| AveragePolicyStd[5]  | 0.24598   |
| AverageReturn        | 941.09    |
| MinReturn            | 278.36    |
| MaxReturn            | 1046.7    |
| StdReturn            | 150.02    |
| AverageEpisodeLength | 959.78    |
| MinEpisodeLength     | 302       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 137.68    |
| TotalNEpisodes       | 18442     |
| TotalNSamples        | 1.777e+06 |
| ExplainedVariance    | 0.0614    |
------------------------------------
[2018-01-21 13:28:32.556337 UTC] Saving snapshot
[2018-01-21 13:28:32.556575 UTC] Starting iteration 356
[2018-01-21 13:28:32.556741 UTC] Start collecting samples
[2018-01-21 13:28:37.131131 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:37.268607 UTC] Performing policy update
[2018-01-21 13:28:37.269556 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:37.386974 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:38.823512 UTC] Performing line search
[2018-01-21 13:28:39.035456 UTC] Updating baseline
[2018-01-21 13:28:41.053555 UTC] Computing logging information
-------------------------------------
| Iteration            | 356        |
| ExpectedImprovement  | 0.016829   |
| ActualImprovement    | 0.016047   |
| ImprovementRatio     | 0.95352    |
| MeanKL               | 0.0075594  |
| Entropy              | 0.49421    |
| Perplexity           | 1.6392     |
| AveragePolicyStd     | 0.26606    |
| AveragePolicyStd[0]  | 0.27525    |
| AveragePolicyStd[1]  | 0.34427    |
| AveragePolicyStd[2]  | 0.20431    |
| AveragePolicyStd[3]  | 0.27861    |
| AveragePolicyStd[4]  | 0.24796    |
| AveragePolicyStd[5]  | 0.24599    |
| AverageReturn        | 932.56     |
| MinReturn            | 91.022     |
| MaxReturn            | 1046.7     |
| StdReturn            | 176.66     |
| AverageEpisodeLength | 949.6      |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.13     |
| TotalNEpisodes       | 18447      |
| TotalNSamples        | 1.7803e+06 |
| ExplainedVariance    | 0.26392    |
-------------------------------------
[2018-01-21 13:28:41.704857 UTC] Saving snapshot
[2018-01-21 13:28:41.705104 UTC] Starting iteration 357
[2018-01-21 13:28:41.705263 UTC] Start collecting samples
[2018-01-21 13:28:46.146285 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:46.277903 UTC] Performing policy update
[2018-01-21 13:28:46.278912 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:46.396692 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:47.788393 UTC] Performing line search
[2018-01-21 13:28:47.977215 UTC] Updating baseline
[2018-01-21 13:28:50.180020 UTC] Computing logging information
-------------------------------------
| Iteration            | 357        |
| ExpectedImprovement  | 0.014976   |
| ActualImprovement    | 0.01465    |
| ImprovementRatio     | 0.97821    |
| MeanKL               | 0.0071719  |
| Entropy              | 0.48739    |
| Perplexity           | 1.6281     |
| AveragePolicyStd     | 0.26576    |
| AveragePolicyStd[0]  | 0.27435    |
| AveragePolicyStd[1]  | 0.34319    |
| AveragePolicyStd[2]  | 0.20375    |
| AveragePolicyStd[3]  | 0.27965    |
| AveragePolicyStd[4]  | 0.24663    |
| AveragePolicyStd[5]  | 0.24697    |
| AverageReturn        | 931.61     |
| MinReturn            | 91.022     |
| MaxReturn            | 1046.7     |
| StdReturn            | 183.18     |
| AverageEpisodeLength | 943.9      |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.63     |
| TotalNEpisodes       | 18450      |
| TotalNSamples        | 1.7825e+06 |
| ExplainedVariance    | 0.24773    |
-------------------------------------
[2018-01-21 13:28:50.883507 UTC] Saving snapshot
[2018-01-21 13:28:50.883740 UTC] Starting iteration 358
[2018-01-21 13:28:50.883884 UTC] Start collecting samples
[2018-01-21 13:28:55.656469 UTC] Computing input variables for policy optimization
[2018-01-21 13:28:55.780936 UTC] Performing policy update
[2018-01-21 13:28:55.781525 UTC] Computing gradient in Euclidean space
[2018-01-21 13:28:55.899153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:28:57.335925 UTC] Performing line search
[2018-01-21 13:28:57.523008 UTC] Updating baseline
[2018-01-21 13:28:59.733801 UTC] Computing logging information
------------------------------------
| Iteration            | 358       |
| ExpectedImprovement  | 0.017228  |
| ActualImprovement    | 0.016127  |
| ImprovementRatio     | 0.93606   |
| MeanKL               | 0.0072096 |
| Entropy              | 0.48872   |
| Perplexity           | 1.6302    |
| AveragePolicyStd     | 0.26588   |
| AveragePolicyStd[0]  | 0.27437   |
| AveragePolicyStd[1]  | 0.34413   |
| AveragePolicyStd[2]  | 0.20326   |
| AveragePolicyStd[3]  | 0.28019   |
| AveragePolicyStd[4]  | 0.24624   |
| AveragePolicyStd[5]  | 0.24712   |
| AverageReturn        | 929.12    |
| MinReturn            | 91.022    |
| MaxReturn            | 1046.7    |
| StdReturn            | 189.73    |
| AverageEpisodeLength | 938.78    |
| MinEpisodeLength     | 93        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.11    |
| TotalNEpisodes       | 18460     |
| TotalNSamples        | 1.792e+06 |
| ExplainedVariance    | 0.10751   |
------------------------------------
[2018-01-21 13:29:00.405178 UTC] Saving snapshot
[2018-01-21 13:29:00.405373 UTC] Starting iteration 359
[2018-01-21 13:29:00.405491 UTC] Start collecting samples
[2018-01-21 13:29:04.860919 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:04.988854 UTC] Performing policy update
[2018-01-21 13:29:04.989475 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:05.105630 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:06.478528 UTC] Performing line search
[2018-01-21 13:29:06.660145 UTC] Updating baseline
[2018-01-21 13:29:08.804329 UTC] Computing logging information
------------------------------------
| Iteration            | 359       |
| ExpectedImprovement  | 0.014383  |
| ActualImprovement    | 0.013543  |
| ImprovementRatio     | 0.94155   |
| MeanKL               | 0.0073855 |
| Entropy              | 0.47959   |
| Perplexity           | 1.6154    |
| AveragePolicyStd     | 0.26549   |
| AveragePolicyStd[0]  | 0.27334   |
| AveragePolicyStd[1]  | 0.34352   |
| AveragePolicyStd[2]  | 0.20319   |
| AveragePolicyStd[3]  | 0.28137   |
| AveragePolicyStd[4]  | 0.24571   |
| AveragePolicyStd[5]  | 0.24581   |
| AverageReturn        | 930.57    |
| MinReturn            | 91.022    |
| MaxReturn            | 1046.7    |
| StdReturn            | 190.1     |
| AverageEpisodeLength | 938.78    |
| MinEpisodeLength     | 93        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.11    |
| TotalNEpisodes       | 18463     |
| TotalNSamples        | 1.795e+06 |
| ExplainedVariance    | 0.0026102 |
------------------------------------
[2018-01-21 13:29:09.477800 UTC] Saving snapshot
[2018-01-21 13:29:09.478005 UTC] Starting iteration 360
[2018-01-21 13:29:09.478213 UTC] Start collecting samples
[2018-01-21 13:29:13.883013 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:14.031241 UTC] Performing policy update
[2018-01-21 13:29:14.032236 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:14.162110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:15.584400 UTC] Performing line search
[2018-01-21 13:29:15.779268 UTC] Updating baseline
[2018-01-21 13:29:17.775130 UTC] Computing logging information
-------------------------------------
| Iteration            | 360        |
| ExpectedImprovement  | 0.015405   |
| ActualImprovement    | 0.014129   |
| ImprovementRatio     | 0.91719    |
| MeanKL               | 0.0077668  |
| Entropy              | 0.4845     |
| Perplexity           | 1.6234     |
| AveragePolicyStd     | 0.26577    |
| AveragePolicyStd[0]  | 0.27409    |
| AveragePolicyStd[1]  | 0.3445     |
| AveragePolicyStd[2]  | 0.20309    |
| AveragePolicyStd[3]  | 0.28193    |
| AveragePolicyStd[4]  | 0.24589    |
| AveragePolicyStd[5]  | 0.24509    |
| AverageReturn        | 931.89     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 190.62     |
| AverageEpisodeLength | 938.78     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.11     |
| TotalNEpisodes       | 18465      |
| TotalNSamples        | 1.797e+06  |
| ExplainedVariance    | -0.0050599 |
-------------------------------------
[2018-01-21 13:29:18.449503 UTC] Saving snapshot
[2018-01-21 13:29:18.456824 UTC] Starting iteration 361
[2018-01-21 13:29:18.457060 UTC] Start collecting samples
[2018-01-21 13:29:23.275717 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:23.420059 UTC] Performing policy update
[2018-01-21 13:29:23.420679 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:23.557396 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:24.950458 UTC] Performing line search
[2018-01-21 13:29:25.133655 UTC] Updating baseline
[2018-01-21 13:29:27.276022 UTC] Computing logging information
------------------------------------
| Iteration            | 361       |
| ExpectedImprovement  | 0.015777  |
| ActualImprovement    | 0.014986  |
| ImprovementRatio     | 0.94988   |
| MeanKL               | 0.0071446 |
| Entropy              | 0.47227   |
| Perplexity           | 1.6036    |
| AveragePolicyStd     | 0.26519   |
| AveragePolicyStd[0]  | 0.27062   |
| AveragePolicyStd[1]  | 0.34401   |
| AveragePolicyStd[2]  | 0.2033    |
| AveragePolicyStd[3]  | 0.28259   |
| AveragePolicyStd[4]  | 0.24594   |
| AveragePolicyStd[5]  | 0.24469   |
| AverageReturn        | 934.43    |
| MinReturn            | 91.022    |
| MaxReturn            | 1067.4    |
| StdReturn            | 191.51    |
| AverageEpisodeLength | 938.78    |
| MinEpisodeLength     | 93        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.11    |
| TotalNEpisodes       | 18472     |
| TotalNSamples        | 1.804e+06 |
| ExplainedVariance    | 0.0089427 |
------------------------------------
[2018-01-21 13:29:27.910008 UTC] Saving snapshot
[2018-01-21 13:29:27.910320 UTC] Starting iteration 362
[2018-01-21 13:29:27.910513 UTC] Start collecting samples
[2018-01-21 13:29:32.419769 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:32.567813 UTC] Performing policy update
[2018-01-21 13:29:32.568924 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:32.690581 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:34.126766 UTC] Performing line search
[2018-01-21 13:29:34.323128 UTC] Updating baseline
[2018-01-21 13:29:36.563290 UTC] Computing logging information
-------------------------------------
| Iteration            | 362        |
| ExpectedImprovement  | 0.017729   |
| ActualImprovement    | 0.016559   |
| ImprovementRatio     | 0.93399    |
| MeanKL               | 0.0071748  |
| Entropy              | 0.47112    |
| Perplexity           | 1.6018     |
| AveragePolicyStd     | 0.2652     |
| AveragePolicyStd[0]  | 0.26943    |
| AveragePolicyStd[1]  | 0.3441     |
| AveragePolicyStd[2]  | 0.20234    |
| AveragePolicyStd[3]  | 0.28429    |
| AveragePolicyStd[4]  | 0.24602    |
| AveragePolicyStd[5]  | 0.24503    |
| AverageReturn        | 934.55     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 196.53     |
| AverageEpisodeLength | 937.16     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.87     |
| TotalNEpisodes       | 18480      |
| TotalNSamples        | 1.8113e+06 |
| ExplainedVariance    | 0.12634    |
-------------------------------------
[2018-01-21 13:29:37.212152 UTC] Saving snapshot
[2018-01-21 13:29:37.212329 UTC] Starting iteration 363
[2018-01-21 13:29:37.212432 UTC] Start collecting samples
[2018-01-21 13:29:41.759266 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:41.910170 UTC] Performing policy update
[2018-01-21 13:29:41.911172 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:42.044785 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:43.432318 UTC] Performing line search
[2018-01-21 13:29:43.629967 UTC] Updating baseline
[2018-01-21 13:29:45.955016 UTC] Computing logging information
-------------------------------------
| Iteration            | 363        |
| ExpectedImprovement  | 0.01881    |
| ActualImprovement    | 0.017751   |
| ImprovementRatio     | 0.94369    |
| MeanKL               | 0.0071209  |
| Entropy              | 0.46951    |
| Perplexity           | 1.5992     |
| AveragePolicyStd     | 0.26502    |
| AveragePolicyStd[0]  | 0.26953    |
| AveragePolicyStd[1]  | 0.34183    |
| AveragePolicyStd[2]  | 0.20259    |
| AveragePolicyStd[3]  | 0.28413    |
| AveragePolicyStd[4]  | 0.24641    |
| AveragePolicyStd[5]  | 0.24563    |
| AverageReturn        | 937.05     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 192.02     |
| AverageEpisodeLength | 939.14     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.96     |
| TotalNEpisodes       | 18483      |
| TotalNSamples        | 1.8139e+06 |
| ExplainedVariance    | 0.18167    |
-------------------------------------
[2018-01-21 13:29:46.576791 UTC] Saving snapshot
[2018-01-21 13:29:46.577081 UTC] Starting iteration 364
[2018-01-21 13:29:46.577300 UTC] Start collecting samples
[2018-01-21 13:29:51.064229 UTC] Computing input variables for policy optimization
[2018-01-21 13:29:51.199290 UTC] Performing policy update
[2018-01-21 13:29:51.199996 UTC] Computing gradient in Euclidean space
[2018-01-21 13:29:51.323857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:29:52.777547 UTC] Performing line search
[2018-01-21 13:29:52.995115 UTC] Updating baseline
[2018-01-21 13:29:56.008169 UTC] Computing logging information
-------------------------------------
| Iteration            | 364        |
| ExpectedImprovement  | 0.01501    |
| ActualImprovement    | 0.014008   |
| ImprovementRatio     | 0.93327    |
| MeanKL               | 0.007916   |
| Entropy              | 0.47016    |
| Perplexity           | 1.6002     |
| AveragePolicyStd     | 0.26502    |
| AveragePolicyStd[0]  | 0.26908    |
| AveragePolicyStd[1]  | 0.34126    |
| AveragePolicyStd[2]  | 0.20259    |
| AveragePolicyStd[3]  | 0.2844     |
| AveragePolicyStd[4]  | 0.24665    |
| AveragePolicyStd[5]  | 0.24614    |
| AverageReturn        | 949.1      |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 181.5      |
| AverageEpisodeLength | 948.03     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.6      |
| TotalNEpisodes       | 18488      |
| TotalNSamples        | 1.8188e+06 |
| ExplainedVariance    | 0.13359    |
-------------------------------------
[2018-01-21 13:29:56.715835 UTC] Saving snapshot
[2018-01-21 13:29:56.716107 UTC] Starting iteration 365
[2018-01-21 13:29:56.716308 UTC] Start collecting samples
[2018-01-21 13:30:01.349240 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:01.476088 UTC] Performing policy update
[2018-01-21 13:30:01.477211 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:01.594360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:02.968858 UTC] Performing line search
[2018-01-21 13:30:03.160807 UTC] Updating baseline
[2018-01-21 13:30:04.961891 UTC] Computing logging information
-------------------------------------
| Iteration            | 365        |
| ExpectedImprovement  | 0.01364    |
| ActualImprovement    | 0.013162   |
| ImprovementRatio     | 0.96498    |
| MeanKL               | 0.0081335  |
| Entropy              | 0.47786    |
| Perplexity           | 1.6126     |
| AveragePolicyStd     | 0.26533    |
| AveragePolicyStd[0]  | 0.26974    |
| AveragePolicyStd[1]  | 0.34098    |
| AveragePolicyStd[2]  | 0.20264    |
| AveragePolicyStd[3]  | 0.28447    |
| AveragePolicyStd[4]  | 0.24702    |
| AveragePolicyStd[5]  | 0.24714    |
| AverageReturn        | 951.76     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 182.37     |
| AverageEpisodeLength | 948.03     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.6      |
| TotalNEpisodes       | 18495      |
| TotalNSamples        | 1.8258e+06 |
| ExplainedVariance    | -0.022716  |
-------------------------------------
[2018-01-21 13:30:05.640845 UTC] Saving snapshot
[2018-01-21 13:30:05.641176 UTC] Starting iteration 366
[2018-01-21 13:30:05.641410 UTC] Start collecting samples
[2018-01-21 13:30:10.162944 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:10.298830 UTC] Performing policy update
[2018-01-21 13:30:10.299891 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:10.420730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:11.837606 UTC] Performing line search
[2018-01-21 13:30:12.048766 UTC] Updating baseline
[2018-01-21 13:30:14.029797 UTC] Computing logging information
-------------------------------------
| Iteration            | 366        |
| ExpectedImprovement  | 0.017288   |
| ActualImprovement    | 0.016641   |
| ImprovementRatio     | 0.9626     |
| MeanKL               | 0.0070036  |
| Entropy              | 0.48089    |
| Perplexity           | 1.6175     |
| AveragePolicyStd     | 0.26546    |
| AveragePolicyStd[0]  | 0.27123    |
| AveragePolicyStd[1]  | 0.33969    |
| AveragePolicyStd[2]  | 0.20179    |
| AveragePolicyStd[3]  | 0.28504    |
| AveragePolicyStd[4]  | 0.24713    |
| AveragePolicyStd[5]  | 0.24789    |
| AverageReturn        | 952.59     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 182.59     |
| AverageEpisodeLength | 948.03     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.6      |
| TotalNEpisodes       | 18499      |
| TotalNSamples        | 1.8298e+06 |
| ExplainedVariance    | 0.0076997  |
-------------------------------------
[2018-01-21 13:30:14.701393 UTC] Saving snapshot
[2018-01-21 13:30:14.701642 UTC] Starting iteration 367
[2018-01-21 13:30:14.701825 UTC] Start collecting samples
[2018-01-21 13:30:19.370694 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:19.514067 UTC] Performing policy update
[2018-01-21 13:30:19.515968 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:19.639199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:21.067097 UTC] Performing line search
[2018-01-21 13:30:21.251224 UTC] Updating baseline
[2018-01-21 13:30:23.595103 UTC] Computing logging information
-------------------------------------
| Iteration            | 367        |
| ExpectedImprovement  | 0.019025   |
| ActualImprovement    | 0.017322   |
| ImprovementRatio     | 0.91045    |
| MeanKL               | 0.0072121  |
| Entropy              | 0.48237    |
| Perplexity           | 1.6199     |
| AveragePolicyStd     | 0.26539    |
| AveragePolicyStd[0]  | 0.27107    |
| AveragePolicyStd[1]  | 0.33784    |
| AveragePolicyStd[2]  | 0.20283    |
| AveragePolicyStd[3]  | 0.28479    |
| AveragePolicyStd[4]  | 0.24795    |
| AveragePolicyStd[5]  | 0.24788    |
| AverageReturn        | 952.94     |
| MinReturn            | 91.022     |
| MaxReturn            | 1067.4     |
| StdReturn            | 185.26     |
| AverageEpisodeLength | 944.53     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.99     |
| TotalNEpisodes       | 18505      |
| TotalNSamples        | 1.8355e+06 |
| ExplainedVariance    | 0.077431   |
-------------------------------------
[2018-01-21 13:30:24.245530 UTC] Saving snapshot
[2018-01-21 13:30:24.245807 UTC] Starting iteration 368
[2018-01-21 13:30:24.245990 UTC] Start collecting samples
[2018-01-21 13:30:28.864808 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:28.997094 UTC] Performing policy update
[2018-01-21 13:30:28.997779 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:29.117146 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:30.507645 UTC] Performing line search
[2018-01-21 13:30:30.691104 UTC] Updating baseline
[2018-01-21 13:30:32.618664 UTC] Computing logging information
-------------------------------------
| Iteration            | 368        |
| ExpectedImprovement  | 0.016646   |
| ActualImprovement    | 0.01586    |
| ImprovementRatio     | 0.95278    |
| MeanKL               | 0.0073413  |
| Entropy              | 0.46956    |
| Perplexity           | 1.5993     |
| AveragePolicyStd     | 0.26482    |
| AveragePolicyStd[0]  | 0.27207    |
| AveragePolicyStd[1]  | 0.33807    |
| AveragePolicyStd[2]  | 0.20297    |
| AveragePolicyStd[3]  | 0.28159    |
| AveragePolicyStd[4]  | 0.24619    |
| AveragePolicyStd[5]  | 0.24803    |
| AverageReturn        | 954.47     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 185.82     |
| AverageEpisodeLength | 944.53     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.99     |
| TotalNEpisodes       | 18510      |
| TotalNSamples        | 1.8405e+06 |
| ExplainedVariance    | 0.013809   |
-------------------------------------
[2018-01-21 13:30:33.307111 UTC] Saving snapshot
[2018-01-21 13:30:33.307355 UTC] Starting iteration 369
[2018-01-21 13:30:33.307510 UTC] Start collecting samples
[2018-01-21 13:30:37.777474 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:37.958803 UTC] Performing policy update
[2018-01-21 13:30:37.959363 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:38.082913 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:39.531982 UTC] Performing line search
[2018-01-21 13:30:39.747284 UTC] Updating baseline
[2018-01-21 13:30:41.730233 UTC] Computing logging information
-------------------------------------
| Iteration            | 369        |
| ExpectedImprovement  | 0.015847   |
| ActualImprovement    | 0.015159   |
| ImprovementRatio     | 0.95662    |
| MeanKL               | 0.006976   |
| Entropy              | 0.46216    |
| Perplexity           | 1.5875     |
| AveragePolicyStd     | 0.26443    |
| AveragePolicyStd[0]  | 0.27174    |
| AveragePolicyStd[1]  | 0.33757    |
| AveragePolicyStd[2]  | 0.20406    |
| AveragePolicyStd[3]  | 0.28055    |
| AveragePolicyStd[4]  | 0.24468    |
| AveragePolicyStd[5]  | 0.24797    |
| AverageReturn        | 946.03     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 203.73     |
| AverageEpisodeLength | 935.83     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.38     |
| TotalNEpisodes       | 18516      |
| TotalNSamples        | 1.8456e+06 |
| ExplainedVariance    | 0.13763    |
-------------------------------------
[2018-01-21 13:30:42.398515 UTC] Saving snapshot
[2018-01-21 13:30:42.398789 UTC] Starting iteration 370
[2018-01-21 13:30:42.398968 UTC] Start collecting samples
[2018-01-21 13:30:46.888379 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:47.039261 UTC] Performing policy update
[2018-01-21 13:30:47.039880 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:47.157558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:48.534775 UTC] Performing line search
[2018-01-21 13:30:48.722935 UTC] Updating baseline
[2018-01-21 13:30:50.948803 UTC] Computing logging information
-------------------------------------
| Iteration            | 370        |
| ExpectedImprovement  | 0.014284   |
| ActualImprovement    | 0.013767   |
| ImprovementRatio     | 0.9638     |
| MeanKL               | 0.0075636  |
| Entropy              | 0.46727    |
| Perplexity           | 1.5956     |
| AveragePolicyStd     | 0.2646     |
| AveragePolicyStd[0]  | 0.2722     |
| AveragePolicyStd[1]  | 0.33656    |
| AveragePolicyStd[2]  | 0.20425    |
| AveragePolicyStd[3]  | 0.28075    |
| AveragePolicyStd[4]  | 0.24494    |
| AveragePolicyStd[5]  | 0.2489     |
| AverageReturn        | 954.56     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 197.09     |
| AverageEpisodeLength | 943.75     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.59     |
| TotalNEpisodes       | 18522      |
| TotalNSamples        | 1.8516e+06 |
| ExplainedVariance    | -0.0016761 |
-------------------------------------
[2018-01-21 13:30:51.648729 UTC] Saving snapshot
[2018-01-21 13:30:51.657977 UTC] Starting iteration 371
[2018-01-21 13:30:51.658207 UTC] Start collecting samples
[2018-01-21 13:30:56.063688 UTC] Computing input variables for policy optimization
[2018-01-21 13:30:56.186624 UTC] Performing policy update
[2018-01-21 13:30:56.187719 UTC] Computing gradient in Euclidean space
[2018-01-21 13:30:56.308167 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:30:57.702862 UTC] Performing line search
[2018-01-21 13:30:57.897733 UTC] Updating baseline
[2018-01-21 13:31:00.219782 UTC] Computing logging information
-------------------------------------
| Iteration            | 371        |
| ExpectedImprovement  | 0.016132   |
| ActualImprovement    | 0.01533    |
| ImprovementRatio     | 0.95027    |
| MeanKL               | 0.0073408  |
| Entropy              | 0.46284    |
| Perplexity           | 1.5886     |
| AveragePolicyStd     | 0.26442    |
| AveragePolicyStd[0]  | 0.27252    |
| AveragePolicyStd[1]  | 0.33614    |
| AveragePolicyStd[2]  | 0.2039     |
| AveragePolicyStd[3]  | 0.28081    |
| AveragePolicyStd[4]  | 0.24429    |
| AveragePolicyStd[5]  | 0.24885    |
| AverageReturn        | 955.11     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 197.23     |
| AverageEpisodeLength | 943.75     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.59     |
| TotalNEpisodes       | 18527      |
| TotalNSamples        | 1.8566e+06 |
| ExplainedVariance    | 0.13532    |
-------------------------------------
[2018-01-21 13:31:00.852416 UTC] Saving snapshot
[2018-01-21 13:31:00.852676 UTC] Starting iteration 372
[2018-01-21 13:31:00.852845 UTC] Start collecting samples
[2018-01-21 13:31:05.328092 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:05.455953 UTC] Performing policy update
[2018-01-21 13:31:05.456876 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:05.576366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:07.043564 UTC] Performing line search
[2018-01-21 13:31:07.233478 UTC] Updating baseline
[2018-01-21 13:31:09.360930 UTC] Computing logging information
-------------------------------------
| Iteration            | 372        |
| ExpectedImprovement  | 0.017347   |
| ActualImprovement    | 0.016285   |
| ImprovementRatio     | 0.93882    |
| MeanKL               | 0.0073545  |
| Entropy              | 0.46371    |
| Perplexity           | 1.59       |
| AveragePolicyStd     | 0.26448    |
| AveragePolicyStd[0]  | 0.27224    |
| AveragePolicyStd[1]  | 0.33672    |
| AveragePolicyStd[2]  | 0.20384    |
| AveragePolicyStd[3]  | 0.28121    |
| AveragePolicyStd[4]  | 0.24436    |
| AveragePolicyStd[5]  | 0.24854    |
| AverageReturn        | 957.85     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 196.66     |
| AverageEpisodeLength | 945.72     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.14     |
| TotalNEpisodes       | 18531      |
| TotalNSamples        | 1.8606e+06 |
| ExplainedVariance    | 0.04417    |
-------------------------------------
[2018-01-21 13:31:09.993937 UTC] Saving snapshot
[2018-01-21 13:31:09.994159 UTC] Starting iteration 373
[2018-01-21 13:31:09.994336 UTC] Start collecting samples
[2018-01-21 13:31:14.588670 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:14.711742 UTC] Performing policy update
[2018-01-21 13:31:14.712711 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:14.840865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:16.275844 UTC] Performing line search
[2018-01-21 13:31:16.456256 UTC] Updating baseline
[2018-01-21 13:31:18.738797 UTC] Computing logging information
-------------------------------------
| Iteration            | 373        |
| ExpectedImprovement  | 0.016292   |
| ActualImprovement    | 0.015417   |
| ImprovementRatio     | 0.94631    |
| MeanKL               | 0.0077705  |
| Entropy              | 0.45048    |
| Perplexity           | 1.5691     |
| AveragePolicyStd     | 0.26385    |
| AveragePolicyStd[0]  | 0.27154    |
| AveragePolicyStd[1]  | 0.33566    |
| AveragePolicyStd[2]  | 0.20389    |
| AveragePolicyStd[3]  | 0.27943    |
| AveragePolicyStd[4]  | 0.24323    |
| AveragePolicyStd[5]  | 0.24933    |
| AverageReturn        | 957.41     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 196.65     |
| AverageEpisodeLength | 945.72     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.14     |
| TotalNEpisodes       | 18535      |
| TotalNSamples        | 1.8646e+06 |
| ExplainedVariance    | 0.21366    |
-------------------------------------
[2018-01-21 13:31:19.372276 UTC] Saving snapshot
[2018-01-21 13:31:19.372513 UTC] Starting iteration 374
[2018-01-21 13:31:19.372661 UTC] Start collecting samples
[2018-01-21 13:31:24.007016 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:24.143363 UTC] Performing policy update
[2018-01-21 13:31:24.144000 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:24.278361 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:25.636792 UTC] Performing line search
[2018-01-21 13:31:25.821685 UTC] Updating baseline
[2018-01-21 13:31:27.603460 UTC] Computing logging information
-------------------------------------
| Iteration            | 374        |
| ExpectedImprovement  | 0.015186   |
| ActualImprovement    | 0.014951   |
| ImprovementRatio     | 0.98449    |
| MeanKL               | 0.0076461  |
| Entropy              | 0.44875    |
| Perplexity           | 1.5664     |
| AveragePolicyStd     | 0.26377    |
| AveragePolicyStd[0]  | 0.27022    |
| AveragePolicyStd[1]  | 0.33553    |
| AveragePolicyStd[2]  | 0.20303    |
| AveragePolicyStd[3]  | 0.27899    |
| AveragePolicyStd[4]  | 0.24454    |
| AveragePolicyStd[5]  | 0.25033    |
| AverageReturn        | 957.52     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 196.95     |
| AverageEpisodeLength | 945.72     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.14     |
| TotalNEpisodes       | 18541      |
| TotalNSamples        | 1.8706e+06 |
| ExplainedVariance    | 0.10626    |
-------------------------------------
[2018-01-21 13:31:28.339586 UTC] Saving snapshot
[2018-01-21 13:31:28.340101 UTC] Starting iteration 375
[2018-01-21 13:31:28.340502 UTC] Start collecting samples
[2018-01-21 13:31:32.823159 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:32.961152 UTC] Performing policy update
[2018-01-21 13:31:32.962161 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:33.089747 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:34.561012 UTC] Performing line search
[2018-01-21 13:31:34.738400 UTC] Updating baseline
[2018-01-21 13:31:36.763703 UTC] Computing logging information
-------------------------------------
| Iteration            | 375        |
| ExpectedImprovement  | 0.017734   |
| ActualImprovement    | 0.016783   |
| ImprovementRatio     | 0.9464     |
| MeanKL               | 0.0071229  |
| Entropy              | 0.46457    |
| Perplexity           | 1.5913     |
| AveragePolicyStd     | 0.26448    |
| AveragePolicyStd[0]  | 0.27135    |
| AveragePolicyStd[1]  | 0.3367     |
| AveragePolicyStd[2]  | 0.20384    |
| AveragePolicyStd[3]  | 0.27967    |
| AveragePolicyStd[4]  | 0.24554    |
| AveragePolicyStd[5]  | 0.24976    |
| AverageReturn        | 966.05     |
| MinReturn            | 91.022     |
| MaxReturn            | 1069.8     |
| StdReturn            | 180.81     |
| AverageEpisodeLength | 953.81     |
| MinEpisodeLength     | 93         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.42     |
| TotalNEpisodes       | 18546      |
| TotalNSamples        | 1.8756e+06 |
| ExplainedVariance    | 0.12721    |
-------------------------------------
[2018-01-21 13:31:37.489216 UTC] Saving snapshot
[2018-01-21 13:31:37.489443 UTC] Starting iteration 376
[2018-01-21 13:31:37.489587 UTC] Start collecting samples
[2018-01-21 13:31:41.995746 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:42.145573 UTC] Performing policy update
[2018-01-21 13:31:42.146167 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:42.274514 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:43.685966 UTC] Performing line search
[2018-01-21 13:31:43.871706 UTC] Updating baseline
[2018-01-21 13:31:45.995232 UTC] Computing logging information
-------------------------------------
| Iteration            | 376        |
| ExpectedImprovement  | 0.016811   |
| ActualImprovement    | 0.016003   |
| ImprovementRatio     | 0.95193    |
| MeanKL               | 0.007393   |
| Entropy              | 0.45626    |
| Perplexity           | 1.5782     |
| AveragePolicyStd     | 0.26417    |
| AveragePolicyStd[0]  | 0.27156    |
| AveragePolicyStd[1]  | 0.33693    |
| AveragePolicyStd[2]  | 0.2032     |
| AveragePolicyStd[3]  | 0.27984    |
| AveragePolicyStd[4]  | 0.24495    |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 983.8      |
| MinReturn            | 116.11     |
| MaxReturn            | 1069.8     |
| StdReturn            | 138.52     |
| AverageEpisodeLength | 970.7      |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.73     |
| TotalNEpisodes       | 18549      |
| TotalNSamples        | 1.8786e+06 |
| ExplainedVariance    | 0.11513    |
-------------------------------------
[2018-01-21 13:31:46.611078 UTC] Saving snapshot
[2018-01-21 13:31:46.611367 UTC] Starting iteration 377
[2018-01-21 13:31:46.611546 UTC] Start collecting samples
[2018-01-21 13:31:51.030736 UTC] Computing input variables for policy optimization
[2018-01-21 13:31:51.156322 UTC] Performing policy update
[2018-01-21 13:31:51.156952 UTC] Computing gradient in Euclidean space
[2018-01-21 13:31:51.276595 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:31:52.709000 UTC] Performing line search
[2018-01-21 13:31:52.914311 UTC] Updating baseline
[2018-01-21 13:31:54.909007 UTC] Computing logging information
-------------------------------------
| Iteration            | 377        |
| ExpectedImprovement  | 0.014646   |
| ActualImprovement    | 0.014044   |
| ImprovementRatio     | 0.95891    |
| MeanKL               | 0.0082533  |
| Entropy              | 0.45012    |
| Perplexity           | 1.5685     |
| AveragePolicyStd     | 0.26389    |
| AveragePolicyStd[0]  | 0.27144    |
| AveragePolicyStd[1]  | 0.33692    |
| AveragePolicyStd[2]  | 0.20351    |
| AveragePolicyStd[3]  | 0.27896    |
| AveragePolicyStd[4]  | 0.24442    |
| AveragePolicyStd[5]  | 0.24808    |
| AverageReturn        | 985.72     |
| MinReturn            | 116.11     |
| MaxReturn            | 1069.8     |
| StdReturn            | 138.9      |
| AverageEpisodeLength | 970.7      |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.73     |
| TotalNEpisodes       | 18554      |
| TotalNSamples        | 1.8836e+06 |
| ExplainedVariance    | -0.018667  |
-------------------------------------
[2018-01-21 13:31:55.580411 UTC] Saving snapshot
[2018-01-21 13:31:55.580647 UTC] Starting iteration 378
[2018-01-21 13:31:55.580857 UTC] Start collecting samples
[2018-01-21 13:32:00.014663 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:00.150594 UTC] Performing policy update
[2018-01-21 13:32:00.151641 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:00.282206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:01.739279 UTC] Performing line search
[2018-01-21 13:32:01.962752 UTC] Updating baseline
[2018-01-21 13:32:03.936780 UTC] Computing logging information
-------------------------------------
| Iteration            | 378        |
| ExpectedImprovement  | 0.015399   |
| ActualImprovement    | 0.014503   |
| ImprovementRatio     | 0.94181    |
| MeanKL               | 0.0077645  |
| Entropy              | 0.4616     |
| Perplexity           | 1.5866     |
| AveragePolicyStd     | 0.26439    |
| AveragePolicyStd[0]  | 0.27027    |
| AveragePolicyStd[1]  | 0.33731    |
| AveragePolicyStd[2]  | 0.20348    |
| AveragePolicyStd[3]  | 0.28031    |
| AveragePolicyStd[4]  | 0.24549    |
| AveragePolicyStd[5]  | 0.24947    |
| AverageReturn        | 991.47     |
| MinReturn            | 116.11     |
| MaxReturn            | 1069.8     |
| StdReturn            | 129.06     |
| AverageEpisodeLength | 975.82     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.5      |
| TotalNEpisodes       | 18560      |
| TotalNSamples        | 1.8896e+06 |
| ExplainedVariance    | 0.039208   |
-------------------------------------
[2018-01-21 13:32:04.667262 UTC] Saving snapshot
[2018-01-21 13:32:04.667497 UTC] Starting iteration 379
[2018-01-21 13:32:04.667680 UTC] Start collecting samples
[2018-01-21 13:32:09.231329 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:09.369632 UTC] Performing policy update
[2018-01-21 13:32:09.370259 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:09.495850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:10.960689 UTC] Performing line search
[2018-01-21 13:32:11.296493 UTC] Updating baseline
[2018-01-21 13:32:13.171198 UTC] Computing logging information
-------------------------------------
| Iteration            | 379        |
| ExpectedImprovement  | 0.012544   |
| ActualImprovement    | 0.012099   |
| ImprovementRatio     | 0.96456    |
| MeanKL               | 0.0081698  |
| Entropy              | 0.4649     |
| Perplexity           | 1.5919     |
| AveragePolicyStd     | 0.2645     |
| AveragePolicyStd[0]  | 0.27072    |
| AveragePolicyStd[1]  | 0.33658    |
| AveragePolicyStd[2]  | 0.20331    |
| AveragePolicyStd[3]  | 0.28014    |
| AveragePolicyStd[4]  | 0.24607    |
| AveragePolicyStd[5]  | 0.25019    |
| AverageReturn        | 991.99     |
| MinReturn            | 116.11     |
| MaxReturn            | 1069.8     |
| StdReturn            | 129.18     |
| AverageEpisodeLength | 975.82     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.5      |
| TotalNEpisodes       | 18564      |
| TotalNSamples        | 1.8936e+06 |
| ExplainedVariance    | -0.001621  |
-------------------------------------
[2018-01-21 13:32:13.902042 UTC] Saving snapshot
[2018-01-21 13:32:13.902273 UTC] Starting iteration 380
[2018-01-21 13:32:13.902417 UTC] Start collecting samples
[2018-01-21 13:32:18.677706 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:18.799664 UTC] Performing policy update
[2018-01-21 13:32:18.800246 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:18.922899 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:20.313020 UTC] Performing line search
[2018-01-21 13:32:20.485010 UTC] Updating baseline
[2018-01-21 13:32:22.388582 UTC] Computing logging information
-------------------------------------
| Iteration            | 380        |
| ExpectedImprovement  | 0.015617   |
| ActualImprovement    | 0.015214   |
| ImprovementRatio     | 0.97424    |
| MeanKL               | 0.0076262  |
| Entropy              | 0.4643     |
| Perplexity           | 1.5909     |
| AveragePolicyStd     | 0.26461    |
| AveragePolicyStd[0]  | 0.27118    |
| AveragePolicyStd[1]  | 0.33807    |
| AveragePolicyStd[2]  | 0.20168    |
| AveragePolicyStd[3]  | 0.28015    |
| AveragePolicyStd[4]  | 0.24728    |
| AveragePolicyStd[5]  | 0.2493     |
| AverageReturn        | 984.94     |
| MinReturn            | 103.73     |
| MaxReturn            | 1082.6     |
| StdReturn            | 157.15     |
| AverageEpisodeLength | 966.95     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.55     |
| TotalNEpisodes       | 18571      |
| TotalNSamples        | 1.8997e+06 |
| ExplainedVariance    | 0.079121   |
-------------------------------------
[2018-01-21 13:32:23.009333 UTC] Saving snapshot
[2018-01-21 13:32:23.015066 UTC] Starting iteration 381
[2018-01-21 13:32:23.015241 UTC] Start collecting samples
[2018-01-21 13:32:27.638034 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:27.788870 UTC] Performing policy update
[2018-01-21 13:32:27.789495 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:27.910688 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:29.311662 UTC] Performing line search
[2018-01-21 13:32:29.526325 UTC] Updating baseline
[2018-01-21 13:32:32.278736 UTC] Computing logging information
-------------------------------------
| Iteration            | 381        |
| ExpectedImprovement  | 0.019456   |
| ActualImprovement    | 0.017882   |
| ImprovementRatio     | 0.91907    |
| MeanKL               | 0.0076534  |
| Entropy              | 0.47188    |
| Perplexity           | 1.603      |
| AveragePolicyStd     | 0.26494    |
| AveragePolicyStd[0]  | 0.27237    |
| AveragePolicyStd[1]  | 0.33677    |
| AveragePolicyStd[2]  | 0.20064    |
| AveragePolicyStd[3]  | 0.28118    |
| AveragePolicyStd[4]  | 0.24832    |
| AveragePolicyStd[5]  | 0.25037    |
| AverageReturn        | 985.48     |
| MinReturn            | 103.73     |
| MaxReturn            | 1082.6     |
| StdReturn            | 157.27     |
| AverageEpisodeLength | 966.95     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.55     |
| TotalNEpisodes       | 18576      |
| TotalNSamples        | 1.9047e+06 |
| ExplainedVariance    | -0.0010316 |
-------------------------------------
[2018-01-21 13:32:32.989211 UTC] Saving snapshot
[2018-01-21 13:32:32.989438 UTC] Starting iteration 382
[2018-01-21 13:32:32.989600 UTC] Start collecting samples
[2018-01-21 13:32:37.543782 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:37.667767 UTC] Performing policy update
[2018-01-21 13:32:37.668393 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:37.788306 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:39.184121 UTC] Performing line search
[2018-01-21 13:32:39.372955 UTC] Updating baseline
[2018-01-21 13:32:43.002881 UTC] Computing logging information
-------------------------------------
| Iteration            | 382        |
| ExpectedImprovement  | 0.015709   |
| ActualImprovement    | 0.014263   |
| ImprovementRatio     | 0.90798    |
| MeanKL               | 0.0073826  |
| Entropy              | 0.45937    |
| Perplexity           | 1.5831     |
| AveragePolicyStd     | 0.26438    |
| AveragePolicyStd[0]  | 0.271      |
| AveragePolicyStd[1]  | 0.33594    |
| AveragePolicyStd[2]  | 0.20054    |
| AveragePolicyStd[3]  | 0.2817     |
| AveragePolicyStd[4]  | 0.24784    |
| AveragePolicyStd[5]  | 0.24927    |
| AverageReturn        | 999        |
| MinReturn            | 103.73     |
| MaxReturn            | 1082.6     |
| StdReturn            | 134.26     |
| AverageEpisodeLength | 978.26     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.41     |
| TotalNEpisodes       | 18581      |
| TotalNSamples        | 1.9097e+06 |
| ExplainedVariance    | -0.02556   |
-------------------------------------
[2018-01-21 13:32:43.708791 UTC] Saving snapshot
[2018-01-21 13:32:43.709073 UTC] Starting iteration 383
[2018-01-21 13:32:43.709251 UTC] Start collecting samples
[2018-01-21 13:32:48.152420 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:48.279560 UTC] Performing policy update
[2018-01-21 13:32:48.280565 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:48.400672 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:49.804633 UTC] Performing line search
[2018-01-21 13:32:49.989205 UTC] Updating baseline
[2018-01-21 13:32:51.870343 UTC] Computing logging information
-------------------------------------
| Iteration            | 383        |
| ExpectedImprovement  | 0.014923   |
| ActualImprovement    | 0.014079   |
| ImprovementRatio     | 0.94348    |
| MeanKL               | 0.0078505  |
| Entropy              | 0.45454    |
| Perplexity           | 1.5754     |
| AveragePolicyStd     | 0.26429    |
| AveragePolicyStd[0]  | 0.27093    |
| AveragePolicyStd[1]  | 0.33726    |
| AveragePolicyStd[2]  | 0.19944    |
| AveragePolicyStd[3]  | 0.2818     |
| AveragePolicyStd[4]  | 0.24639    |
| AveragePolicyStd[5]  | 0.24991    |
| AverageReturn        | 1001.4     |
| MinReturn            | 103.73     |
| MaxReturn            | 1082.6     |
| StdReturn            | 134.58     |
| AverageEpisodeLength | 978.93     |
| MinEpisodeLength     | 113        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.35     |
| TotalNEpisodes       | 18586      |
| TotalNSamples        | 1.9147e+06 |
| ExplainedVariance    | 8.6889e-08 |
-------------------------------------
[2018-01-21 13:32:52.534743 UTC] Saving snapshot
[2018-01-21 13:32:52.534980 UTC] Starting iteration 384
[2018-01-21 13:32:52.535136 UTC] Start collecting samples
[2018-01-21 13:32:57.082104 UTC] Computing input variables for policy optimization
[2018-01-21 13:32:57.220081 UTC] Performing policy update
[2018-01-21 13:32:57.220736 UTC] Computing gradient in Euclidean space
[2018-01-21 13:32:57.332019 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:32:58.741973 UTC] Performing line search
[2018-01-21 13:32:58.953503 UTC] Updating baseline
[2018-01-21 13:33:00.640725 UTC] Computing logging information
-------------------------------------
| Iteration            | 384        |
| ExpectedImprovement  | 0.016327   |
| ActualImprovement    | 0.015722   |
| ImprovementRatio     | 0.96294    |
| MeanKL               | 0.0074286  |
| Entropy              | 0.44853    |
| Perplexity           | 1.566      |
| AveragePolicyStd     | 0.26392    |
| AveragePolicyStd[0]  | 0.27103    |
| AveragePolicyStd[1]  | 0.33533    |
| AveragePolicyStd[2]  | 0.19989    |
| AveragePolicyStd[3]  | 0.2807     |
| AveragePolicyStd[4]  | 0.2471     |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 986.74     |
| MinReturn            | 91.157     |
| MaxReturn            | 1082.6     |
| StdReturn            | 173.77     |
| AverageEpisodeLength | 963.81     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.99     |
| TotalNEpisodes       | 18594      |
| TotalNSamples        | 1.9212e+06 |
| ExplainedVariance    | 0.16014    |
-------------------------------------
[2018-01-21 13:33:01.271764 UTC] Saving snapshot
[2018-01-21 13:33:01.271976 UTC] Starting iteration 385
[2018-01-21 13:33:01.272124 UTC] Start collecting samples
[2018-01-21 13:33:05.893405 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:06.036945 UTC] Performing policy update
[2018-01-21 13:33:06.037562 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:06.154003 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:07.601277 UTC] Performing line search
[2018-01-21 13:33:07.787149 UTC] Updating baseline
[2018-01-21 13:33:09.834181 UTC] Computing logging information
-------------------------------------
| Iteration            | 385        |
| ExpectedImprovement  | 0.014809   |
| ActualImprovement    | 0.014013   |
| ImprovementRatio     | 0.94626    |
| MeanKL               | 0.0070622  |
| Entropy              | 0.45577    |
| Perplexity           | 1.5774     |
| AveragePolicyStd     | 0.2642     |
| AveragePolicyStd[0]  | 0.27114    |
| AveragePolicyStd[1]  | 0.3347     |
| AveragePolicyStd[2]  | 0.20004    |
| AveragePolicyStd[3]  | 0.2814     |
| AveragePolicyStd[4]  | 0.24846    |
| AveragePolicyStd[5]  | 0.24944    |
| AverageReturn        | 978.84     |
| MinReturn            | 91.157     |
| MaxReturn            | 1082.6     |
| StdReturn            | 189.07     |
| AverageEpisodeLength | 954.42     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.88     |
| TotalNEpisodes       | 18601      |
| TotalNSamples        | 1.9269e+06 |
| ExplainedVariance    | 0.27142    |
-------------------------------------
[2018-01-21 13:33:10.578498 UTC] Saving snapshot
[2018-01-21 13:33:10.578782 UTC] Starting iteration 386
[2018-01-21 13:33:10.578975 UTC] Start collecting samples
[2018-01-21 13:33:15.144436 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:15.292785 UTC] Performing policy update
[2018-01-21 13:33:15.293403 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:15.414113 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:16.848098 UTC] Performing line search
[2018-01-21 13:33:17.034280 UTC] Updating baseline
[2018-01-21 13:33:19.069843 UTC] Computing logging information
-------------------------------------
| Iteration            | 386        |
| ExpectedImprovement  | 0.015066   |
| ActualImprovement    | 0.014525   |
| ImprovementRatio     | 0.96409    |
| MeanKL               | 0.0078289  |
| Entropy              | 0.44922    |
| Perplexity           | 1.5671     |
| AveragePolicyStd     | 0.26389    |
| AveragePolicyStd[0]  | 0.27293    |
| AveragePolicyStd[1]  | 0.33415    |
| AveragePolicyStd[2]  | 0.20028    |
| AveragePolicyStd[3]  | 0.28008    |
| AveragePolicyStd[4]  | 0.24752    |
| AveragePolicyStd[5]  | 0.2484     |
| AverageReturn        | 974.3      |
| MinReturn            | 91.157     |
| MaxReturn            | 1082.6     |
| StdReturn            | 194.28     |
| AverageEpisodeLength | 949.36     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.55     |
| TotalNEpisodes       | 18605      |
| TotalNSamples        | 1.9304e+06 |
| ExplainedVariance    | 0.1235     |
-------------------------------------
[2018-01-21 13:33:19.708968 UTC] Saving snapshot
[2018-01-21 13:33:19.709211 UTC] Starting iteration 387
[2018-01-21 13:33:19.709423 UTC] Start collecting samples
[2018-01-21 13:33:24.287993 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:24.426455 UTC] Performing policy update
[2018-01-21 13:33:24.427400 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:24.537838 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:26.012056 UTC] Performing line search
[2018-01-21 13:33:26.208775 UTC] Updating baseline
[2018-01-21 13:33:28.105139 UTC] Computing logging information
-------------------------------------
| Iteration            | 387        |
| ExpectedImprovement  | 0.014772   |
| ActualImprovement    | 0.014399   |
| ImprovementRatio     | 0.97477    |
| MeanKL               | 0.0078829  |
| Entropy              | 0.43985    |
| Perplexity           | 1.5525     |
| AveragePolicyStd     | 0.2635     |
| AveragePolicyStd[0]  | 0.27255    |
| AveragePolicyStd[1]  | 0.33404    |
| AveragePolicyStd[2]  | 0.19998    |
| AveragePolicyStd[3]  | 0.2796     |
| AveragePolicyStd[4]  | 0.24775    |
| AveragePolicyStd[5]  | 0.24706    |
| AverageReturn        | 975.26     |
| MinReturn            | 91.157     |
| MaxReturn            | 1082.6     |
| StdReturn            | 194.55     |
| AverageEpisodeLength | 949.36     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.55     |
| TotalNEpisodes       | 18611      |
| TotalNSamples        | 1.9364e+06 |
| ExplainedVariance    | 0.0074427  |
-------------------------------------
[2018-01-21 13:33:28.773852 UTC] Saving snapshot
[2018-01-21 13:33:28.774069 UTC] Starting iteration 388
[2018-01-21 13:33:28.774272 UTC] Start collecting samples
[2018-01-21 13:33:33.303625 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:33.421912 UTC] Performing policy update
[2018-01-21 13:33:33.422523 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:33.538518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:34.912503 UTC] Performing line search
[2018-01-21 13:33:35.099125 UTC] Updating baseline
[2018-01-21 13:33:37.101445 UTC] Computing logging information
-------------------------------------
| Iteration            | 388        |
| ExpectedImprovement  | 0.016491   |
| ActualImprovement    | 0.015891   |
| ImprovementRatio     | 0.96359    |
| MeanKL               | 0.0081081  |
| Entropy              | 0.42975    |
| Perplexity           | 1.5369     |
| AveragePolicyStd     | 0.26303    |
| AveragePolicyStd[0]  | 0.27308    |
| AveragePolicyStd[1]  | 0.3327     |
| AveragePolicyStd[2]  | 0.2        |
| AveragePolicyStd[3]  | 0.27956    |
| AveragePolicyStd[4]  | 0.24553    |
| AveragePolicyStd[5]  | 0.24733    |
| AverageReturn        | 975.81     |
| MinReturn            | 72.994     |
| MaxReturn            | 1082.6     |
| StdReturn            | 196.68     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18617      |
| TotalNSamples        | 1.9415e+06 |
| ExplainedVariance    | 0.071684   |
-------------------------------------
[2018-01-21 13:33:37.745412 UTC] Saving snapshot
[2018-01-21 13:33:37.745683 UTC] Starting iteration 389
[2018-01-21 13:33:37.745853 UTC] Start collecting samples
[2018-01-21 13:33:42.188106 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:42.310823 UTC] Performing policy update
[2018-01-21 13:33:42.311577 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:42.450849 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:43.867279 UTC] Performing line search
[2018-01-21 13:33:44.060215 UTC] Updating baseline
[2018-01-21 13:33:46.220109 UTC] Computing logging information
-------------------------------------
| Iteration            | 389        |
| ExpectedImprovement  | 0.016818   |
| ActualImprovement    | 0.015333   |
| ImprovementRatio     | 0.91171    |
| MeanKL               | 0.0071507  |
| Entropy              | 0.42771    |
| Perplexity           | 1.5337     |
| AveragePolicyStd     | 0.26304    |
| AveragePolicyStd[0]  | 0.27207    |
| AveragePolicyStd[1]  | 0.33456    |
| AveragePolicyStd[2]  | 0.19918    |
| AveragePolicyStd[3]  | 0.27947    |
| AveragePolicyStd[4]  | 0.24537    |
| AveragePolicyStd[5]  | 0.24762    |
| AverageReturn        | 977.28     |
| MinReturn            | 72.994     |
| MaxReturn            | 1082.6     |
| StdReturn            | 197.02     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18621      |
| TotalNSamples        | 1.9455e+06 |
| ExplainedVariance    | -0.016695  |
-------------------------------------
[2018-01-21 13:33:46.929675 UTC] Saving snapshot
[2018-01-21 13:33:46.929910 UTC] Starting iteration 390
[2018-01-21 13:33:46.930059 UTC] Start collecting samples
[2018-01-21 13:33:51.490330 UTC] Computing input variables for policy optimization
[2018-01-21 13:33:51.630209 UTC] Performing policy update
[2018-01-21 13:33:51.631360 UTC] Computing gradient in Euclidean space
[2018-01-21 13:33:51.745981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:33:53.178473 UTC] Performing line search
[2018-01-21 13:33:53.372732 UTC] Updating baseline
[2018-01-21 13:33:56.548996 UTC] Computing logging information
-------------------------------------
| Iteration            | 390        |
| ExpectedImprovement  | 0.016885   |
| ActualImprovement    | 0.015994   |
| ImprovementRatio     | 0.94722    |
| MeanKL               | 0.0070494  |
| Entropy              | 0.42403    |
| Perplexity           | 1.5281     |
| AveragePolicyStd     | 0.26288    |
| AveragePolicyStd[0]  | 0.27029    |
| AveragePolicyStd[1]  | 0.33368    |
| AveragePolicyStd[2]  | 0.19881    |
| AveragePolicyStd[3]  | 0.28124    |
| AveragePolicyStd[4]  | 0.24489    |
| AveragePolicyStd[5]  | 0.24835    |
| AverageReturn        | 979.32     |
| MinReturn            | 72.994     |
| MaxReturn            | 1082.6     |
| StdReturn            | 197.68     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18626      |
| TotalNSamples        | 1.9505e+06 |
| ExplainedVariance    | 0.01564    |
-------------------------------------
[2018-01-21 13:33:57.266325 UTC] Saving snapshot
[2018-01-21 13:33:57.272135 UTC] Starting iteration 391
[2018-01-21 13:33:57.272323 UTC] Start collecting samples
[2018-01-21 13:34:01.885643 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:02.010503 UTC] Performing policy update
[2018-01-21 13:34:02.011225 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:02.140277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:03.534304 UTC] Performing line search
[2018-01-21 13:34:03.727362 UTC] Updating baseline
[2018-01-21 13:34:05.985638 UTC] Computing logging information
-------------------------------------
| Iteration            | 391        |
| ExpectedImprovement  | 0.015555   |
| ActualImprovement    | 0.015013   |
| ImprovementRatio     | 0.96514    |
| MeanKL               | 0.007292   |
| Entropy              | 0.42379    |
| Perplexity           | 1.5277     |
| AveragePolicyStd     | 0.26285    |
| AveragePolicyStd[0]  | 0.27041    |
| AveragePolicyStd[1]  | 0.33313    |
| AveragePolicyStd[2]  | 0.19854    |
| AveragePolicyStd[3]  | 0.28104    |
| AveragePolicyStd[4]  | 0.24519    |
| AveragePolicyStd[5]  | 0.24881    |
| AverageReturn        | 981.8      |
| MinReturn            | 72.994     |
| MaxReturn            | 1089.3     |
| StdReturn            | 198.52     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18631      |
| TotalNSamples        | 1.9555e+06 |
| ExplainedVariance    | -0.038621  |
-------------------------------------
[2018-01-21 13:34:06.649116 UTC] Saving snapshot
[2018-01-21 13:34:06.649355 UTC] Starting iteration 392
[2018-01-21 13:34:06.649502 UTC] Start collecting samples
[2018-01-21 13:34:11.132347 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:11.265135 UTC] Performing policy update
[2018-01-21 13:34:11.265784 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:11.389961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:12.791072 UTC] Performing line search
[2018-01-21 13:34:12.982381 UTC] Updating baseline
[2018-01-21 13:34:15.536093 UTC] Computing logging information
-------------------------------------
| Iteration            | 392        |
| ExpectedImprovement  | 0.015431   |
| ActualImprovement    | 0.014221   |
| ImprovementRatio     | 0.92157    |
| MeanKL               | 0.0076115  |
| Entropy              | 0.42284    |
| Perplexity           | 1.5263     |
| AveragePolicyStd     | 0.26278    |
| AveragePolicyStd[0]  | 0.27174    |
| AveragePolicyStd[1]  | 0.33127    |
| AveragePolicyStd[2]  | 0.19758    |
| AveragePolicyStd[3]  | 0.28028    |
| AveragePolicyStd[4]  | 0.24544    |
| AveragePolicyStd[5]  | 0.25038    |
| AverageReturn        | 985.4      |
| MinReturn            | 72.994     |
| MaxReturn            | 1089.3     |
| StdReturn            | 199.21     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18635      |
| TotalNSamples        | 1.9595e+06 |
| ExplainedVariance    | 0.0087111  |
-------------------------------------
[2018-01-21 13:34:16.241382 UTC] Saving snapshot
[2018-01-21 13:34:16.241650 UTC] Starting iteration 393
[2018-01-21 13:34:16.241835 UTC] Start collecting samples
[2018-01-21 13:34:20.935996 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:21.056968 UTC] Performing policy update
[2018-01-21 13:34:21.058092 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:21.176959 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:22.566249 UTC] Performing line search
[2018-01-21 13:34:22.767502 UTC] Updating baseline
[2018-01-21 13:34:25.100769 UTC] Computing logging information
-------------------------------------
| Iteration            | 393        |
| ExpectedImprovement  | 0.015427   |
| ActualImprovement    | 0.014638   |
| ImprovementRatio     | 0.94891    |
| MeanKL               | 0.0081927  |
| Entropy              | 0.41379    |
| Perplexity           | 1.5125     |
| AveragePolicyStd     | 0.26232    |
| AveragePolicyStd[0]  | 0.27149    |
| AveragePolicyStd[1]  | 0.33025    |
| AveragePolicyStd[2]  | 0.19798    |
| AveragePolicyStd[3]  | 0.27896    |
| AveragePolicyStd[4]  | 0.24552    |
| AveragePolicyStd[5]  | 0.24971    |
| AverageReturn        | 987.96     |
| MinReturn            | 72.994     |
| MaxReturn            | 1089.3     |
| StdReturn            | 199.45     |
| AverageEpisodeLength | 948.87     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.77     |
| TotalNEpisodes       | 18638      |
| TotalNSamples        | 1.9625e+06 |
| ExplainedVariance    | 0.040989   |
-------------------------------------
[2018-01-21 13:34:25.731944 UTC] Saving snapshot
[2018-01-21 13:34:25.732186 UTC] Starting iteration 394
[2018-01-21 13:34:25.732354 UTC] Start collecting samples
[2018-01-21 13:34:30.435020 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:30.575813 UTC] Performing policy update
[2018-01-21 13:34:30.576425 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:30.693499 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:32.114337 UTC] Performing line search
[2018-01-21 13:34:32.301833 UTC] Updating baseline
[2018-01-21 13:34:34.474183 UTC] Computing logging information
-------------------------------------
| Iteration            | 394        |
| ExpectedImprovement  | 0.015938   |
| ActualImprovement    | 0.015216   |
| ImprovementRatio     | 0.95471    |
| MeanKL               | 0.0069806  |
| Entropy              | 0.40952    |
| Perplexity           | 1.5061     |
| AveragePolicyStd     | 0.2621     |
| AveragePolicyStd[0]  | 0.27074    |
| AveragePolicyStd[1]  | 0.33006    |
| AveragePolicyStd[2]  | 0.19786    |
| AveragePolicyStd[3]  | 0.27742    |
| AveragePolicyStd[4]  | 0.24684    |
| AveragePolicyStd[5]  | 0.24967    |
| AverageReturn        | 984.39     |
| MinReturn            | 72.994     |
| MaxReturn            | 1095.2     |
| StdReturn            | 209.54     |
| AverageEpisodeLength | 941.48     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.68     |
| TotalNEpisodes       | 18648      |
| TotalNSamples        | 1.9718e+06 |
| ExplainedVariance    | 0.1531     |
-------------------------------------
[2018-01-21 13:34:35.136309 UTC] Saving snapshot
[2018-01-21 13:34:35.136525 UTC] Starting iteration 395
[2018-01-21 13:34:35.136708 UTC] Start collecting samples
[2018-01-21 13:34:39.848884 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:39.978958 UTC] Performing policy update
[2018-01-21 13:34:39.979549 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:40.096247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:41.479450 UTC] Performing line search
[2018-01-21 13:34:41.663744 UTC] Updating baseline
[2018-01-21 13:34:43.591222 UTC] Computing logging information
-------------------------------------
| Iteration            | 395        |
| ExpectedImprovement  | 0.016869   |
| ActualImprovement    | 0.015571   |
| ImprovementRatio     | 0.92304    |
| MeanKL               | 0.0069442  |
| Entropy              | 0.41741    |
| Perplexity           | 1.518      |
| AveragePolicyStd     | 0.26255    |
| AveragePolicyStd[0]  | 0.27226    |
| AveragePolicyStd[1]  | 0.33183    |
| AveragePolicyStd[2]  | 0.19724    |
| AveragePolicyStd[3]  | 0.27782    |
| AveragePolicyStd[4]  | 0.24654    |
| AveragePolicyStd[5]  | 0.24964    |
| AverageReturn        | 984.09     |
| MinReturn            | 72.994     |
| MaxReturn            | 1095.2     |
| StdReturn            | 209.58     |
| AverageEpisodeLength | 939.88     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.53     |
| TotalNEpisodes       | 18652      |
| TotalNSamples        | 1.9756e+06 |
| ExplainedVariance    | 0.38188    |
-------------------------------------
[2018-01-21 13:34:44.324653 UTC] Saving snapshot
[2018-01-21 13:34:44.324892 UTC] Starting iteration 396
[2018-01-21 13:34:44.325064 UTC] Start collecting samples
[2018-01-21 13:34:48.867081 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:49.006306 UTC] Performing policy update
[2018-01-21 13:34:49.007064 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:49.124278 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:50.547595 UTC] Performing line search
[2018-01-21 13:34:50.745586 UTC] Updating baseline
[2018-01-21 13:34:52.659080 UTC] Computing logging information
------------------------------------
| Iteration            | 396       |
| ExpectedImprovement  | 0.017797  |
| ActualImprovement    | 0.016268  |
| ImprovementRatio     | 0.91409   |
| MeanKL               | 0.0073537 |
| Entropy              | 0.40572   |
| Perplexity           | 1.5004    |
| AveragePolicyStd     | 0.26205   |
| AveragePolicyStd[0]  | 0.27327   |
| AveragePolicyStd[1]  | 0.33021   |
| AveragePolicyStd[2]  | 0.19655   |
| AveragePolicyStd[3]  | 0.27783   |
| AveragePolicyStd[4]  | 0.24724   |
| AveragePolicyStd[5]  | 0.24719   |
| AverageReturn        | 979.88    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 216.58    |
| AverageEpisodeLength | 934.28    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 202.62    |
| TotalNEpisodes       | 18655     |
| TotalNSamples        | 1.978e+06 |
| ExplainedVariance    | 0.27564   |
------------------------------------
[2018-01-21 13:34:53.325638 UTC] Saving snapshot
[2018-01-21 13:34:53.325842 UTC] Starting iteration 397
[2018-01-21 13:34:53.326001 UTC] Start collecting samples
[2018-01-21 13:34:57.878447 UTC] Computing input variables for policy optimization
[2018-01-21 13:34:58.015330 UTC] Performing policy update
[2018-01-21 13:34:58.015956 UTC] Computing gradient in Euclidean space
[2018-01-21 13:34:58.129194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:34:59.521991 UTC] Performing line search
[2018-01-21 13:34:59.707040 UTC] Updating baseline
[2018-01-21 13:35:01.939362 UTC] Computing logging information
------------------------------------
| Iteration            | 397       |
| ExpectedImprovement  | 0.015628  |
| ActualImprovement    | 0.014832  |
| ImprovementRatio     | 0.94904   |
| MeanKL               | 0.0074851 |
| Entropy              | 0.39739   |
| Perplexity           | 1.4879    |
| AveragePolicyStd     | 0.26173   |
| AveragePolicyStd[0]  | 0.27297   |
| AveragePolicyStd[1]  | 0.3302    |
| AveragePolicyStd[2]  | 0.19566   |
| AveragePolicyStd[3]  | 0.27745   |
| AveragePolicyStd[4]  | 0.24715   |
| AveragePolicyStd[5]  | 0.24695   |
| AverageReturn        | 984.5     |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 217.89    |
| AverageEpisodeLength | 934.28    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 202.62    |
| TotalNEpisodes       | 18663     |
| TotalNSamples        | 1.986e+06 |
| ExplainedVariance    | 0.018311  |
------------------------------------
[2018-01-21 13:35:02.640745 UTC] Saving snapshot
[2018-01-21 13:35:02.640979 UTC] Starting iteration 398
[2018-01-21 13:35:02.641158 UTC] Start collecting samples
[2018-01-21 13:35:07.259318 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:07.388136 UTC] Performing policy update
[2018-01-21 13:35:07.388750 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:07.513677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:09.032817 UTC] Performing line search
[2018-01-21 13:35:09.231452 UTC] Updating baseline
[2018-01-21 13:35:10.961986 UTC] Computing logging information
------------------------------------
| Iteration            | 398       |
| ExpectedImprovement  | 0.01598   |
| ActualImprovement    | 0.015562  |
| ImprovementRatio     | 0.97387   |
| MeanKL               | 0.0072264 |
| Entropy              | 0.38432   |
| Perplexity           | 1.4686    |
| AveragePolicyStd     | 0.26111   |
| AveragePolicyStd[0]  | 0.27197   |
| AveragePolicyStd[1]  | 0.32879   |
| AveragePolicyStd[2]  | 0.19582   |
| AveragePolicyStd[3]  | 0.27732   |
| AveragePolicyStd[4]  | 0.24665   |
| AveragePolicyStd[5]  | 0.24612   |
| AverageReturn        | 985.62    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 218.31    |
| AverageEpisodeLength | 934.28    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 202.62    |
| TotalNEpisodes       | 18667     |
| TotalNSamples        | 1.99e+06  |
| ExplainedVariance    | -0.011988 |
------------------------------------
[2018-01-21 13:35:11.667155 UTC] Saving snapshot
[2018-01-21 13:35:11.667446 UTC] Starting iteration 399
[2018-01-21 13:35:11.667641 UTC] Start collecting samples
[2018-01-21 13:35:16.320691 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:16.456003 UTC] Performing policy update
[2018-01-21 13:35:16.456618 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:16.582892 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:18.007451 UTC] Performing line search
[2018-01-21 13:35:18.194049 UTC] Updating baseline
[2018-01-21 13:35:20.822765 UTC] Computing logging information
------------------------------------
| Iteration            | 399       |
| ExpectedImprovement  | 0.016505  |
| ActualImprovement    | 0.015768  |
| ImprovementRatio     | 0.95535   |
| MeanKL               | 0.007325  |
| Entropy              | 0.38366   |
| Perplexity           | 1.4676    |
| AveragePolicyStd     | 0.26113   |
| AveragePolicyStd[0]  | 0.27281   |
| AveragePolicyStd[1]  | 0.32922   |
| AveragePolicyStd[2]  | 0.19569   |
| AveragePolicyStd[3]  | 0.27806   |
| AveragePolicyStd[4]  | 0.24456   |
| AveragePolicyStd[5]  | 0.24647   |
| AverageReturn        | 995.83    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 199.85    |
| AverageEpisodeLength | 943.15    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.13    |
| TotalNEpisodes       | 18671     |
| TotalNSamples        | 1.994e+06 |
| ExplainedVariance    | 0.0071313 |
------------------------------------
[2018-01-21 13:35:21.533642 UTC] Saving snapshot
[2018-01-21 13:35:21.533902 UTC] Starting iteration 400
[2018-01-21 13:35:21.534148 UTC] Start collecting samples
[2018-01-21 13:35:26.101530 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:26.235704 UTC] Performing policy update
[2018-01-21 13:35:26.236381 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:26.353762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:27.736903 UTC] Performing line search
[2018-01-21 13:35:27.933896 UTC] Updating baseline
[2018-01-21 13:35:30.047603 UTC] Computing logging information
------------------------------------
| Iteration            | 400       |
| ExpectedImprovement  | 0.016183  |
| ActualImprovement    | 0.015276  |
| ImprovementRatio     | 0.94395   |
| MeanKL               | 0.0077204 |
| Entropy              | 0.37717   |
| Perplexity           | 1.4582    |
| AveragePolicyStd     | 0.26091   |
| AveragePolicyStd[0]  | 0.27213   |
| AveragePolicyStd[1]  | 0.32983   |
| AveragePolicyStd[2]  | 0.19499   |
| AveragePolicyStd[3]  | 0.2779    |
| AveragePolicyStd[4]  | 0.24443   |
| AveragePolicyStd[5]  | 0.24618   |
| AverageReturn        | 999.12    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 200.71    |
| AverageEpisodeLength | 943.15    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.13    |
| TotalNEpisodes       | 18678     |
| TotalNSamples        | 2.001e+06 |
| ExplainedVariance    | 0.017332  |
------------------------------------
[2018-01-21 13:35:30.761049 UTC] Saving snapshot
[2018-01-21 13:35:30.770389 UTC] Starting iteration 401
[2018-01-21 13:35:30.770631 UTC] Start collecting samples
[2018-01-21 13:35:35.152210 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:35.290014 UTC] Performing policy update
[2018-01-21 13:35:35.290709 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:35.407338 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:36.792367 UTC] Performing line search
[2018-01-21 13:35:36.983354 UTC] Updating baseline
[2018-01-21 13:35:39.161772 UTC] Computing logging information
------------------------------------
| Iteration            | 401       |
| ExpectedImprovement  | 0.014781  |
| ActualImprovement    | 0.013958  |
| ImprovementRatio     | 0.94429   |
| MeanKL               | 0.0077223 |
| Entropy              | 0.37837   |
| Perplexity           | 1.4599    |
| AveragePolicyStd     | 0.26093   |
| AveragePolicyStd[0]  | 0.27193   |
| AveragePolicyStd[1]  | 0.32902   |
| AveragePolicyStd[2]  | 0.19503   |
| AveragePolicyStd[3]  | 0.27845   |
| AveragePolicyStd[4]  | 0.24359   |
| AveragePolicyStd[5]  | 0.24757   |
| AverageReturn        | 1000.3    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 201.11    |
| AverageEpisodeLength | 943.15    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.13    |
| TotalNEpisodes       | 18683     |
| TotalNSamples        | 2.006e+06 |
| ExplainedVariance    | 0.12889   |
------------------------------------
[2018-01-21 13:35:39.842171 UTC] Saving snapshot
[2018-01-21 13:35:39.842469 UTC] Starting iteration 402
[2018-01-21 13:35:39.842654 UTC] Start collecting samples
[2018-01-21 13:35:44.344252 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:44.465471 UTC] Performing policy update
[2018-01-21 13:35:44.466123 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:44.585699 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:45.989776 UTC] Performing line search
[2018-01-21 13:35:46.174696 UTC] Updating baseline
[2018-01-21 13:35:47.970020 UTC] Computing logging information
------------------------------------
| Iteration            | 402       |
| ExpectedImprovement  | 0.015673  |
| ActualImprovement    | 0.015406  |
| ImprovementRatio     | 0.98296   |
| MeanKL               | 0.0077057 |
| Entropy              | 0.37958   |
| Perplexity           | 1.4617    |
| AveragePolicyStd     | 0.26109   |
| AveragePolicyStd[0]  | 0.27311   |
| AveragePolicyStd[1]  | 0.33069   |
| AveragePolicyStd[2]  | 0.19432   |
| AveragePolicyStd[3]  | 0.27746   |
| AveragePolicyStd[4]  | 0.24305   |
| AveragePolicyStd[5]  | 0.24788   |
| AverageReturn        | 1000.9    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 201.28    |
| AverageEpisodeLength | 943.15    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.13    |
| TotalNEpisodes       | 18686     |
| TotalNSamples        | 2.009e+06 |
| ExplainedVariance    | -0.05021  |
------------------------------------
[2018-01-21 13:35:48.597763 UTC] Saving snapshot
[2018-01-21 13:35:48.598011 UTC] Starting iteration 403
[2018-01-21 13:35:48.598192 UTC] Start collecting samples
[2018-01-21 13:35:53.215422 UTC] Computing input variables for policy optimization
[2018-01-21 13:35:53.341185 UTC] Performing policy update
[2018-01-21 13:35:53.341894 UTC] Computing gradient in Euclidean space
[2018-01-21 13:35:53.464579 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:35:54.866553 UTC] Performing line search
[2018-01-21 13:35:55.051840 UTC] Updating baseline
[2018-01-21 13:35:56.842945 UTC] Computing logging information
------------------------------------
| Iteration            | 403       |
| ExpectedImprovement  | 0.015098  |
| ActualImprovement    | 0.014152  |
| ImprovementRatio     | 0.93735   |
| MeanKL               | 0.0074918 |
| Entropy              | 0.38165   |
| Perplexity           | 1.4647    |
| AveragePolicyStd     | 0.26119   |
| AveragePolicyStd[0]  | 0.27306   |
| AveragePolicyStd[1]  | 0.33128   |
| AveragePolicyStd[2]  | 0.19436   |
| AveragePolicyStd[3]  | 0.27717   |
| AveragePolicyStd[4]  | 0.2428    |
| AveragePolicyStd[5]  | 0.24848   |
| AverageReturn        | 1018.1    |
| MinReturn            | 72.994    |
| MaxReturn            | 1117.3    |
| StdReturn            | 168.05    |
| AverageEpisodeLength | 958.27    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 154.58    |
| TotalNEpisodes       | 18694     |
| TotalNSamples        | 2.017e+06 |
| ExplainedVariance    | 0.013005  |
------------------------------------
[2018-01-21 13:35:57.491888 UTC] Saving snapshot
[2018-01-21 13:35:57.492164 UTC] Starting iteration 404
[2018-01-21 13:35:57.492368 UTC] Start collecting samples
[2018-01-21 13:36:02.103409 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:02.243122 UTC] Performing policy update
[2018-01-21 13:36:02.243793 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:02.380296 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:03.804893 UTC] Performing line search
[2018-01-21 13:36:03.995763 UTC] Updating baseline
[2018-01-21 13:36:05.802121 UTC] Computing logging information
-------------------------------------
| Iteration            | 404        |
| ExpectedImprovement  | 0.014716   |
| ActualImprovement    | 0.014045   |
| ImprovementRatio     | 0.95441    |
| MeanKL               | 0.0078363  |
| Entropy              | 0.39732    |
| Perplexity           | 1.4878     |
| AveragePolicyStd     | 0.26192    |
| AveragePolicyStd[0]  | 0.27442    |
| AveragePolicyStd[1]  | 0.33252    |
| AveragePolicyStd[2]  | 0.19487    |
| AveragePolicyStd[3]  | 0.27922    |
| AveragePolicyStd[4]  | 0.2427     |
| AveragePolicyStd[5]  | 0.24783    |
| AverageReturn        | 1025.5     |
| MinReturn            | 72.994     |
| MaxReturn            | 1120.9     |
| StdReturn            | 167.18     |
| AverageEpisodeLength | 963.16     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.46     |
| TotalNEpisodes       | 18700      |
| TotalNSamples        | 2.0222e+06 |
| ExplainedVariance    | 0.11737    |
-------------------------------------
[2018-01-21 13:36:06.510245 UTC] Saving snapshot
[2018-01-21 13:36:06.510542 UTC] Starting iteration 405
[2018-01-21 13:36:06.510733 UTC] Start collecting samples
[2018-01-21 13:36:10.931315 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:11.070939 UTC] Performing policy update
[2018-01-21 13:36:11.071537 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:11.195772 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:12.670184 UTC] Performing line search
[2018-01-21 13:36:12.867514 UTC] Updating baseline
[2018-01-21 13:36:15.019690 UTC] Computing logging information
-------------------------------------
| Iteration            | 405        |
| ExpectedImprovement  | 0.01702    |
| ActualImprovement    | 0.015418   |
| ImprovementRatio     | 0.9059     |
| MeanKL               | 0.0070645  |
| Entropy              | 0.38993    |
| Perplexity           | 1.4769     |
| AveragePolicyStd     | 0.26155    |
| AveragePolicyStd[0]  | 0.27461    |
| AveragePolicyStd[1]  | 0.33026    |
| AveragePolicyStd[2]  | 0.19486    |
| AveragePolicyStd[3]  | 0.28       |
| AveragePolicyStd[4]  | 0.24182    |
| AveragePolicyStd[5]  | 0.24773    |
| AverageReturn        | 1025.4     |
| MinReturn            | 72.994     |
| MaxReturn            | 1120.9     |
| StdReturn            | 167.18     |
| AverageEpisodeLength | 963.16     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.46     |
| TotalNEpisodes       | 18702      |
| TotalNSamples        | 2.0242e+06 |
| ExplainedVariance    | 0.015098   |
-------------------------------------
[2018-01-21 13:36:15.751097 UTC] Saving snapshot
[2018-01-21 13:36:15.751386 UTC] Starting iteration 406
[2018-01-21 13:36:15.751570 UTC] Start collecting samples
[2018-01-21 13:36:20.364793 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:20.497462 UTC] Performing policy update
[2018-01-21 13:36:20.498410 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:20.638610 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:22.078200 UTC] Performing line search
[2018-01-21 13:36:22.264278 UTC] Updating baseline
[2018-01-21 13:36:24.331538 UTC] Computing logging information
-------------------------------------
| Iteration            | 406        |
| ExpectedImprovement  | 0.015028   |
| ActualImprovement    | 0.014293   |
| ImprovementRatio     | 0.95108    |
| MeanKL               | 0.0076072  |
| Entropy              | 0.38498    |
| Perplexity           | 1.4696     |
| AveragePolicyStd     | 0.26141    |
| AveragePolicyStd[0]  | 0.27416    |
| AveragePolicyStd[1]  | 0.33073    |
| AveragePolicyStd[2]  | 0.19437    |
| AveragePolicyStd[3]  | 0.28137    |
| AveragePolicyStd[4]  | 0.24073    |
| AveragePolicyStd[5]  | 0.2471     |
| AverageReturn        | 1032.9     |
| MinReturn            | 72.994     |
| MaxReturn            | 1120.9     |
| StdReturn            | 160.24     |
| AverageEpisodeLength | 968.22     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.07     |
| TotalNEpisodes       | 18708      |
| TotalNSamples        | 2.0302e+06 |
| ExplainedVariance    | 0.020192   |
-------------------------------------
[2018-01-21 13:36:25.014615 UTC] Saving snapshot
[2018-01-21 13:36:25.014900 UTC] Starting iteration 407
[2018-01-21 13:36:25.015081 UTC] Start collecting samples
[2018-01-21 13:36:29.538534 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:29.673929 UTC] Performing policy update
[2018-01-21 13:36:29.674618 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:29.794011 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:31.212012 UTC] Performing line search
[2018-01-21 13:36:31.408098 UTC] Updating baseline
[2018-01-21 13:36:33.604581 UTC] Computing logging information
-------------------------------------
| Iteration            | 407        |
| ExpectedImprovement  | 0.014418   |
| ActualImprovement    | 0.013863   |
| ImprovementRatio     | 0.96152    |
| MeanKL               | 0.0067959  |
| Entropy              | 0.38509    |
| Perplexity           | 1.4697     |
| AveragePolicyStd     | 0.26135    |
| AveragePolicyStd[0]  | 0.27392    |
| AveragePolicyStd[1]  | 0.33023    |
| AveragePolicyStd[2]  | 0.1946     |
| AveragePolicyStd[3]  | 0.28044    |
| AveragePolicyStd[4]  | 0.24176    |
| AveragePolicyStd[5]  | 0.24718    |
| AverageReturn        | 1027.2     |
| MinReturn            | 72.994     |
| MaxReturn            | 1133.2     |
| StdReturn            | 183.89     |
| AverageEpisodeLength | 959.63     |
| MinEpisodeLength     | 81         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.62     |
| TotalNEpisodes       | 18715      |
| TotalNSamples        | 2.0364e+06 |
| ExplainedVariance    | 0.089573   |
-------------------------------------
[2018-01-21 13:36:34.309011 UTC] Saving snapshot
[2018-01-21 13:36:34.309251 UTC] Starting iteration 408
[2018-01-21 13:36:34.309400 UTC] Start collecting samples
[2018-01-21 13:36:38.651063 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:38.794288 UTC] Performing policy update
[2018-01-21 13:36:38.794980 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:38.925138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:40.328783 UTC] Performing line search
[2018-01-21 13:36:40.512222 UTC] Updating baseline
[2018-01-21 13:36:42.295640 UTC] Computing logging information
-------------------------------------
| Iteration            | 408        |
| ExpectedImprovement  | 0.015259   |
| ActualImprovement    | 0.014332   |
| ImprovementRatio     | 0.93929    |
| MeanKL               | 0.007234   |
| Entropy              | 0.39106    |
| Perplexity           | 1.4785     |
| AveragePolicyStd     | 0.26145    |
| AveragePolicyStd[0]  | 0.27304    |
| AveragePolicyStd[1]  | 0.32963    |
| AveragePolicyStd[2]  | 0.19649    |
| AveragePolicyStd[3]  | 0.27908    |
| AveragePolicyStd[4]  | 0.24215    |
| AveragePolicyStd[5]  | 0.24832    |
| AverageReturn        | 1038.2     |
| MinReturn            | 140.34     |
| MaxReturn            | 1133.2     |
| StdReturn            | 157.08     |
| AverageEpisodeLength | 968.82     |
| MinEpisodeLength     | 141        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.51     |
| TotalNEpisodes       | 18719      |
| TotalNSamples        | 2.0404e+06 |
| ExplainedVariance    | 0.072661   |
-------------------------------------
[2018-01-21 13:36:43.006923 UTC] Saving snapshot
[2018-01-21 13:36:43.007207 UTC] Starting iteration 409
[2018-01-21 13:36:43.007358 UTC] Start collecting samples
[2018-01-21 13:36:47.544773 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:47.685597 UTC] Performing policy update
[2018-01-21 13:36:47.686276 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:47.807166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:49.280411 UTC] Performing line search
[2018-01-21 13:36:49.476293 UTC] Updating baseline
[2018-01-21 13:36:52.314951 UTC] Computing logging information
-------------------------------------
| Iteration            | 409        |
| ExpectedImprovement  | 0.017758   |
| ActualImprovement    | 0.017026   |
| ImprovementRatio     | 0.95875    |
| MeanKL               | 0.0069224  |
| Entropy              | 0.39018    |
| Perplexity           | 1.4773     |
| AveragePolicyStd     | 0.26142    |
| AveragePolicyStd[0]  | 0.27303    |
| AveragePolicyStd[1]  | 0.32913    |
| AveragePolicyStd[2]  | 0.19629    |
| AveragePolicyStd[3]  | 0.2801     |
| AveragePolicyStd[4]  | 0.24185    |
| AveragePolicyStd[5]  | 0.24813    |
| AverageReturn        | 1039.1     |
| MinReturn            | 140.34     |
| MaxReturn            | 1133.2     |
| StdReturn            | 157.13     |
| AverageEpisodeLength | 968.82     |
| MinEpisodeLength     | 141        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.51     |
| TotalNEpisodes       | 18721      |
| TotalNSamples        | 2.0424e+06 |
| ExplainedVariance    | -0.21598   |
-------------------------------------
[2018-01-21 13:36:52.998018 UTC] Saving snapshot
[2018-01-21 13:36:52.998214 UTC] Starting iteration 410
[2018-01-21 13:36:52.998384 UTC] Start collecting samples
[2018-01-21 13:36:57.623967 UTC] Computing input variables for policy optimization
[2018-01-21 13:36:57.752157 UTC] Performing policy update
[2018-01-21 13:36:57.752797 UTC] Computing gradient in Euclidean space
[2018-01-21 13:36:57.871381 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:36:59.270041 UTC] Performing line search
[2018-01-21 13:36:59.453983 UTC] Updating baseline
[2018-01-21 13:37:01.387292 UTC] Computing logging information
-------------------------------------
| Iteration            | 410        |
| ExpectedImprovement  | 0.016851   |
| ActualImprovement    | 0.015982   |
| ImprovementRatio     | 0.94842    |
| MeanKL               | 0.0076298  |
| Entropy              | 0.38196    |
| Perplexity           | 1.4652     |
| AveragePolicyStd     | 0.26112    |
| AveragePolicyStd[0]  | 0.27327    |
| AveragePolicyStd[1]  | 0.3298     |
| AveragePolicyStd[2]  | 0.1958     |
| AveragePolicyStd[3]  | 0.27917    |
| AveragePolicyStd[4]  | 0.24091    |
| AveragePolicyStd[5]  | 0.24778    |
| AverageReturn        | 1041.3     |
| MinReturn            | 140.34     |
| MaxReturn            | 1133.2     |
| StdReturn            | 157.6      |
| AverageEpisodeLength | 968.82     |
| MinEpisodeLength     | 141        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.51     |
| TotalNEpisodes       | 18731      |
| TotalNSamples        | 2.0524e+06 |
| ExplainedVariance    | 0.016175   |
-------------------------------------
[2018-01-21 13:37:02.036079 UTC] Saving snapshot
[2018-01-21 13:37:02.046473 UTC] Starting iteration 411
[2018-01-21 13:37:02.046710 UTC] Start collecting samples
[2018-01-21 13:37:06.530313 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:06.655043 UTC] Performing policy update
[2018-01-21 13:37:06.655643 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:06.779428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:08.215851 UTC] Performing line search
[2018-01-21 13:37:08.432338 UTC] Updating baseline
[2018-01-21 13:37:10.560620 UTC] Computing logging information
-------------------------------------
| Iteration            | 411        |
| ExpectedImprovement  | 0.018016   |
| ActualImprovement    | 0.017431   |
| ImprovementRatio     | 0.96753    |
| MeanKL               | 0.0070134  |
| Entropy              | 0.36884    |
| Perplexity           | 1.4461     |
| AveragePolicyStd     | 0.26057    |
| AveragePolicyStd[0]  | 0.27252    |
| AveragePolicyStd[1]  | 0.32892    |
| AveragePolicyStd[2]  | 0.19509    |
| AveragePolicyStd[3]  | 0.27924    |
| AveragePolicyStd[4]  | 0.241      |
| AveragePolicyStd[5]  | 0.24663    |
| AverageReturn        | 1025.7     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 190.39     |
| AverageEpisodeLength | 953.88     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.52     |
| TotalNEpisodes       | 18737      |
| TotalNSamples        | 2.0569e+06 |
| ExplainedVariance    | 0.25241    |
-------------------------------------
[2018-01-21 13:37:11.211901 UTC] Saving snapshot
[2018-01-21 13:37:11.212129 UTC] Starting iteration 412
[2018-01-21 13:37:11.212311 UTC] Start collecting samples
[2018-01-21 13:37:15.802366 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:15.951308 UTC] Performing policy update
[2018-01-21 13:37:15.952500 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:16.072122 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:17.486791 UTC] Performing line search
[2018-01-21 13:37:17.683010 UTC] Updating baseline
[2018-01-21 13:37:19.458715 UTC] Computing logging information
-------------------------------------
| Iteration            | 412        |
| ExpectedImprovement  | 0.015076   |
| ActualImprovement    | 0.014105   |
| ImprovementRatio     | 0.9356     |
| MeanKL               | 0.0075618  |
| Entropy              | 0.34615    |
| Perplexity           | 1.4136     |
| AveragePolicyStd     | 0.25951    |
| AveragePolicyStd[0]  | 0.27043    |
| AveragePolicyStd[1]  | 0.32713    |
| AveragePolicyStd[2]  | 0.19487    |
| AveragePolicyStd[3]  | 0.27757    |
| AveragePolicyStd[4]  | 0.24029    |
| AveragePolicyStd[5]  | 0.24676    |
| AverageReturn        | 1011.4     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 211.2      |
| AverageEpisodeLength | 940.39     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.82     |
| TotalNEpisodes       | 18742      |
| TotalNSamples        | 2.0604e+06 |
| ExplainedVariance    | 0.4842     |
-------------------------------------
[2018-01-21 13:37:20.173164 UTC] Saving snapshot
[2018-01-21 13:37:20.173399 UTC] Starting iteration 413
[2018-01-21 13:37:20.173560 UTC] Start collecting samples
[2018-01-21 13:37:24.697830 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:24.849918 UTC] Performing policy update
[2018-01-21 13:37:24.850389 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:24.966945 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:26.416097 UTC] Performing line search
[2018-01-21 13:37:26.601016 UTC] Updating baseline
[2018-01-21 13:37:28.415897 UTC] Computing logging information
-------------------------------------
| Iteration            | 413        |
| ExpectedImprovement  | 0.014501   |
| ActualImprovement    | 0.014105   |
| ImprovementRatio     | 0.97269    |
| MeanKL               | 0.0072424  |
| Entropy              | 0.3375     |
| Perplexity           | 1.4014     |
| AveragePolicyStd     | 0.25916    |
| AveragePolicyStd[0]  | 0.27005    |
| AveragePolicyStd[1]  | 0.32759    |
| AveragePolicyStd[2]  | 0.19437    |
| AveragePolicyStd[3]  | 0.27612    |
| AveragePolicyStd[4]  | 0.2402     |
| AveragePolicyStd[5]  | 0.24666    |
| AverageReturn        | 1019.2     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 201.76     |
| AverageEpisodeLength | 946.83     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.74     |
| TotalNEpisodes       | 18748      |
| TotalNSamples        | 2.0664e+06 |
| ExplainedVariance    | 0.011015   |
-------------------------------------
[2018-01-21 13:37:29.079155 UTC] Saving snapshot
[2018-01-21 13:37:29.079444 UTC] Starting iteration 414
[2018-01-21 13:37:29.079657 UTC] Start collecting samples
[2018-01-21 13:37:33.782880 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:33.904020 UTC] Performing policy update
[2018-01-21 13:37:33.904699 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:34.026059 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:35.400792 UTC] Performing line search
[2018-01-21 13:37:35.583733 UTC] Updating baseline
[2018-01-21 13:37:37.588633 UTC] Computing logging information
-------------------------------------
| Iteration            | 414        |
| ExpectedImprovement  | 0.015712   |
| ActualImprovement    | 0.01449    |
| ImprovementRatio     | 0.92227    |
| MeanKL               | 0.0076664  |
| Entropy              | 0.33807    |
| Perplexity           | 1.4022     |
| AveragePolicyStd     | 0.25927    |
| AveragePolicyStd[0]  | 0.27002    |
| AveragePolicyStd[1]  | 0.33027    |
| AveragePolicyStd[2]  | 0.19368    |
| AveragePolicyStd[3]  | 0.27292    |
| AveragePolicyStd[4]  | 0.24084    |
| AveragePolicyStd[5]  | 0.24791    |
| AverageReturn        | 1022.1     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 202.28     |
| AverageEpisodeLength | 948.43     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.85     |
| TotalNEpisodes       | 18752      |
| TotalNSamples        | 2.0704e+06 |
| ExplainedVariance    | -0.008188  |
-------------------------------------
[2018-01-21 13:37:38.233446 UTC] Saving snapshot
[2018-01-21 13:37:38.233687 UTC] Starting iteration 415
[2018-01-21 13:37:38.233832 UTC] Start collecting samples
[2018-01-21 13:37:42.847147 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:42.983839 UTC] Performing policy update
[2018-01-21 13:37:42.984431 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:43.107461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:44.518301 UTC] Performing line search
[2018-01-21 13:37:44.717878 UTC] Updating baseline
[2018-01-21 13:37:47.010752 UTC] Computing logging information
-------------------------------------
| Iteration            | 415        |
| ExpectedImprovement  | 0.0172     |
| ActualImprovement    | 0.015999   |
| ImprovementRatio     | 0.93019    |
| MeanKL               | 0.0072949  |
| Entropy              | 0.33062    |
| Perplexity           | 1.3918     |
| AveragePolicyStd     | 0.25902    |
| AveragePolicyStd[0]  | 0.26808    |
| AveragePolicyStd[1]  | 0.33131    |
| AveragePolicyStd[2]  | 0.19328    |
| AveragePolicyStd[3]  | 0.27351    |
| AveragePolicyStd[4]  | 0.23975    |
| AveragePolicyStd[5]  | 0.24817    |
| AverageReturn        | 1026.5     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 194.23     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18757      |
| TotalNSamples        | 2.0753e+06 |
| ExplainedVariance    | 0.14305    |
-------------------------------------
[2018-01-21 13:37:47.656447 UTC] Saving snapshot
[2018-01-21 13:37:47.656714 UTC] Starting iteration 416
[2018-01-21 13:37:47.656898 UTC] Start collecting samples
[2018-01-21 13:37:52.097995 UTC] Computing input variables for policy optimization
[2018-01-21 13:37:52.239383 UTC] Performing policy update
[2018-01-21 13:37:52.240076 UTC] Computing gradient in Euclidean space
[2018-01-21 13:37:52.356139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:37:53.754027 UTC] Performing line search
[2018-01-21 13:37:53.940036 UTC] Updating baseline
[2018-01-21 13:37:56.201277 UTC] Computing logging information
-------------------------------------
| Iteration            | 416        |
| ExpectedImprovement  | 0.017053   |
| ActualImprovement    | 0.015649   |
| ImprovementRatio     | 0.91765    |
| MeanKL               | 0.0071507  |
| Entropy              | 0.30873    |
| Perplexity           | 1.3617     |
| AveragePolicyStd     | 0.25812    |
| AveragePolicyStd[0]  | 0.2672     |
| AveragePolicyStd[1]  | 0.33112    |
| AveragePolicyStd[2]  | 0.19227    |
| AveragePolicyStd[3]  | 0.27177    |
| AveragePolicyStd[4]  | 0.23894    |
| AveragePolicyStd[5]  | 0.24742    |
| AverageReturn        | 1027.6     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 194.52     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18762      |
| TotalNSamples        | 2.0803e+06 |
| ExplainedVariance    | 0.016208   |
-------------------------------------
[2018-01-21 13:37:56.862903 UTC] Saving snapshot
[2018-01-21 13:37:56.863132 UTC] Starting iteration 417
[2018-01-21 13:37:56.863275 UTC] Start collecting samples
[2018-01-21 13:38:01.411864 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:01.535296 UTC] Performing policy update
[2018-01-21 13:38:01.536431 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:01.654673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:03.051478 UTC] Performing line search
[2018-01-21 13:38:03.245898 UTC] Updating baseline
[2018-01-21 13:38:05.343346 UTC] Computing logging information
-------------------------------------
| Iteration            | 417        |
| ExpectedImprovement  | 0.015505   |
| ActualImprovement    | 0.014132   |
| ImprovementRatio     | 0.91143    |
| MeanKL               | 0.0072252  |
| Entropy              | 0.29645    |
| Perplexity           | 1.3451     |
| AveragePolicyStd     | 0.25764    |
| AveragePolicyStd[0]  | 0.26715    |
| AveragePolicyStd[1]  | 0.33137    |
| AveragePolicyStd[2]  | 0.19148    |
| AveragePolicyStd[3]  | 0.27032    |
| AveragePolicyStd[4]  | 0.23862    |
| AveragePolicyStd[5]  | 0.24693    |
| AverageReturn        | 1029.4     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 195.14     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18768      |
| TotalNSamples        | 2.0863e+06 |
| ExplainedVariance    | -0.017043  |
-------------------------------------
[2018-01-21 13:38:05.985558 UTC] Saving snapshot
[2018-01-21 13:38:05.985797 UTC] Starting iteration 418
[2018-01-21 13:38:05.985950 UTC] Start collecting samples
[2018-01-21 13:38:10.303481 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:10.436291 UTC] Performing policy update
[2018-01-21 13:38:10.436898 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:10.553258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:11.937632 UTC] Performing line search
[2018-01-21 13:38:12.129205 UTC] Updating baseline
[2018-01-21 13:38:14.181411 UTC] Computing logging information
-------------------------------------
| Iteration            | 418        |
| ExpectedImprovement  | 0.013901   |
| ActualImprovement    | 0.013114   |
| ImprovementRatio     | 0.94342    |
| MeanKL               | 0.0076731  |
| Entropy              | 0.28542    |
| Perplexity           | 1.3303     |
| AveragePolicyStd     | 0.25735    |
| AveragePolicyStd[0]  | 0.26745    |
| AveragePolicyStd[1]  | 0.33296    |
| AveragePolicyStd[2]  | 0.18968    |
| AveragePolicyStd[3]  | 0.27005    |
| AveragePolicyStd[4]  | 0.23825    |
| AveragePolicyStd[5]  | 0.2457     |
| AverageReturn        | 1030.4     |
| MinReturn            | 128.06     |
| MaxReturn            | 1133.2     |
| StdReturn            | 195.46     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18771      |
| TotalNSamples        | 2.0893e+06 |
| ExplainedVariance    | 0.026457   |
-------------------------------------
[2018-01-21 13:38:14.841163 UTC] Saving snapshot
[2018-01-21 13:38:14.841408 UTC] Starting iteration 419
[2018-01-21 13:38:14.841560 UTC] Start collecting samples
[2018-01-21 13:38:19.400174 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:19.539866 UTC] Performing policy update
[2018-01-21 13:38:19.540477 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:19.660931 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:21.089847 UTC] Performing line search
[2018-01-21 13:38:21.280436 UTC] Updating baseline
[2018-01-21 13:38:23.186903 UTC] Computing logging information
-------------------------------------
| Iteration            | 419        |
| ExpectedImprovement  | 0.01623    |
| ActualImprovement    | 0.015228   |
| ImprovementRatio     | 0.93825    |
| MeanKL               | 0.0069884  |
| Entropy              | 0.27262    |
| Perplexity           | 1.3134     |
| AveragePolicyStd     | 0.25681    |
| AveragePolicyStd[0]  | 0.26712    |
| AveragePolicyStd[1]  | 0.33219    |
| AveragePolicyStd[2]  | 0.18887    |
| AveragePolicyStd[3]  | 0.26901    |
| AveragePolicyStd[4]  | 0.23847    |
| AveragePolicyStd[5]  | 0.2452     |
| AverageReturn        | 1032.9     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 196.39     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18777      |
| TotalNSamples        | 2.0953e+06 |
| ExplainedVariance    | 0.041453   |
-------------------------------------
[2018-01-21 13:38:23.809723 UTC] Saving snapshot
[2018-01-21 13:38:23.809922 UTC] Starting iteration 420
[2018-01-21 13:38:23.810060 UTC] Start collecting samples
[2018-01-21 13:38:28.317402 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:28.440459 UTC] Performing policy update
[2018-01-21 13:38:28.441319 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:28.563168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:29.992355 UTC] Performing line search
[2018-01-21 13:38:30.184238 UTC] Updating baseline
[2018-01-21 13:38:32.097486 UTC] Computing logging information
-------------------------------------
| Iteration            | 420        |
| ExpectedImprovement  | 0.013849   |
| ActualImprovement    | 0.013345   |
| ImprovementRatio     | 0.96359    |
| MeanKL               | 0.0072831  |
| Entropy              | 0.26652    |
| Perplexity           | 1.3054     |
| AveragePolicyStd     | 0.25652    |
| AveragePolicyStd[0]  | 0.26859    |
| AveragePolicyStd[1]  | 0.32958    |
| AveragePolicyStd[2]  | 0.18793    |
| AveragePolicyStd[3]  | 0.2694     |
| AveragePolicyStd[4]  | 0.23985    |
| AveragePolicyStd[5]  | 0.24374    |
| AverageReturn        | 1034.6     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 196.76     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18784      |
| TotalNSamples        | 2.1023e+06 |
| ExplainedVariance    | 0.007876   |
-------------------------------------
[2018-01-21 13:38:32.770960 UTC] Saving snapshot
[2018-01-21 13:38:32.781887 UTC] Starting iteration 421
[2018-01-21 13:38:32.782121 UTC] Start collecting samples
[2018-01-21 13:38:37.249739 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:37.373767 UTC] Performing policy update
[2018-01-21 13:38:37.374497 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:37.491360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:38.956404 UTC] Performing line search
[2018-01-21 13:38:39.165672 UTC] Updating baseline
[2018-01-21 13:38:41.716838 UTC] Computing logging information
-------------------------------------
| Iteration            | 421        |
| ExpectedImprovement  | 0.017672   |
| ActualImprovement    | 0.015725   |
| ImprovementRatio     | 0.88982    |
| MeanKL               | 0.0076824  |
| Entropy              | 0.27257    |
| Perplexity           | 1.3133     |
| AveragePolicyStd     | 0.25679    |
| AveragePolicyStd[0]  | 0.26937    |
| AveragePolicyStd[1]  | 0.33076    |
| AveragePolicyStd[2]  | 0.18804    |
| AveragePolicyStd[3]  | 0.26794    |
| AveragePolicyStd[4]  | 0.24108    |
| AveragePolicyStd[5]  | 0.24358    |
| AverageReturn        | 1034.8     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 196.8      |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18785      |
| TotalNSamples        | 2.1033e+06 |
| ExplainedVariance    | -0.029604  |
-------------------------------------
[2018-01-21 13:38:42.339206 UTC] Saving snapshot
[2018-01-21 13:38:42.339451 UTC] Starting iteration 422
[2018-01-21 13:38:42.339632 UTC] Start collecting samples
[2018-01-21 13:38:46.864846 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:47.000386 UTC] Performing policy update
[2018-01-21 13:38:47.000983 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:47.121965 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:48.519254 UTC] Performing line search
[2018-01-21 13:38:48.707508 UTC] Updating baseline
[2018-01-21 13:38:50.502231 UTC] Computing logging information
-------------------------------------
| Iteration            | 422        |
| ExpectedImprovement  | 0.016454   |
| ActualImprovement    | 0.015866   |
| ImprovementRatio     | 0.96429    |
| MeanKL               | 0.0073899  |
| Entropy              | 0.26728    |
| Perplexity           | 1.3064     |
| AveragePolicyStd     | 0.2566     |
| AveragePolicyStd[0]  | 0.26944    |
| AveragePolicyStd[1]  | 0.33126    |
| AveragePolicyStd[2]  | 0.18787    |
| AveragePolicyStd[3]  | 0.26686    |
| AveragePolicyStd[4]  | 0.24053    |
| AveragePolicyStd[5]  | 0.24363    |
| AverageReturn        | 1035.5     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 197.22     |
| AverageEpisodeLength | 952.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.78     |
| TotalNEpisodes       | 18792      |
| TotalNSamples        | 2.1103e+06 |
| ExplainedVariance    | -0.0029691 |
-------------------------------------
[2018-01-21 13:38:51.134988 UTC] Saving snapshot
[2018-01-21 13:38:51.135225 UTC] Starting iteration 423
[2018-01-21 13:38:51.135366 UTC] Start collecting samples
[2018-01-21 13:38:55.784404 UTC] Computing input variables for policy optimization
[2018-01-21 13:38:55.924646 UTC] Performing policy update
[2018-01-21 13:38:55.925665 UTC] Computing gradient in Euclidean space
[2018-01-21 13:38:56.046106 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:38:57.454832 UTC] Performing line search
[2018-01-21 13:38:57.643580 UTC] Updating baseline
[2018-01-21 13:38:59.550390 UTC] Computing logging information
-------------------------------------
| Iteration            | 423        |
| ExpectedImprovement  | 0.015587   |
| ActualImprovement    | 0.014389   |
| ImprovementRatio     | 0.9231     |
| MeanKL               | 0.0074271  |
| Entropy              | 0.25196    |
| Perplexity           | 1.2865     |
| AveragePolicyStd     | 0.25591    |
| AveragePolicyStd[0]  | 0.2682     |
| AveragePolicyStd[1]  | 0.3301     |
| AveragePolicyStd[2]  | 0.18768    |
| AveragePolicyStd[3]  | 0.26652    |
| AveragePolicyStd[4]  | 0.23932    |
| AveragePolicyStd[5]  | 0.24364    |
| AverageReturn        | 1044.4     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 178.19     |
| AverageEpisodeLength | 960.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.82     |
| TotalNEpisodes       | 18797      |
| TotalNSamples        | 2.1153e+06 |
| ExplainedVariance    | -0.0033013 |
-------------------------------------
[2018-01-21 13:39:00.241439 UTC] Saving snapshot
[2018-01-21 13:39:00.241699 UTC] Starting iteration 424
[2018-01-21 13:39:00.241917 UTC] Start collecting samples
[2018-01-21 13:39:04.809444 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:04.941207 UTC] Performing policy update
[2018-01-21 13:39:04.941819 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:05.063045 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:06.530078 UTC] Performing line search
[2018-01-21 13:39:06.721134 UTC] Updating baseline
[2018-01-21 13:39:09.517254 UTC] Computing logging information
--------------------------------------
| Iteration            | 424         |
| ExpectedImprovement  | 0.015907    |
| ActualImprovement    | 0.014763    |
| ImprovementRatio     | 0.92805     |
| MeanKL               | 0.0081529   |
| Entropy              | 0.2422      |
| Perplexity           | 1.2741      |
| AveragePolicyStd     | 0.25555     |
| AveragePolicyStd[0]  | 0.26686     |
| AveragePolicyStd[1]  | 0.33075     |
| AveragePolicyStd[2]  | 0.18718     |
| AveragePolicyStd[3]  | 0.26635     |
| AveragePolicyStd[4]  | 0.23927     |
| AveragePolicyStd[5]  | 0.24288     |
| AverageReturn        | 1046        |
| MinReturn            | 128.06      |
| MaxReturn            | 1160.2      |
| StdReturn            | 178.55      |
| AverageEpisodeLength | 960.77      |
| MinEpisodeLength     | 121         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 159.82      |
| TotalNEpisodes       | 18801       |
| TotalNSamples        | 2.1193e+06  |
| ExplainedVariance    | -5.9359e-05 |
--------------------------------------
[2018-01-21 13:39:10.217017 UTC] Saving snapshot
[2018-01-21 13:39:10.217267 UTC] Starting iteration 425
[2018-01-21 13:39:10.217418 UTC] Start collecting samples
[2018-01-21 13:39:14.846739 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:14.976423 UTC] Performing policy update
[2018-01-21 13:39:14.977498 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:15.098797 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:16.540624 UTC] Performing line search
[2018-01-21 13:39:16.732008 UTC] Updating baseline
[2018-01-21 13:39:18.672923 UTC] Computing logging information
-------------------------------------
| Iteration            | 425        |
| ExpectedImprovement  | 0.015094   |
| ActualImprovement    | 0.014274   |
| ImprovementRatio     | 0.9457     |
| MeanKL               | 0.0077964  |
| Entropy              | 0.24469    |
| Perplexity           | 1.2772     |
| AveragePolicyStd     | 0.25571    |
| AveragePolicyStd[0]  | 0.26558    |
| AveragePolicyStd[1]  | 0.33176    |
| AveragePolicyStd[2]  | 0.1871     |
| AveragePolicyStd[3]  | 0.26819    |
| AveragePolicyStd[4]  | 0.23887    |
| AveragePolicyStd[5]  | 0.24274    |
| AverageReturn        | 1047.6     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 179.06     |
| AverageEpisodeLength | 960.77     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.82     |
| TotalNEpisodes       | 18807      |
| TotalNSamples        | 2.1253e+06 |
| ExplainedVariance    | 0.0044142  |
-------------------------------------
[2018-01-21 13:39:19.308447 UTC] Saving snapshot
[2018-01-21 13:39:19.308807 UTC] Starting iteration 426
[2018-01-21 13:39:19.309047 UTC] Start collecting samples
[2018-01-21 13:39:23.920912 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:24.053707 UTC] Performing policy update
[2018-01-21 13:39:24.054391 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:24.173243 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:25.591308 UTC] Performing line search
[2018-01-21 13:39:25.782965 UTC] Updating baseline
[2018-01-21 13:39:27.587167 UTC] Computing logging information
-------------------------------------
| Iteration            | 426        |
| ExpectedImprovement  | 0.01659    |
| ActualImprovement    | 0.015856   |
| ImprovementRatio     | 0.95577    |
| MeanKL               | 0.0072298  |
| Entropy              | 0.23849    |
| Perplexity           | 1.2693     |
| AveragePolicyStd     | 0.25548    |
| AveragePolicyStd[0]  | 0.26496    |
| AveragePolicyStd[1]  | 0.33142    |
| AveragePolicyStd[2]  | 0.18594    |
| AveragePolicyStd[3]  | 0.26769    |
| AveragePolicyStd[4]  | 0.23982    |
| AveragePolicyStd[5]  | 0.24306    |
| AverageReturn        | 1059.6     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 154.83     |
| AverageEpisodeLength | 969.36     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.98     |
| TotalNEpisodes       | 18813      |
| TotalNSamples        | 2.1313e+06 |
| ExplainedVariance    | -0.0028834 |
-------------------------------------
[2018-01-21 13:39:28.289088 UTC] Saving snapshot
[2018-01-21 13:39:28.289391 UTC] Starting iteration 427
[2018-01-21 13:39:28.289617 UTC] Start collecting samples
[2018-01-21 13:39:32.746925 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:32.874395 UTC] Performing policy update
[2018-01-21 13:39:32.875435 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:33.003638 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:34.421003 UTC] Performing line search
[2018-01-21 13:39:34.604120 UTC] Updating baseline
[2018-01-21 13:39:36.857313 UTC] Computing logging information
-------------------------------------
| Iteration            | 427        |
| ExpectedImprovement  | 0.016379   |
| ActualImprovement    | 0.015533   |
| ImprovementRatio     | 0.94832    |
| MeanKL               | 0.0074252  |
| Entropy              | 0.2428     |
| Perplexity           | 1.2748     |
| AveragePolicyStd     | 0.25562    |
| AveragePolicyStd[0]  | 0.2646     |
| AveragePolicyStd[1]  | 0.33097    |
| AveragePolicyStd[2]  | 0.1861     |
| AveragePolicyStd[3]  | 0.2676     |
| AveragePolicyStd[4]  | 0.24053    |
| AveragePolicyStd[5]  | 0.24392    |
| AverageReturn        | 1059.5     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 154.75     |
| AverageEpisodeLength | 969.36     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.98     |
| TotalNEpisodes       | 18817      |
| TotalNSamples        | 2.1353e+06 |
| ExplainedVariance    | 0.0016545  |
-------------------------------------
[2018-01-21 13:39:37.477785 UTC] Saving snapshot
[2018-01-21 13:39:37.478050 UTC] Starting iteration 428
[2018-01-21 13:39:37.478225 UTC] Start collecting samples
[2018-01-21 13:39:42.101535 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:42.227635 UTC] Performing policy update
[2018-01-21 13:39:42.228716 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:42.355131 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:43.798668 UTC] Performing line search
[2018-01-21 13:39:43.998582 UTC] Updating baseline
[2018-01-21 13:39:46.052200 UTC] Computing logging information
-------------------------------------
| Iteration            | 428        |
| ExpectedImprovement  | 0.016119   |
| ActualImprovement    | 0.015234   |
| ImprovementRatio     | 0.94511    |
| MeanKL               | 0.0074699  |
| Entropy              | 0.24178    |
| Perplexity           | 1.2735     |
| AveragePolicyStd     | 0.25566    |
| AveragePolicyStd[0]  | 0.26409    |
| AveragePolicyStd[1]  | 0.33221    |
| AveragePolicyStd[2]  | 0.18557    |
| AveragePolicyStd[3]  | 0.26793    |
| AveragePolicyStd[4]  | 0.23999    |
| AveragePolicyStd[5]  | 0.24417    |
| AverageReturn        | 1061.2     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 155.09     |
| AverageEpisodeLength | 969.36     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.98     |
| TotalNEpisodes       | 18822      |
| TotalNSamples        | 2.1403e+06 |
| ExplainedVariance    | 0.0048596  |
-------------------------------------
[2018-01-21 13:39:46.768219 UTC] Saving snapshot
[2018-01-21 13:39:46.768465 UTC] Starting iteration 429
[2018-01-21 13:39:46.768616 UTC] Start collecting samples
[2018-01-21 13:39:51.588917 UTC] Computing input variables for policy optimization
[2018-01-21 13:39:51.735096 UTC] Performing policy update
[2018-01-21 13:39:51.735775 UTC] Computing gradient in Euclidean space
[2018-01-21 13:39:51.853129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:39:53.247891 UTC] Performing line search
[2018-01-21 13:39:53.454905 UTC] Updating baseline
[2018-01-21 13:39:55.577688 UTC] Computing logging information
-------------------------------------
| Iteration            | 429        |
| ExpectedImprovement  | 0.015427   |
| ActualImprovement    | 0.014716   |
| ImprovementRatio     | 0.95388    |
| MeanKL               | 0.0082447  |
| Entropy              | 0.24622    |
| Perplexity           | 1.2792     |
| AveragePolicyStd     | 0.25577    |
| AveragePolicyStd[0]  | 0.26428    |
| AveragePolicyStd[1]  | 0.33059    |
| AveragePolicyStd[2]  | 0.18577    |
| AveragePolicyStd[3]  | 0.26863    |
| AveragePolicyStd[4]  | 0.23994    |
| AveragePolicyStd[5]  | 0.24544    |
| AverageReturn        | 1061.1     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 155.46     |
| AverageEpisodeLength | 969.36     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.98     |
| TotalNEpisodes       | 18828      |
| TotalNSamples        | 2.1463e+06 |
| ExplainedVariance    | -0.012731  |
-------------------------------------
[2018-01-21 13:39:56.312068 UTC] Saving snapshot
[2018-01-21 13:39:56.312370 UTC] Starting iteration 430
[2018-01-21 13:39:56.312541 UTC] Start collecting samples
[2018-01-21 13:40:01.037342 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:01.157689 UTC] Performing policy update
[2018-01-21 13:40:01.158316 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:01.274217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:02.626619 UTC] Performing line search
[2018-01-21 13:40:02.808829 UTC] Updating baseline
[2018-01-21 13:40:04.867117 UTC] Computing logging information
-------------------------------------
| Iteration            | 430        |
| ExpectedImprovement  | 0.014353   |
| ActualImprovement    | 0.013609   |
| ImprovementRatio     | 0.94817    |
| MeanKL               | 0.0078371  |
| Entropy              | 0.25149    |
| Perplexity           | 1.2859     |
| AveragePolicyStd     | 0.25603    |
| AveragePolicyStd[0]  | 0.26496    |
| AveragePolicyStd[1]  | 0.33029    |
| AveragePolicyStd[2]  | 0.18527    |
| AveragePolicyStd[3]  | 0.2696     |
| AveragePolicyStd[4]  | 0.23952    |
| AveragePolicyStd[5]  | 0.24651    |
| AverageReturn        | 1053.4     |
| MinReturn            | 128.06     |
| MaxReturn            | 1160.2     |
| StdReturn            | 175.72     |
| AverageEpisodeLength | 961.67     |
| MinEpisodeLength     | 121        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.39     |
| TotalNEpisodes       | 18833      |
| TotalNSamples        | 2.1505e+06 |
| ExplainedVariance    | 0.13399    |
-------------------------------------
[2018-01-21 13:40:05.537935 UTC] Saving snapshot
[2018-01-21 13:40:05.543780 UTC] Starting iteration 431
[2018-01-21 13:40:05.543960 UTC] Start collecting samples
[2018-01-21 13:40:10.055767 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:10.186034 UTC] Performing policy update
[2018-01-21 13:40:10.186704 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:10.299226 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:11.663435 UTC] Performing line search
[2018-01-21 13:40:11.859495 UTC] Updating baseline
[2018-01-21 13:40:13.742316 UTC] Computing logging information
-------------------------------------
| Iteration            | 431        |
| ExpectedImprovement  | 0.016667   |
| ActualImprovement    | 0.016016   |
| ImprovementRatio     | 0.96093    |
| MeanKL               | 0.0078816  |
| Entropy              | 0.26223    |
| Perplexity           | 1.2998     |
| AveragePolicyStd     | 0.25643    |
| AveragePolicyStd[0]  | 0.2646     |
| AveragePolicyStd[1]  | 0.33028    |
| AveragePolicyStd[2]  | 0.18602    |
| AveragePolicyStd[3]  | 0.27016    |
| AveragePolicyStd[4]  | 0.24014    |
| AveragePolicyStd[5]  | 0.24738    |
| AverageReturn        | 1071.7     |
| MinReturn            | 233.65     |
| MaxReturn            | 1160.2     |
| StdReturn            | 136.87     |
| AverageEpisodeLength | 976.61     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.81     |
| TotalNEpisodes       | 18838      |
| TotalNSamples        | 2.1555e+06 |
| ExplainedVariance    | 0.14907    |
-------------------------------------
[2018-01-21 13:40:14.444558 UTC] Saving snapshot
[2018-01-21 13:40:14.444851 UTC] Starting iteration 432
[2018-01-21 13:40:14.445053 UTC] Start collecting samples
[2018-01-21 13:40:19.006692 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:19.125067 UTC] Performing policy update
[2018-01-21 13:40:19.125681 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:19.243456 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:20.622602 UTC] Performing line search
[2018-01-21 13:40:20.822419 UTC] Updating baseline
[2018-01-21 13:40:22.926675 UTC] Computing logging information
-------------------------------------
| Iteration            | 432        |
| ExpectedImprovement  | 0.01932    |
| ActualImprovement    | 0.017654   |
| ImprovementRatio     | 0.91378    |
| MeanKL               | 0.0069009  |
| Entropy              | 0.26055    |
| Perplexity           | 1.2976     |
| AveragePolicyStd     | 0.2564     |
| AveragePolicyStd[0]  | 0.2654     |
| AveragePolicyStd[1]  | 0.33067    |
| AveragePolicyStd[2]  | 0.18567    |
| AveragePolicyStd[3]  | 0.26969    |
| AveragePolicyStd[4]  | 0.23985    |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1088.1     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.91      |
| AverageEpisodeLength | 989.93     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.079     |
| TotalNEpisodes       | 18842      |
| TotalNSamples        | 2.1594e+06 |
| ExplainedVariance    | 0.13293    |
-------------------------------------
[2018-01-21 13:40:23.594974 UTC] Saving snapshot
[2018-01-21 13:40:23.595222 UTC] Starting iteration 433
[2018-01-21 13:40:23.595402 UTC] Start collecting samples
[2018-01-21 13:40:28.062660 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:28.196728 UTC] Performing policy update
[2018-01-21 13:40:28.197336 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:28.316998 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:29.767683 UTC] Performing line search
[2018-01-21 13:40:29.963890 UTC] Updating baseline
[2018-01-21 13:40:31.872798 UTC] Computing logging information
-------------------------------------
| Iteration            | 433        |
| ExpectedImprovement  | 0.016311   |
| ActualImprovement    | 0.014884   |
| ImprovementRatio     | 0.91249    |
| MeanKL               | 0.0071368  |
| Entropy              | 0.25234    |
| Perplexity           | 1.287      |
| AveragePolicyStd     | 0.25603    |
| AveragePolicyStd[0]  | 0.26558    |
| AveragePolicyStd[1]  | 0.32965    |
| AveragePolicyStd[2]  | 0.18553    |
| AveragePolicyStd[3]  | 0.26991    |
| AveragePolicyStd[4]  | 0.24034    |
| AveragePolicyStd[5]  | 0.24516    |
| AverageReturn        | 1089.9     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.889     |
| AverageEpisodeLength | 989.93     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.079     |
| TotalNEpisodes       | 18849      |
| TotalNSamples        | 2.1664e+06 |
| ExplainedVariance    | -0.027737  |
-------------------------------------
[2018-01-21 13:40:32.555105 UTC] Saving snapshot
[2018-01-21 13:40:32.555286 UTC] Starting iteration 434
[2018-01-21 13:40:32.555389 UTC] Start collecting samples
[2018-01-21 13:40:37.056908 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:37.176832 UTC] Performing policy update
[2018-01-21 13:40:37.177465 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:37.295506 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:38.675469 UTC] Performing line search
[2018-01-21 13:40:38.860136 UTC] Updating baseline
[2018-01-21 13:40:42.693451 UTC] Computing logging information
-------------------------------------
| Iteration            | 434        |
| ExpectedImprovement  | 0.014596   |
| ActualImprovement    | 0.013833   |
| ImprovementRatio     | 0.94773    |
| MeanKL               | 0.007761   |
| Entropy              | 0.24383    |
| Perplexity           | 1.2761     |
| AveragePolicyStd     | 0.25565    |
| AveragePolicyStd[0]  | 0.26367    |
| AveragePolicyStd[1]  | 0.32917    |
| AveragePolicyStd[2]  | 0.18544    |
| AveragePolicyStd[3]  | 0.27026    |
| AveragePolicyStd[4]  | 0.24027    |
| AveragePolicyStd[5]  | 0.24509    |
| AverageReturn        | 1089.9     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 93.005     |
| AverageEpisodeLength | 989.93     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.079     |
| TotalNEpisodes       | 18852      |
| TotalNSamples        | 2.1694e+06 |
| ExplainedVariance    | 4.2887e-06 |
-------------------------------------
[2018-01-21 13:40:43.384910 UTC] Saving snapshot
[2018-01-21 13:40:43.385157 UTC] Starting iteration 435
[2018-01-21 13:40:43.385372 UTC] Start collecting samples
[2018-01-21 13:40:48.036802 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:48.213996 UTC] Performing policy update
[2018-01-21 13:40:48.214633 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:48.348064 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:49.866571 UTC] Performing line search
[2018-01-21 13:40:50.065233 UTC] Updating baseline
[2018-01-21 13:40:52.095969 UTC] Computing logging information
-------------------------------------
| Iteration            | 435        |
| ExpectedImprovement  | 0.014817   |
| ActualImprovement    | 0.014138   |
| ImprovementRatio     | 0.95417    |
| MeanKL               | 0.0081242  |
| Entropy              | 0.25141    |
| Perplexity           | 1.2858     |
| AveragePolicyStd     | 0.25597    |
| AveragePolicyStd[0]  | 0.26396    |
| AveragePolicyStd[1]  | 0.32914    |
| AveragePolicyStd[2]  | 0.18535    |
| AveragePolicyStd[3]  | 0.27056    |
| AveragePolicyStd[4]  | 0.24122    |
| AveragePolicyStd[5]  | 0.24557    |
| AverageReturn        | 1093.3     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.348     |
| AverageEpisodeLength | 991.19     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 77.21      |
| TotalNEpisodes       | 18858      |
| TotalNSamples        | 2.1754e+06 |
| ExplainedVariance    | -0.024728  |
-------------------------------------
[2018-01-21 13:40:52.773957 UTC] Saving snapshot
[2018-01-21 13:40:52.774163 UTC] Starting iteration 436
[2018-01-21 13:40:52.774347 UTC] Start collecting samples
[2018-01-21 13:40:57.675216 UTC] Computing input variables for policy optimization
[2018-01-21 13:40:57.799100 UTC] Performing policy update
[2018-01-21 13:40:57.800401 UTC] Computing gradient in Euclidean space
[2018-01-21 13:40:57.927305 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:40:59.323946 UTC] Performing line search
[2018-01-21 13:40:59.515219 UTC] Updating baseline
[2018-01-21 13:41:02.005749 UTC] Computing logging information
-------------------------------------
| Iteration            | 436        |
| ExpectedImprovement  | 0.017051   |
| ActualImprovement    | 0.01659    |
| ImprovementRatio     | 0.97296    |
| MeanKL               | 0.0069581  |
| Entropy              | 0.24181    |
| Perplexity           | 1.2736     |
| AveragePolicyStd     | 0.25556    |
| AveragePolicyStd[0]  | 0.26388    |
| AveragePolicyStd[1]  | 0.32773    |
| AveragePolicyStd[2]  | 0.18469    |
| AveragePolicyStd[3]  | 0.27084    |
| AveragePolicyStd[4]  | 0.24083    |
| AveragePolicyStd[5]  | 0.24536    |
| AverageReturn        | 1094.9     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.447     |
| AverageEpisodeLength | 991.19     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 77.21      |
| TotalNEpisodes       | 18865      |
| TotalNSamples        | 2.1824e+06 |
| ExplainedVariance    | 0.085539   |
-------------------------------------
[2018-01-21 13:41:02.645200 UTC] Saving snapshot
[2018-01-21 13:41:02.645450 UTC] Starting iteration 437
[2018-01-21 13:41:02.645633 UTC] Start collecting samples
[2018-01-21 13:41:07.074423 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:07.199331 UTC] Performing policy update
[2018-01-21 13:41:07.200286 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:07.317891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:08.811419 UTC] Performing line search
[2018-01-21 13:41:09.004567 UTC] Updating baseline
[2018-01-21 13:41:12.039068 UTC] Computing logging information
-------------------------------------
| Iteration            | 437        |
| ExpectedImprovement  | 0.018266   |
| ActualImprovement    | 0.017248   |
| ImprovementRatio     | 0.94424    |
| MeanKL               | 0.0069839  |
| Entropy              | 0.23776    |
| Perplexity           | 1.2684     |
| AveragePolicyStd     | 0.25535    |
| AveragePolicyStd[0]  | 0.26285    |
| AveragePolicyStd[1]  | 0.32767    |
| AveragePolicyStd[2]  | 0.18486    |
| AveragePolicyStd[3]  | 0.26982    |
| AveragePolicyStd[4]  | 0.24005    |
| AveragePolicyStd[5]  | 0.24688    |
| AverageReturn        | 1094.8     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.422     |
| AverageEpisodeLength | 991.19     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 77.21      |
| TotalNEpisodes       | 18866      |
| TotalNSamples        | 2.1834e+06 |
| ExplainedVariance    | -0.11001   |
-------------------------------------
[2018-01-21 13:41:12.717602 UTC] Saving snapshot
[2018-01-21 13:41:12.717784 UTC] Starting iteration 438
[2018-01-21 13:41:12.717888 UTC] Start collecting samples
[2018-01-21 13:41:17.333858 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:17.455480 UTC] Performing policy update
[2018-01-21 13:41:17.456076 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:17.573084 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:19.002040 UTC] Performing line search
[2018-01-21 13:41:19.183165 UTC] Updating baseline
[2018-01-21 13:41:21.316037 UTC] Computing logging information
-------------------------------------
| Iteration            | 438        |
| ExpectedImprovement  | 0.016436   |
| ActualImprovement    | 0.015465   |
| ImprovementRatio     | 0.94089    |
| MeanKL               | 0.0070518  |
| Entropy              | 0.23067    |
| Perplexity           | 1.2594     |
| AveragePolicyStd     | 0.25501    |
| AveragePolicyStd[0]  | 0.26191    |
| AveragePolicyStd[1]  | 0.32678    |
| AveragePolicyStd[2]  | 0.18486    |
| AveragePolicyStd[3]  | 0.26907    |
| AveragePolicyStd[4]  | 0.24021    |
| AveragePolicyStd[5]  | 0.24721    |
| AverageReturn        | 1094.9     |
| MinReturn            | 241.21     |
| MaxReturn            | 1160.2     |
| StdReturn            | 92.883     |
| AverageEpisodeLength | 990.82     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 77.256     |
| TotalNEpisodes       | 18873      |
| TotalNSamples        | 2.1904e+06 |
| ExplainedVariance    | 0.096012   |
-------------------------------------
[2018-01-21 13:41:22.026509 UTC] Saving snapshot
[2018-01-21 13:41:22.026739 UTC] Starting iteration 439
[2018-01-21 13:41:22.026889 UTC] Start collecting samples
[2018-01-21 13:41:26.373114 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:26.501637 UTC] Performing policy update
[2018-01-21 13:41:26.502466 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:26.624198 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:28.059672 UTC] Performing line search
[2018-01-21 13:41:28.253733 UTC] Updating baseline
[2018-01-21 13:41:30.424791 UTC] Computing logging information
-------------------------------------
| Iteration            | 439        |
| ExpectedImprovement  | 0.018079   |
| ActualImprovement    | 0.016792   |
| ImprovementRatio     | 0.92877    |
| MeanKL               | 0.0071954  |
| Entropy              | 0.22935    |
| Perplexity           | 1.2578     |
| AveragePolicyStd     | 0.25495    |
| AveragePolicyStd[0]  | 0.26111    |
| AveragePolicyStd[1]  | 0.32789    |
| AveragePolicyStd[2]  | 0.18515    |
| AveragePolicyStd[3]  | 0.26741    |
| AveragePolicyStd[4]  | 0.23992    |
| AveragePolicyStd[5]  | 0.24823    |
| AverageReturn        | 1096.8     |
| MinReturn            | 241.21     |
| MaxReturn            | 1159.8     |
| StdReturn            | 92.996     |
| AverageEpisodeLength | 990.82     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 77.256     |
| TotalNEpisodes       | 18878      |
| TotalNSamples        | 2.1954e+06 |
| ExplainedVariance    | -0.024286  |
-------------------------------------
[2018-01-21 13:41:31.124171 UTC] Saving snapshot
[2018-01-21 13:41:31.124414 UTC] Starting iteration 440
[2018-01-21 13:41:31.124597 UTC] Start collecting samples
[2018-01-21 13:41:35.864671 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:36.008831 UTC] Performing policy update
[2018-01-21 13:41:36.009509 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:36.124281 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:37.550663 UTC] Performing line search
[2018-01-21 13:41:37.740331 UTC] Updating baseline
[2018-01-21 13:41:40.423924 UTC] Computing logging information
-------------------------------------
| Iteration            | 440        |
| ExpectedImprovement  | 0.018376   |
| ActualImprovement    | 0.0172     |
| ImprovementRatio     | 0.93602    |
| MeanKL               | 0.0069749  |
| Entropy              | 0.22264    |
| Perplexity           | 1.2494     |
| AveragePolicyStd     | 0.2547     |
| AveragePolicyStd[0]  | 0.26059    |
| AveragePolicyStd[1]  | 0.3277     |
| AveragePolicyStd[2]  | 0.18447    |
| AveragePolicyStd[3]  | 0.26734    |
| AveragePolicyStd[4]  | 0.24042    |
| AveragePolicyStd[5]  | 0.24768    |
| AverageReturn        | 1089.1     |
| MinReturn            | 241.21     |
| MaxReturn            | 1159.8     |
| StdReturn            | 125.89     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18883      |
| TotalNSamples        | 2.1996e+06 |
| ExplainedVariance    | -0.02042   |
-------------------------------------
[2018-01-21 13:41:41.129690 UTC] Saving snapshot
[2018-01-21 13:41:41.137975 UTC] Starting iteration 441
[2018-01-21 13:41:41.138171 UTC] Start collecting samples
[2018-01-21 13:41:45.602358 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:45.727604 UTC] Performing policy update
[2018-01-21 13:41:45.728271 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:45.849143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:47.242028 UTC] Performing line search
[2018-01-21 13:41:47.433039 UTC] Updating baseline
[2018-01-21 13:41:50.318109 UTC] Computing logging information
--------------------------------------
| Iteration            | 441         |
| ExpectedImprovement  | 0.016342    |
| ActualImprovement    | 0.015086    |
| ImprovementRatio     | 0.92316     |
| MeanKL               | 0.0070002   |
| Entropy              | 0.22997     |
| Perplexity           | 1.2586      |
| AveragePolicyStd     | 0.25505     |
| AveragePolicyStd[0]  | 0.26073     |
| AveragePolicyStd[1]  | 0.32844     |
| AveragePolicyStd[2]  | 0.18431     |
| AveragePolicyStd[3]  | 0.26799     |
| AveragePolicyStd[4]  | 0.24054     |
| AveragePolicyStd[5]  | 0.24828     |
| AverageReturn        | 1093.3      |
| MinReturn            | 241.21      |
| MaxReturn            | 1165.1      |
| StdReturn            | 125.44      |
| AverageEpisodeLength | 983.04      |
| MinEpisodeLength     | 222         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 108.71      |
| TotalNEpisodes       | 18890       |
| TotalNSamples        | 2.2066e+06  |
| ExplainedVariance    | -0.00046165 |
--------------------------------------
[2018-01-21 13:41:51.034036 UTC] Saving snapshot
[2018-01-21 13:41:51.034273 UTC] Starting iteration 442
[2018-01-21 13:41:51.034496 UTC] Start collecting samples
[2018-01-21 13:41:55.705326 UTC] Computing input variables for policy optimization
[2018-01-21 13:41:55.852553 UTC] Performing policy update
[2018-01-21 13:41:55.853165 UTC] Computing gradient in Euclidean space
[2018-01-21 13:41:55.983278 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:41:57.379535 UTC] Performing line search
[2018-01-21 13:41:57.562089 UTC] Updating baseline
[2018-01-21 13:41:59.691669 UTC] Computing logging information
-------------------------------------
| Iteration            | 442        |
| ExpectedImprovement  | 0.016586   |
| ActualImprovement    | 0.016009   |
| ImprovementRatio     | 0.96524    |
| MeanKL               | 0.0073488  |
| Entropy              | 0.21795    |
| Perplexity           | 1.2435     |
| AveragePolicyStd     | 0.25462    |
| AveragePolicyStd[0]  | 0.2607     |
| AveragePolicyStd[1]  | 0.32865    |
| AveragePolicyStd[2]  | 0.18303    |
| AveragePolicyStd[3]  | 0.26661    |
| AveragePolicyStd[4]  | 0.24006    |
| AveragePolicyStd[5]  | 0.24868    |
| AverageReturn        | 1095.5     |
| MinReturn            | 241.21     |
| MaxReturn            | 1169.6     |
| StdReturn            | 125.78     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18894      |
| TotalNSamples        | 2.2106e+06 |
| ExplainedVariance    | 0.0011912  |
-------------------------------------
[2018-01-21 13:42:00.344327 UTC] Saving snapshot
[2018-01-21 13:42:00.344513 UTC] Starting iteration 443
[2018-01-21 13:42:00.344619 UTC] Start collecting samples
[2018-01-21 13:42:04.857542 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:04.997192 UTC] Performing policy update
[2018-01-21 13:42:04.997966 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:05.111443 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:06.487541 UTC] Performing line search
[2018-01-21 13:42:06.672205 UTC] Updating baseline
[2018-01-21 13:42:09.191978 UTC] Computing logging information
-------------------------------------
| Iteration            | 443        |
| ExpectedImprovement  | 0.015275   |
| ActualImprovement    | 0.015322   |
| ImprovementRatio     | 1.0031     |
| MeanKL               | 0.0081835  |
| Entropy              | 0.21779    |
| Perplexity           | 1.2433     |
| AveragePolicyStd     | 0.25456    |
| AveragePolicyStd[0]  | 0.26006    |
| AveragePolicyStd[1]  | 0.32809    |
| AveragePolicyStd[2]  | 0.18348    |
| AveragePolicyStd[3]  | 0.26679    |
| AveragePolicyStd[4]  | 0.24005    |
| AveragePolicyStd[5]  | 0.24891    |
| AverageReturn        | 1098.5     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 126.5      |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18899      |
| TotalNSamples        | 2.2156e+06 |
| ExplainedVariance    | -0.0037405 |
-------------------------------------
[2018-01-21 13:42:09.903321 UTC] Saving snapshot
[2018-01-21 13:42:09.903601 UTC] Starting iteration 444
[2018-01-21 13:42:09.903789 UTC] Start collecting samples
[2018-01-21 13:42:14.319304 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:14.474146 UTC] Performing policy update
[2018-01-21 13:42:14.474822 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:14.593464 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:16.022689 UTC] Performing line search
[2018-01-21 13:42:16.236737 UTC] Updating baseline
[2018-01-21 13:42:18.348396 UTC] Computing logging information
-------------------------------------
| Iteration            | 444        |
| ExpectedImprovement  | 0.015816   |
| ActualImprovement    | 0.014838   |
| ImprovementRatio     | 0.93816    |
| MeanKL               | 0.0072186  |
| Entropy              | 0.22371    |
| Perplexity           | 1.2507     |
| AveragePolicyStd     | 0.25474    |
| AveragePolicyStd[0]  | 0.26122    |
| AveragePolicyStd[1]  | 0.32739    |
| AveragePolicyStd[2]  | 0.1846     |
| AveragePolicyStd[3]  | 0.26724    |
| AveragePolicyStd[4]  | 0.23891    |
| AveragePolicyStd[5]  | 0.24905    |
| AverageReturn        | 1100.4     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 127.02     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18905      |
| TotalNSamples        | 2.2216e+06 |
| ExplainedVariance    | 0.0065645  |
-------------------------------------
[2018-01-21 13:42:19.021358 UTC] Saving snapshot
[2018-01-21 13:42:19.021588 UTC] Starting iteration 445
[2018-01-21 13:42:19.021733 UTC] Start collecting samples
[2018-01-21 13:42:23.323827 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:23.440639 UTC] Performing policy update
[2018-01-21 13:42:23.441229 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:23.557494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:24.953469 UTC] Performing line search
[2018-01-21 13:42:25.139789 UTC] Updating baseline
[2018-01-21 13:42:26.997385 UTC] Computing logging information
-------------------------------------
| Iteration            | 445        |
| ExpectedImprovement  | 0.018561   |
| ActualImprovement    | 0.017396   |
| ImprovementRatio     | 0.93722    |
| MeanKL               | 0.0074345  |
| Entropy              | 0.2225     |
| Perplexity           | 1.2492     |
| AveragePolicyStd     | 0.25465    |
| AveragePolicyStd[0]  | 0.26104    |
| AveragePolicyStd[1]  | 0.32802    |
| AveragePolicyStd[2]  | 0.18539    |
| AveragePolicyStd[3]  | 0.26616    |
| AveragePolicyStd[4]  | 0.23922    |
| AveragePolicyStd[5]  | 0.24807    |
| AverageReturn        | 1101.5     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 127.32     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18909      |
| TotalNSamples        | 2.2256e+06 |
| ExplainedVariance    | 0.037626   |
-------------------------------------
[2018-01-21 13:42:27.664266 UTC] Saving snapshot
[2018-01-21 13:42:27.664465 UTC] Starting iteration 446
[2018-01-21 13:42:27.664582 UTC] Start collecting samples
[2018-01-21 13:42:32.257107 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:32.384397 UTC] Performing policy update
[2018-01-21 13:42:32.385018 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:32.500728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:34.022967 UTC] Performing line search
[2018-01-21 13:42:34.246206 UTC] Updating baseline
[2018-01-21 13:42:36.049649 UTC] Computing logging information
-------------------------------------
| Iteration            | 446        |
| ExpectedImprovement  | 0.016565   |
| ActualImprovement    | 0.015507   |
| ImprovementRatio     | 0.93614    |
| MeanKL               | 0.0076591  |
| Entropy              | 0.22435    |
| Perplexity           | 1.2515     |
| AveragePolicyStd     | 0.25473    |
| AveragePolicyStd[0]  | 0.26099    |
| AveragePolicyStd[1]  | 0.32756    |
| AveragePolicyStd[2]  | 0.1854     |
| AveragePolicyStd[3]  | 0.26735    |
| AveragePolicyStd[4]  | 0.23838    |
| AveragePolicyStd[5]  | 0.24868    |
| AverageReturn        | 1103       |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 127.73     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18913      |
| TotalNSamples        | 2.2296e+06 |
| ExplainedVariance    | -0.0092533 |
-------------------------------------
[2018-01-21 13:42:36.718507 UTC] Saving snapshot
[2018-01-21 13:42:36.718801 UTC] Starting iteration 447
[2018-01-21 13:42:36.719024 UTC] Start collecting samples
[2018-01-21 13:42:41.375191 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:41.538233 UTC] Performing policy update
[2018-01-21 13:42:41.538850 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:41.659096 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:43.050898 UTC] Performing line search
[2018-01-21 13:42:43.242347 UTC] Updating baseline
[2018-01-21 13:42:45.038953 UTC] Computing logging information
-------------------------------------
| Iteration            | 447        |
| ExpectedImprovement  | 0.016084   |
| ActualImprovement    | 0.01512    |
| ImprovementRatio     | 0.94011    |
| MeanKL               | 0.0071262  |
| Entropy              | 0.22622    |
| Perplexity           | 1.2539     |
| AveragePolicyStd     | 0.25481    |
| AveragePolicyStd[0]  | 0.26129    |
| AveragePolicyStd[1]  | 0.32759    |
| AveragePolicyStd[2]  | 0.18537    |
| AveragePolicyStd[3]  | 0.2676     |
| AveragePolicyStd[4]  | 0.23824    |
| AveragePolicyStd[5]  | 0.2488     |
| AverageReturn        | 1106.6     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 128.21     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18920      |
| TotalNSamples        | 2.2366e+06 |
| ExplainedVariance    | 0.014262   |
-------------------------------------
[2018-01-21 13:42:45.701939 UTC] Saving snapshot
[2018-01-21 13:42:45.702132 UTC] Starting iteration 448
[2018-01-21 13:42:45.702250 UTC] Start collecting samples
[2018-01-21 13:42:50.015452 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:50.154365 UTC] Performing policy update
[2018-01-21 13:42:50.155131 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:50.281046 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:42:51.770520 UTC] Performing line search
[2018-01-21 13:42:51.968574 UTC] Updating baseline
[2018-01-21 13:42:54.002464 UTC] Computing logging information
-------------------------------------
| Iteration            | 448        |
| ExpectedImprovement  | 0.015369   |
| ActualImprovement    | 0.01442    |
| ImprovementRatio     | 0.93825    |
| MeanKL               | 0.0078353  |
| Entropy              | 0.21639    |
| Perplexity           | 1.2416     |
| AveragePolicyStd     | 0.25433    |
| AveragePolicyStd[0]  | 0.26106    |
| AveragePolicyStd[1]  | 0.3256     |
| AveragePolicyStd[2]  | 0.18519    |
| AveragePolicyStd[3]  | 0.26706    |
| AveragePolicyStd[4]  | 0.23764    |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 1107.9     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 128.35     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18923      |
| TotalNSamples        | 2.2396e+06 |
| ExplainedVariance    | 0.023339   |
-------------------------------------
[2018-01-21 13:42:54.651591 UTC] Saving snapshot
[2018-01-21 13:42:54.651800 UTC] Starting iteration 449
[2018-01-21 13:42:54.651974 UTC] Start collecting samples
[2018-01-21 13:42:59.465676 UTC] Computing input variables for policy optimization
[2018-01-21 13:42:59.585894 UTC] Performing policy update
[2018-01-21 13:42:59.586539 UTC] Computing gradient in Euclidean space
[2018-01-21 13:42:59.706784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:01.177046 UTC] Performing line search
[2018-01-21 13:43:01.377562 UTC] Updating baseline
[2018-01-21 13:43:03.569100 UTC] Computing logging information
-------------------------------------
| Iteration            | 449        |
| ExpectedImprovement  | 0.017849   |
| ActualImprovement    | 0.016723   |
| ImprovementRatio     | 0.93694    |
| MeanKL               | 0.0083051  |
| Entropy              | 0.22952    |
| Perplexity           | 1.258      |
| AveragePolicyStd     | 0.25486    |
| AveragePolicyStd[0]  | 0.2605     |
| AveragePolicyStd[1]  | 0.32676    |
| AveragePolicyStd[2]  | 0.18622    |
| AveragePolicyStd[3]  | 0.26761    |
| AveragePolicyStd[4]  | 0.23838    |
| AveragePolicyStd[5]  | 0.2497     |
| AverageReturn        | 1110.2     |
| MinReturn            | 241.21     |
| MaxReturn            | 1182.7     |
| StdReturn            | 128.09     |
| AverageEpisodeLength | 983.04     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.71     |
| TotalNEpisodes       | 18929      |
| TotalNSamples        | 2.2456e+06 |
| ExplainedVariance    | 0.012657   |
-------------------------------------
[2018-01-21 13:43:04.214030 UTC] Saving snapshot
[2018-01-21 13:43:04.214276 UTC] Starting iteration 450
[2018-01-21 13:43:04.214448 UTC] Start collecting samples
[2018-01-21 13:43:08.622983 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:08.749316 UTC] Performing policy update
[2018-01-21 13:43:08.749906 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:08.871557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:10.277840 UTC] Performing line search
[2018-01-21 13:43:10.482013 UTC] Updating baseline
[2018-01-21 13:43:12.677143 UTC] Computing logging information
-------------------------------------
| Iteration            | 450        |
| ExpectedImprovement  | 0.014398   |
| ActualImprovement    | 0.013567   |
| ImprovementRatio     | 0.94226    |
| MeanKL               | 0.0072536  |
| Entropy              | 0.22947    |
| Perplexity           | 1.2579     |
| AveragePolicyStd     | 0.25492    |
| AveragePolicyStd[0]  | 0.26082    |
| AveragePolicyStd[1]  | 0.32769    |
| AveragePolicyStd[2]  | 0.18561    |
| AveragePolicyStd[3]  | 0.26654    |
| AveragePolicyStd[4]  | 0.23895    |
| AveragePolicyStd[5]  | 0.24989    |
| AverageReturn        | 1120.9     |
| MinReturn            | 246.09     |
| MaxReturn            | 1184.7     |
| StdReturn            | 94.033     |
| AverageEpisodeLength | 990.73     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.141     |
| TotalNEpisodes       | 18933      |
| TotalNSamples        | 2.2496e+06 |
| ExplainedVariance    | 0.0028385  |
-------------------------------------
[2018-01-21 13:43:13.329206 UTC] Saving snapshot
[2018-01-21 13:43:13.338769 UTC] Starting iteration 451
[2018-01-21 13:43:13.339001 UTC] Start collecting samples
[2018-01-21 13:43:17.794610 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:17.934810 UTC] Performing policy update
[2018-01-21 13:43:17.935411 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:18.060515 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:19.486716 UTC] Performing line search
[2018-01-21 13:43:19.668996 UTC] Updating baseline
[2018-01-21 13:43:21.832462 UTC] Computing logging information
-------------------------------------
| Iteration            | 451        |
| ExpectedImprovement  | 0.017865   |
| ActualImprovement    | 0.016943   |
| ImprovementRatio     | 0.9484     |
| MeanKL               | 0.0069115  |
| Entropy              | 0.22956    |
| Perplexity           | 1.258      |
| AveragePolicyStd     | 0.25487    |
| AveragePolicyStd[0]  | 0.26079    |
| AveragePolicyStd[1]  | 0.32708    |
| AveragePolicyStd[2]  | 0.18617    |
| AveragePolicyStd[3]  | 0.26645    |
| AveragePolicyStd[4]  | 0.23881    |
| AveragePolicyStd[5]  | 0.24989    |
| AverageReturn        | 1122.8     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 94.294     |
| AverageEpisodeLength | 990.73     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 78.141     |
| TotalNEpisodes       | 18939      |
| TotalNSamples        | 2.2556e+06 |
| ExplainedVariance    | 0.01172    |
-------------------------------------
[2018-01-21 13:43:22.580084 UTC] Saving snapshot
[2018-01-21 13:43:22.580561 UTC] Starting iteration 452
[2018-01-21 13:43:22.580924 UTC] Start collecting samples
[2018-01-21 13:43:27.270460 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:27.406356 UTC] Performing policy update
[2018-01-21 13:43:27.407763 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:27.531385 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:28.991243 UTC] Performing line search
[2018-01-21 13:43:29.197569 UTC] Updating baseline
[2018-01-21 13:43:31.878986 UTC] Computing logging information
-------------------------------------
| Iteration            | 452        |
| ExpectedImprovement  | 0.015541   |
| ActualImprovement    | 0.014727   |
| ImprovementRatio     | 0.94764    |
| MeanKL               | 0.0068584  |
| Entropy              | 0.22831    |
| Perplexity           | 1.2565     |
| AveragePolicyStd     | 0.25475    |
| AveragePolicyStd[0]  | 0.26279    |
| AveragePolicyStd[1]  | 0.32625    |
| AveragePolicyStd[2]  | 0.187      |
| AveragePolicyStd[3]  | 0.2658     |
| AveragePolicyStd[4]  | 0.23781    |
| AveragePolicyStd[5]  | 0.24885    |
| AverageReturn        | 1121.7     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 104.37     |
| AverageEpisodeLength | 987.7      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.393     |
| TotalNEpisodes       | 18946      |
| TotalNSamples        | 2.2622e+06 |
| ExplainedVariance    | 0.036592   |
-------------------------------------
[2018-01-21 13:43:32.551073 UTC] Saving snapshot
[2018-01-21 13:43:32.551304 UTC] Starting iteration 453
[2018-01-21 13:43:32.551513 UTC] Start collecting samples
[2018-01-21 13:43:36.905312 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:37.050108 UTC] Performing policy update
[2018-01-21 13:43:37.050941 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:37.173548 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:38.612196 UTC] Performing line search
[2018-01-21 13:43:38.800461 UTC] Updating baseline
[2018-01-21 13:43:40.766668 UTC] Computing logging information
-------------------------------------
| Iteration            | 453        |
| ExpectedImprovement  | 0.0163     |
| ActualImprovement    | 0.015531   |
| ImprovementRatio     | 0.95286    |
| MeanKL               | 0.0070358  |
| Entropy              | 0.2116     |
| Perplexity           | 1.2357     |
| AveragePolicyStd     | 0.25394    |
| AveragePolicyStd[0]  | 0.2609     |
| AveragePolicyStd[1]  | 0.32333    |
| AveragePolicyStd[2]  | 0.18674    |
| AveragePolicyStd[3]  | 0.26573    |
| AveragePolicyStd[4]  | 0.23817    |
| AveragePolicyStd[5]  | 0.24877    |
| AverageReturn        | 1122.3     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 104.42     |
| AverageEpisodeLength | 987.7      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.393     |
| TotalNEpisodes       | 18949      |
| TotalNSamples        | 2.2652e+06 |
| ExplainedVariance    | 0.15346    |
-------------------------------------
[2018-01-21 13:43:41.408489 UTC] Saving snapshot
[2018-01-21 13:43:41.408819 UTC] Starting iteration 454
[2018-01-21 13:43:41.408978 UTC] Start collecting samples
[2018-01-21 13:43:46.037947 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:46.159324 UTC] Performing policy update
[2018-01-21 13:43:46.159913 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:46.277766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:47.679050 UTC] Performing line search
[2018-01-21 13:43:47.873519 UTC] Updating baseline
[2018-01-21 13:43:49.558564 UTC] Computing logging information
-------------------------------------
| Iteration            | 454        |
| ExpectedImprovement  | 0.014583   |
| ActualImprovement    | 0.014112   |
| ImprovementRatio     | 0.96774    |
| MeanKL               | 0.0077181  |
| Entropy              | 0.20926    |
| Perplexity           | 1.2328     |
| AveragePolicyStd     | 0.25381    |
| AveragePolicyStd[0]  | 0.26132    |
| AveragePolicyStd[1]  | 0.32189    |
| AveragePolicyStd[2]  | 0.18636    |
| AveragePolicyStd[3]  | 0.26583    |
| AveragePolicyStd[4]  | 0.23794    |
| AveragePolicyStd[5]  | 0.24955    |
| AverageReturn        | 1117.2     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 129.14     |
| AverageEpisodeLength | 981.13     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.39     |
| TotalNEpisodes       | 18955      |
| TotalNSamples        | 2.2705e+06 |
| ExplainedVariance    | -0.023364  |
-------------------------------------
[2018-01-21 13:43:50.284425 UTC] Saving snapshot
[2018-01-21 13:43:50.284712 UTC] Starting iteration 455
[2018-01-21 13:43:50.284940 UTC] Start collecting samples
[2018-01-21 13:43:54.947097 UTC] Computing input variables for policy optimization
[2018-01-21 13:43:55.081382 UTC] Performing policy update
[2018-01-21 13:43:55.081997 UTC] Computing gradient in Euclidean space
[2018-01-21 13:43:55.204359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:43:56.664608 UTC] Performing line search
[2018-01-21 13:43:56.848975 UTC] Updating baseline
[2018-01-21 13:43:58.736435 UTC] Computing logging information
-------------------------------------
| Iteration            | 455        |
| ExpectedImprovement  | 0.015724   |
| ActualImprovement    | 0.014984   |
| ImprovementRatio     | 0.95295    |
| MeanKL               | 0.0075569  |
| Entropy              | 0.20526    |
| Perplexity           | 1.2278     |
| AveragePolicyStd     | 0.25361    |
| AveragePolicyStd[0]  | 0.26195    |
| AveragePolicyStd[1]  | 0.32126    |
| AveragePolicyStd[2]  | 0.18642    |
| AveragePolicyStd[3]  | 0.26431    |
| AveragePolicyStd[4]  | 0.23812    |
| AveragePolicyStd[5]  | 0.2496     |
| AverageReturn        | 1119.4     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 129.5      |
| AverageEpisodeLength | 981.13     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.39     |
| TotalNEpisodes       | 18961      |
| TotalNSamples        | 2.2765e+06 |
| ExplainedVariance    | 0.0059612  |
-------------------------------------
[2018-01-21 13:43:59.396638 UTC] Saving snapshot
[2018-01-21 13:43:59.396879 UTC] Starting iteration 456
[2018-01-21 13:43:59.397029 UTC] Start collecting samples
[2018-01-21 13:44:03.980565 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:04.112639 UTC] Performing policy update
[2018-01-21 13:44:04.113650 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:04.237044 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:05.668864 UTC] Performing line search
[2018-01-21 13:44:05.869244 UTC] Updating baseline
[2018-01-21 13:44:08.254372 UTC] Computing logging information
-------------------------------------
| Iteration            | 456        |
| ExpectedImprovement  | 0.016069   |
| ActualImprovement    | 0.015084   |
| ImprovementRatio     | 0.93872    |
| MeanKL               | 0.0075228  |
| Entropy              | 0.1942     |
| Perplexity           | 1.2143     |
| AveragePolicyStd     | 0.25313    |
| AveragePolicyStd[0]  | 0.26117    |
| AveragePolicyStd[1]  | 0.32085    |
| AveragePolicyStd[2]  | 0.1866     |
| AveragePolicyStd[3]  | 0.26438    |
| AveragePolicyStd[4]  | 0.23642    |
| AveragePolicyStd[5]  | 0.24938    |
| AverageReturn        | 1120.8     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 129.78     |
| AverageEpisodeLength | 981.13     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.39     |
| TotalNEpisodes       | 18964      |
| TotalNSamples        | 2.2795e+06 |
| ExplainedVariance    | 0.045846   |
-------------------------------------
[2018-01-21 13:44:08.973235 UTC] Saving snapshot
[2018-01-21 13:44:08.973481 UTC] Starting iteration 457
[2018-01-21 13:44:08.973656 UTC] Start collecting samples
[2018-01-21 13:44:13.521233 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:13.661584 UTC] Performing policy update
[2018-01-21 13:44:13.662220 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:13.799389 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:15.215421 UTC] Performing line search
[2018-01-21 13:44:15.401800 UTC] Updating baseline
[2018-01-21 13:44:17.202575 UTC] Computing logging information
-------------------------------------
| Iteration            | 457        |
| ExpectedImprovement  | 0.017476   |
| ActualImprovement    | 0.016373   |
| ImprovementRatio     | 0.93686    |
| MeanKL               | 0.0071227  |
| Entropy              | 0.20003    |
| Perplexity           | 1.2214     |
| AveragePolicyStd     | 0.25337    |
| AveragePolicyStd[0]  | 0.26128    |
| AveragePolicyStd[1]  | 0.32075    |
| AveragePolicyStd[2]  | 0.18696    |
| AveragePolicyStd[3]  | 0.26563    |
| AveragePolicyStd[4]  | 0.23644    |
| AveragePolicyStd[5]  | 0.24914    |
| AverageReturn        | 1117.6     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 139.81     |
| AverageEpisodeLength | 976.72     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 117.62     |
| TotalNEpisodes       | 18972      |
| TotalNSamples        | 2.2871e+06 |
| ExplainedVariance    | 0.034394   |
-------------------------------------
[2018-01-21 13:44:17.855926 UTC] Saving snapshot
[2018-01-21 13:44:17.856142 UTC] Starting iteration 458
[2018-01-21 13:44:17.856326 UTC] Start collecting samples
[2018-01-21 13:44:22.080285 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:22.201269 UTC] Performing policy update
[2018-01-21 13:44:22.201914 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:22.322705 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:23.806075 UTC] Performing line search
[2018-01-21 13:44:24.011094 UTC] Updating baseline
[2018-01-21 13:44:26.577859 UTC] Computing logging information
-------------------------------------
| Iteration            | 458        |
| ExpectedImprovement  | 0.017403   |
| ActualImprovement    | 0.01687    |
| ImprovementRatio     | 0.96935    |
| MeanKL               | 0.0071217  |
| Entropy              | 0.19327    |
| Perplexity           | 1.2132     |
| AveragePolicyStd     | 0.25307    |
| AveragePolicyStd[0]  | 0.26089    |
| AveragePolicyStd[1]  | 0.32085    |
| AveragePolicyStd[2]  | 0.18716    |
| AveragePolicyStd[3]  | 0.26516    |
| AveragePolicyStd[4]  | 0.23656    |
| AveragePolicyStd[5]  | 0.24779    |
| AverageReturn        | 1109.8     |
| MinReturn            | 246.09     |
| MaxReturn            | 1195.6     |
| StdReturn            | 159.62     |
| AverageEpisodeLength | 969.79     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.15     |
| TotalNEpisodes       | 18977      |
| TotalNSamples        | 2.2914e+06 |
| ExplainedVariance    | 0.086668   |
-------------------------------------
[2018-01-21 13:44:27.310019 UTC] Saving snapshot
[2018-01-21 13:44:27.310247 UTC] Starting iteration 459
[2018-01-21 13:44:27.310391 UTC] Start collecting samples
[2018-01-21 13:44:31.866967 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:32.020911 UTC] Performing policy update
[2018-01-21 13:44:32.024519 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:32.140929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:33.535766 UTC] Performing line search
[2018-01-21 13:44:33.721787 UTC] Updating baseline
[2018-01-21 13:44:35.872022 UTC] Computing logging information
-------------------------------------
| Iteration            | 459        |
| ExpectedImprovement  | 0.015421   |
| ActualImprovement    | 0.014938   |
| ImprovementRatio     | 0.96872    |
| MeanKL               | 0.0074263  |
| Entropy              | 0.19182    |
| Perplexity           | 1.2115     |
| AveragePolicyStd     | 0.253      |
| AveragePolicyStd[0]  | 0.2604     |
| AveragePolicyStd[1]  | 0.3208     |
| AveragePolicyStd[2]  | 0.18692    |
| AveragePolicyStd[3]  | 0.26438    |
| AveragePolicyStd[4]  | 0.23716    |
| AveragePolicyStd[5]  | 0.24836    |
| AverageReturn        | 1106.3     |
| MinReturn            | 135.89     |
| MaxReturn            | 1195.6     |
| StdReturn            | 169.59     |
| AverageEpisodeLength | 966.44     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.8      |
| TotalNEpisodes       | 18983      |
| TotalNSamples        | 2.2963e+06 |
| ExplainedVariance    | 0.33707    |
-------------------------------------
[2018-01-21 13:44:36.527175 UTC] Saving snapshot
[2018-01-21 13:44:36.527512 UTC] Starting iteration 460
[2018-01-21 13:44:36.527755 UTC] Start collecting samples
[2018-01-21 13:44:40.946754 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:41.083176 UTC] Performing policy update
[2018-01-21 13:44:41.084140 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:41.202231 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:42.629857 UTC] Performing line search
[2018-01-21 13:44:42.825165 UTC] Updating baseline
[2018-01-21 13:44:44.658920 UTC] Computing logging information
-------------------------------------
| Iteration            | 460        |
| ExpectedImprovement  | 0.016856   |
| ActualImprovement    | 0.015932   |
| ImprovementRatio     | 0.94517    |
| MeanKL               | 0.0073292  |
| Entropy              | 0.18139    |
| Perplexity           | 1.1989     |
| AveragePolicyStd     | 0.25258    |
| AveragePolicyStd[0]  | 0.2603     |
| AveragePolicyStd[1]  | 0.3207     |
| AveragePolicyStd[2]  | 0.18669    |
| AveragePolicyStd[3]  | 0.26379    |
| AveragePolicyStd[4]  | 0.23673    |
| AveragePolicyStd[5]  | 0.24726    |
| AverageReturn        | 1107.7     |
| MinReturn            | 135.89     |
| MaxReturn            | 1195.6     |
| StdReturn            | 169.81     |
| AverageEpisodeLength | 966.44     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.8      |
| TotalNEpisodes       | 18988      |
| TotalNSamples        | 2.3013e+06 |
| ExplainedVariance    | 0.085091   |
-------------------------------------
[2018-01-21 13:44:45.392734 UTC] Saving snapshot
[2018-01-21 13:44:45.402102 UTC] Starting iteration 461
[2018-01-21 13:44:45.402326 UTC] Start collecting samples
[2018-01-21 13:44:50.011998 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:50.141216 UTC] Performing policy update
[2018-01-21 13:44:50.142284 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:50.266950 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:44:51.673876 UTC] Performing line search
[2018-01-21 13:44:51.872972 UTC] Updating baseline
[2018-01-21 13:44:53.680574 UTC] Computing logging information
-------------------------------------
| Iteration            | 461        |
| ExpectedImprovement  | 0.015083   |
| ActualImprovement    | 0.01428    |
| ImprovementRatio     | 0.94674    |
| MeanKL               | 0.007676   |
| Entropy              | 0.17623    |
| Perplexity           | 1.1927     |
| AveragePolicyStd     | 0.25237    |
| AveragePolicyStd[0]  | 0.26042    |
| AveragePolicyStd[1]  | 0.32047    |
| AveragePolicyStd[2]  | 0.18652    |
| AveragePolicyStd[3]  | 0.26383    |
| AveragePolicyStd[4]  | 0.23676    |
| AveragePolicyStd[5]  | 0.2462     |
| AverageReturn        | 1107.8     |
| MinReturn            | 135.89     |
| MaxReturn            | 1195.6     |
| StdReturn            | 169.82     |
| AverageEpisodeLength | 966.44     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.8      |
| TotalNEpisodes       | 18992      |
| TotalNSamples        | 2.3053e+06 |
| ExplainedVariance    | 0.016553   |
-------------------------------------
[2018-01-21 13:44:54.311403 UTC] Saving snapshot
[2018-01-21 13:44:54.311599 UTC] Starting iteration 462
[2018-01-21 13:44:54.311712 UTC] Start collecting samples
[2018-01-21 13:44:59.211163 UTC] Computing input variables for policy optimization
[2018-01-21 13:44:59.338778 UTC] Performing policy update
[2018-01-21 13:44:59.339368 UTC] Computing gradient in Euclidean space
[2018-01-21 13:44:59.454902 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:00.872339 UTC] Performing line search
[2018-01-21 13:45:01.058872 UTC] Updating baseline
[2018-01-21 13:45:02.785108 UTC] Computing logging information
-------------------------------------
| Iteration            | 462        |
| ExpectedImprovement  | 0.017935   |
| ActualImprovement    | 0.016957   |
| ImprovementRatio     | 0.94545    |
| MeanKL               | 0.007749   |
| Entropy              | 0.16107    |
| Perplexity           | 1.1748     |
| AveragePolicyStd     | 0.25181    |
| AveragePolicyStd[0]  | 0.25942    |
| AveragePolicyStd[1]  | 0.32117    |
| AveragePolicyStd[2]  | 0.18556    |
| AveragePolicyStd[3]  | 0.26313    |
| AveragePolicyStd[4]  | 0.23702    |
| AveragePolicyStd[5]  | 0.24455    |
| AverageReturn        | 1101.3     |
| MinReturn            | 135.89     |
| MaxReturn            | 1195.6     |
| StdReturn            | 178.78     |
| AverageEpisodeLength | 960.31     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.85     |
| TotalNEpisodes       | 18999      |
| TotalNSamples        | 2.3116e+06 |
| ExplainedVariance    | 0.22914    |
-------------------------------------
[2018-01-21 13:45:03.498549 UTC] Saving snapshot
[2018-01-21 13:45:03.499058 UTC] Starting iteration 463
[2018-01-21 13:45:03.499473 UTC] Start collecting samples
[2018-01-21 13:45:08.001130 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:08.136203 UTC] Performing policy update
[2018-01-21 13:45:08.137314 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:08.260612 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:09.691495 UTC] Performing line search
[2018-01-21 13:45:09.893495 UTC] Updating baseline
[2018-01-21 13:45:12.166941 UTC] Computing logging information
-------------------------------------
| Iteration            | 463        |
| ExpectedImprovement  | 0.015748   |
| ActualImprovement    | 0.015104   |
| ImprovementRatio     | 0.95908    |
| MeanKL               | 0.0076522  |
| Entropy              | 0.15021    |
| Perplexity           | 1.1621     |
| AveragePolicyStd     | 0.25139    |
| AveragePolicyStd[0]  | 0.25799    |
| AveragePolicyStd[1]  | 0.32147    |
| AveragePolicyStd[2]  | 0.18508    |
| AveragePolicyStd[3]  | 0.26265    |
| AveragePolicyStd[4]  | 0.2366     |
| AveragePolicyStd[5]  | 0.24453    |
| AverageReturn        | 1102.3     |
| MinReturn            | 135.89     |
| MaxReturn            | 1201.4     |
| StdReturn            | 179.06     |
| AverageEpisodeLength | 960.31     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.85     |
| TotalNEpisodes       | 19004      |
| TotalNSamples        | 2.3166e+06 |
| ExplainedVariance    | 0.038737   |
-------------------------------------
[2018-01-21 13:45:12.791021 UTC] Saving snapshot
[2018-01-21 13:45:12.791342 UTC] Starting iteration 464
[2018-01-21 13:45:12.791617 UTC] Start collecting samples
[2018-01-21 13:45:17.420333 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:17.549254 UTC] Performing policy update
[2018-01-21 13:45:17.550169 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:17.671160 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:19.137828 UTC] Performing line search
[2018-01-21 13:45:19.333469 UTC] Updating baseline
[2018-01-21 13:45:21.051215 UTC] Computing logging information
-------------------------------------
| Iteration            | 464        |
| ExpectedImprovement  | 0.016748   |
| ActualImprovement    | 0.016106   |
| ImprovementRatio     | 0.96168    |
| MeanKL               | 0.0074364  |
| Entropy              | 0.14439    |
| Perplexity           | 1.1553     |
| AveragePolicyStd     | 0.25113    |
| AveragePolicyStd[0]  | 0.25847    |
| AveragePolicyStd[1]  | 0.32185    |
| AveragePolicyStd[2]  | 0.18561    |
| AveragePolicyStd[3]  | 0.26145    |
| AveragePolicyStd[4]  | 0.23569    |
| AveragePolicyStd[5]  | 0.24371    |
| AverageReturn        | 1092.6     |
| MinReturn            | 135.89     |
| MaxReturn            | 1201.4     |
| StdReturn            | 191.61     |
| AverageEpisodeLength | 951.43     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.85     |
| TotalNEpisodes       | 19009      |
| TotalNSamples        | 2.3208e+06 |
| ExplainedVariance    | 0.22478    |
-------------------------------------
[2018-01-21 13:45:21.740200 UTC] Saving snapshot
[2018-01-21 13:45:21.740404 UTC] Starting iteration 465
[2018-01-21 13:45:21.740548 UTC] Start collecting samples
[2018-01-21 13:45:26.320686 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:26.463529 UTC] Performing policy update
[2018-01-21 13:45:26.464364 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:26.597596 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:28.073222 UTC] Performing line search
[2018-01-21 13:45:28.270043 UTC] Updating baseline
[2018-01-21 13:45:30.451611 UTC] Computing logging information
-------------------------------------
| Iteration            | 465        |
| ExpectedImprovement  | 0.015634   |
| ActualImprovement    | 0.014277   |
| ImprovementRatio     | 0.91321    |
| MeanKL               | 0.0070538  |
| Entropy              | 0.14353    |
| Perplexity           | 1.1543     |
| AveragePolicyStd     | 0.2511     |
| AveragePolicyStd[0]  | 0.25738    |
| AveragePolicyStd[1]  | 0.32185    |
| AveragePolicyStd[2]  | 0.18507    |
| AveragePolicyStd[3]  | 0.2611     |
| AveragePolicyStd[4]  | 0.23696    |
| AveragePolicyStd[5]  | 0.24427    |
| AverageReturn        | 1093.4     |
| MinReturn            | 135.89     |
| MaxReturn            | 1217.5     |
| StdReturn            | 192.5      |
| AverageEpisodeLength | 950.28     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.91     |
| TotalNEpisodes       | 19015      |
| TotalNSamples        | 2.3266e+06 |
| ExplainedVariance    | 0.13646    |
-------------------------------------
[2018-01-21 13:45:31.148378 UTC] Saving snapshot
[2018-01-21 13:45:31.148630 UTC] Starting iteration 466
[2018-01-21 13:45:31.148808 UTC] Start collecting samples
[2018-01-21 13:45:35.751491 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:35.882877 UTC] Performing policy update
[2018-01-21 13:45:35.883956 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:36.013043 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:37.447022 UTC] Performing line search
[2018-01-21 13:45:37.639270 UTC] Updating baseline
[2018-01-21 13:45:40.741724 UTC] Computing logging information
-------------------------------------
| Iteration            | 466        |
| ExpectedImprovement  | 0.017608   |
| ActualImprovement    | 0.016515   |
| ImprovementRatio     | 0.93795    |
| MeanKL               | 0.0072886  |
| Entropy              | 0.13562    |
| Perplexity           | 1.1452     |
| AveragePolicyStd     | 0.25085    |
| AveragePolicyStd[0]  | 0.25614    |
| AveragePolicyStd[1]  | 0.32275    |
| AveragePolicyStd[2]  | 0.18423    |
| AveragePolicyStd[3]  | 0.26086    |
| AveragePolicyStd[4]  | 0.23737    |
| AveragePolicyStd[5]  | 0.24374    |
| AverageReturn        | 1093       |
| MinReturn            | 135.89     |
| MaxReturn            | 1217.5     |
| StdReturn            | 192.45     |
| AverageEpisodeLength | 950.28     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.91     |
| TotalNEpisodes       | 19018      |
| TotalNSamples        | 2.3296e+06 |
| ExplainedVariance    | -0.0090221 |
-------------------------------------
[2018-01-21 13:45:41.444989 UTC] Saving snapshot
[2018-01-21 13:45:41.445254 UTC] Starting iteration 467
[2018-01-21 13:45:41.445441 UTC] Start collecting samples
[2018-01-21 13:45:46.216322 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:46.359474 UTC] Performing policy update
[2018-01-21 13:45:46.360307 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:46.476542 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:47.894666 UTC] Performing line search
[2018-01-21 13:45:48.083724 UTC] Updating baseline
[2018-01-21 13:45:50.380365 UTC] Computing logging information
-------------------------------------
| Iteration            | 467        |
| ExpectedImprovement  | 0.017631   |
| ActualImprovement    | 0.01681    |
| ImprovementRatio     | 0.95341    |
| MeanKL               | 0.0072295  |
| Entropy              | 0.12978    |
| Perplexity           | 1.1386     |
| AveragePolicyStd     | 0.2506     |
| AveragePolicyStd[0]  | 0.25545    |
| AveragePolicyStd[1]  | 0.32188    |
| AveragePolicyStd[2]  | 0.18371    |
| AveragePolicyStd[3]  | 0.26058    |
| AveragePolicyStd[4]  | 0.23712    |
| AveragePolicyStd[5]  | 0.24486    |
| AverageReturn        | 1094.1     |
| MinReturn            | 135.89     |
| MaxReturn            | 1217.5     |
| StdReturn            | 192.84     |
| AverageEpisodeLength | 950.28     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.91     |
| TotalNEpisodes       | 19024      |
| TotalNSamples        | 2.3356e+06 |
| ExplainedVariance    | -0.0021582 |
-------------------------------------
[2018-01-21 13:45:51.078237 UTC] Saving snapshot
[2018-01-21 13:45:51.078496 UTC] Starting iteration 468
[2018-01-21 13:45:51.078692 UTC] Start collecting samples
[2018-01-21 13:45:55.679625 UTC] Computing input variables for policy optimization
[2018-01-21 13:45:55.808355 UTC] Performing policy update
[2018-01-21 13:45:55.809508 UTC] Computing gradient in Euclidean space
[2018-01-21 13:45:55.942097 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:45:57.468327 UTC] Performing line search
[2018-01-21 13:45:57.653094 UTC] Updating baseline
[2018-01-21 13:45:59.944019 UTC] Computing logging information
-------------------------------------
| Iteration            | 468        |
| ExpectedImprovement  | 0.017712   |
| ActualImprovement    | 0.016437   |
| ImprovementRatio     | 0.92804    |
| MeanKL               | 0.0070623  |
| Entropy              | 0.1279     |
| Perplexity           | 1.1364     |
| AveragePolicyStd     | 0.25054    |
| AveragePolicyStd[0]  | 0.25547    |
| AveragePolicyStd[1]  | 0.3225     |
| AveragePolicyStd[2]  | 0.18363    |
| AveragePolicyStd[3]  | 0.2594     |
| AveragePolicyStd[4]  | 0.23747    |
| AveragePolicyStd[5]  | 0.24475    |
| AverageReturn        | 1097.6     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 193.97     |
| AverageEpisodeLength | 950.28     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.91     |
| TotalNEpisodes       | 19031      |
| TotalNSamples        | 2.3426e+06 |
| ExplainedVariance    | 0.0053394  |
-------------------------------------
[2018-01-21 13:46:00.677849 UTC] Saving snapshot
[2018-01-21 13:46:00.678088 UTC] Starting iteration 469
[2018-01-21 13:46:00.678255 UTC] Start collecting samples
[2018-01-21 13:46:05.091034 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:05.247877 UTC] Performing policy update
[2018-01-21 13:46:05.248949 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:05.368161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:06.828855 UTC] Performing line search
[2018-01-21 13:46:07.029100 UTC] Updating baseline
[2018-01-21 13:46:09.677544 UTC] Computing logging information
-------------------------------------
| Iteration            | 469        |
| ExpectedImprovement  | 0.01863    |
| ActualImprovement    | 0.016383   |
| ImprovementRatio     | 0.8794     |
| MeanKL               | 0.0070108  |
| Entropy              | 0.11516    |
| Perplexity           | 1.122      |
| AveragePolicyStd     | 0.25       |
| AveragePolicyStd[0]  | 0.25505    |
| AveragePolicyStd[1]  | 0.32136    |
| AveragePolicyStd[2]  | 0.18306    |
| AveragePolicyStd[3]  | 0.25883    |
| AveragePolicyStd[4]  | 0.23634    |
| AveragePolicyStd[5]  | 0.24537    |
| AverageReturn        | 1092       |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 202.22     |
| AverageEpisodeLength | 944.92     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.98     |
| TotalNEpisodes       | 19034      |
| TotalNSamples        | 2.3451e+06 |
| ExplainedVariance    | 0.11922    |
-------------------------------------
[2018-01-21 13:46:10.311790 UTC] Saving snapshot
[2018-01-21 13:46:10.312031 UTC] Starting iteration 470
[2018-01-21 13:46:10.312196 UTC] Start collecting samples
[2018-01-21 13:46:14.971532 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:15.126001 UTC] Performing policy update
[2018-01-21 13:46:15.126650 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:15.249307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:16.745264 UTC] Performing line search
[2018-01-21 13:46:16.949329 UTC] Updating baseline
[2018-01-21 13:46:18.865816 UTC] Computing logging information
-------------------------------------
| Iteration            | 470        |
| ExpectedImprovement  | 0.016954   |
| ActualImprovement    | 0.015798   |
| ImprovementRatio     | 0.93183    |
| MeanKL               | 0.0074486  |
| Entropy              | 0.10604    |
| Perplexity           | 1.1119     |
| AveragePolicyStd     | 0.24965    |
| AveragePolicyStd[0]  | 0.25468    |
| AveragePolicyStd[1]  | 0.32214    |
| AveragePolicyStd[2]  | 0.18314    |
| AveragePolicyStd[3]  | 0.25739    |
| AveragePolicyStd[4]  | 0.23559    |
| AveragePolicyStd[5]  | 0.24494    |
| AverageReturn        | 1094.2     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 203.01     |
| AverageEpisodeLength | 944.92     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.98     |
| TotalNEpisodes       | 19040      |
| TotalNSamples        | 2.3511e+06 |
| ExplainedVariance    | -0.020337  |
-------------------------------------
[2018-01-21 13:46:19.588944 UTC] Saving snapshot
[2018-01-21 13:46:19.599076 UTC] Starting iteration 471
[2018-01-21 13:46:19.600177 UTC] Start collecting samples
[2018-01-21 13:46:24.219620 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:24.359350 UTC] Performing policy update
[2018-01-21 13:46:24.359989 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:24.481050 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:25.871652 UTC] Performing line search
[2018-01-21 13:46:26.060345 UTC] Updating baseline
[2018-01-21 13:46:28.138237 UTC] Computing logging information
-------------------------------------
| Iteration            | 471        |
| ExpectedImprovement  | 0.01813    |
| ActualImprovement    | 0.017067   |
| ImprovementRatio     | 0.94137    |
| MeanKL               | 0.0074546  |
| Entropy              | 0.11116    |
| Perplexity           | 1.1176     |
| AveragePolicyStd     | 0.24991    |
| AveragePolicyStd[0]  | 0.25517    |
| AveragePolicyStd[1]  | 0.32285    |
| AveragePolicyStd[2]  | 0.18286    |
| AveragePolicyStd[3]  | 0.2583     |
| AveragePolicyStd[4]  | 0.23549    |
| AveragePolicyStd[5]  | 0.24479    |
| AverageReturn        | 1094.6     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 206.6      |
| AverageEpisodeLength | 943.73     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.94     |
| TotalNEpisodes       | 19046      |
| TotalNSamples        | 2.3566e+06 |
| ExplainedVariance    | 0.13748    |
-------------------------------------
[2018-01-21 13:46:28.848380 UTC] Saving snapshot
[2018-01-21 13:46:28.848613 UTC] Starting iteration 472
[2018-01-21 13:46:28.848763 UTC] Start collecting samples
[2018-01-21 13:46:33.333419 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:33.464196 UTC] Performing policy update
[2018-01-21 13:46:33.465168 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:33.587160 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:35.022494 UTC] Performing line search
[2018-01-21 13:46:35.245243 UTC] Updating baseline
[2018-01-21 13:46:37.003017 UTC] Computing logging information
-------------------------------------
| Iteration            | 472        |
| ExpectedImprovement  | 0.01608    |
| ActualImprovement    | 0.015008   |
| ImprovementRatio     | 0.93332    |
| MeanKL               | 0.0077752  |
| Entropy              | 0.11507    |
| Perplexity           | 1.122      |
| AveragePolicyStd     | 0.25013    |
| AveragePolicyStd[0]  | 0.25556    |
| AveragePolicyStd[1]  | 0.32379    |
| AveragePolicyStd[2]  | 0.18252    |
| AveragePolicyStd[3]  | 0.25801    |
| AveragePolicyStd[4]  | 0.2355     |
| AveragePolicyStd[5]  | 0.24539    |
| AverageReturn        | 1096.9     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 205.32     |
| AverageEpisodeLength | 944.01     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.97     |
| TotalNEpisodes       | 19051      |
| TotalNSamples        | 2.3609e+06 |
| ExplainedVariance    | 0.20225    |
-------------------------------------
[2018-01-21 13:46:37.740009 UTC] Saving snapshot
[2018-01-21 13:46:37.740237 UTC] Starting iteration 473
[2018-01-21 13:46:37.740417 UTC] Start collecting samples
[2018-01-21 13:46:42.300036 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:42.429283 UTC] Performing policy update
[2018-01-21 13:46:42.429899 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:42.549907 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:43.953396 UTC] Performing line search
[2018-01-21 13:46:44.160075 UTC] Updating baseline
[2018-01-21 13:46:46.318743 UTC] Computing logging information
-------------------------------------
| Iteration            | 473        |
| ExpectedImprovement  | 0.016257   |
| ActualImprovement    | 0.015762   |
| ImprovementRatio     | 0.96954    |
| MeanKL               | 0.0073963  |
| Entropy              | 0.11811    |
| Perplexity           | 1.1254     |
| AveragePolicyStd     | 0.25032    |
| AveragePolicyStd[0]  | 0.25587    |
| AveragePolicyStd[1]  | 0.3248     |
| AveragePolicyStd[2]  | 0.18221    |
| AveragePolicyStd[3]  | 0.25815    |
| AveragePolicyStd[4]  | 0.23494    |
| AveragePolicyStd[5]  | 0.24594    |
| AverageReturn        | 1092.5     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 211.62     |
| AverageEpisodeLength | 939.81     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.71     |
| TotalNEpisodes       | 19057      |
| TotalNSamples        | 2.3665e+06 |
| ExplainedVariance    | 0.22838    |
-------------------------------------
[2018-01-21 13:46:46.980699 UTC] Saving snapshot
[2018-01-21 13:46:46.980976 UTC] Starting iteration 474
[2018-01-21 13:46:46.981175 UTC] Start collecting samples
[2018-01-21 13:46:51.380842 UTC] Computing input variables for policy optimization
[2018-01-21 13:46:51.503925 UTC] Performing policy update
[2018-01-21 13:46:51.504516 UTC] Computing gradient in Euclidean space
[2018-01-21 13:46:51.621237 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:46:53.002966 UTC] Performing line search
[2018-01-21 13:46:53.185438 UTC] Updating baseline
[2018-01-21 13:46:55.045351 UTC] Computing logging information
-------------------------------------
| Iteration            | 474        |
| ExpectedImprovement  | 0.019077   |
| ActualImprovement    | 0.017636   |
| ImprovementRatio     | 0.92443    |
| MeanKL               | 0.0073881  |
| Entropy              | 0.10923    |
| Perplexity           | 1.1154     |
| AveragePolicyStd     | 0.24989    |
| AveragePolicyStd[0]  | 0.25655    |
| AveragePolicyStd[1]  | 0.32345    |
| AveragePolicyStd[2]  | 0.18243    |
| AveragePolicyStd[3]  | 0.25734    |
| AveragePolicyStd[4]  | 0.23422    |
| AveragePolicyStd[5]  | 0.24537    |
| AverageReturn        | 1077.5     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 231.08     |
| AverageEpisodeLength | 928.01     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.82     |
| TotalNEpisodes       | 19063      |
| TotalNSamples        | 2.3713e+06 |
| ExplainedVariance    | 0.41973    |
-------------------------------------
[2018-01-21 13:46:55.689260 UTC] Saving snapshot
[2018-01-21 13:46:55.689469 UTC] Starting iteration 475
[2018-01-21 13:46:55.689581 UTC] Start collecting samples
[2018-01-21 13:47:00.462126 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:00.589813 UTC] Performing policy update
[2018-01-21 13:47:00.590476 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:00.719751 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:02.172111 UTC] Performing line search
[2018-01-21 13:47:02.366183 UTC] Updating baseline
[2018-01-21 13:47:05.201263 UTC] Computing logging information
-------------------------------------
| Iteration            | 475        |
| ExpectedImprovement  | 0.015203   |
| ActualImprovement    | 0.014851   |
| ImprovementRatio     | 0.97685    |
| MeanKL               | 0.0072039  |
| Entropy              | 0.098365   |
| Perplexity           | 1.1034     |
| AveragePolicyStd     | 0.2494     |
| AveragePolicyStd[0]  | 0.2555     |
| AveragePolicyStd[1]  | 0.32236    |
| AveragePolicyStd[2]  | 0.18225    |
| AveragePolicyStd[3]  | 0.25674    |
| AveragePolicyStd[4]  | 0.2346     |
| AveragePolicyStd[5]  | 0.24495    |
| AverageReturn        | 1081.2     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 232.54     |
| AverageEpisodeLength | 928.01     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.82     |
| TotalNEpisodes       | 19068      |
| TotalNSamples        | 2.3763e+06 |
| ExplainedVariance    | -0.054054  |
-------------------------------------
[2018-01-21 13:47:05.836043 UTC] Saving snapshot
[2018-01-21 13:47:05.836320 UTC] Starting iteration 476
[2018-01-21 13:47:05.836501 UTC] Start collecting samples
[2018-01-21 13:47:10.292420 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:10.426764 UTC] Performing policy update
[2018-01-21 13:47:10.427385 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:10.545772 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:11.919186 UTC] Performing line search
[2018-01-21 13:47:12.104773 UTC] Updating baseline
[2018-01-21 13:47:14.482864 UTC] Computing logging information
-------------------------------------
| Iteration            | 476        |
| ExpectedImprovement  | 0.016971   |
| ActualImprovement    | 0.015771   |
| ImprovementRatio     | 0.92928    |
| MeanKL               | 0.0076237  |
| Entropy              | 0.10182    |
| Perplexity           | 1.1072     |
| AveragePolicyStd     | 0.24953    |
| AveragePolicyStd[0]  | 0.25551    |
| AveragePolicyStd[1]  | 0.32199    |
| AveragePolicyStd[2]  | 0.18239    |
| AveragePolicyStd[3]  | 0.25821    |
| AveragePolicyStd[4]  | 0.23471    |
| AveragePolicyStd[5]  | 0.24437    |
| AverageReturn        | 1087.4     |
| MinReturn            | 135.89     |
| MaxReturn            | 1230       |
| StdReturn            | 227.65     |
| AverageEpisodeLength | 932.79     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.45     |
| TotalNEpisodes       | 19072      |
| TotalNSamples        | 2.3803e+06 |
| ExplainedVariance    | 0.0038038  |
-------------------------------------
[2018-01-21 13:47:15.162800 UTC] Saving snapshot
[2018-01-21 13:47:15.163124 UTC] Starting iteration 477
[2018-01-21 13:47:15.163268 UTC] Start collecting samples
[2018-01-21 13:47:19.645690 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:19.778525 UTC] Performing policy update
[2018-01-21 13:47:19.779495 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:19.901072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:21.312175 UTC] Performing line search
[2018-01-21 13:47:21.498085 UTC] Updating baseline
[2018-01-21 13:47:23.573293 UTC] Computing logging information
-------------------------------------
| Iteration            | 477        |
| ExpectedImprovement  | 0.015343   |
| ActualImprovement    | 0.014766   |
| ImprovementRatio     | 0.96242    |
| MeanKL               | 0.0082066  |
| Entropy              | 0.1023     |
| Perplexity           | 1.1077     |
| AveragePolicyStd     | 0.24957    |
| AveragePolicyStd[0]  | 0.25609    |
| AveragePolicyStd[1]  | 0.32224    |
| AveragePolicyStd[2]  | 0.18231    |
| AveragePolicyStd[3]  | 0.25794    |
| AveragePolicyStd[4]  | 0.23462    |
| AveragePolicyStd[5]  | 0.24419    |
| AverageReturn        | 1106.3     |
| MinReturn            | 362.99     |
| MaxReturn            | 1230       |
| StdReturn            | 193.99     |
| AverageEpisodeLength | 945.84     |
| MinEpisodeLength     | 363        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.49     |
| TotalNEpisodes       | 19079      |
| TotalNSamples        | 2.3871e+06 |
| ExplainedVariance    | 0.20188    |
-------------------------------------
[2018-01-21 13:47:24.232469 UTC] Saving snapshot
[2018-01-21 13:47:24.232807 UTC] Starting iteration 478
[2018-01-21 13:47:24.233060 UTC] Start collecting samples
[2018-01-21 13:47:28.835832 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:28.981371 UTC] Performing policy update
[2018-01-21 13:47:28.982003 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:29.094640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:30.476816 UTC] Performing line search
[2018-01-21 13:47:30.666924 UTC] Updating baseline
[2018-01-21 13:47:32.515666 UTC] Computing logging information
-------------------------------------
| Iteration            | 478        |
| ExpectedImprovement  | 0.014422   |
| ActualImprovement    | 0.013716   |
| ImprovementRatio     | 0.95106    |
| MeanKL               | 0.0078098  |
| Entropy              | 0.10638    |
| Perplexity           | 1.1122     |
| AveragePolicyStd     | 0.24975    |
| AveragePolicyStd[0]  | 0.25657    |
| AveragePolicyStd[1]  | 0.3227     |
| AveragePolicyStd[2]  | 0.18248    |
| AveragePolicyStd[3]  | 0.25821    |
| AveragePolicyStd[4]  | 0.23437    |
| AveragePolicyStd[5]  | 0.24417    |
| AverageReturn        | 1104.7     |
| MinReturn            | 362.99     |
| MaxReturn            | 1230       |
| StdReturn            | 201.62     |
| AverageEpisodeLength | 942.97     |
| MinEpisodeLength     | 363        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.02     |
| TotalNEpisodes       | 19084      |
| TotalNSamples        | 2.3916e+06 |
| ExplainedVariance    | 0.208      |
-------------------------------------
[2018-01-21 13:47:33.211545 UTC] Saving snapshot
[2018-01-21 13:47:33.211784 UTC] Starting iteration 479
[2018-01-21 13:47:33.211961 UTC] Start collecting samples
[2018-01-21 13:47:37.749269 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:37.880830 UTC] Performing policy update
[2018-01-21 13:47:37.881546 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:38.016817 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:39.436362 UTC] Performing line search
[2018-01-21 13:47:39.623364 UTC] Updating baseline
[2018-01-21 13:47:41.731852 UTC] Computing logging information
-------------------------------------
| Iteration            | 479        |
| ExpectedImprovement  | 0.017229   |
| ActualImprovement    | 0.016087   |
| ImprovementRatio     | 0.93372    |
| MeanKL               | 0.0075204  |
| Entropy              | 0.10038    |
| Perplexity           | 1.1056     |
| AveragePolicyStd     | 0.24943    |
| AveragePolicyStd[0]  | 0.25631    |
| AveragePolicyStd[1]  | 0.32121    |
| AveragePolicyStd[2]  | 0.18278    |
| AveragePolicyStd[3]  | 0.25822    |
| AveragePolicyStd[4]  | 0.23434    |
| AveragePolicyStd[5]  | 0.2437     |
| AverageReturn        | 1096.9     |
| MinReturn            | 362.99     |
| MaxReturn            | 1233.5     |
| StdReturn            | 210.02     |
| AverageEpisodeLength | 935.64     |
| MinEpisodeLength     | 363        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.87     |
| TotalNEpisodes       | 19089      |
| TotalNSamples        | 2.3958e+06 |
| ExplainedVariance    | 0.41925    |
-------------------------------------
[2018-01-21 13:47:42.328267 UTC] Saving snapshot
[2018-01-21 13:47:42.328558 UTC] Starting iteration 480
[2018-01-21 13:47:42.328768 UTC] Start collecting samples
[2018-01-21 13:47:46.878137 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:47.022623 UTC] Performing policy update
[2018-01-21 13:47:47.023296 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:47.141901 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:48.546096 UTC] Performing line search
[2018-01-21 13:47:48.732127 UTC] Updating baseline
[2018-01-21 13:47:50.538726 UTC] Computing logging information
-------------------------------------
| Iteration            | 480        |
| ExpectedImprovement  | 0.013907   |
| ActualImprovement    | 0.013485   |
| ImprovementRatio     | 0.96969    |
| MeanKL               | 0.0074848  |
| Entropy              | 0.10083    |
| Perplexity           | 1.1061     |
| AveragePolicyStd     | 0.24946    |
| AveragePolicyStd[0]  | 0.25604    |
| AveragePolicyStd[1]  | 0.32172    |
| AveragePolicyStd[2]  | 0.18243    |
| AveragePolicyStd[3]  | 0.25671    |
| AveragePolicyStd[4]  | 0.23525    |
| AveragePolicyStd[5]  | 0.24463    |
| AverageReturn        | 1090.9     |
| MinReturn            | 213.85     |
| MaxReturn            | 1233.5     |
| StdReturn            | 226.13     |
| AverageEpisodeLength | 928.66     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.64     |
| TotalNEpisodes       | 19096      |
| TotalNSamples        | 2.4016e+06 |
| ExplainedVariance    | 0.28283    |
-------------------------------------
[2018-01-21 13:47:51.269265 UTC] Saving snapshot
[2018-01-21 13:47:51.279011 UTC] Starting iteration 481
[2018-01-21 13:47:51.279252 UTC] Start collecting samples
[2018-01-21 13:47:55.789272 UTC] Computing input variables for policy optimization
[2018-01-21 13:47:55.920459 UTC] Performing policy update
[2018-01-21 13:47:55.920948 UTC] Computing gradient in Euclidean space
[2018-01-21 13:47:56.068545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:47:57.472386 UTC] Performing line search
[2018-01-21 13:47:57.661282 UTC] Updating baseline
[2018-01-21 13:47:59.592820 UTC] Computing logging information
-------------------------------------
| Iteration            | 481        |
| ExpectedImprovement  | 0.016141   |
| ActualImprovement    | 0.015807   |
| ImprovementRatio     | 0.9793     |
| MeanKL               | 0.0077074  |
| Entropy              | 0.10186    |
| Perplexity           | 1.1072     |
| AveragePolicyStd     | 0.24949    |
| AveragePolicyStd[0]  | 0.25447    |
| AveragePolicyStd[1]  | 0.32185    |
| AveragePolicyStd[2]  | 0.18244    |
| AveragePolicyStd[3]  | 0.25643    |
| AveragePolicyStd[4]  | 0.23623    |
| AveragePolicyStd[5]  | 0.24553    |
| AverageReturn        | 1093.8     |
| MinReturn            | 213.85     |
| MaxReturn            | 1233.5     |
| StdReturn            | 226.92     |
| AverageEpisodeLength | 929.6      |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.76     |
| TotalNEpisodes       | 19102      |
| TotalNSamples        | 2.4076e+06 |
| ExplainedVariance    | -0.059715  |
-------------------------------------
[2018-01-21 13:48:00.290538 UTC] Saving snapshot
[2018-01-21 13:48:00.290797 UTC] Starting iteration 482
[2018-01-21 13:48:00.290978 UTC] Start collecting samples
[2018-01-21 13:48:04.649763 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:04.790674 UTC] Performing policy update
[2018-01-21 13:48:04.791643 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:04.920495 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:06.326605 UTC] Performing line search
[2018-01-21 13:48:06.514931 UTC] Updating baseline
[2018-01-21 13:48:08.569584 UTC] Computing logging information
-------------------------------------
| Iteration            | 482        |
| ExpectedImprovement  | 0.014899   |
| ActualImprovement    | 0.014557   |
| ImprovementRatio     | 0.97701    |
| MeanKL               | 0.0075915  |
| Entropy              | 0.097247   |
| Perplexity           | 1.1021     |
| AveragePolicyStd     | 0.24933    |
| AveragePolicyStd[0]  | 0.25394    |
| AveragePolicyStd[1]  | 0.32263    |
| AveragePolicyStd[2]  | 0.18248    |
| AveragePolicyStd[3]  | 0.25558    |
| AveragePolicyStd[4]  | 0.23552    |
| AveragePolicyStd[5]  | 0.24581    |
| AverageReturn        | 1094       |
| MinReturn            | 213.85     |
| MaxReturn            | 1233.5     |
| StdReturn            | 226.97     |
| AverageEpisodeLength | 929.6      |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.76     |
| TotalNEpisodes       | 19103      |
| TotalNSamples        | 2.4086e+06 |
| ExplainedVariance    | 0.067555   |
-------------------------------------
[2018-01-21 13:48:09.244565 UTC] Saving snapshot
[2018-01-21 13:48:09.244822 UTC] Starting iteration 483
[2018-01-21 13:48:09.245003 UTC] Start collecting samples
[2018-01-21 13:48:13.748110 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:13.875008 UTC] Performing policy update
[2018-01-21 13:48:13.876105 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:14.004840 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:15.512134 UTC] Performing line search
[2018-01-21 13:48:15.706464 UTC] Updating baseline
[2018-01-21 13:48:17.624077 UTC] Computing logging information
-------------------------------------
| Iteration            | 483        |
| ExpectedImprovement  | 0.016541   |
| ActualImprovement    | 0.015756   |
| ImprovementRatio     | 0.95252    |
| MeanKL               | 0.0079223  |
| Entropy              | 0.084326   |
| Perplexity           | 1.088      |
| AveragePolicyStd     | 0.24883    |
| AveragePolicyStd[0]  | 0.25208    |
| AveragePolicyStd[1]  | 0.32316    |
| AveragePolicyStd[2]  | 0.18201    |
| AveragePolicyStd[3]  | 0.25484    |
| AveragePolicyStd[4]  | 0.23559    |
| AveragePolicyStd[5]  | 0.24531    |
| AverageReturn        | 1102       |
| MinReturn            | 213.85     |
| MaxReturn            | 1233.5     |
| StdReturn            | 217.93     |
| AverageEpisodeLength | 936.18     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.89     |
| TotalNEpisodes       | 19111      |
| TotalNSamples        | 2.4164e+06 |
| ExplainedVariance    | 0.13977    |
-------------------------------------
[2018-01-21 13:48:18.289292 UTC] Saving snapshot
[2018-01-21 13:48:18.289565 UTC] Starting iteration 484
[2018-01-21 13:48:18.289736 UTC] Start collecting samples
[2018-01-21 13:48:22.695140 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:22.832374 UTC] Performing policy update
[2018-01-21 13:48:22.833006 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:22.957875 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:24.364622 UTC] Performing line search
[2018-01-21 13:48:24.550085 UTC] Updating baseline
[2018-01-21 13:48:26.616559 UTC] Computing logging information
-------------------------------------
| Iteration            | 484        |
| ExpectedImprovement  | 0.017336   |
| ActualImprovement    | 0.016163   |
| ImprovementRatio     | 0.9323     |
| MeanKL               | 0.0081337  |
| Entropy              | 0.080503   |
| Perplexity           | 1.0838     |
| AveragePolicyStd     | 0.2487     |
| AveragePolicyStd[0]  | 0.25156    |
| AveragePolicyStd[1]  | 0.3237     |
| AveragePolicyStd[2]  | 0.1821     |
| AveragePolicyStd[3]  | 0.25503    |
| AveragePolicyStd[4]  | 0.23498    |
| AveragePolicyStd[5]  | 0.2448     |
| AverageReturn        | 1105.8     |
| MinReturn            | 213.85     |
| MaxReturn            | 1233.5     |
| StdReturn            | 218.6      |
| AverageEpisodeLength | 937.29     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.92     |
| TotalNEpisodes       | 19117      |
| TotalNSamples        | 2.4224e+06 |
| ExplainedVariance    | 0.12672    |
-------------------------------------
[2018-01-21 13:48:27.339557 UTC] Saving snapshot
[2018-01-21 13:48:27.340106 UTC] Starting iteration 485
[2018-01-21 13:48:27.340505 UTC] Start collecting samples
[2018-01-21 13:48:31.845783 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:31.969482 UTC] Performing policy update
[2018-01-21 13:48:31.970116 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:32.089031 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:33.471636 UTC] Performing line search
[2018-01-21 13:48:33.656703 UTC] Updating baseline
[2018-01-21 13:48:36.108865 UTC] Computing logging information
-------------------------------------
| Iteration            | 485        |
| ExpectedImprovement  | 0.014766   |
| ActualImprovement    | 0.013965   |
| ImprovementRatio     | 0.94578    |
| MeanKL               | 0.0079129  |
| Entropy              | 0.066334   |
| Perplexity           | 1.0686     |
| AveragePolicyStd     | 0.24807    |
| AveragePolicyStd[0]  | 0.25134    |
| AveragePolicyStd[1]  | 0.322      |
| AveragePolicyStd[2]  | 0.18168    |
| AveragePolicyStd[3]  | 0.25457    |
| AveragePolicyStd[4]  | 0.23452    |
| AveragePolicyStd[5]  | 0.24432    |
| AverageReturn        | 1100.2     |
| MinReturn            | 213.85     |
| MaxReturn            | 1238.6     |
| StdReturn            | 225.87     |
| AverageEpisodeLength | 931.89     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.23     |
| TotalNEpisodes       | 19120      |
| TotalNSamples        | 2.4248e+06 |
| ExplainedVariance    | 0.32655    |
-------------------------------------
[2018-01-21 13:48:36.824581 UTC] Saving snapshot
[2018-01-21 13:48:36.824848 UTC] Starting iteration 486
[2018-01-21 13:48:36.825032 UTC] Start collecting samples
[2018-01-21 13:48:41.306547 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:41.448919 UTC] Performing policy update
[2018-01-21 13:48:41.449992 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:41.568984 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:43.017665 UTC] Performing line search
[2018-01-21 13:48:43.228440 UTC] Updating baseline
[2018-01-21 13:48:45.019295 UTC] Computing logging information
-------------------------------------
| Iteration            | 486        |
| ExpectedImprovement  | 0.017148   |
| ActualImprovement    | 0.016603   |
| ImprovementRatio     | 0.96821    |
| MeanKL               | 0.0075404  |
| Entropy              | 0.068622   |
| Perplexity           | 1.071      |
| AveragePolicyStd     | 0.24815    |
| AveragePolicyStd[0]  | 0.25219    |
| AveragePolicyStd[1]  | 0.32199    |
| AveragePolicyStd[2]  | 0.18199    |
| AveragePolicyStd[3]  | 0.25404    |
| AveragePolicyStd[4]  | 0.23458    |
| AveragePolicyStd[5]  | 0.2441     |
| AverageReturn        | 1098.1     |
| MinReturn            | 213.85     |
| MaxReturn            | 1238.6     |
| StdReturn            | 228.29     |
| AverageEpisodeLength | 928.62     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.94     |
| TotalNEpisodes       | 19128      |
| TotalNSamples        | 2.4325e+06 |
| ExplainedVariance    | 0.13606    |
-------------------------------------
[2018-01-21 13:48:45.758543 UTC] Saving snapshot
[2018-01-21 13:48:45.758849 UTC] Starting iteration 487
[2018-01-21 13:48:45.759022 UTC] Start collecting samples
[2018-01-21 13:48:50.197252 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:50.340710 UTC] Performing policy update
[2018-01-21 13:48:50.341747 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:50.461899 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:48:51.880635 UTC] Performing line search
[2018-01-21 13:48:52.111817 UTC] Updating baseline
[2018-01-21 13:48:54.446222 UTC] Computing logging information
-------------------------------------
| Iteration            | 487        |
| ExpectedImprovement  | 0.016058   |
| ActualImprovement    | 0.015242   |
| ImprovementRatio     | 0.9492     |
| MeanKL               | 0.0076265  |
| Entropy              | 0.073735   |
| Perplexity           | 1.0765     |
| AveragePolicyStd     | 0.24837    |
| AveragePolicyStd[0]  | 0.25213    |
| AveragePolicyStd[1]  | 0.32269    |
| AveragePolicyStd[2]  | 0.1822     |
| AveragePolicyStd[3]  | 0.25393    |
| AveragePolicyStd[4]  | 0.23466    |
| AveragePolicyStd[5]  | 0.24462    |
| AverageReturn        | 1098       |
| MinReturn            | 213.85     |
| MaxReturn            | 1238.6     |
| StdReturn            | 228.21     |
| AverageEpisodeLength | 928.62     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.94     |
| TotalNEpisodes       | 19131      |
| TotalNSamples        | 2.4355e+06 |
| ExplainedVariance    | -0.067525  |
-------------------------------------
[2018-01-21 13:48:55.083029 UTC] Saving snapshot
[2018-01-21 13:48:55.083316 UTC] Starting iteration 488
[2018-01-21 13:48:55.083481 UTC] Start collecting samples
[2018-01-21 13:48:59.580211 UTC] Computing input variables for policy optimization
[2018-01-21 13:48:59.705492 UTC] Performing policy update
[2018-01-21 13:48:59.706172 UTC] Computing gradient in Euclidean space
[2018-01-21 13:48:59.833730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:01.291778 UTC] Performing line search
[2018-01-21 13:49:01.490069 UTC] Updating baseline
[2018-01-21 13:49:03.661165 UTC] Computing logging information
-------------------------------------
| Iteration            | 488        |
| ExpectedImprovement  | 0.018032   |
| ActualImprovement    | 0.016989   |
| ImprovementRatio     | 0.94211    |
| MeanKL               | 0.0071711  |
| Entropy              | 0.060854   |
| Perplexity           | 1.0627     |
| AveragePolicyStd     | 0.24776    |
| AveragePolicyStd[0]  | 0.24999    |
| AveragePolicyStd[1]  | 0.32134    |
| AveragePolicyStd[2]  | 0.18252    |
| AveragePolicyStd[3]  | 0.25308    |
| AveragePolicyStd[4]  | 0.23403    |
| AveragePolicyStd[5]  | 0.24562    |
| AverageReturn        | 1103.8     |
| MinReturn            | 213.85     |
| MaxReturn            | 1238.6     |
| StdReturn            | 221.17     |
| AverageEpisodeLength | 933.98     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.9      |
| TotalNEpisodes       | 19136      |
| TotalNSamples        | 2.4405e+06 |
| ExplainedVariance    | 0.018733   |
-------------------------------------
[2018-01-21 13:49:04.393704 UTC] Saving snapshot
[2018-01-21 13:49:04.393949 UTC] Starting iteration 489
[2018-01-21 13:49:04.394100 UTC] Start collecting samples
[2018-01-21 13:49:09.022530 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:09.146538 UTC] Performing policy update
[2018-01-21 13:49:09.147178 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:09.273862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:10.673953 UTC] Performing line search
[2018-01-21 13:49:10.857835 UTC] Updating baseline
[2018-01-21 13:49:13.156898 UTC] Computing logging information
-------------------------------------
| Iteration            | 489        |
| ExpectedImprovement  | 0.016074   |
| ActualImprovement    | 0.015142   |
| ImprovementRatio     | 0.94199    |
| MeanKL               | 0.0080145  |
| Entropy              | 0.055687   |
| Perplexity           | 1.0573     |
| AveragePolicyStd     | 0.24759    |
| AveragePolicyStd[0]  | 0.24865    |
| AveragePolicyStd[1]  | 0.32166    |
| AveragePolicyStd[2]  | 0.18214    |
| AveragePolicyStd[3]  | 0.25357    |
| AveragePolicyStd[4]  | 0.23349    |
| AveragePolicyStd[5]  | 0.24604    |
| AverageReturn        | 1106.5     |
| MinReturn            | 213.85     |
| MaxReturn            | 1238.6     |
| StdReturn            | 222.22     |
| AverageEpisodeLength | 933.98     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.9      |
| TotalNEpisodes       | 19143      |
| TotalNSamples        | 2.4475e+06 |
| ExplainedVariance    | 0.07537    |
-------------------------------------
[2018-01-21 13:49:13.890613 UTC] Saving snapshot
[2018-01-21 13:49:13.890849 UTC] Starting iteration 490
[2018-01-21 13:49:13.890996 UTC] Start collecting samples
[2018-01-21 13:49:18.474465 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:18.600342 UTC] Performing policy update
[2018-01-21 13:49:18.601005 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:18.724328 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:20.154514 UTC] Performing line search
[2018-01-21 13:49:20.343625 UTC] Updating baseline
[2018-01-21 13:49:22.544407 UTC] Computing logging information
------------------------------------
| Iteration            | 490       |
| ExpectedImprovement  | 0.017285  |
| ActualImprovement    | 0.016127  |
| ImprovementRatio     | 0.93297   |
| MeanKL               | 0.0074791 |
| Entropy              | 0.050005  |
| Perplexity           | 1.0513    |
| AveragePolicyStd     | 0.24741   |
| AveragePolicyStd[0]  | 0.24902   |
| AveragePolicyStd[1]  | 0.32124   |
| AveragePolicyStd[2]  | 0.1813    |
| AveragePolicyStd[3]  | 0.2541    |
| AveragePolicyStd[4]  | 0.23181   |
| AveragePolicyStd[5]  | 0.24701   |
| AverageReturn        | 1106.6    |
| MinReturn            | 213.85    |
| MaxReturn            | 1238.6    |
| StdReturn            | 221.76    |
| AverageEpisodeLength | 934.21    |
| MinEpisodeLength     | 210       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 173.3     |
| TotalNEpisodes       | 19147     |
| TotalNSamples        | 2.451e+06 |
| ExplainedVariance    | 0.2184    |
------------------------------------
[2018-01-21 13:49:23.211309 UTC] Saving snapshot
[2018-01-21 13:49:23.220977 UTC] Starting iteration 491
[2018-01-21 13:49:23.221209 UTC] Start collecting samples
[2018-01-21 13:49:27.690145 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:27.813489 UTC] Performing policy update
[2018-01-21 13:49:27.814565 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:27.935567 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:29.381846 UTC] Performing line search
[2018-01-21 13:49:29.577761 UTC] Updating baseline
[2018-01-21 13:49:31.745835 UTC] Computing logging information
-------------------------------------
| Iteration            | 491        |
| ExpectedImprovement  | 0.017388   |
| ActualImprovement    | 0.016197   |
| ImprovementRatio     | 0.93152    |
| MeanKL               | 0.0080558  |
| Entropy              | 0.028422   |
| Perplexity           | 1.0288     |
| AveragePolicyStd     | 0.24653    |
| AveragePolicyStd[0]  | 0.24784    |
| AveragePolicyStd[1]  | 0.32055    |
| AveragePolicyStd[2]  | 0.18106    |
| AveragePolicyStd[3]  | 0.25378    |
| AveragePolicyStd[4]  | 0.22998    |
| AveragePolicyStd[5]  | 0.24597    |
| AverageReturn        | 1113.7     |
| MinReturn            | 213.85     |
| MaxReturn            | 1239.7     |
| StdReturn            | 210.8      |
| AverageEpisodeLength | 939.62     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.81     |
| TotalNEpisodes       | 19152      |
| TotalNSamples        | 2.4559e+06 |
| ExplainedVariance    | 0.19827    |
-------------------------------------
[2018-01-21 13:49:32.414724 UTC] Saving snapshot
[2018-01-21 13:49:32.415235 UTC] Starting iteration 492
[2018-01-21 13:49:32.415645 UTC] Start collecting samples
[2018-01-21 13:49:36.908087 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:37.039174 UTC] Performing policy update
[2018-01-21 13:49:37.039972 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:37.175759 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:38.591001 UTC] Performing line search
[2018-01-21 13:49:38.775760 UTC] Updating baseline
[2018-01-21 13:49:43.452962 UTC] Computing logging information
-------------------------------------
| Iteration            | 492        |
| ExpectedImprovement  | 0.017182   |
| ActualImprovement    | 0.016456   |
| ImprovementRatio     | 0.9577     |
| MeanKL               | 0.0076106  |
| Entropy              | 0.021394   |
| Perplexity           | 1.0216     |
| AveragePolicyStd     | 0.24623    |
| AveragePolicyStd[0]  | 0.24781    |
| AveragePolicyStd[1]  | 0.3199     |
| AveragePolicyStd[2]  | 0.18095    |
| AveragePolicyStd[3]  | 0.2534     |
| AveragePolicyStd[4]  | 0.22908    |
| AveragePolicyStd[5]  | 0.24626    |
| AverageReturn        | 1122.5     |
| MinReturn            | 213.85     |
| MaxReturn            | 1243.8     |
| StdReturn            | 205.34     |
| AverageEpisodeLength | 943.82     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.88     |
| TotalNEpisodes       | 19158      |
| TotalNSamples        | 2.4619e+06 |
| ExplainedVariance    | -0.054596  |
-------------------------------------
[2018-01-21 13:49:44.129553 UTC] Saving snapshot
[2018-01-21 13:49:44.129837 UTC] Starting iteration 493
[2018-01-21 13:49:44.130017 UTC] Start collecting samples
[2018-01-21 13:49:48.603565 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:48.723401 UTC] Performing policy update
[2018-01-21 13:49:48.724175 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:48.841544 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:50.278860 UTC] Performing line search
[2018-01-21 13:49:50.489445 UTC] Updating baseline
[2018-01-21 13:49:52.271226 UTC] Computing logging information
-------------------------------------
| Iteration            | 493        |
| ExpectedImprovement  | 0.017356   |
| ActualImprovement    | 0.016312   |
| ImprovementRatio     | 0.93986    |
| MeanKL               | 0.0074673  |
| Entropy              | 0.0050209  |
| Perplexity           | 1.005      |
| AveragePolicyStd     | 0.24547    |
| AveragePolicyStd[0]  | 0.24777    |
| AveragePolicyStd[1]  | 0.31686    |
| AveragePolicyStd[2]  | 0.18055    |
| AveragePolicyStd[3]  | 0.25292    |
| AveragePolicyStd[4]  | 0.22877    |
| AveragePolicyStd[5]  | 0.24595    |
| AverageReturn        | 1139.6     |
| MinReturn            | 213.85     |
| MaxReturn            | 1244.5     |
| StdReturn            | 180.24     |
| AverageEpisodeLength | 955.62     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.5      |
| TotalNEpisodes       | 19163      |
| TotalNSamples        | 2.4669e+06 |
| ExplainedVariance    | 0.0022044  |
-------------------------------------
[2018-01-21 13:49:52.997113 UTC] Saving snapshot
[2018-01-21 13:49:52.997397 UTC] Starting iteration 494
[2018-01-21 13:49:52.997574 UTC] Start collecting samples
[2018-01-21 13:49:57.439638 UTC] Computing input variables for policy optimization
[2018-01-21 13:49:57.557507 UTC] Performing policy update
[2018-01-21 13:49:57.558154 UTC] Computing gradient in Euclidean space
[2018-01-21 13:49:57.676645 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:49:59.076589 UTC] Performing line search
[2018-01-21 13:49:59.264724 UTC] Updating baseline
[2018-01-21 13:50:01.310185 UTC] Computing logging information
--------------------------------------
| Iteration            | 494         |
| ExpectedImprovement  | 0.016129    |
| ActualImprovement    | 0.015206    |
| ImprovementRatio     | 0.94279     |
| MeanKL               | 0.0079419   |
| Entropy              | 0.0032718   |
| Perplexity           | 1.0033      |
| AveragePolicyStd     | 0.2454      |
| AveragePolicyStd[0]  | 0.24754     |
| AveragePolicyStd[1]  | 0.3165      |
| AveragePolicyStd[2]  | 0.18021     |
| AveragePolicyStd[3]  | 0.25308     |
| AveragePolicyStd[4]  | 0.22946     |
| AveragePolicyStd[5]  | 0.2456      |
| AverageReturn        | 1139.7      |
| MinReturn            | 213.85      |
| MaxReturn            | 1244.5      |
| StdReturn            | 180.26      |
| AverageEpisodeLength | 955.62      |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 140.5       |
| TotalNEpisodes       | 19167       |
| TotalNSamples        | 2.4709e+06  |
| ExplainedVariance    | -0.00082521 |
--------------------------------------
[2018-01-21 13:50:01.998717 UTC] Saving snapshot
[2018-01-21 13:50:01.998897 UTC] Starting iteration 495
[2018-01-21 13:50:01.999010 UTC] Start collecting samples
[2018-01-21 13:50:06.314418 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:06.446246 UTC] Performing policy update
[2018-01-21 13:50:06.447223 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:06.569944 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:08.002916 UTC] Performing line search
[2018-01-21 13:50:08.196584 UTC] Updating baseline
[2018-01-21 13:50:09.844648 UTC] Computing logging information
-------------------------------------
| Iteration            | 495        |
| ExpectedImprovement  | 0.014526   |
| ActualImprovement    | 0.013957   |
| ImprovementRatio     | 0.9608     |
| MeanKL               | 0.0079568  |
| Entropy              | 0.0097343  |
| Perplexity           | 1.0098     |
| AveragePolicyStd     | 0.2456     |
| AveragePolicyStd[0]  | 0.24864    |
| AveragePolicyStd[1]  | 0.31564    |
| AveragePolicyStd[2]  | 0.18068    |
| AveragePolicyStd[3]  | 0.2538     |
| AveragePolicyStd[4]  | 0.22993    |
| AveragePolicyStd[5]  | 0.24492    |
| AverageReturn        | 1141.2     |
| MinReturn            | 213.85     |
| MaxReturn            | 1244.5     |
| StdReturn            | 180.7      |
| AverageEpisodeLength | 955.62     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.5      |
| TotalNEpisodes       | 19170      |
| TotalNSamples        | 2.4739e+06 |
| ExplainedVariance    | -0.013713  |
-------------------------------------
[2018-01-21 13:50:10.546985 UTC] Saving snapshot
[2018-01-21 13:50:10.547554 UTC] Starting iteration 496
[2018-01-21 13:50:10.547982 UTC] Start collecting samples
[2018-01-21 13:50:15.173896 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:15.318286 UTC] Performing policy update
[2018-01-21 13:50:15.318851 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:15.448957 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:16.886062 UTC] Performing line search
[2018-01-21 13:50:17.087657 UTC] Updating baseline
[2018-01-21 13:50:20.276633 UTC] Computing logging information
-------------------------------------
| Iteration            | 496        |
| ExpectedImprovement  | 0.015816   |
| ActualImprovement    | 0.015401   |
| ImprovementRatio     | 0.97378    |
| MeanKL               | 0.007792   |
| Entropy              | 0.015476   |
| Perplexity           | 1.0156     |
| AveragePolicyStd     | 0.24577    |
| AveragePolicyStd[0]  | 0.24899    |
| AveragePolicyStd[1]  | 0.31503    |
| AveragePolicyStd[2]  | 0.18167    |
| AveragePolicyStd[3]  | 0.25365    |
| AveragePolicyStd[4]  | 0.22919    |
| AveragePolicyStd[5]  | 0.24608    |
| AverageReturn        | 1147       |
| MinReturn            | 213.85     |
| MaxReturn            | 1249.5     |
| StdReturn            | 180.38     |
| AverageEpisodeLength | 957.92     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.36     |
| TotalNEpisodes       | 19179      |
| TotalNSamples        | 2.4829e+06 |
| ExplainedVariance    | 0.00017196 |
-------------------------------------
[2018-01-21 13:50:20.943232 UTC] Saving snapshot
[2018-01-21 13:50:20.943471 UTC] Starting iteration 497
[2018-01-21 13:50:20.943617 UTC] Start collecting samples
[2018-01-21 13:50:25.493042 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:25.617329 UTC] Performing policy update
[2018-01-21 13:50:25.618455 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:25.739553 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:27.172642 UTC] Performing line search
[2018-01-21 13:50:27.355906 UTC] Updating baseline
[2018-01-21 13:50:30.601017 UTC] Computing logging information
-------------------------------------
| Iteration            | 497        |
| ExpectedImprovement  | 0.018251   |
| ActualImprovement    | 0.0175     |
| ImprovementRatio     | 0.95883    |
| MeanKL               | 0.0072578  |
| Entropy              | 0.019723   |
| Perplexity           | 1.0199     |
| AveragePolicyStd     | 0.2459     |
| AveragePolicyStd[0]  | 0.24896    |
| AveragePolicyStd[1]  | 0.3143     |
| AveragePolicyStd[2]  | 0.18199    |
| AveragePolicyStd[3]  | 0.25395    |
| AveragePolicyStd[4]  | 0.22867    |
| AveragePolicyStd[5]  | 0.24756    |
| AverageReturn        | 1147.8     |
| MinReturn            | 213.85     |
| MaxReturn            | 1249.5     |
| StdReturn            | 180.63     |
| AverageEpisodeLength | 957.92     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.36     |
| TotalNEpisodes       | 19183      |
| TotalNSamples        | 2.4869e+06 |
| ExplainedVariance    | -0.011728  |
-------------------------------------
[2018-01-21 13:50:31.257030 UTC] Saving snapshot
[2018-01-21 13:50:31.257238 UTC] Starting iteration 498
[2018-01-21 13:50:31.257405 UTC] Start collecting samples
[2018-01-21 13:50:35.703847 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:35.821121 UTC] Performing policy update
[2018-01-21 13:50:35.822190 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:35.945073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:37.350012 UTC] Performing line search
[2018-01-21 13:50:37.542731 UTC] Updating baseline
[2018-01-21 13:50:39.411629 UTC] Computing logging information
-------------------------------------
| Iteration            | 498        |
| ExpectedImprovement  | 0.018104   |
| ActualImprovement    | 0.016084   |
| ImprovementRatio     | 0.88839    |
| MeanKL               | 0.0072762  |
| Entropy              | 0.02253    |
| Perplexity           | 1.0228     |
| AveragePolicyStd     | 0.24597    |
| AveragePolicyStd[0]  | 0.24904    |
| AveragePolicyStd[1]  | 0.31377    |
| AveragePolicyStd[2]  | 0.18255    |
| AveragePolicyStd[3]  | 0.25449    |
| AveragePolicyStd[4]  | 0.22849    |
| AveragePolicyStd[5]  | 0.24749    |
| AverageReturn        | 1155.9     |
| MinReturn            | 213.85     |
| MaxReturn            | 1272.4     |
| StdReturn            | 167.82     |
| AverageEpisodeLength | 963.5      |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.4      |
| TotalNEpisodes       | 19185      |
| TotalNSamples        | 2.4889e+06 |
| ExplainedVariance    | 0.023524   |
-------------------------------------
[2018-01-21 13:50:40.041405 UTC] Saving snapshot
[2018-01-21 13:50:40.041630 UTC] Starting iteration 499
[2018-01-21 13:50:40.041808 UTC] Start collecting samples
[2018-01-21 13:50:44.635239 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:44.760964 UTC] Performing policy update
[2018-01-21 13:50:44.761627 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:44.894645 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:46.356082 UTC] Performing line search
[2018-01-21 13:50:46.560716 UTC] Updating baseline
[2018-01-21 13:50:48.348339 UTC] Computing logging information
-------------------------------------
| Iteration            | 499        |
| ExpectedImprovement  | 0.016456   |
| ActualImprovement    | 0.0159     |
| ImprovementRatio     | 0.9662     |
| MeanKL               | 0.0072887  |
| Entropy              | 0.016236   |
| Perplexity           | 1.0164     |
| AveragePolicyStd     | 0.24574    |
| AveragePolicyStd[0]  | 0.24849    |
| AveragePolicyStd[1]  | 0.31354    |
| AveragePolicyStd[2]  | 0.18197    |
| AveragePolicyStd[3]  | 0.25463    |
| AveragePolicyStd[4]  | 0.22816    |
| AveragePolicyStd[5]  | 0.24767    |
| AverageReturn        | 1178.3     |
| MinReturn            | 541.62     |
| MaxReturn            | 1272.4     |
| StdReturn            | 121.92     |
| AverageEpisodeLength | 978.73     |
| MinEpisodeLength     | 460        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 92.596     |
| TotalNEpisodes       | 19194      |
| TotalNSamples        | 2.4979e+06 |
| ExplainedVariance    | 0.084956   |
-------------------------------------
[2018-01-21 13:50:49.096254 UTC] Saving snapshot
[2018-01-21 13:50:49.096516 UTC] Starting iteration 500
[2018-01-21 13:50:49.096714 UTC] Start collecting samples
[2018-01-21 13:50:53.605050 UTC] Computing input variables for policy optimization
[2018-01-21 13:50:53.723082 UTC] Performing policy update
[2018-01-21 13:50:53.723708 UTC] Computing gradient in Euclidean space
[2018-01-21 13:50:53.842972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:50:55.271244 UTC] Performing line search
[2018-01-21 13:50:55.456648 UTC] Updating baseline
[2018-01-21 13:50:57.249685 UTC] Computing logging information
-------------------------------------
| Iteration            | 500        |
| ExpectedImprovement  | 0.016393   |
| ActualImprovement    | 0.015541   |
| ImprovementRatio     | 0.94805    |
| MeanKL               | 0.007627   |
| Entropy              | 0.012571   |
| Perplexity           | 1.0127     |
| AveragePolicyStd     | 0.24564    |
| AveragePolicyStd[0]  | 0.24802    |
| AveragePolicyStd[1]  | 0.31437    |
| AveragePolicyStd[2]  | 0.18182    |
| AveragePolicyStd[3]  | 0.25492    |
| AveragePolicyStd[4]  | 0.22766    |
| AveragePolicyStd[5]  | 0.24704    |
| AverageReturn        | 1178       |
| MinReturn            | 451.64     |
| MaxReturn            | 1272.4     |
| StdReturn            | 132.14     |
| AverageEpisodeLength | 977.19     |
| MinEpisodeLength     | 419        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.29     |
| TotalNEpisodes       | 19199      |
| TotalNSamples        | 2.5023e+06 |
| ExplainedVariance    | 0.16929    |
-------------------------------------
[2018-01-21 13:50:57.887779 UTC] Saving snapshot
[2018-01-21 13:50:57.894817 UTC] Starting iteration 501
[2018-01-21 13:50:57.895013 UTC] Start collecting samples
[2018-01-21 13:51:02.380884 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:02.506653 UTC] Performing policy update
[2018-01-21 13:51:02.507277 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:02.639160 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:04.131505 UTC] Performing line search
[2018-01-21 13:51:04.354497 UTC] Updating baseline
[2018-01-21 13:51:06.254920 UTC] Computing logging information
------------------------------------
| Iteration            | 501       |
| ExpectedImprovement  | 0.017835  |
| ActualImprovement    | 0.016124  |
| ImprovementRatio     | 0.90404   |
| MeanKL               | 0.0075682 |
| Entropy              | 0.009791  |
| Perplexity           | 1.0098    |
| AveragePolicyStd     | 0.24552   |
| AveragePolicyStd[0]  | 0.24803   |
| AveragePolicyStd[1]  | 0.31427   |
| AveragePolicyStd[2]  | 0.18175   |
| AveragePolicyStd[3]  | 0.25395   |
| AveragePolicyStd[4]  | 0.22757   |
| AveragePolicyStd[5]  | 0.24756   |
| AverageReturn        | 1174.7    |
| MinReturn            | 451.64    |
| MaxReturn            | 1272.4    |
| StdReturn            | 136.8     |
| AverageEpisodeLength | 974.16    |
| MinEpisodeLength     | 419       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 104.06    |
| TotalNEpisodes       | 19202     |
| TotalNSamples        | 2.505e+06 |
| ExplainedVariance    | 0.38395   |
------------------------------------
[2018-01-21 13:51:06.951847 UTC] Saving snapshot
[2018-01-21 13:51:06.952131 UTC] Starting iteration 502
[2018-01-21 13:51:06.952322 UTC] Start collecting samples
[2018-01-21 13:51:11.546952 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:11.673998 UTC] Performing policy update
[2018-01-21 13:51:11.674600 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:11.801513 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:13.221774 UTC] Performing line search
[2018-01-21 13:51:13.408753 UTC] Updating baseline
[2018-01-21 13:51:16.426219 UTC] Computing logging information
------------------------------------
| Iteration            | 502       |
| ExpectedImprovement  | 0.016729  |
| ActualImprovement    | 0.016349  |
| ImprovementRatio     | 0.97728   |
| MeanKL               | 0.0073121 |
| Entropy              | 0.0030292 |
| Perplexity           | 1.003     |
| AveragePolicyStd     | 0.24515   |
| AveragePolicyStd[0]  | 0.2486    |
| AveragePolicyStd[1]  | 0.31195   |
| AveragePolicyStd[2]  | 0.18179   |
| AveragePolicyStd[3]  | 0.25291   |
| AveragePolicyStd[4]  | 0.22777   |
| AveragePolicyStd[5]  | 0.2479    |
| AverageReturn        | 1181.1    |
| MinReturn            | 451.64    |
| MaxReturn            | 1272.4    |
| StdReturn            | 134.07    |
| AverageEpisodeLength | 976.46    |
| MinEpisodeLength     | 419       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 102.05    |
| TotalNEpisodes       | 19208     |
| TotalNSamples        | 2.511e+06 |
| ExplainedVariance    | -0.081554 |
------------------------------------
[2018-01-21 13:51:17.088781 UTC] Saving snapshot
[2018-01-21 13:51:17.089048 UTC] Starting iteration 503
[2018-01-21 13:51:17.089203 UTC] Start collecting samples
[2018-01-21 13:51:21.662188 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:21.796740 UTC] Performing policy update
[2018-01-21 13:51:21.797430 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:21.922488 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:23.344391 UTC] Performing line search
[2018-01-21 13:51:23.552398 UTC] Updating baseline
[2018-01-21 13:51:25.323324 UTC] Computing logging information
------------------------------------
| Iteration            | 503       |
| ExpectedImprovement  | 0.016608  |
| ActualImprovement    | 0.015572  |
| ImprovementRatio     | 0.93759   |
| MeanKL               | 0.0075796 |
| Entropy              | 0.0052176 |
| Perplexity           | 1.0052    |
| AveragePolicyStd     | 0.24528   |
| AveragePolicyStd[0]  | 0.24903   |
| AveragePolicyStd[1]  | 0.31151   |
| AveragePolicyStd[2]  | 0.18088   |
| AveragePolicyStd[3]  | 0.25357   |
| AveragePolicyStd[4]  | 0.2283    |
| AveragePolicyStd[5]  | 0.24837   |
| AverageReturn        | 1181.9    |
| MinReturn            | 451.64    |
| MaxReturn            | 1272.4    |
| StdReturn            | 134.22    |
| AverageEpisodeLength | 976.5     |
| MinEpisodeLength     | 419       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 102.06    |
| TotalNEpisodes       | 19213     |
| TotalNSamples        | 2.516e+06 |
| ExplainedVariance    | 0.0033807 |
------------------------------------
[2018-01-21 13:51:26.038273 UTC] Saving snapshot
[2018-01-21 13:51:26.038518 UTC] Starting iteration 504
[2018-01-21 13:51:26.038665 UTC] Start collecting samples
[2018-01-21 13:51:30.630239 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:30.771559 UTC] Performing policy update
[2018-01-21 13:51:30.772584 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:30.893671 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:32.284135 UTC] Performing line search
[2018-01-21 13:51:32.473081 UTC] Updating baseline
[2018-01-21 13:51:34.696411 UTC] Computing logging information
-------------------------------------
| Iteration            | 504        |
| ExpectedImprovement  | 0.017283   |
| ActualImprovement    | 0.016173   |
| ImprovementRatio     | 0.93582    |
| MeanKL               | 0.0071221  |
| Entropy              | 0.0042634  |
| Perplexity           | 1.0043     |
| AveragePolicyStd     | 0.24521    |
| AveragePolicyStd[0]  | 0.24896    |
| AveragePolicyStd[1]  | 0.3109     |
| AveragePolicyStd[2]  | 0.18089    |
| AveragePolicyStd[3]  | 0.2535     |
| AveragePolicyStd[4]  | 0.22898    |
| AveragePolicyStd[5]  | 0.24802    |
| AverageReturn        | 1181.6     |
| MinReturn            | 390.19     |
| MaxReturn            | 1272.4     |
| StdReturn            | 142.21     |
| AverageEpisodeLength | 975.28     |
| MinEpisodeLength     | 338        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.73     |
| TotalNEpisodes       | 19219      |
| TotalNSamples        | 2.5214e+06 |
| ExplainedVariance    | 0.082717   |
-------------------------------------
[2018-01-21 13:51:35.432211 UTC] Saving snapshot
[2018-01-21 13:51:35.432407 UTC] Starting iteration 505
[2018-01-21 13:51:35.432588 UTC] Start collecting samples
[2018-01-21 13:51:40.043763 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:40.172835 UTC] Performing policy update
[2018-01-21 13:51:40.173449 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:40.291688 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:41.696906 UTC] Performing line search
[2018-01-21 13:51:41.892211 UTC] Updating baseline
[2018-01-21 13:51:43.824873 UTC] Computing logging information
-------------------------------------
| Iteration            | 505        |
| ExpectedImprovement  | 0.017352   |
| ActualImprovement    | 0.016451   |
| ImprovementRatio     | 0.94808    |
| MeanKL               | 0.007574   |
| Entropy              | 0.0056     |
| Perplexity           | 1.0056     |
| AveragePolicyStd     | 0.24528    |
| AveragePolicyStd[0]  | 0.24872    |
| AveragePolicyStd[1]  | 0.3109     |
| AveragePolicyStd[2]  | 0.18074    |
| AveragePolicyStd[3]  | 0.25383    |
| AveragePolicyStd[4]  | 0.22891    |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 1177.2     |
| MinReturn            | 210.67     |
| MaxReturn            | 1272.4     |
| StdReturn            | 168.27     |
| AverageEpisodeLength | 970.71     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.05     |
| TotalNEpisodes       | 19225      |
| TotalNSamples        | 2.5266e+06 |
| ExplainedVariance    | 0.13969    |
-------------------------------------
[2018-01-21 13:51:44.477119 UTC] Saving snapshot
[2018-01-21 13:51:44.477369 UTC] Starting iteration 506
[2018-01-21 13:51:44.477550 UTC] Start collecting samples
[2018-01-21 13:51:49.043374 UTC] Computing input variables for policy optimization
[2018-01-21 13:51:49.173446 UTC] Performing policy update
[2018-01-21 13:51:49.174069 UTC] Computing gradient in Euclidean space
[2018-01-21 13:51:49.301718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:51:50.751606 UTC] Performing line search
[2018-01-21 13:51:50.942222 UTC] Updating baseline
[2018-01-21 13:51:54.692775 UTC] Computing logging information
-------------------------------------
| Iteration            | 506        |
| ExpectedImprovement  | 0.017803   |
| ActualImprovement    | 0.016968   |
| ImprovementRatio     | 0.95313    |
| MeanKL               | 0.0077194  |
| Entropy              | 0.010977   |
| Perplexity           | 1.011      |
| AveragePolicyStd     | 0.24543    |
| AveragePolicyStd[0]  | 0.24965    |
| AveragePolicyStd[1]  | 0.31001    |
| AveragePolicyStd[2]  | 0.18146    |
| AveragePolicyStd[3]  | 0.25405    |
| AveragePolicyStd[4]  | 0.22882    |
| AveragePolicyStd[5]  | 0.24857    |
| AverageReturn        | 1178.4     |
| MinReturn            | 210.67     |
| MaxReturn            | 1272.4     |
| StdReturn            | 168.44     |
| AverageEpisodeLength | 970.71     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.05     |
| TotalNEpisodes       | 19229      |
| TotalNSamples        | 2.5306e+06 |
| ExplainedVariance    | -0.047489  |
-------------------------------------
[2018-01-21 13:51:55.316655 UTC] Saving snapshot
[2018-01-21 13:51:55.316891 UTC] Starting iteration 507
[2018-01-21 13:51:55.317039 UTC] Start collecting samples
[2018-01-21 13:51:59.924273 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:00.071276 UTC] Performing policy update
[2018-01-21 13:52:00.072241 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:00.191495 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:01.684207 UTC] Performing line search
[2018-01-21 13:52:01.880446 UTC] Updating baseline
[2018-01-21 13:52:04.150297 UTC] Computing logging information
-------------------------------------
| Iteration            | 507        |
| ExpectedImprovement  | 0.017503   |
| ActualImprovement    | 0.016728   |
| ImprovementRatio     | 0.95575    |
| MeanKL               | 0.0077686  |
| Entropy              | 0.0088934  |
| Perplexity           | 1.0089     |
| AveragePolicyStd     | 0.24537    |
| AveragePolicyStd[0]  | 0.24895    |
| AveragePolicyStd[1]  | 0.3103     |
| AveragePolicyStd[2]  | 0.18137    |
| AveragePolicyStd[3]  | 0.25435    |
| AveragePolicyStd[4]  | 0.22798    |
| AveragePolicyStd[5]  | 0.24926    |
| AverageReturn        | 1179.1     |
| MinReturn            | 210.67     |
| MaxReturn            | 1272.4     |
| StdReturn            | 168.63     |
| AverageEpisodeLength | 970.71     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.05     |
| TotalNEpisodes       | 19235      |
| TotalNSamples        | 2.5366e+06 |
| ExplainedVariance    | 0.0033967  |
-------------------------------------
[2018-01-21 13:52:04.813944 UTC] Saving snapshot
[2018-01-21 13:52:04.814171 UTC] Starting iteration 508
[2018-01-21 13:52:04.814345 UTC] Start collecting samples
[2018-01-21 13:52:09.415926 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:09.576967 UTC] Performing policy update
[2018-01-21 13:52:09.577645 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:09.697744 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:11.100549 UTC] Performing line search
[2018-01-21 13:52:11.286037 UTC] Updating baseline
[2018-01-21 13:52:13.140735 UTC] Computing logging information
-------------------------------------
| Iteration            | 508        |
| ExpectedImprovement  | 0.017959   |
| ActualImprovement    | 0.016485   |
| ImprovementRatio     | 0.91794    |
| MeanKL               | 0.0075976  |
| Entropy              | 0.0092233  |
| Perplexity           | 1.0093     |
| AveragePolicyStd     | 0.24533    |
| AveragePolicyStd[0]  | 0.2494     |
| AveragePolicyStd[1]  | 0.30817    |
| AveragePolicyStd[2]  | 0.18116    |
| AveragePolicyStd[3]  | 0.25416    |
| AveragePolicyStd[4]  | 0.22804    |
| AveragePolicyStd[5]  | 0.25103    |
| AverageReturn        | 1182.9     |
| MinReturn            | 210.67     |
| MaxReturn            | 1278.2     |
| StdReturn            | 168.96     |
| AverageEpisodeLength | 970.71     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.05     |
| TotalNEpisodes       | 19240      |
| TotalNSamples        | 2.5416e+06 |
| ExplainedVariance    | 0.010497   |
-------------------------------------
[2018-01-21 13:52:13.763491 UTC] Saving snapshot
[2018-01-21 13:52:13.763697 UTC] Starting iteration 509
[2018-01-21 13:52:13.763838 UTC] Start collecting samples
[2018-01-21 13:52:18.266969 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:18.397568 UTC] Performing policy update
[2018-01-21 13:52:18.398210 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:18.518795 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:19.946138 UTC] Performing line search
[2018-01-21 13:52:20.132425 UTC] Updating baseline
[2018-01-21 13:52:22.020191 UTC] Computing logging information
-------------------------------------
| Iteration            | 509        |
| ExpectedImprovement  | 0.016207   |
| ActualImprovement    | 0.015409   |
| ImprovementRatio     | 0.95072    |
| MeanKL               | 0.0074465  |
| Entropy              | 0.006017   |
| Perplexity           | 1.006      |
| AveragePolicyStd     | 0.24522    |
| AveragePolicyStd[0]  | 0.24982    |
| AveragePolicyStd[1]  | 0.30795    |
| AveragePolicyStd[2]  | 0.18052    |
| AveragePolicyStd[3]  | 0.25419    |
| AveragePolicyStd[4]  | 0.22841    |
| AveragePolicyStd[5]  | 0.25043    |
| AverageReturn        | 1188.9     |
| MinReturn            | 210.67     |
| MaxReturn            | 1278.2     |
| StdReturn            | 157.27     |
| AverageEpisodeLength | 974.59     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.02     |
| TotalNEpisodes       | 19245      |
| TotalNSamples        | 2.5464e+06 |
| ExplainedVariance    | 0.077013   |
-------------------------------------
[2018-01-21 13:52:22.673980 UTC] Saving snapshot
[2018-01-21 13:52:22.674217 UTC] Starting iteration 510
[2018-01-21 13:52:22.674371 UTC] Start collecting samples
[2018-01-21 13:52:27.297078 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:27.424007 UTC] Performing policy update
[2018-01-21 13:52:27.425130 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:27.544960 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:28.971014 UTC] Performing line search
[2018-01-21 13:52:29.158135 UTC] Updating baseline
[2018-01-21 13:52:31.583449 UTC] Computing logging information
-------------------------------------
| Iteration            | 510        |
| ExpectedImprovement  | 0.018266   |
| ActualImprovement    | 0.01721    |
| ImprovementRatio     | 0.9422     |
| MeanKL               | 0.0074448  |
| Entropy              | 0.0047028  |
| Perplexity           | 1.0047     |
| AveragePolicyStd     | 0.24517    |
| AveragePolicyStd[0]  | 0.25063    |
| AveragePolicyStd[1]  | 0.30733    |
| AveragePolicyStd[2]  | 0.1799     |
| AveragePolicyStd[3]  | 0.25407    |
| AveragePolicyStd[4]  | 0.22927    |
| AveragePolicyStd[5]  | 0.24984    |
| AverageReturn        | 1190.3     |
| MinReturn            | 210.67     |
| MaxReturn            | 1278.2     |
| StdReturn            | 157.52     |
| AverageEpisodeLength | 974.59     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.02     |
| TotalNEpisodes       | 19249      |
| TotalNSamples        | 2.5504e+06 |
| ExplainedVariance    | -0.0088743 |
-------------------------------------
[2018-01-21 13:52:32.252992 UTC] Saving snapshot
[2018-01-21 13:52:32.260989 UTC] Starting iteration 511
[2018-01-21 13:52:32.261179 UTC] Start collecting samples
[2018-01-21 13:52:36.627936 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:36.757940 UTC] Performing policy update
[2018-01-21 13:52:36.758568 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:36.888344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:38.312689 UTC] Performing line search
[2018-01-21 13:52:38.505578 UTC] Updating baseline
[2018-01-21 13:52:41.201437 UTC] Computing logging information
-------------------------------------
| Iteration            | 511        |
| ExpectedImprovement  | 0.014642   |
| ActualImprovement    | 0.013709   |
| ImprovementRatio     | 0.93627    |
| MeanKL               | 0.0075337  |
| Entropy              | 0.0015607  |
| Perplexity           | 1.0016     |
| AveragePolicyStd     | 0.24502    |
| AveragePolicyStd[0]  | 0.25065    |
| AveragePolicyStd[1]  | 0.30675    |
| AveragePolicyStd[2]  | 0.17993    |
| AveragePolicyStd[3]  | 0.2541     |
| AveragePolicyStd[4]  | 0.22915    |
| AveragePolicyStd[5]  | 0.24956    |
| AverageReturn        | 1191.3     |
| MinReturn            | 210.67     |
| MaxReturn            | 1289.3     |
| StdReturn            | 160.07     |
| AverageEpisodeLength | 972.86     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.14     |
| TotalNEpisodes       | 19254      |
| TotalNSamples        | 2.5552e+06 |
| ExplainedVariance    | 0.15387    |
-------------------------------------
[2018-01-21 13:52:41.872240 UTC] Saving snapshot
[2018-01-21 13:52:41.872494 UTC] Starting iteration 512
[2018-01-21 13:52:41.872657 UTC] Start collecting samples
[2018-01-21 13:52:46.487795 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:46.607199 UTC] Performing policy update
[2018-01-21 13:52:46.607792 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:46.728182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:48.188289 UTC] Performing line search
[2018-01-21 13:52:48.399515 UTC] Updating baseline
[2018-01-21 13:52:50.210619 UTC] Computing logging information
-------------------------------------
| Iteration            | 512        |
| ExpectedImprovement  | 0.01625    |
| ActualImprovement    | 0.015854   |
| ImprovementRatio     | 0.97564    |
| MeanKL               | 0.0078527  |
| Entropy              | 0.0021118  |
| Perplexity           | 1.0021     |
| AveragePolicyStd     | 0.24503    |
| AveragePolicyStd[0]  | 0.25062    |
| AveragePolicyStd[1]  | 0.30665    |
| AveragePolicyStd[2]  | 0.18038    |
| AveragePolicyStd[3]  | 0.25463    |
| AveragePolicyStd[4]  | 0.22834    |
| AveragePolicyStd[5]  | 0.24955    |
| AverageReturn        | 1194.6     |
| MinReturn            | 210.67     |
| MaxReturn            | 1304.9     |
| StdReturn            | 160.86     |
| AverageEpisodeLength | 972.86     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.14     |
| TotalNEpisodes       | 19261      |
| TotalNSamples        | 2.5622e+06 |
| ExplainedVariance    | 0.013616   |
-------------------------------------
[2018-01-21 13:52:50.909342 UTC] Saving snapshot
[2018-01-21 13:52:50.909657 UTC] Starting iteration 513
[2018-01-21 13:52:50.909865 UTC] Start collecting samples
[2018-01-21 13:52:55.417176 UTC] Computing input variables for policy optimization
[2018-01-21 13:52:55.562765 UTC] Performing policy update
[2018-01-21 13:52:55.563379 UTC] Computing gradient in Euclidean space
[2018-01-21 13:52:55.683610 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:52:57.109152 UTC] Performing line search
[2018-01-21 13:52:57.299148 UTC] Updating baseline
[2018-01-21 13:53:00.076802 UTC] Computing logging information
-------------------------------------
| Iteration            | 513        |
| ExpectedImprovement  | 0.016075   |
| ActualImprovement    | 0.014785   |
| ImprovementRatio     | 0.91974    |
| MeanKL               | 0.0072118  |
| Entropy              | 0.0041492  |
| Perplexity           | 1.0042     |
| AveragePolicyStd     | 0.24508    |
| AveragePolicyStd[0]  | 0.25137    |
| AveragePolicyStd[1]  | 0.30678    |
| AveragePolicyStd[2]  | 0.18104    |
| AveragePolicyStd[3]  | 0.25427    |
| AveragePolicyStd[4]  | 0.22779    |
| AveragePolicyStd[5]  | 0.24926    |
| AverageReturn        | 1192.2     |
| MinReturn            | 210.67     |
| MaxReturn            | 1304.9     |
| StdReturn            | 166.55     |
| AverageEpisodeLength | 969.53     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.84     |
| TotalNEpisodes       | 19266      |
| TotalNSamples        | 2.5669e+06 |
| ExplainedVariance    | 0.16241    |
-------------------------------------
[2018-01-21 13:53:00.754807 UTC] Saving snapshot
[2018-01-21 13:53:00.755154 UTC] Starting iteration 514
[2018-01-21 13:53:00.755385 UTC] Start collecting samples
[2018-01-21 13:53:05.249218 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:05.385880 UTC] Performing policy update
[2018-01-21 13:53:05.387066 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:05.510182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:06.961455 UTC] Performing line search
[2018-01-21 13:53:07.151192 UTC] Updating baseline
[2018-01-21 13:53:08.955997 UTC] Computing logging information
-------------------------------------
| Iteration            | 514        |
| ExpectedImprovement  | 0.015535   |
| ActualImprovement    | 0.015156   |
| ImprovementRatio     | 0.97559    |
| MeanKL               | 0.0075517  |
| Entropy              | -0.0093585 |
| Perplexity           | 0.99069    |
| AveragePolicyStd     | 0.24456    |
| AveragePolicyStd[0]  | 0.25117    |
| AveragePolicyStd[1]  | 0.30579    |
| AveragePolicyStd[2]  | 0.18039    |
| AveragePolicyStd[3]  | 0.25491    |
| AveragePolicyStd[4]  | 0.22618    |
| AveragePolicyStd[5]  | 0.24893    |
| AverageReturn        | 1193.3     |
| MinReturn            | 210.67     |
| MaxReturn            | 1304.9     |
| StdReturn            | 166.92     |
| AverageEpisodeLength | 969.53     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.84     |
| TotalNEpisodes       | 19269      |
| TotalNSamples        | 2.5699e+06 |
| ExplainedVariance    | -0.034957  |
-------------------------------------
[2018-01-21 13:53:09.639052 UTC] Saving snapshot
[2018-01-21 13:53:09.639351 UTC] Starting iteration 515
[2018-01-21 13:53:09.639557 UTC] Start collecting samples
[2018-01-21 13:53:14.118662 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:14.273622 UTC] Performing policy update
[2018-01-21 13:53:14.274266 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:14.397926 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:15.805372 UTC] Performing line search
[2018-01-21 13:53:16.005144 UTC] Updating baseline
[2018-01-21 13:53:18.307482 UTC] Computing logging information
-------------------------------------
| Iteration            | 515        |
| ExpectedImprovement  | 0.016444   |
| ActualImprovement    | 0.015364   |
| ImprovementRatio     | 0.93433    |
| MeanKL               | 0.0077778  |
| Entropy              | -0.0074522 |
| Perplexity           | 0.99258    |
| AveragePolicyStd     | 0.24462    |
| AveragePolicyStd[0]  | 0.25022    |
| AveragePolicyStd[1]  | 0.30542    |
| AveragePolicyStd[2]  | 0.18054    |
| AveragePolicyStd[3]  | 0.25537    |
| AveragePolicyStd[4]  | 0.22637    |
| AveragePolicyStd[5]  | 0.24978    |
| AverageReturn        | 1195.7     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 167.66     |
| AverageEpisodeLength | 969.53     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.84     |
| TotalNEpisodes       | 19277      |
| TotalNSamples        | 2.5779e+06 |
| ExplainedVariance    | 0.010988   |
-------------------------------------
[2018-01-21 13:53:18.990806 UTC] Saving snapshot
[2018-01-21 13:53:18.991056 UTC] Starting iteration 516
[2018-01-21 13:53:18.991240 UTC] Start collecting samples
[2018-01-21 13:53:23.403198 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:23.544397 UTC] Performing policy update
[2018-01-21 13:53:23.545004 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:23.666709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:25.081260 UTC] Performing line search
[2018-01-21 13:53:25.271891 UTC] Updating baseline
[2018-01-21 13:53:27.150013 UTC] Computing logging information
-------------------------------------
| Iteration            | 516        |
| ExpectedImprovement  | 0.01608    |
| ActualImprovement    | 0.015564   |
| ImprovementRatio     | 0.9679     |
| MeanKL               | 0.0075144  |
| Entropy              | -0.010764  |
| Perplexity           | 0.98929    |
| AveragePolicyStd     | 0.24448    |
| AveragePolicyStd[0]  | 0.25047    |
| AveragePolicyStd[1]  | 0.30487    |
| AveragePolicyStd[2]  | 0.18052    |
| AveragePolicyStd[3]  | 0.25501    |
| AveragePolicyStd[4]  | 0.22558    |
| AveragePolicyStd[5]  | 0.25041    |
| AverageReturn        | 1199.2     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 168.62     |
| AverageEpisodeLength | 969.53     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.84     |
| TotalNEpisodes       | 19282      |
| TotalNSamples        | 2.5829e+06 |
| ExplainedVariance    | 0.046678   |
-------------------------------------
[2018-01-21 13:53:27.886990 UTC] Saving snapshot
[2018-01-21 13:53:27.887252 UTC] Starting iteration 517
[2018-01-21 13:53:27.887437 UTC] Start collecting samples
[2018-01-21 13:53:32.372418 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:32.494841 UTC] Performing policy update
[2018-01-21 13:53:32.495484 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:32.616720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:34.090704 UTC] Performing line search
[2018-01-21 13:53:34.301623 UTC] Updating baseline
[2018-01-21 13:53:36.313455 UTC] Computing logging information
-------------------------------------
| Iteration            | 517        |
| ExpectedImprovement  | 0.018263   |
| ActualImprovement    | 0.016608   |
| ImprovementRatio     | 0.90936    |
| MeanKL               | 0.0070674  |
| Entropy              | -0.0082217 |
| Perplexity           | 0.99181    |
| AveragePolicyStd     | 0.24454    |
| AveragePolicyStd[0]  | 0.25092    |
| AveragePolicyStd[1]  | 0.30431    |
| AveragePolicyStd[2]  | 0.18101    |
| AveragePolicyStd[3]  | 0.25524    |
| AveragePolicyStd[4]  | 0.22526    |
| AveragePolicyStd[5]  | 0.2505     |
| AverageReturn        | 1199       |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 168.52     |
| AverageEpisodeLength | 969.53     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.84     |
| TotalNEpisodes       | 19284      |
| TotalNSamples        | 2.5849e+06 |
| ExplainedVariance    | 0.057827   |
-------------------------------------
[2018-01-21 13:53:36.989625 UTC] Saving snapshot
[2018-01-21 13:53:36.989805 UTC] Starting iteration 518
[2018-01-21 13:53:36.989908 UTC] Start collecting samples
[2018-01-21 13:53:41.620712 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:41.750110 UTC] Performing policy update
[2018-01-21 13:53:41.751174 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:41.877789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:43.291724 UTC] Performing line search
[2018-01-21 13:53:43.484371 UTC] Updating baseline
[2018-01-21 13:53:46.515039 UTC] Computing logging information
-------------------------------------
| Iteration            | 518        |
| ExpectedImprovement  | 0.016983   |
| ActualImprovement    | 0.016522   |
| ImprovementRatio     | 0.97282    |
| MeanKL               | 0.0074475  |
| Entropy              | -0.016095  |
| Perplexity           | 0.98403    |
| AveragePolicyStd     | 0.24417    |
| AveragePolicyStd[0]  | 0.24959    |
| AveragePolicyStd[1]  | 0.30383    |
| AveragePolicyStd[2]  | 0.18126    |
| AveragePolicyStd[3]  | 0.25535    |
| AveragePolicyStd[4]  | 0.22588    |
| AveragePolicyStd[5]  | 0.24913    |
| AverageReturn        | 1197       |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 172.3      |
| AverageEpisodeLength | 966.78     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.13     |
| TotalNEpisodes       | 19292      |
| TotalNSamples        | 2.5926e+06 |
| ExplainedVariance    | 0.056555   |
-------------------------------------
[2018-01-21 13:53:47.240124 UTC] Saving snapshot
[2018-01-21 13:53:47.240354 UTC] Starting iteration 519
[2018-01-21 13:53:47.240497 UTC] Start collecting samples
[2018-01-21 13:53:51.851186 UTC] Computing input variables for policy optimization
[2018-01-21 13:53:51.979202 UTC] Performing policy update
[2018-01-21 13:53:51.980180 UTC] Computing gradient in Euclidean space
[2018-01-21 13:53:52.110032 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:53:53.574058 UTC] Performing line search
[2018-01-21 13:53:53.770504 UTC] Updating baseline
[2018-01-21 13:53:55.817915 UTC] Computing logging information
-------------------------------------
| Iteration            | 519        |
| ExpectedImprovement  | 0.017481   |
| ActualImprovement    | 0.016126   |
| ImprovementRatio     | 0.92245    |
| MeanKL               | 0.0071769  |
| Entropy              | -0.023103  |
| Perplexity           | 0.97716    |
| AveragePolicyStd     | 0.24387    |
| AveragePolicyStd[0]  | 0.24993    |
| AveragePolicyStd[1]  | 0.30296    |
| AveragePolicyStd[2]  | 0.18121    |
| AveragePolicyStd[3]  | 0.25515    |
| AveragePolicyStd[4]  | 0.22517    |
| AveragePolicyStd[5]  | 0.24879    |
| AverageReturn        | 1198.3     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 172.61     |
| AverageEpisodeLength | 966.78     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.13     |
| TotalNEpisodes       | 19295      |
| TotalNSamples        | 2.5956e+06 |
| ExplainedVariance    | -0.019025  |
-------------------------------------
[2018-01-21 13:53:56.555572 UTC] Saving snapshot
[2018-01-21 13:53:56.555819 UTC] Starting iteration 520
[2018-01-21 13:53:56.555971 UTC] Start collecting samples
[2018-01-21 13:54:01.011877 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:01.147833 UTC] Performing policy update
[2018-01-21 13:54:01.148954 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:01.261722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:02.886773 UTC] Performing line search
[2018-01-21 13:54:03.107385 UTC] Updating baseline
[2018-01-21 13:54:04.973491 UTC] Computing logging information
-------------------------------------
| Iteration            | 520        |
| ExpectedImprovement  | 0.016013   |
| ActualImprovement    | 0.015001   |
| ImprovementRatio     | 0.93682    |
| MeanKL               | 0.0074052  |
| Entropy              | -0.026127  |
| Perplexity           | 0.97421    |
| AveragePolicyStd     | 0.24372    |
| AveragePolicyStd[0]  | 0.2498     |
| AveragePolicyStd[1]  | 0.30247    |
| AveragePolicyStd[2]  | 0.18159    |
| AveragePolicyStd[3]  | 0.25532    |
| AveragePolicyStd[4]  | 0.2244     |
| AveragePolicyStd[5]  | 0.24874    |
| AverageReturn        | 1207.6     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 155.88     |
| AverageEpisodeLength | 972.59     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.73     |
| TotalNEpisodes       | 19300      |
| TotalNSamples        | 2.6006e+06 |
| ExplainedVariance    | 0.024498   |
-------------------------------------
[2018-01-21 13:54:05.675743 UTC] Saving snapshot
[2018-01-21 13:54:05.684062 UTC] Starting iteration 521
[2018-01-21 13:54:05.684310 UTC] Start collecting samples
[2018-01-21 13:54:10.118675 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:10.265299 UTC] Performing policy update
[2018-01-21 13:54:10.265927 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:10.394112 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:11.826968 UTC] Performing line search
[2018-01-21 13:54:12.014366 UTC] Updating baseline
[2018-01-21 13:54:14.553630 UTC] Computing logging information
-------------------------------------
| Iteration            | 521        |
| ExpectedImprovement  | 0.019303   |
| ActualImprovement    | 0.018455   |
| ImprovementRatio     | 0.95605    |
| MeanKL               | 0.0072211  |
| Entropy              | -0.027535  |
| Perplexity           | 0.97284    |
| AveragePolicyStd     | 0.2437     |
| AveragePolicyStd[0]  | 0.24989    |
| AveragePolicyStd[1]  | 0.30284    |
| AveragePolicyStd[2]  | 0.18115    |
| AveragePolicyStd[3]  | 0.25493    |
| AveragePolicyStd[4]  | 0.22465    |
| AveragePolicyStd[5]  | 0.24872    |
| AverageReturn        | 1197.9     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 179.67     |
| AverageEpisodeLength | 964.37     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.42     |
| TotalNEpisodes       | 19308      |
| TotalNSamples        | 2.6075e+06 |
| ExplainedVariance    | 0.19129    |
-------------------------------------
[2018-01-21 13:54:15.199113 UTC] Saving snapshot
[2018-01-21 13:54:15.199329 UTC] Starting iteration 522
[2018-01-21 13:54:15.199499 UTC] Start collecting samples
[2018-01-21 13:54:19.726889 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:19.861448 UTC] Performing policy update
[2018-01-21 13:54:19.862037 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:19.977839 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:21.411152 UTC] Performing line search
[2018-01-21 13:54:21.610529 UTC] Updating baseline
[2018-01-21 13:54:26.302836 UTC] Computing logging information
-------------------------------------
| Iteration            | 522        |
| ExpectedImprovement  | 0.017196   |
| ActualImprovement    | 0.016292   |
| ImprovementRatio     | 0.94745    |
| MeanKL               | 0.0078213  |
| Entropy              | -0.035662  |
| Perplexity           | 0.96497    |
| AveragePolicyStd     | 0.24341    |
| AveragePolicyStd[0]  | 0.25002    |
| AveragePolicyStd[1]  | 0.30208    |
| AveragePolicyStd[2]  | 0.1802     |
| AveragePolicyStd[3]  | 0.25491    |
| AveragePolicyStd[4]  | 0.22392    |
| AveragePolicyStd[5]  | 0.24933    |
| AverageReturn        | 1199.3     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 180.01     |
| AverageEpisodeLength | 964.37     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.42     |
| TotalNEpisodes       | 19312      |
| TotalNSamples        | 2.6115e+06 |
| ExplainedVariance    | -0.088541  |
-------------------------------------
[2018-01-21 13:54:26.973960 UTC] Saving snapshot
[2018-01-21 13:54:26.974228 UTC] Starting iteration 523
[2018-01-21 13:54:26.974411 UTC] Start collecting samples
[2018-01-21 13:54:31.567290 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:31.696702 UTC] Performing policy update
[2018-01-21 13:54:31.697314 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:31.816428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:33.272434 UTC] Performing line search
[2018-01-21 13:54:33.474597 UTC] Updating baseline
[2018-01-21 13:54:35.956220 UTC] Computing logging information
-------------------------------------
| Iteration            | 523        |
| ExpectedImprovement  | 0.016671   |
| ActualImprovement    | 0.015183   |
| ImprovementRatio     | 0.91073    |
| MeanKL               | 0.0076205  |
| Entropy              | -0.033119  |
| Perplexity           | 0.96742    |
| AveragePolicyStd     | 0.24352    |
| AveragePolicyStd[0]  | 0.24907    |
| AveragePolicyStd[1]  | 0.30286    |
| AveragePolicyStd[2]  | 0.1803     |
| AveragePolicyStd[3]  | 0.25397    |
| AveragePolicyStd[4]  | 0.22416    |
| AveragePolicyStd[5]  | 0.25078    |
| AverageReturn        | 1191.5     |
| MinReturn            | 210.67     |
| MaxReturn            | 1307.6     |
| StdReturn            | 197.66     |
| AverageEpisodeLength | 957.52     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.99     |
| TotalNEpisodes       | 19316      |
| TotalNSamples        | 2.6148e+06 |
| ExplainedVariance    | 0.10255    |
-------------------------------------
[2018-01-21 13:54:36.603961 UTC] Saving snapshot
[2018-01-21 13:54:36.604174 UTC] Starting iteration 524
[2018-01-21 13:54:36.604280 UTC] Start collecting samples
[2018-01-21 13:54:41.161699 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:41.281847 UTC] Performing policy update
[2018-01-21 13:54:41.282463 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:41.402369 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:42.796680 UTC] Performing line search
[2018-01-21 13:54:42.996118 UTC] Updating baseline
[2018-01-21 13:54:45.153795 UTC] Computing logging information
-------------------------------------
| Iteration            | 524        |
| ExpectedImprovement  | 0.017288   |
| ActualImprovement    | 0.016562   |
| ImprovementRatio     | 0.95803    |
| MeanKL               | 0.0074823  |
| Entropy              | -0.025494  |
| Perplexity           | 0.97483    |
| AveragePolicyStd     | 0.24385    |
| AveragePolicyStd[0]  | 0.2486     |
| AveragePolicyStd[1]  | 0.30317    |
| AveragePolicyStd[2]  | 0.18014    |
| AveragePolicyStd[3]  | 0.25466    |
| AveragePolicyStd[4]  | 0.225      |
| AveragePolicyStd[5]  | 0.25152    |
| AverageReturn        | 1196       |
| MinReturn            | 265.32     |
| MaxReturn            | 1307.6     |
| StdReturn            | 187.94     |
| AverageEpisodeLength | 959.45     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.63     |
| TotalNEpisodes       | 19325      |
| TotalNSamples        | 2.6225e+06 |
| ExplainedVariance    | 0.17645    |
-------------------------------------
[2018-01-21 13:54:45.785077 UTC] Saving snapshot
[2018-01-21 13:54:45.785309 UTC] Starting iteration 525
[2018-01-21 13:54:45.785492 UTC] Start collecting samples
[2018-01-21 13:54:50.270699 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:50.405446 UTC] Performing policy update
[2018-01-21 13:54:50.405933 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:50.524974 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:54:51.925645 UTC] Performing line search
[2018-01-21 13:54:52.111220 UTC] Updating baseline
[2018-01-21 13:54:54.151206 UTC] Computing logging information
------------------------------------
| Iteration            | 525       |
| ExpectedImprovement  | 0.016243  |
| ActualImprovement    | 0.015315  |
| ImprovementRatio     | 0.94288   |
| MeanKL               | 0.0079652 |
| Entropy              | -0.024769 |
| Perplexity           | 0.97554   |
| AveragePolicyStd     | 0.2439    |
| AveragePolicyStd[0]  | 0.24871   |
| AveragePolicyStd[1]  | 0.30313   |
| AveragePolicyStd[2]  | 0.17964   |
| AveragePolicyStd[3]  | 0.25394   |
| AveragePolicyStd[4]  | 0.22537   |
| AveragePolicyStd[5]  | 0.25261   |
| AverageReturn        | 1190.9    |
| MinReturn            | 265.32    |
| MaxReturn            | 1307.6    |
| StdReturn            | 199.27    |
| AverageEpisodeLength | 953.97    |
| MinEpisodeLength     | 238       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 151.24    |
| TotalNEpisodes       | 19331     |
| TotalNSamples        | 2.628e+06 |
| ExplainedVariance    | 0.19237   |
------------------------------------
[2018-01-21 13:54:54.820522 UTC] Saving snapshot
[2018-01-21 13:54:54.820745 UTC] Starting iteration 526
[2018-01-21 13:54:54.820927 UTC] Start collecting samples
[2018-01-21 13:54:59.395037 UTC] Computing input variables for policy optimization
[2018-01-21 13:54:59.543340 UTC] Performing policy update
[2018-01-21 13:54:59.544022 UTC] Computing gradient in Euclidean space
[2018-01-21 13:54:59.674329 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:01.087963 UTC] Performing line search
[2018-01-21 13:55:01.271996 UTC] Updating baseline
[2018-01-21 13:55:03.442115 UTC] Computing logging information
-------------------------------------
| Iteration            | 526        |
| ExpectedImprovement  | 0.018135   |
| ActualImprovement    | 0.016903   |
| ImprovementRatio     | 0.93204    |
| MeanKL               | 0.0072049  |
| Entropy              | -0.029982  |
| Perplexity           | 0.97046    |
| AveragePolicyStd     | 0.24366    |
| AveragePolicyStd[0]  | 0.24872    |
| AveragePolicyStd[1]  | 0.30215    |
| AveragePolicyStd[2]  | 0.17934    |
| AveragePolicyStd[3]  | 0.25295    |
| AveragePolicyStd[4]  | 0.22611    |
| AveragePolicyStd[5]  | 0.25269    |
| AverageReturn        | 1173.6     |
| MinReturn            | 265.32     |
| MaxReturn            | 1307.6     |
| StdReturn            | 221.75     |
| AverageEpisodeLength | 939.61     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.11     |
| TotalNEpisodes       | 19336      |
| TotalNSamples        | 2.6315e+06 |
| ExplainedVariance    | 0.40726    |
-------------------------------------
[2018-01-21 13:55:04.165265 UTC] Saving snapshot
[2018-01-21 13:55:04.165494 UTC] Starting iteration 527
[2018-01-21 13:55:04.165637 UTC] Start collecting samples
[2018-01-21 13:55:08.752850 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:08.871029 UTC] Performing policy update
[2018-01-21 13:55:08.872165 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:08.993031 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:10.454121 UTC] Performing line search
[2018-01-21 13:55:10.642929 UTC] Updating baseline
[2018-01-21 13:55:12.648206 UTC] Computing logging information
-------------------------------------
| Iteration            | 527        |
| ExpectedImprovement  | 0.015148   |
| ActualImprovement    | 0.014248   |
| ImprovementRatio     | 0.94054    |
| MeanKL               | 0.0073154  |
| Entropy              | -0.038386  |
| Perplexity           | 0.96234    |
| AveragePolicyStd     | 0.24328    |
| AveragePolicyStd[0]  | 0.24875    |
| AveragePolicyStd[1]  | 0.30149    |
| AveragePolicyStd[2]  | 0.17948    |
| AveragePolicyStd[3]  | 0.25162    |
| AveragePolicyStd[4]  | 0.22601    |
| AveragePolicyStd[5]  | 0.25233    |
| AverageReturn        | 1170.3     |
| MinReturn            | 265.32     |
| MaxReturn            | 1307.6     |
| StdReturn            | 222.37     |
| AverageEpisodeLength | 937.59     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.58     |
| TotalNEpisodes       | 19340      |
| TotalNSamples        | 2.6353e+06 |
| ExplainedVariance    | 0.16654    |
-------------------------------------
[2018-01-21 13:55:13.318932 UTC] Saving snapshot
[2018-01-21 13:55:13.319196 UTC] Starting iteration 528
[2018-01-21 13:55:13.319365 UTC] Start collecting samples
[2018-01-21 13:55:17.955459 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:18.109294 UTC] Performing policy update
[2018-01-21 13:55:18.109955 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:18.237671 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:19.628733 UTC] Performing line search
[2018-01-21 13:55:19.815424 UTC] Updating baseline
[2018-01-21 13:55:22.272342 UTC] Computing logging information
-------------------------------------
| Iteration            | 528        |
| ExpectedImprovement  | 0.018228   |
| ActualImprovement    | 0.017103   |
| ImprovementRatio     | 0.93825    |
| MeanKL               | 0.0072257  |
| Entropy              | -0.048557  |
| Perplexity           | 0.9526     |
| AveragePolicyStd     | 0.24291    |
| AveragePolicyStd[0]  | 0.2486     |
| AveragePolicyStd[1]  | 0.30196    |
| AveragePolicyStd[2]  | 0.17894    |
| AveragePolicyStd[3]  | 0.251      |
| AveragePolicyStd[4]  | 0.22584    |
| AveragePolicyStd[5]  | 0.25111    |
| AverageReturn        | 1165.3     |
| MinReturn            | 265.32     |
| MaxReturn            | 1307.6     |
| StdReturn            | 235.35     |
| AverageEpisodeLength | 932.22     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.62     |
| TotalNEpisodes       | 19348      |
| TotalNSamples        | 2.6427e+06 |
| ExplainedVariance    | 0.095776   |
-------------------------------------
[2018-01-21 13:55:23.007151 UTC] Saving snapshot
[2018-01-21 13:55:23.007497 UTC] Starting iteration 529
[2018-01-21 13:55:23.007758 UTC] Start collecting samples
[2018-01-21 13:55:27.608482 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:27.748834 UTC] Performing policy update
[2018-01-21 13:55:27.749963 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:27.864407 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:29.298413 UTC] Performing line search
[2018-01-21 13:55:29.492971 UTC] Updating baseline
[2018-01-21 13:55:31.322413 UTC] Computing logging information
------------------------------------
| Iteration            | 529       |
| ExpectedImprovement  | 0.016235  |
| ActualImprovement    | 0.015161  |
| ImprovementRatio     | 0.93379   |
| MeanKL               | 0.0079228 |
| Entropy              | -0.055868 |
| Perplexity           | 0.94566   |
| AveragePolicyStd     | 0.24261   |
| AveragePolicyStd[0]  | 0.24858   |
| AveragePolicyStd[1]  | 0.30135   |
| AveragePolicyStd[2]  | 0.17881   |
| AveragePolicyStd[3]  | 0.2506    |
| AveragePolicyStd[4]  | 0.22502   |
| AveragePolicyStd[5]  | 0.2513    |
| AverageReturn        | 1159.6    |
| MinReturn            | 265.32    |
| MaxReturn            | 1307.6    |
| StdReturn            | 246.28    |
| AverageEpisodeLength | 928.42    |
| MinEpisodeLength     | 238       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.52    |
| TotalNEpisodes       | 19353     |
| TotalNSamples        | 2.647e+06 |
| ExplainedVariance    | 0.20505   |
------------------------------------
[2018-01-21 13:55:32.035108 UTC] Saving snapshot
[2018-01-21 13:55:32.035426 UTC] Starting iteration 530
[2018-01-21 13:55:32.035641 UTC] Start collecting samples
[2018-01-21 13:55:36.587844 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:36.708057 UTC] Performing policy update
[2018-01-21 13:55:36.708690 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:36.830090 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:38.284853 UTC] Performing line search
[2018-01-21 13:55:38.474135 UTC] Updating baseline
[2018-01-21 13:55:40.702126 UTC] Computing logging information
-------------------------------------
| Iteration            | 530        |
| ExpectedImprovement  | 0.016753   |
| ActualImprovement    | 0.015476   |
| ImprovementRatio     | 0.92379    |
| MeanKL               | 0.0074886  |
| Entropy              | -0.056791  |
| Perplexity           | 0.94479    |
| AveragePolicyStd     | 0.24259    |
| AveragePolicyStd[0]  | 0.24904    |
| AveragePolicyStd[1]  | 0.30097    |
| AveragePolicyStd[2]  | 0.17833    |
| AveragePolicyStd[3]  | 0.25054    |
| AveragePolicyStd[4]  | 0.22521    |
| AveragePolicyStd[5]  | 0.25144    |
| AverageReturn        | 1157.5     |
| MinReturn            | 265.32     |
| MaxReturn            | 1307.6     |
| StdReturn            | 246.25     |
| AverageEpisodeLength | 927.04     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.49     |
| TotalNEpisodes       | 19358      |
| TotalNSamples        | 2.6519e+06 |
| ExplainedVariance    | 0.21033    |
-------------------------------------
[2018-01-21 13:55:41.428329 UTC] Saving snapshot
[2018-01-21 13:55:41.437700 UTC] Starting iteration 531
[2018-01-21 13:55:41.437935 UTC] Start collecting samples
[2018-01-21 13:55:45.986243 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:46.135074 UTC] Performing policy update
[2018-01-21 13:55:46.135704 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:46.252748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:47.653505 UTC] Performing line search
[2018-01-21 13:55:47.837658 UTC] Updating baseline
[2018-01-21 13:55:49.816998 UTC] Computing logging information
-------------------------------------
| Iteration            | 531        |
| ExpectedImprovement  | 0.017957   |
| ActualImprovement    | 0.017307   |
| ImprovementRatio     | 0.96379    |
| MeanKL               | 0.0071608  |
| Entropy              | -0.061448  |
| Perplexity           | 0.9404     |
| AveragePolicyStd     | 0.24245    |
| AveragePolicyStd[0]  | 0.24944    |
| AveragePolicyStd[1]  | 0.30055    |
| AveragePolicyStd[2]  | 0.17737    |
| AveragePolicyStd[3]  | 0.25072    |
| AveragePolicyStd[4]  | 0.225      |
| AveragePolicyStd[5]  | 0.25162    |
| AverageReturn        | 1156.7     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 249.92     |
| AverageEpisodeLength | 925.84     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.53     |
| TotalNEpisodes       | 19365      |
| TotalNSamples        | 2.6584e+06 |
| ExplainedVariance    | 0.1481     |
-------------------------------------
[2018-01-21 13:55:50.488574 UTC] Saving snapshot
[2018-01-21 13:55:50.488872 UTC] Starting iteration 532
[2018-01-21 13:55:50.489108 UTC] Start collecting samples
[2018-01-21 13:55:54.901617 UTC] Computing input variables for policy optimization
[2018-01-21 13:55:55.035157 UTC] Performing policy update
[2018-01-21 13:55:55.036021 UTC] Computing gradient in Euclidean space
[2018-01-21 13:55:55.158542 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:55:56.578495 UTC] Performing line search
[2018-01-21 13:55:56.775405 UTC] Updating baseline
[2018-01-21 13:55:58.912927 UTC] Computing logging information
-------------------------------------
| Iteration            | 532        |
| ExpectedImprovement  | 0.01783    |
| ActualImprovement    | 0.016517   |
| ImprovementRatio     | 0.92637    |
| MeanKL               | 0.0075776  |
| Entropy              | -0.065951  |
| Perplexity           | 0.93618    |
| AveragePolicyStd     | 0.24234    |
| AveragePolicyStd[0]  | 0.24871    |
| AveragePolicyStd[1]  | 0.30183    |
| AveragePolicyStd[2]  | 0.17686    |
| AveragePolicyStd[3]  | 0.25039    |
| AveragePolicyStd[4]  | 0.22446    |
| AveragePolicyStd[5]  | 0.25182    |
| AverageReturn        | 1158.1     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 250.51     |
| AverageEpisodeLength | 925.84     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.53     |
| TotalNEpisodes       | 19368      |
| TotalNSamples        | 2.6614e+06 |
| ExplainedVariance    | 0.029775   |
-------------------------------------
[2018-01-21 13:55:59.634650 UTC] Saving snapshot
[2018-01-21 13:55:59.634896 UTC] Starting iteration 533
[2018-01-21 13:55:59.635044 UTC] Start collecting samples
[2018-01-21 13:56:04.129607 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:04.266870 UTC] Performing policy update
[2018-01-21 13:56:04.267547 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:04.395079 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:05.836015 UTC] Performing line search
[2018-01-21 13:56:06.031408 UTC] Updating baseline
[2018-01-21 13:56:08.133157 UTC] Computing logging information
-------------------------------------
| Iteration            | 533        |
| ExpectedImprovement  | 0.015209   |
| ActualImprovement    | 0.014558   |
| ImprovementRatio     | 0.95719    |
| MeanKL               | 0.0075758  |
| Entropy              | -0.07859   |
| Perplexity           | 0.92442    |
| AveragePolicyStd     | 0.24189    |
| AveragePolicyStd[0]  | 0.24805    |
| AveragePolicyStd[1]  | 0.30167    |
| AveragePolicyStd[2]  | 0.17576    |
| AveragePolicyStd[3]  | 0.25064    |
| AveragePolicyStd[4]  | 0.22442    |
| AveragePolicyStd[5]  | 0.25082    |
| AverageReturn        | 1150.8     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 255.43     |
| AverageEpisodeLength | 920.53     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.64     |
| TotalNEpisodes       | 19373      |
| TotalNSamples        | 2.6659e+06 |
| ExplainedVariance    | 0.33814    |
-------------------------------------
[2018-01-21 13:56:08.820097 UTC] Saving snapshot
[2018-01-21 13:56:08.820427 UTC] Starting iteration 534
[2018-01-21 13:56:08.820666 UTC] Start collecting samples
[2018-01-21 13:56:13.270348 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:13.412098 UTC] Performing policy update
[2018-01-21 13:56:13.412735 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:13.527775 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:14.952864 UTC] Performing line search
[2018-01-21 13:56:15.140155 UTC] Updating baseline
[2018-01-21 13:56:16.871889 UTC] Computing logging information
-------------------------------------
| Iteration            | 534        |
| ExpectedImprovement  | 0.015837   |
| ActualImprovement    | 0.015331   |
| ImprovementRatio     | 0.9681     |
| MeanKL               | 0.00741    |
| Entropy              | -0.061584  |
| Perplexity           | 0.94027    |
| AveragePolicyStd     | 0.24269    |
| AveragePolicyStd[0]  | 0.2496     |
| AveragePolicyStd[1]  | 0.30348    |
| AveragePolicyStd[2]  | 0.17513    |
| AveragePolicyStd[3]  | 0.25161    |
| AveragePolicyStd[4]  | 0.22466    |
| AveragePolicyStd[5]  | 0.25168    |
| AverageReturn        | 1146.8     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 255.03     |
| AverageEpisodeLength | 918.8      |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.69     |
| TotalNEpisodes       | 19380      |
| TotalNSamples        | 2.6727e+06 |
| ExplainedVariance    | 0.14976    |
-------------------------------------
[2018-01-21 13:56:17.568626 UTC] Saving snapshot
[2018-01-21 13:56:17.568859 UTC] Starting iteration 535
[2018-01-21 13:56:17.569041 UTC] Start collecting samples
[2018-01-21 13:56:21.985672 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:22.111065 UTC] Performing policy update
[2018-01-21 13:56:22.111661 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:22.235558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:23.639394 UTC] Performing line search
[2018-01-21 13:56:23.823865 UTC] Updating baseline
[2018-01-21 13:56:26.141686 UTC] Computing logging information
------------------------------------
| Iteration            | 535       |
| ExpectedImprovement  | 0.01678   |
| ActualImprovement    | 0.015333  |
| ImprovementRatio     | 0.91372   |
| MeanKL               | 0.0073432 |
| Entropy              | -0.065269 |
| Perplexity           | 0.93682   |
| AveragePolicyStd     | 0.24255   |
| AveragePolicyStd[0]  | 0.24926   |
| AveragePolicyStd[1]  | 0.30365   |
| AveragePolicyStd[2]  | 0.17501   |
| AveragePolicyStd[3]  | 0.25004   |
| AveragePolicyStd[4]  | 0.22506   |
| AveragePolicyStd[5]  | 0.25225   |
| AverageReturn        | 1137.4    |
| MinReturn            | 265.32    |
| MaxReturn            | 1332.6    |
| StdReturn            | 269.44    |
| AverageEpisodeLength | 911.2     |
| MinEpisodeLength     | 238       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 204.94    |
| TotalNEpisodes       | 19384     |
| TotalNSamples        | 2.676e+06 |
| ExplainedVariance    | 0.24799   |
------------------------------------
[2018-01-21 13:56:26.797413 UTC] Saving snapshot
[2018-01-21 13:56:26.797651 UTC] Starting iteration 536
[2018-01-21 13:56:26.797812 UTC] Start collecting samples
[2018-01-21 13:56:31.290863 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:31.423971 UTC] Performing policy update
[2018-01-21 13:56:31.424742 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:31.543311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:32.963016 UTC] Performing line search
[2018-01-21 13:56:33.147025 UTC] Updating baseline
[2018-01-21 13:56:35.179124 UTC] Computing logging information
------------------------------------
| Iteration            | 536       |
| ExpectedImprovement  | 0.01937   |
| ActualImprovement    | 0.017174  |
| ImprovementRatio     | 0.88662   |
| MeanKL               | 0.0068897 |
| Entropy              | -0.070216 |
| Perplexity           | 0.93219   |
| AveragePolicyStd     | 0.24235   |
| AveragePolicyStd[0]  | 0.24939   |
| AveragePolicyStd[1]  | 0.30418   |
| AveragePolicyStd[2]  | 0.17496   |
| AveragePolicyStd[3]  | 0.24753   |
| AveragePolicyStd[4]  | 0.22588   |
| AveragePolicyStd[5]  | 0.25214   |
| AverageReturn        | 1137.2    |
| MinReturn            | 265.32    |
| MaxReturn            | 1332.6    |
| StdReturn            | 269.47    |
| AverageEpisodeLength | 911.2     |
| MinEpisodeLength     | 238       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 204.94    |
| TotalNEpisodes       | 19388     |
| TotalNSamples        | 2.68e+06  |
| ExplainedVariance    | 0.049084  |
------------------------------------
[2018-01-21 13:56:35.904093 UTC] Saving snapshot
[2018-01-21 13:56:35.904402 UTC] Starting iteration 537
[2018-01-21 13:56:35.904625 UTC] Start collecting samples
[2018-01-21 13:56:40.281596 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:40.403522 UTC] Performing policy update
[2018-01-21 13:56:40.404215 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:40.521258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:41.969105 UTC] Performing line search
[2018-01-21 13:56:42.170803 UTC] Updating baseline
[2018-01-21 13:56:44.091328 UTC] Computing logging information
-------------------------------------
| Iteration            | 537        |
| ExpectedImprovement  | 0.015883   |
| ActualImprovement    | 0.015301   |
| ImprovementRatio     | 0.96333    |
| MeanKL               | 0.0075182  |
| Entropy              | -0.064455  |
| Perplexity           | 0.93758    |
| AveragePolicyStd     | 0.24254    |
| AveragePolicyStd[0]  | 0.24967    |
| AveragePolicyStd[1]  | 0.3043     |
| AveragePolicyStd[2]  | 0.17547    |
| AveragePolicyStd[3]  | 0.24742    |
| AveragePolicyStd[4]  | 0.22651    |
| AveragePolicyStd[5]  | 0.25189    |
| AverageReturn        | 1133.1     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 274.55     |
| AverageEpisodeLength | 906.9      |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.17     |
| TotalNEpisodes       | 19397      |
| TotalNSamples        | 2.6883e+06 |
| ExplainedVariance    | 0.18132    |
-------------------------------------
[2018-01-21 13:56:44.742779 UTC] Saving snapshot
[2018-01-21 13:56:44.743017 UTC] Starting iteration 538
[2018-01-21 13:56:44.743166 UTC] Start collecting samples
[2018-01-21 13:56:49.068737 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:49.202802 UTC] Performing policy update
[2018-01-21 13:56:49.203437 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:49.336054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:50.782698 UTC] Performing line search
[2018-01-21 13:56:50.975184 UTC] Updating baseline
[2018-01-21 13:56:52.754759 UTC] Computing logging information
-------------------------------------
| Iteration            | 538        |
| ExpectedImprovement  | 0.019672   |
| ActualImprovement    | 0.017606   |
| ImprovementRatio     | 0.89499    |
| MeanKL               | 0.0075089  |
| Entropy              | -0.061765  |
| Perplexity           | 0.9401     |
| AveragePolicyStd     | 0.24266    |
| AveragePolicyStd[0]  | 0.24975    |
| AveragePolicyStd[1]  | 0.30501    |
| AveragePolicyStd[2]  | 0.17545    |
| AveragePolicyStd[3]  | 0.24646    |
| AveragePolicyStd[4]  | 0.22768    |
| AveragePolicyStd[5]  | 0.2516     |
| AverageReturn        | 1128.3     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 277.2      |
| AverageEpisodeLength | 902.84     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.25     |
| TotalNEpisodes       | 19399      |
| TotalNSamples        | 2.6899e+06 |
| ExplainedVariance    | 0.36676    |
-------------------------------------
[2018-01-21 13:56:53.488066 UTC] Saving snapshot
[2018-01-21 13:56:53.488392 UTC] Starting iteration 539
[2018-01-21 13:56:53.488605 UTC] Start collecting samples
[2018-01-21 13:56:57.884632 UTC] Computing input variables for policy optimization
[2018-01-21 13:56:58.044399 UTC] Performing policy update
[2018-01-21 13:56:58.045528 UTC] Computing gradient in Euclidean space
[2018-01-21 13:56:58.164073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:56:59.631999 UTC] Performing line search
[2018-01-21 13:56:59.834745 UTC] Updating baseline
[2018-01-21 13:57:01.617328 UTC] Computing logging information
-------------------------------------
| Iteration            | 539        |
| ExpectedImprovement  | 0.018545   |
| ActualImprovement    | 0.016766   |
| ImprovementRatio     | 0.9041     |
| MeanKL               | 0.0073115  |
| Entropy              | -0.069467  |
| Perplexity           | 0.93289    |
| AveragePolicyStd     | 0.24236    |
| AveragePolicyStd[0]  | 0.24915    |
| AveragePolicyStd[1]  | 0.30447    |
| AveragePolicyStd[2]  | 0.17508    |
| AveragePolicyStd[3]  | 0.24658    |
| AveragePolicyStd[4]  | 0.22722    |
| AveragePolicyStd[5]  | 0.25164    |
| AverageReturn        | 1128.4     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 277.26     |
| AverageEpisodeLength | 902.84     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.25     |
| TotalNEpisodes       | 19403      |
| TotalNSamples        | 2.6939e+06 |
| ExplainedVariance    | 0.20214    |
-------------------------------------
[2018-01-21 13:57:02.371127 UTC] Saving snapshot
[2018-01-21 13:57:02.371364 UTC] Starting iteration 540
[2018-01-21 13:57:02.371526 UTC] Start collecting samples
[2018-01-21 13:57:07.035892 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:07.191660 UTC] Performing policy update
[2018-01-21 13:57:07.192259 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:07.314095 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:08.714490 UTC] Performing line search
[2018-01-21 13:57:08.900575 UTC] Updating baseline
[2018-01-21 13:57:10.699708 UTC] Computing logging information
-------------------------------------
| Iteration            | 540        |
| ExpectedImprovement  | 0.018653   |
| ActualImprovement    | 0.017895   |
| ImprovementRatio     | 0.95937    |
| MeanKL               | 0.0075767  |
| Entropy              | -0.082357  |
| Perplexity           | 0.92094    |
| AveragePolicyStd     | 0.24186    |
| AveragePolicyStd[0]  | 0.24875    |
| AveragePolicyStd[1]  | 0.3037     |
| AveragePolicyStd[2]  | 0.17422    |
| AveragePolicyStd[3]  | 0.24713    |
| AveragePolicyStd[4]  | 0.22695    |
| AveragePolicyStd[5]  | 0.25041    |
| AverageReturn        | 1145.4     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 264.7      |
| AverageEpisodeLength | 913.17     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.48     |
| TotalNEpisodes       | 19413      |
| TotalNSamples        | 2.7038e+06 |
| ExplainedVariance    | 0.19905    |
-------------------------------------
[2018-01-21 13:57:11.443356 UTC] Saving snapshot
[2018-01-21 13:57:11.452527 UTC] Starting iteration 541
[2018-01-21 13:57:11.452755 UTC] Start collecting samples
[2018-01-21 13:57:15.911584 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:16.066788 UTC] Performing policy update
[2018-01-21 13:57:16.067449 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:16.194745 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:17.635988 UTC] Performing line search
[2018-01-21 13:57:17.842925 UTC] Updating baseline
[2018-01-21 13:57:20.080540 UTC] Computing logging information
-------------------------------------
| Iteration            | 541        |
| ExpectedImprovement  | 0.015584   |
| ActualImprovement    | 0.014755   |
| ImprovementRatio     | 0.94682    |
| MeanKL               | 0.0069368  |
| Entropy              | -0.087857  |
| Perplexity           | 0.91589    |
| AveragePolicyStd     | 0.24168    |
| AveragePolicyStd[0]  | 0.24901    |
| AveragePolicyStd[1]  | 0.30453    |
| AveragePolicyStd[2]  | 0.17412    |
| AveragePolicyStd[3]  | 0.24665    |
| AveragePolicyStd[4]  | 0.22573    |
| AveragePolicyStd[5]  | 0.25007    |
| AverageReturn        | 1154.9     |
| MinReturn            | 265.32     |
| MaxReturn            | 1332.6     |
| StdReturn            | 253.83     |
| AverageEpisodeLength | 920.02     |
| MinEpisodeLength     | 238        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.42     |
| TotalNEpisodes       | 19415      |
| TotalNSamples        | 2.7058e+06 |
| ExplainedVariance    | -0.0089897 |
-------------------------------------
[2018-01-21 13:57:20.767724 UTC] Saving snapshot
[2018-01-21 13:57:20.767903 UTC] Starting iteration 542
[2018-01-21 13:57:20.768010 UTC] Start collecting samples
[2018-01-21 13:57:25.217421 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:25.375913 UTC] Performing policy update
[2018-01-21 13:57:25.376510 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:25.495784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:26.941111 UTC] Performing line search
[2018-01-21 13:57:27.129862 UTC] Updating baseline
[2018-01-21 13:57:28.939690 UTC] Computing logging information
------------------------------------
| Iteration            | 542       |
| ExpectedImprovement  | 0.016803  |
| ActualImprovement    | 0.016149  |
| ImprovementRatio     | 0.9611    |
| MeanKL               | 0.0071385 |
| Entropy              | -0.087918 |
| Perplexity           | 0.91584   |
| AveragePolicyStd     | 0.24161   |
| AveragePolicyStd[0]  | 0.24955   |
| AveragePolicyStd[1]  | 0.30331   |
| AveragePolicyStd[2]  | 0.17455   |
| AveragePolicyStd[3]  | 0.24712   |
| AveragePolicyStd[4]  | 0.22595   |
| AveragePolicyStd[5]  | 0.24918   |
| AverageReturn        | 1144      |
| MinReturn            | 255.47    |
| MaxReturn            | 1332.6    |
| StdReturn            | 260.96    |
| AverageEpisodeLength | 909.97    |
| MinEpisodeLength     | 209       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 197.65    |
| TotalNEpisodes       | 19423     |
| TotalNSamples        | 2.712e+06 |
| ExplainedVariance    | 0.33198   |
------------------------------------
[2018-01-21 13:57:29.595191 UTC] Saving snapshot
[2018-01-21 13:57:29.595439 UTC] Starting iteration 543
[2018-01-21 13:57:29.595595 UTC] Start collecting samples
[2018-01-21 13:57:34.238324 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:34.381842 UTC] Performing policy update
[2018-01-21 13:57:34.382541 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:34.501973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:35.981115 UTC] Performing line search
[2018-01-21 13:57:36.182565 UTC] Updating baseline
[2018-01-21 13:57:38.218269 UTC] Computing logging information
------------------------------------
| Iteration            | 543       |
| ExpectedImprovement  | 0.016329  |
| ActualImprovement    | 0.016073  |
| ImprovementRatio     | 0.98435   |
| MeanKL               | 0.0076729 |
| Entropy              | -0.094432 |
| Perplexity           | 0.90989   |
| AveragePolicyStd     | 0.24139   |
| AveragePolicyStd[0]  | 0.25051   |
| AveragePolicyStd[1]  | 0.30349   |
| AveragePolicyStd[2]  | 0.17398   |
| AveragePolicyStd[3]  | 0.24688   |
| AveragePolicyStd[4]  | 0.22552   |
| AveragePolicyStd[5]  | 0.24799   |
| AverageReturn        | 1153      |
| MinReturn            | 255.47    |
| MaxReturn            | 1343      |
| StdReturn            | 256.45    |
| AverageEpisodeLength | 914.88    |
| MinEpisodeLength     | 209       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 193.68    |
| TotalNEpisodes       | 19429     |
| TotalNSamples        | 2.718e+06 |
| ExplainedVariance    | -0.083865 |
------------------------------------
[2018-01-21 13:57:38.922692 UTC] Saving snapshot
[2018-01-21 13:57:38.922935 UTC] Starting iteration 544
[2018-01-21 13:57:38.923091 UTC] Start collecting samples
[2018-01-21 13:57:43.453810 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:43.582679 UTC] Performing policy update
[2018-01-21 13:57:43.583412 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:43.719807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:45.173985 UTC] Performing line search
[2018-01-21 13:57:45.358496 UTC] Updating baseline
[2018-01-21 13:57:47.329086 UTC] Computing logging information
------------------------------------
| Iteration            | 544       |
| ExpectedImprovement  | 0.015281  |
| ActualImprovement    | 0.013955  |
| ImprovementRatio     | 0.91319   |
| MeanKL               | 0.0081012 |
| Entropy              | -0.094554 |
| Perplexity           | 0.90978   |
| AveragePolicyStd     | 0.24139   |
| AveragePolicyStd[0]  | 0.25067   |
| AveragePolicyStd[1]  | 0.30323   |
| AveragePolicyStd[2]  | 0.17389   |
| AveragePolicyStd[3]  | 0.24678   |
| AveragePolicyStd[4]  | 0.22546   |
| AveragePolicyStd[5]  | 0.24831   |
| AverageReturn        | 1161.9    |
| MinReturn            | 255.47    |
| MaxReturn            | 1343      |
| StdReturn            | 250.04    |
| AverageEpisodeLength | 920.36    |
| MinEpisodeLength     | 209       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 188.18    |
| TotalNEpisodes       | 19432     |
| TotalNSamples        | 2.721e+06 |
| ExplainedVariance    | 0.018669  |
------------------------------------
[2018-01-21 13:57:47.983553 UTC] Saving snapshot
[2018-01-21 13:57:47.983761 UTC] Starting iteration 545
[2018-01-21 13:57:47.983936 UTC] Start collecting samples
[2018-01-21 13:57:52.541508 UTC] Computing input variables for policy optimization
[2018-01-21 13:57:52.664539 UTC] Performing policy update
[2018-01-21 13:57:52.665662 UTC] Computing gradient in Euclidean space
[2018-01-21 13:57:52.783776 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:57:54.222105 UTC] Performing line search
[2018-01-21 13:57:54.414398 UTC] Updating baseline
[2018-01-21 13:57:56.373847 UTC] Computing logging information
------------------------------------
| Iteration            | 545       |
| ExpectedImprovement  | 0.016724  |
| ActualImprovement    | 0.016124  |
| ImprovementRatio     | 0.96413   |
| MeanKL               | 0.0074808 |
| Entropy              | -0.096081 |
| Perplexity           | 0.90839   |
| AveragePolicyStd     | 0.2413    |
| AveragePolicyStd[0]  | 0.25071   |
| AveragePolicyStd[1]  | 0.30336   |
| AveragePolicyStd[2]  | 0.17444   |
| AveragePolicyStd[3]  | 0.24559   |
| AveragePolicyStd[4]  | 0.22531   |
| AveragePolicyStd[5]  | 0.24837   |
| AverageReturn        | 1183.2    |
| MinReturn            | 255.47    |
| MaxReturn            | 1343      |
| StdReturn            | 232.59    |
| AverageEpisodeLength | 934.72    |
| MinEpisodeLength     | 209       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 173.9     |
| TotalNEpisodes       | 19437     |
| TotalNSamples        | 2.726e+06 |
| ExplainedVariance    | 0.0042437 |
------------------------------------
[2018-01-21 13:57:57.038061 UTC] Saving snapshot
[2018-01-21 13:57:57.038306 UTC] Starting iteration 546
[2018-01-21 13:57:57.038473 UTC] Start collecting samples
[2018-01-21 13:58:01.462395 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:01.582054 UTC] Performing policy update
[2018-01-21 13:58:01.583147 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:01.706637 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:03.124659 UTC] Performing line search
[2018-01-21 13:58:03.308807 UTC] Updating baseline
[2018-01-21 13:58:05.426116 UTC] Computing logging information
------------------------------------
| Iteration            | 546       |
| ExpectedImprovement  | 0.017909  |
| ActualImprovement    | 0.016537  |
| ImprovementRatio     | 0.92337   |
| MeanKL               | 0.0070172 |
| Entropy              | -0.097737 |
| Perplexity           | 0.90689   |
| AveragePolicyStd     | 0.24123   |
| AveragePolicyStd[0]  | 0.24986   |
| AveragePolicyStd[1]  | 0.3038    |
| AveragePolicyStd[2]  | 0.17462   |
| AveragePolicyStd[3]  | 0.24464   |
| AveragePolicyStd[4]  | 0.22543   |
| AveragePolicyStd[5]  | 0.24901   |
| AverageReturn        | 1184.4    |
| MinReturn            | 213.71    |
| MaxReturn            | 1343      |
| StdReturn            | 240.07    |
| AverageEpisodeLength | 933.7     |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 179.35    |
| TotalNEpisodes       | 19445     |
| TotalNSamples        | 2.733e+06 |
| ExplainedVariance    | 0.15555   |
------------------------------------
[2018-01-21 13:58:06.142902 UTC] Saving snapshot
[2018-01-21 13:58:06.143082 UTC] Starting iteration 547
[2018-01-21 13:58:06.143186 UTC] Start collecting samples
[2018-01-21 13:58:10.710809 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:10.844588 UTC] Performing policy update
[2018-01-21 13:58:10.845208 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:10.966656 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:12.385308 UTC] Performing line search
[2018-01-21 13:58:12.578850 UTC] Updating baseline
[2018-01-21 13:58:14.953592 UTC] Computing logging information
-------------------------------------
| Iteration            | 547        |
| ExpectedImprovement  | 0.016827   |
| ActualImprovement    | 0.016146   |
| ImprovementRatio     | 0.95948    |
| MeanKL               | 0.0072507  |
| Entropy              | -0.10166   |
| Perplexity           | 0.90333    |
| AveragePolicyStd     | 0.24101    |
| AveragePolicyStd[0]  | 0.25057    |
| AveragePolicyStd[1]  | 0.30199    |
| AveragePolicyStd[2]  | 0.17476    |
| AveragePolicyStd[3]  | 0.24348    |
| AveragePolicyStd[4]  | 0.22529    |
| AveragePolicyStd[5]  | 0.24996    |
| AverageReturn        | 1161.3     |
| MinReturn            | 213.71     |
| MaxReturn            | 1343       |
| StdReturn            | 268.78     |
| AverageEpisodeLength | 915.16     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.51     |
| TotalNEpisodes       | 19453      |
| TotalNSamples        | 2.7385e+06 |
| ExplainedVariance    | 0.32623    |
-------------------------------------
[2018-01-21 13:58:15.695212 UTC] Saving snapshot
[2018-01-21 13:58:15.695458 UTC] Starting iteration 548
[2018-01-21 13:58:15.696381 UTC] Start collecting samples
[2018-01-21 13:58:20.160355 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:20.279738 UTC] Performing policy update
[2018-01-21 13:58:20.280475 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:20.399196 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:21.808984 UTC] Performing line search
[2018-01-21 13:58:21.999635 UTC] Updating baseline
[2018-01-21 13:58:23.929264 UTC] Computing logging information
-------------------------------------
| Iteration            | 548        |
| ExpectedImprovement  | 0.016441   |
| ActualImprovement    | 0.015691   |
| ImprovementRatio     | 0.95439    |
| MeanKL               | 0.0076915  |
| Entropy              | -0.10697   |
| Perplexity           | 0.89855    |
| AveragePolicyStd     | 0.24081    |
| AveragePolicyStd[0]  | 0.25037    |
| AveragePolicyStd[1]  | 0.30231    |
| AveragePolicyStd[2]  | 0.17477    |
| AveragePolicyStd[3]  | 0.24306    |
| AveragePolicyStd[4]  | 0.22478    |
| AveragePolicyStd[5]  | 0.24954    |
| AverageReturn        | 1164.5     |
| MinReturn            | 213.71     |
| MaxReturn            | 1343       |
| StdReturn            | 269.22     |
| AverageEpisodeLength | 916.54     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.62     |
| TotalNEpisodes       | 19456      |
| TotalNSamples        | 2.7415e+06 |
| ExplainedVariance    | -0.0024961 |
-------------------------------------
[2018-01-21 13:58:24.606828 UTC] Saving snapshot
[2018-01-21 13:58:24.607076 UTC] Starting iteration 549
[2018-01-21 13:58:24.607264 UTC] Start collecting samples
[2018-01-21 13:58:28.961243 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:29.098357 UTC] Performing policy update
[2018-01-21 13:58:29.099010 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:29.219033 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:30.690898 UTC] Performing line search
[2018-01-21 13:58:30.887119 UTC] Updating baseline
[2018-01-21 13:58:32.901959 UTC] Computing logging information
-------------------------------------
| Iteration            | 549        |
| ExpectedImprovement  | 0.016001   |
| ActualImprovement    | 0.014426   |
| ImprovementRatio     | 0.90157    |
| MeanKL               | 0.0079525  |
| Entropy              | -0.099227  |
| Perplexity           | 0.90554    |
| AveragePolicyStd     | 0.24113    |
| AveragePolicyStd[0]  | 0.25158    |
| AveragePolicyStd[1]  | 0.30301    |
| AveragePolicyStd[2]  | 0.17499    |
| AveragePolicyStd[3]  | 0.24315    |
| AveragePolicyStd[4]  | 0.22505    |
| AveragePolicyStd[5]  | 0.24899    |
| AverageReturn        | 1171.7     |
| MinReturn            | 213.71     |
| MaxReturn            | 1343       |
| StdReturn            | 263.86     |
| AverageEpisodeLength | 921.07     |
| MinEpisodeLength     | 188        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.34     |
| TotalNEpisodes       | 19459      |
| TotalNSamples        | 2.7445e+06 |
| ExplainedVariance    | -0.0033326 |
-------------------------------------
[2018-01-21 13:58:33.587374 UTC] Saving snapshot
[2018-01-21 13:58:33.587591 UTC] Starting iteration 550
[2018-01-21 13:58:33.587830 UTC] Start collecting samples
[2018-01-21 13:58:38.210031 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:38.344549 UTC] Performing policy update
[2018-01-21 13:58:38.345266 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:38.470766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:39.933803 UTC] Performing line search
[2018-01-21 13:58:40.134113 UTC] Updating baseline
[2018-01-21 13:58:42.320045 UTC] Computing logging information
------------------------------------
| Iteration            | 550       |
| ExpectedImprovement  | 0.017406  |
| ActualImprovement    | 0.01684   |
| ImprovementRatio     | 0.96748   |
| MeanKL               | 0.0072464 |
| Entropy              | -0.10683  |
| Perplexity           | 0.89868   |
| AveragePolicyStd     | 0.24081   |
| AveragePolicyStd[0]  | 0.25204   |
| AveragePolicyStd[1]  | 0.30189   |
| AveragePolicyStd[2]  | 0.1747    |
| AveragePolicyStd[3]  | 0.24267   |
| AveragePolicyStd[4]  | 0.22488   |
| AveragePolicyStd[5]  | 0.24866   |
| AverageReturn        | 1166.7    |
| MinReturn            | 213.71    |
| MaxReturn            | 1343      |
| StdReturn            | 269.73    |
| AverageEpisodeLength | 916.09    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 203.49    |
| TotalNEpisodes       | 19468     |
| TotalNSamples        | 2.753e+06 |
| ExplainedVariance    | 0.044986  |
------------------------------------
[2018-01-21 13:58:43.003404 UTC] Saving snapshot
[2018-01-21 13:58:43.013177 UTC] Starting iteration 551
[2018-01-21 13:58:43.013414 UTC] Start collecting samples
[2018-01-21 13:58:47.645267 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:47.776524 UTC] Performing policy update
[2018-01-21 13:58:47.777188 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:47.891406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:49.285044 UTC] Performing line search
[2018-01-21 13:58:49.478690 UTC] Updating baseline
[2018-01-21 13:58:51.451999 UTC] Computing logging information
-------------------------------------
| Iteration            | 551        |
| ExpectedImprovement  | 0.017725   |
| ActualImprovement    | 0.016898   |
| ImprovementRatio     | 0.95336    |
| MeanKL               | 0.0080162  |
| Entropy              | -0.10895   |
| Perplexity           | 0.89678    |
| AveragePolicyStd     | 0.2407     |
| AveragePolicyStd[0]  | 0.25082    |
| AveragePolicyStd[1]  | 0.30206    |
| AveragePolicyStd[2]  | 0.17505    |
| AveragePolicyStd[3]  | 0.24304    |
| AveragePolicyStd[4]  | 0.22472    |
| AveragePolicyStd[5]  | 0.24851    |
| AverageReturn        | 1164.4     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 285.51     |
| AverageEpisodeLength | 912.49     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.26     |
| TotalNEpisodes       | 19474      |
| TotalNSamples        | 2.7582e+06 |
| ExplainedVariance    | 0.22078    |
-------------------------------------
[2018-01-21 13:58:52.126948 UTC] Saving snapshot
[2018-01-21 13:58:52.127126 UTC] Starting iteration 552
[2018-01-21 13:58:52.127230 UTC] Start collecting samples
[2018-01-21 13:58:56.745275 UTC] Computing input variables for policy optimization
[2018-01-21 13:58:56.869823 UTC] Performing policy update
[2018-01-21 13:58:56.870537 UTC] Computing gradient in Euclidean space
[2018-01-21 13:58:56.999961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:58:58.507485 UTC] Performing line search
[2018-01-21 13:58:58.704881 UTC] Updating baseline
[2018-01-21 13:59:00.525731 UTC] Computing logging information
-------------------------------------
| Iteration            | 552        |
| ExpectedImprovement  | 0.016942   |
| ActualImprovement    | 0.016116   |
| ImprovementRatio     | 0.95125    |
| MeanKL               | 0.0077438  |
| Entropy              | -0.10937   |
| Perplexity           | 0.8964     |
| AveragePolicyStd     | 0.2407     |
| AveragePolicyStd[0]  | 0.25119    |
| AveragePolicyStd[1]  | 0.30226    |
| AveragePolicyStd[2]  | 0.17508    |
| AveragePolicyStd[3]  | 0.24288    |
| AveragePolicyStd[4]  | 0.2241     |
| AveragePolicyStd[5]  | 0.24867    |
| AverageReturn        | 1155       |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 295.86     |
| AverageEpisodeLength | 905.13     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.5      |
| TotalNEpisodes       | 19478      |
| TotalNSamples        | 2.7612e+06 |
| ExplainedVariance    | 0.36119    |
-------------------------------------
[2018-01-21 13:59:01.221410 UTC] Saving snapshot
[2018-01-21 13:59:01.221699 UTC] Starting iteration 553
[2018-01-21 13:59:01.221901 UTC] Start collecting samples
[2018-01-21 13:59:05.593848 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:05.715337 UTC] Performing policy update
[2018-01-21 13:59:05.715989 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:05.836063 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:07.223936 UTC] Performing line search
[2018-01-21 13:59:07.408120 UTC] Updating baseline
[2018-01-21 13:59:09.331094 UTC] Computing logging information
-------------------------------------
| Iteration            | 553        |
| ExpectedImprovement  | 0.016121   |
| ActualImprovement    | 0.015686   |
| ImprovementRatio     | 0.97301    |
| MeanKL               | 0.0079712  |
| Entropy              | -0.11869   |
| Perplexity           | 0.88808    |
| AveragePolicyStd     | 0.24026    |
| AveragePolicyStd[0]  | 0.25171    |
| AveragePolicyStd[1]  | 0.29995    |
| AveragePolicyStd[2]  | 0.17474    |
| AveragePolicyStd[3]  | 0.24276    |
| AveragePolicyStd[4]  | 0.22454    |
| AveragePolicyStd[5]  | 0.24787    |
| AverageReturn        | 1164.4     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 282.25     |
| AverageEpisodeLength | 912.48     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.32     |
| TotalNEpisodes       | 19482      |
| TotalNSamples        | 2.7652e+06 |
| ExplainedVariance    | 0.34949    |
-------------------------------------
[2018-01-21 13:59:09.977632 UTC] Saving snapshot
[2018-01-21 13:59:09.977843 UTC] Starting iteration 554
[2018-01-21 13:59:09.978023 UTC] Start collecting samples
[2018-01-21 13:59:14.528269 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:14.660517 UTC] Performing policy update
[2018-01-21 13:59:14.661641 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:14.782549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:16.212141 UTC] Performing line search
[2018-01-21 13:59:16.416898 UTC] Updating baseline
[2018-01-21 13:59:18.227702 UTC] Computing logging information
-------------------------------------
| Iteration            | 554        |
| ExpectedImprovement  | 0.015054   |
| ActualImprovement    | 0.015305   |
| ImprovementRatio     | 1.0167     |
| MeanKL               | 0.0080874  |
| Entropy              | -0.11536   |
| Perplexity           | 0.89105    |
| AveragePolicyStd     | 0.24045    |
| AveragePolicyStd[0]  | 0.25184    |
| AveragePolicyStd[1]  | 0.29979    |
| AveragePolicyStd[2]  | 0.17394    |
| AveragePolicyStd[3]  | 0.24265    |
| AveragePolicyStd[4]  | 0.22491    |
| AveragePolicyStd[5]  | 0.24954    |
| AverageReturn        | 1168       |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 283.61     |
| AverageEpisodeLength | 912.48     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.32     |
| TotalNEpisodes       | 19487      |
| TotalNSamples        | 2.7702e+06 |
| ExplainedVariance    | 0.12471    |
-------------------------------------
[2018-01-21 13:59:18.936550 UTC] Saving snapshot
[2018-01-21 13:59:18.936796 UTC] Starting iteration 555
[2018-01-21 13:59:18.936953 UTC] Start collecting samples
[2018-01-21 13:59:23.551498 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:23.673154 UTC] Performing policy update
[2018-01-21 13:59:23.674153 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:23.794805 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:25.238485 UTC] Performing line search
[2018-01-21 13:59:25.422759 UTC] Updating baseline
[2018-01-21 13:59:27.464288 UTC] Computing logging information
-------------------------------------
| Iteration            | 555        |
| ExpectedImprovement  | 0.016284   |
| ActualImprovement    | 0.015206   |
| ImprovementRatio     | 0.9338     |
| MeanKL               | 0.0073133  |
| Entropy              | -0.11844   |
| Perplexity           | 0.88831    |
| AveragePolicyStd     | 0.2403     |
| AveragePolicyStd[0]  | 0.25178    |
| AveragePolicyStd[1]  | 0.299      |
| AveragePolicyStd[2]  | 0.17422    |
| AveragePolicyStd[3]  | 0.24287    |
| AveragePolicyStd[4]  | 0.2237     |
| AveragePolicyStd[5]  | 0.25022    |
| AverageReturn        | 1167.5     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 287.2      |
| AverageEpisodeLength | 911.89     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.69     |
| TotalNEpisodes       | 19494      |
| TotalNSamples        | 2.7766e+06 |
| ExplainedVariance    | 0.15457    |
-------------------------------------
[2018-01-21 13:59:28.228796 UTC] Saving snapshot
[2018-01-21 13:59:28.229582 UTC] Starting iteration 556
[2018-01-21 13:59:28.230022 UTC] Start collecting samples
[2018-01-21 13:59:32.683645 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:32.807901 UTC] Performing policy update
[2018-01-21 13:59:32.808493 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:32.932161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:34.410901 UTC] Performing line search
[2018-01-21 13:59:34.596455 UTC] Updating baseline
[2018-01-21 13:59:37.649167 UTC] Computing logging information
-------------------------------------
| Iteration            | 556        |
| ExpectedImprovement  | 0.015629   |
| ActualImprovement    | 0.015154   |
| ImprovementRatio     | 0.96964    |
| MeanKL               | 0.0074843  |
| Entropy              | -0.11332   |
| Perplexity           | 0.89286    |
| AveragePolicyStd     | 0.24049    |
| AveragePolicyStd[0]  | 0.25239    |
| AveragePolicyStd[1]  | 0.29879    |
| AveragePolicyStd[2]  | 0.17453    |
| AveragePolicyStd[3]  | 0.24384    |
| AveragePolicyStd[4]  | 0.22324    |
| AveragePolicyStd[5]  | 0.25013    |
| AverageReturn        | 1166.3     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 295.71     |
| AverageEpisodeLength | 908.99     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.64     |
| TotalNEpisodes       | 19500      |
| TotalNSamples        | 2.7818e+06 |
| ExplainedVariance    | 0.26891    |
-------------------------------------
[2018-01-21 13:59:38.356316 UTC] Saving snapshot
[2018-01-21 13:59:38.356650 UTC] Starting iteration 557
[2018-01-21 13:59:38.356902 UTC] Start collecting samples
[2018-01-21 13:59:43.032534 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:43.181422 UTC] Performing policy update
[2018-01-21 13:59:43.182600 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:43.307438 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:44.708559 UTC] Performing line search
[2018-01-21 13:59:44.894895 UTC] Updating baseline
[2018-01-21 13:59:46.627352 UTC] Computing logging information
-------------------------------------
| Iteration            | 557        |
| ExpectedImprovement  | 0.016899   |
| ActualImprovement    | 0.016436   |
| ImprovementRatio     | 0.97263    |
| MeanKL               | 0.0073382  |
| Entropy              | -0.10413   |
| Perplexity           | 0.9011     |
| AveragePolicyStd     | 0.2409     |
| AveragePolicyStd[0]  | 0.25209    |
| AveragePolicyStd[1]  | 0.30064    |
| AveragePolicyStd[2]  | 0.17493    |
| AveragePolicyStd[3]  | 0.24444    |
| AveragePolicyStd[4]  | 0.22257    |
| AveragePolicyStd[5]  | 0.25075    |
| AverageReturn        | 1152.1     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 312.21     |
| AverageEpisodeLength | 897.29     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.98     |
| TotalNEpisodes       | 19506      |
| TotalNSamples        | 2.7866e+06 |
| ExplainedVariance    | 0.16617    |
-------------------------------------
[2018-01-21 13:59:47.358649 UTC] Saving snapshot
[2018-01-21 13:59:47.358949 UTC] Starting iteration 558
[2018-01-21 13:59:47.359211 UTC] Start collecting samples
[2018-01-21 13:59:51.802598 UTC] Computing input variables for policy optimization
[2018-01-21 13:59:51.966925 UTC] Performing policy update
[2018-01-21 13:59:51.967533 UTC] Computing gradient in Euclidean space
[2018-01-21 13:59:52.086143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 13:59:53.581494 UTC] Performing line search
[2018-01-21 13:59:53.774614 UTC] Updating baseline
[2018-01-21 13:59:55.823775 UTC] Computing logging information
-------------------------------------
| Iteration            | 558        |
| ExpectedImprovement  | 0.014963   |
| ActualImprovement    | 0.014355   |
| ImprovementRatio     | 0.95937    |
| MeanKL               | 0.0073304  |
| Entropy              | -0.11716   |
| Perplexity           | 0.88945    |
| AveragePolicyStd     | 0.24045    |
| AveragePolicyStd[0]  | 0.25129    |
| AveragePolicyStd[1]  | 0.30074    |
| AveragePolicyStd[2]  | 0.17392    |
| AveragePolicyStd[3]  | 0.24405    |
| AveragePolicyStd[4]  | 0.22191    |
| AveragePolicyStd[5]  | 0.2508     |
| AverageReturn        | 1143.3     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 329.41     |
| AverageEpisodeLength | 889.42     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 244.58     |
| TotalNEpisodes       | 19511      |
| TotalNSamples        | 2.7907e+06 |
| ExplainedVariance    | 0.16077    |
-------------------------------------
[2018-01-21 13:59:56.583375 UTC] Saving snapshot
[2018-01-21 13:59:56.583614 UTC] Starting iteration 559
[2018-01-21 13:59:56.583796 UTC] Start collecting samples
[2018-01-21 14:00:01.246796 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:01.393140 UTC] Performing policy update
[2018-01-21 14:00:01.394228 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:01.516577 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:02.930380 UTC] Performing line search
[2018-01-21 14:00:03.122494 UTC] Updating baseline
[2018-01-21 14:00:05.068635 UTC] Computing logging information
-------------------------------------
| Iteration            | 559        |
| ExpectedImprovement  | 0.01586    |
| ActualImprovement    | 0.015106   |
| ImprovementRatio     | 0.95246    |
| MeanKL               | 0.0078428  |
| Entropy              | -0.12644   |
| Perplexity           | 0.88122    |
| AveragePolicyStd     | 0.24007    |
| AveragePolicyStd[0]  | 0.25084    |
| AveragePolicyStd[1]  | 0.29973    |
| AveragePolicyStd[2]  | 0.17363    |
| AveragePolicyStd[3]  | 0.24323    |
| AveragePolicyStd[4]  | 0.22168    |
| AveragePolicyStd[5]  | 0.2513     |
| AverageReturn        | 1151.8     |
| MinReturn            | 113.33     |
| MaxReturn            | 1343       |
| StdReturn            | 326.86     |
| AverageEpisodeLength | 894.19     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 242.02     |
| TotalNEpisodes       | 19517      |
| TotalNSamples        | 2.7967e+06 |
| ExplainedVariance    | 0.068051   |
-------------------------------------
[2018-01-21 14:00:05.759555 UTC] Saving snapshot
[2018-01-21 14:00:05.759816 UTC] Starting iteration 560
[2018-01-21 14:00:05.760019 UTC] Start collecting samples
[2018-01-21 14:00:10.355741 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:10.491764 UTC] Performing policy update
[2018-01-21 14:00:10.492444 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:10.618084 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:12.074184 UTC] Performing line search
[2018-01-21 14:00:12.271083 UTC] Updating baseline
[2018-01-21 14:00:14.074641 UTC] Computing logging information
-------------------------------------
| Iteration            | 560        |
| ExpectedImprovement  | 0.016228   |
| ActualImprovement    | 0.016098   |
| ImprovementRatio     | 0.99196    |
| MeanKL               | 0.0071991  |
| Entropy              | -0.1316    |
| Perplexity           | 0.87669    |
| AveragePolicyStd     | 0.23981    |
| AveragePolicyStd[0]  | 0.24922    |
| AveragePolicyStd[1]  | 0.2998     |
| AveragePolicyStd[2]  | 0.17427    |
| AveragePolicyStd[3]  | 0.24254    |
| AveragePolicyStd[4]  | 0.22186    |
| AveragePolicyStd[5]  | 0.25113    |
| AverageReturn        | 1166.3     |
| MinReturn            | 113.33     |
| MaxReturn            | 1339       |
| StdReturn            | 313.16     |
| AverageEpisodeLength | 905.2      |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.25     |
| TotalNEpisodes       | 19524      |
| TotalNSamples        | 2.8035e+06 |
| ExplainedVariance    | -0.020456  |
-------------------------------------
[2018-01-21 14:00:14.725464 UTC] Saving snapshot
[2018-01-21 14:00:14.735216 UTC] Starting iteration 561
[2018-01-21 14:00:14.735454 UTC] Start collecting samples
[2018-01-21 14:00:19.146284 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:19.275699 UTC] Performing policy update
[2018-01-21 14:00:19.276453 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:19.403204 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:20.831092 UTC] Performing line search
[2018-01-21 14:00:21.019430 UTC] Updating baseline
[2018-01-21 14:00:23.332974 UTC] Computing logging information
-------------------------------------
| Iteration            | 561        |
| ExpectedImprovement  | 0.016245   |
| ActualImprovement    | 0.015454   |
| ImprovementRatio     | 0.95133    |
| MeanKL               | 0.0072309  |
| Entropy              | -0.13252   |
| Perplexity           | 0.87588    |
| AveragePolicyStd     | 0.23975    |
| AveragePolicyStd[0]  | 0.24944    |
| AveragePolicyStd[1]  | 0.29895    |
| AveragePolicyStd[2]  | 0.17424    |
| AveragePolicyStd[3]  | 0.24274    |
| AveragePolicyStd[4]  | 0.22151    |
| AveragePolicyStd[5]  | 0.25163    |
| AverageReturn        | 1166.7     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 313.31     |
| AverageEpisodeLength | 905.2      |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.25     |
| TotalNEpisodes       | 19527      |
| TotalNSamples        | 2.8065e+06 |
| ExplainedVariance    | -0.053512  |
-------------------------------------
[2018-01-21 14:00:24.031792 UTC] Saving snapshot
[2018-01-21 14:00:24.031967 UTC] Starting iteration 562
[2018-01-21 14:00:24.032085 UTC] Start collecting samples
[2018-01-21 14:00:28.590807 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:28.711031 UTC] Performing policy update
[2018-01-21 14:00:28.711649 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:28.831056 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:30.278709 UTC] Performing line search
[2018-01-21 14:00:30.467334 UTC] Updating baseline
[2018-01-21 14:00:32.274017 UTC] Computing logging information
-------------------------------------
| Iteration            | 562        |
| ExpectedImprovement  | 0.01788    |
| ActualImprovement    | 0.017054   |
| ImprovementRatio     | 0.95379    |
| MeanKL               | 0.0069343  |
| Entropy              | -0.13958   |
| Perplexity           | 0.86972    |
| AveragePolicyStd     | 0.23945    |
| AveragePolicyStd[0]  | 0.24841    |
| AveragePolicyStd[1]  | 0.29843    |
| AveragePolicyStd[2]  | 0.17405    |
| AveragePolicyStd[3]  | 0.24269    |
| AveragePolicyStd[4]  | 0.22172    |
| AveragePolicyStd[5]  | 0.25143    |
| AverageReturn        | 1153.2     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 329.57     |
| AverageEpisodeLength | 895.67     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 243.54     |
| TotalNEpisodes       | 19533      |
| TotalNSamples        | 2.8116e+06 |
| ExplainedVariance    | 0.20981    |
-------------------------------------
[2018-01-21 14:00:33.015743 UTC] Saving snapshot
[2018-01-21 14:00:33.015971 UTC] Starting iteration 563
[2018-01-21 14:00:33.016144 UTC] Start collecting samples
[2018-01-21 14:00:37.687139 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:37.843473 UTC] Performing policy update
[2018-01-21 14:00:37.844084 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:37.965153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:39.368779 UTC] Performing line search
[2018-01-21 14:00:39.557713 UTC] Updating baseline
[2018-01-21 14:00:41.373214 UTC] Computing logging information
-------------------------------------
| Iteration            | 563        |
| ExpectedImprovement  | 0.016832   |
| ActualImprovement    | 0.016207   |
| ImprovementRatio     | 0.96287    |
| MeanKL               | 0.0077097  |
| Entropy              | -0.15008   |
| Perplexity           | 0.86064    |
| AveragePolicyStd     | 0.23906    |
| AveragePolicyStd[0]  | 0.24833    |
| AveragePolicyStd[1]  | 0.29862    |
| AveragePolicyStd[2]  | 0.17372    |
| AveragePolicyStd[3]  | 0.242      |
| AveragePolicyStd[4]  | 0.2211     |
| AveragePolicyStd[5]  | 0.25061    |
| AverageReturn        | 1148.8     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 329.2      |
| AverageEpisodeLength | 894.49     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 244.06     |
| TotalNEpisodes       | 19541      |
| TotalNSamples        | 2.8193e+06 |
| ExplainedVariance    | 0.094227   |
-------------------------------------
[2018-01-21 14:00:42.032744 UTC] Saving snapshot
[2018-01-21 14:00:42.033021 UTC] Starting iteration 564
[2018-01-21 14:00:42.033201 UTC] Start collecting samples
[2018-01-21 14:00:46.427794 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:46.571899 UTC] Performing policy update
[2018-01-21 14:00:46.572515 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:46.709861 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:48.161448 UTC] Performing line search
[2018-01-21 14:00:48.360192 UTC] Updating baseline
[2018-01-21 14:00:50.263402 UTC] Computing logging information
-------------------------------------
| Iteration            | 564        |
| ExpectedImprovement  | 0.016175   |
| ActualImprovement    | 0.01472    |
| ImprovementRatio     | 0.91008    |
| MeanKL               | 0.0073974  |
| Entropy              | -0.15421   |
| Perplexity           | 0.85709    |
| AveragePolicyStd     | 0.23887    |
| AveragePolicyStd[0]  | 0.24808    |
| AveragePolicyStd[1]  | 0.2981     |
| AveragePolicyStd[2]  | 0.17397    |
| AveragePolicyStd[3]  | 0.2416     |
| AveragePolicyStd[4]  | 0.22064    |
| AveragePolicyStd[5]  | 0.25083    |
| AverageReturn        | 1159.1     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 315.6      |
| AverageEpisodeLength | 902.61     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.71     |
| TotalNEpisodes       | 19544      |
| TotalNSamples        | 2.8223e+06 |
| ExplainedVariance    | 0.08183    |
-------------------------------------
[2018-01-21 14:00:50.958164 UTC] Saving snapshot
[2018-01-21 14:00:50.958400 UTC] Starting iteration 565
[2018-01-21 14:00:50.958575 UTC] Start collecting samples
[2018-01-21 14:00:55.397663 UTC] Computing input variables for policy optimization
[2018-01-21 14:00:55.521974 UTC] Performing policy update
[2018-01-21 14:00:55.522616 UTC] Computing gradient in Euclidean space
[2018-01-21 14:00:55.648294 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:00:57.118625 UTC] Performing line search
[2018-01-21 14:00:57.310894 UTC] Updating baseline
[2018-01-21 14:00:59.325639 UTC] Computing logging information
-------------------------------------
| Iteration            | 565        |
| ExpectedImprovement  | 0.01792    |
| ActualImprovement    | 0.016763   |
| ImprovementRatio     | 0.93544    |
| MeanKL               | 0.0077216  |
| Entropy              | -0.15814   |
| Perplexity           | 0.85373    |
| AveragePolicyStd     | 0.23877    |
| AveragePolicyStd[0]  | 0.24765    |
| AveragePolicyStd[1]  | 0.29883    |
| AveragePolicyStd[2]  | 0.17348    |
| AveragePolicyStd[3]  | 0.24146    |
| AveragePolicyStd[4]  | 0.22051    |
| AveragePolicyStd[5]  | 0.25067    |
| AverageReturn        | 1159.3     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 315.68     |
| AverageEpisodeLength | 902.61     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.71     |
| TotalNEpisodes       | 19546      |
| TotalNSamples        | 2.8243e+06 |
| ExplainedVariance    | 0.035798   |
-------------------------------------
[2018-01-21 14:01:00.081257 UTC] Saving snapshot
[2018-01-21 14:01:00.081519 UTC] Starting iteration 566
[2018-01-21 14:01:00.081690 UTC] Start collecting samples
[2018-01-21 14:01:04.759331 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:04.886207 UTC] Performing policy update
[2018-01-21 14:01:04.886862 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:05.007331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:06.406314 UTC] Performing line search
[2018-01-21 14:01:06.592793 UTC] Updating baseline
[2018-01-21 14:01:09.026081 UTC] Computing logging information
-------------------------------------
| Iteration            | 566        |
| ExpectedImprovement  | 0.019234   |
| ActualImprovement    | 0.01824    |
| ImprovementRatio     | 0.94831    |
| MeanKL               | 0.0070809  |
| Entropy              | -0.17337   |
| Perplexity           | 0.84083    |
| AveragePolicyStd     | 0.23819    |
| AveragePolicyStd[0]  | 0.24804    |
| AveragePolicyStd[1]  | 0.29828    |
| AveragePolicyStd[2]  | 0.17299    |
| AveragePolicyStd[3]  | 0.24042    |
| AveragePolicyStd[4]  | 0.21936    |
| AveragePolicyStd[5]  | 0.25003    |
| AverageReturn        | 1183.3     |
| MinReturn            | 113.33     |
| MaxReturn            | 1344.7     |
| StdReturn            | 284.28     |
| AverageEpisodeLength | 922.03     |
| MinEpisodeLength     | 109        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.15     |
| TotalNEpisodes       | 19557      |
| TotalNSamples        | 2.8347e+06 |
| ExplainedVariance    | 0.11704    |
-------------------------------------
[2018-01-21 14:01:09.689265 UTC] Saving snapshot
[2018-01-21 14:01:09.689513 UTC] Starting iteration 567
[2018-01-21 14:01:09.689691 UTC] Start collecting samples
[2018-01-21 14:01:14.199236 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:14.338045 UTC] Performing policy update
[2018-01-21 14:01:14.338671 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:14.459082 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:15.873732 UTC] Performing line search
[2018-01-21 14:01:16.066483 UTC] Updating baseline
[2018-01-21 14:01:18.195750 UTC] Computing logging information
-------------------------------------
| Iteration            | 567        |
| ExpectedImprovement  | 0.02053    |
| ActualImprovement    | 0.018618   |
| ImprovementRatio     | 0.90685    |
| MeanKL               | 0.0067579  |
| Entropy              | -0.18738   |
| Perplexity           | 0.82913    |
| AveragePolicyStd     | 0.23761    |
| AveragePolicyStd[0]  | 0.24782    |
| AveragePolicyStd[1]  | 0.29743    |
| AveragePolicyStd[2]  | 0.17273    |
| AveragePolicyStd[3]  | 0.24019    |
| AveragePolicyStd[4]  | 0.21882    |
| AveragePolicyStd[5]  | 0.24871    |
| AverageReturn        | 1171.3     |
| MinReturn            | 12.297     |
| MaxReturn            | 1344.7     |
| StdReturn            | 307.28     |
| AverageEpisodeLength | 912.29     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.11     |
| TotalNEpisodes       | 19560      |
| TotalNSamples        | 2.8368e+06 |
| ExplainedVariance    | 0.10595    |
-------------------------------------
[2018-01-21 14:01:18.887631 UTC] Saving snapshot
[2018-01-21 14:01:18.887834 UTC] Starting iteration 568
[2018-01-21 14:01:18.888062 UTC] Start collecting samples
[2018-01-21 14:01:23.377303 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:23.509841 UTC] Performing policy update
[2018-01-21 14:01:23.510795 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:23.629865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:25.078395 UTC] Performing line search
[2018-01-21 14:01:25.277136 UTC] Updating baseline
[2018-01-21 14:01:27.225206 UTC] Computing logging information
-------------------------------------
| Iteration            | 568        |
| ExpectedImprovement  | 0.016933   |
| ActualImprovement    | 0.015635   |
| ImprovementRatio     | 0.92336    |
| MeanKL               | 0.0074285  |
| Entropy              | -0.19433   |
| Perplexity           | 0.82339    |
| AveragePolicyStd     | 0.23732    |
| AveragePolicyStd[0]  | 0.24687    |
| AveragePolicyStd[1]  | 0.29684    |
| AveragePolicyStd[2]  | 0.17266    |
| AveragePolicyStd[3]  | 0.24008    |
| AveragePolicyStd[4]  | 0.21861    |
| AveragePolicyStd[5]  | 0.24886    |
| AverageReturn        | 1171.1     |
| MinReturn            | 12.297     |
| MaxReturn            | 1344.7     |
| StdReturn            | 307.19     |
| AverageEpisodeLength | 912.29     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.11     |
| TotalNEpisodes       | 19562      |
| TotalNSamples        | 2.8388e+06 |
| ExplainedVariance    | 0.10421    |
-------------------------------------
[2018-01-21 14:01:28.000715 UTC] Saving snapshot
[2018-01-21 14:01:28.001004 UTC] Starting iteration 569
[2018-01-21 14:01:28.001231 UTC] Start collecting samples
[2018-01-21 14:01:32.744540 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:32.872185 UTC] Performing policy update
[2018-01-21 14:01:32.872907 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:32.992730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:34.389770 UTC] Performing line search
[2018-01-21 14:01:34.584625 UTC] Updating baseline
[2018-01-21 14:01:36.541158 UTC] Computing logging information
-------------------------------------
| Iteration            | 569        |
| ExpectedImprovement  | 0.016539   |
| ActualImprovement    | 0.015636   |
| ImprovementRatio     | 0.94541    |
| MeanKL               | 0.0075375  |
| Entropy              | -0.19711   |
| Perplexity           | 0.8211     |
| AveragePolicyStd     | 0.23719    |
| AveragePolicyStd[0]  | 0.24709    |
| AveragePolicyStd[1]  | 0.29629    |
| AveragePolicyStd[2]  | 0.17283    |
| AveragePolicyStd[3]  | 0.24034    |
| AveragePolicyStd[4]  | 0.21798    |
| AveragePolicyStd[5]  | 0.24862    |
| AverageReturn        | 1186.2     |
| MinReturn            | 12.297     |
| MaxReturn            | 1344.7     |
| StdReturn            | 283.16     |
| AverageEpisodeLength | 924.44     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.53     |
| TotalNEpisodes       | 19571      |
| TotalNSamples        | 2.8476e+06 |
| ExplainedVariance    | 0.091702   |
-------------------------------------
[2018-01-21 14:01:37.287868 UTC] Saving snapshot
[2018-01-21 14:01:37.288045 UTC] Starting iteration 570
[2018-01-21 14:01:37.288148 UTC] Start collecting samples
[2018-01-21 14:01:41.751102 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:41.890966 UTC] Performing policy update
[2018-01-21 14:01:41.891585 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:42.024410 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:43.458586 UTC] Performing line search
[2018-01-21 14:01:43.659270 UTC] Updating baseline
[2018-01-21 14:01:45.968502 UTC] Computing logging information
-------------------------------------
| Iteration            | 570        |
| ExpectedImprovement  | 0.01749    |
| ActualImprovement    | 0.016799   |
| ImprovementRatio     | 0.96051    |
| MeanKL               | 0.0075699  |
| Entropy              | -0.19878   |
| Perplexity           | 0.81973    |
| AveragePolicyStd     | 0.23715    |
| AveragePolicyStd[0]  | 0.24652    |
| AveragePolicyStd[1]  | 0.29668    |
| AveragePolicyStd[2]  | 0.17268    |
| AveragePolicyStd[3]  | 0.24052    |
| AveragePolicyStd[4]  | 0.21761    |
| AveragePolicyStd[5]  | 0.24891    |
| AverageReturn        | 1187       |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 285.89     |
| AverageEpisodeLength | 923.94     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.93     |
| TotalNEpisodes       | 19576      |
| TotalNSamples        | 2.8519e+06 |
| ExplainedVariance    | 0.17923    |
-------------------------------------
[2018-01-21 14:01:46.630268 UTC] Saving snapshot
[2018-01-21 14:01:46.641511 UTC] Starting iteration 571
[2018-01-21 14:01:46.641745 UTC] Start collecting samples
[2018-01-21 14:01:51.072723 UTC] Computing input variables for policy optimization
[2018-01-21 14:01:51.227337 UTC] Performing policy update
[2018-01-21 14:01:51.227929 UTC] Computing gradient in Euclidean space
[2018-01-21 14:01:51.344433 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:01:52.823392 UTC] Performing line search
[2018-01-21 14:01:53.031730 UTC] Updating baseline
[2018-01-21 14:01:54.972881 UTC] Computing logging information
-------------------------------------
| Iteration            | 571        |
| ExpectedImprovement  | 0.015045   |
| ActualImprovement    | 0.014554   |
| ImprovementRatio     | 0.96739    |
| MeanKL               | 0.0086066  |
| Entropy              | -0.20942   |
| Perplexity           | 0.81106    |
| AveragePolicyStd     | 0.23674    |
| AveragePolicyStd[0]  | 0.24607    |
| AveragePolicyStd[1]  | 0.29566    |
| AveragePolicyStd[2]  | 0.17187    |
| AveragePolicyStd[3]  | 0.24102    |
| AveragePolicyStd[4]  | 0.2179     |
| AveragePolicyStd[5]  | 0.24789    |
| AverageReturn        | 1183.1     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 299.13     |
| AverageEpisodeLength | 919        |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.82     |
| TotalNEpisodes       | 19580      |
| TotalNSamples        | 2.8551e+06 |
| ExplainedVariance    | 0.17004    |
-------------------------------------
[2018-01-21 14:01:55.626863 UTC] Saving snapshot
[2018-01-21 14:01:55.627091 UTC] Starting iteration 572
[2018-01-21 14:01:55.627284 UTC] Start collecting samples
[2018-01-21 14:02:00.085364 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:00.240122 UTC] Performing policy update
[2018-01-21 14:02:00.240736 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:00.361118 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:01.759690 UTC] Performing line search
[2018-01-21 14:02:01.945830 UTC] Updating baseline
[2018-01-21 14:02:03.742413 UTC] Computing logging information
-------------------------------------
| Iteration            | 572        |
| ExpectedImprovement  | 0.01592    |
| ActualImprovement    | 0.015231   |
| ImprovementRatio     | 0.9567     |
| MeanKL               | 0.0075616  |
| Entropy              | -0.20935   |
| Perplexity           | 0.81111    |
| AveragePolicyStd     | 0.23673    |
| AveragePolicyStd[0]  | 0.24597    |
| AveragePolicyStd[1]  | 0.29602    |
| AveragePolicyStd[2]  | 0.17229    |
| AveragePolicyStd[3]  | 0.24149    |
| AveragePolicyStd[4]  | 0.21742    |
| AveragePolicyStd[5]  | 0.24718    |
| AverageReturn        | 1181.2     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 299.39     |
| AverageEpisodeLength | 917.08     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.95     |
| TotalNEpisodes       | 19587      |
| TotalNSamples        | 2.8619e+06 |
| ExplainedVariance    | 0.082665   |
-------------------------------------
[2018-01-21 14:02:04.402831 UTC] Saving snapshot
[2018-01-21 14:02:04.403078 UTC] Starting iteration 573
[2018-01-21 14:02:04.403199 UTC] Start collecting samples
[2018-01-21 14:02:08.892351 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:09.021366 UTC] Performing policy update
[2018-01-21 14:02:09.021961 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:09.139958 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:10.534605 UTC] Performing line search
[2018-01-21 14:02:10.720752 UTC] Updating baseline
[2018-01-21 14:02:12.727296 UTC] Computing logging information
-------------------------------------
| Iteration            | 573        |
| ExpectedImprovement  | 0.016738   |
| ActualImprovement    | 0.015396   |
| ImprovementRatio     | 0.9198     |
| MeanKL               | 0.0075966  |
| Entropy              | -0.21132   |
| Perplexity           | 0.80951    |
| AveragePolicyStd     | 0.23663    |
| AveragePolicyStd[0]  | 0.2464     |
| AveragePolicyStd[1]  | 0.29564    |
| AveragePolicyStd[2]  | 0.17222    |
| AveragePolicyStd[3]  | 0.24063    |
| AveragePolicyStd[4]  | 0.21797    |
| AveragePolicyStd[5]  | 0.24693    |
| AverageReturn        | 1190.5     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 289.79     |
| AverageEpisodeLength | 923.2      |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.62     |
| TotalNEpisodes       | 19593      |
| TotalNSamples        | 2.8679e+06 |
| ExplainedVariance    | -0.0090836 |
-------------------------------------
[2018-01-21 14:02:13.400577 UTC] Saving snapshot
[2018-01-21 14:02:13.400863 UTC] Starting iteration 574
[2018-01-21 14:02:13.401072 UTC] Start collecting samples
[2018-01-21 14:02:17.835737 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:17.981020 UTC] Performing policy update
[2018-01-21 14:02:17.981764 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:18.097925 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:19.503489 UTC] Performing line search
[2018-01-21 14:02:19.702014 UTC] Updating baseline
[2018-01-21 14:02:21.880163 UTC] Computing logging information
-------------------------------------
| Iteration            | 574        |
| ExpectedImprovement  | 0.015733   |
| ActualImprovement    | 0.014946   |
| ImprovementRatio     | 0.94998    |
| MeanKL               | 0.0074359  |
| Entropy              | -0.21351   |
| Perplexity           | 0.80774    |
| AveragePolicyStd     | 0.23654    |
| AveragePolicyStd[0]  | 0.24664    |
| AveragePolicyStd[1]  | 0.29463    |
| AveragePolicyStd[2]  | 0.17183    |
| AveragePolicyStd[3]  | 0.24096    |
| AveragePolicyStd[4]  | 0.2181     |
| AveragePolicyStd[5]  | 0.24705    |
| AverageReturn        | 1190.1     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 289.68     |
| AverageEpisodeLength | 923.2      |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.62     |
| TotalNEpisodes       | 19596      |
| TotalNSamples        | 2.8709e+06 |
| ExplainedVariance    | 0.035879   |
-------------------------------------
[2018-01-21 14:02:22.612669 UTC] Saving snapshot
[2018-01-21 14:02:22.612898 UTC] Starting iteration 575
[2018-01-21 14:02:22.613057 UTC] Start collecting samples
[2018-01-21 14:02:27.244372 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:27.400979 UTC] Performing policy update
[2018-01-21 14:02:27.401663 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:27.519858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:28.947842 UTC] Performing line search
[2018-01-21 14:02:29.145204 UTC] Updating baseline
[2018-01-21 14:02:31.282109 UTC] Computing logging information
-------------------------------------
| Iteration            | 575        |
| ExpectedImprovement  | 0.018003   |
| ActualImprovement    | 0.016642   |
| ImprovementRatio     | 0.92443    |
| MeanKL               | 0.0073605  |
| Entropy              | -0.22171   |
| Perplexity           | 0.80115    |
| AveragePolicyStd     | 0.23621    |
| AveragePolicyStd[0]  | 0.24649    |
| AveragePolicyStd[1]  | 0.2935     |
| AveragePolicyStd[2]  | 0.17142    |
| AveragePolicyStd[3]  | 0.24118    |
| AveragePolicyStd[4]  | 0.21776    |
| AveragePolicyStd[5]  | 0.24688    |
| AverageReturn        | 1201.7     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 278.05     |
| AverageEpisodeLength | 931.68     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.75     |
| TotalNEpisodes       | 19602      |
| TotalNSamples        | 2.8769e+06 |
| ExplainedVariance    | -0.01969   |
-------------------------------------
[2018-01-21 14:02:31.967212 UTC] Saving snapshot
[2018-01-21 14:02:31.967388 UTC] Starting iteration 576
[2018-01-21 14:02:31.967489 UTC] Start collecting samples
[2018-01-21 14:02:36.683451 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:36.817993 UTC] Performing policy update
[2018-01-21 14:02:36.818595 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:36.946523 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:38.395324 UTC] Performing line search
[2018-01-21 14:02:38.578902 UTC] Updating baseline
[2018-01-21 14:02:41.188559 UTC] Computing logging information
-------------------------------------
| Iteration            | 576        |
| ExpectedImprovement  | 0.016803   |
| ActualImprovement    | 0.015592   |
| ImprovementRatio     | 0.92795    |
| MeanKL               | 0.0074144  |
| Entropy              | -0.23437   |
| Perplexity           | 0.79107    |
| AveragePolicyStd     | 0.23569    |
| AveragePolicyStd[0]  | 0.24691    |
| AveragePolicyStd[1]  | 0.29284    |
| AveragePolicyStd[2]  | 0.1713     |
| AveragePolicyStd[3]  | 0.2413     |
| AveragePolicyStd[4]  | 0.21714    |
| AveragePolicyStd[5]  | 0.24466    |
| AverageReturn        | 1213       |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 256.37     |
| AverageEpisodeLength | 939.58     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.09     |
| TotalNEpisodes       | 19611      |
| TotalNSamples        | 2.8847e+06 |
| ExplainedVariance    | 0.15548    |
-------------------------------------
[2018-01-21 14:02:41.899728 UTC] Saving snapshot
[2018-01-21 14:02:41.899981 UTC] Starting iteration 577
[2018-01-21 14:02:41.900162 UTC] Start collecting samples
[2018-01-21 14:02:46.307345 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:46.443581 UTC] Performing policy update
[2018-01-21 14:02:46.444208 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:46.562664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:48.003417 UTC] Performing line search
[2018-01-21 14:02:48.203741 UTC] Updating baseline
[2018-01-21 14:02:50.141180 UTC] Computing logging information
-------------------------------------
| Iteration            | 577        |
| ExpectedImprovement  | 0.019276   |
| ActualImprovement    | 0.018547   |
| ImprovementRatio     | 0.9622     |
| MeanKL               | 0.0071456  |
| Entropy              | -0.23255   |
| Perplexity           | 0.79251    |
| AveragePolicyStd     | 0.23573    |
| AveragePolicyStd[0]  | 0.24638    |
| AveragePolicyStd[1]  | 0.29245    |
| AveragePolicyStd[2]  | 0.17165    |
| AveragePolicyStd[3]  | 0.24096    |
| AveragePolicyStd[4]  | 0.21718    |
| AveragePolicyStd[5]  | 0.24576    |
| AverageReturn        | 1209.7     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 256.56     |
| AverageEpisodeLength | 937.51     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.54     |
| TotalNEpisodes       | 19613      |
| TotalNSamples        | 2.8865e+06 |
| ExplainedVariance    | 0.27482    |
-------------------------------------
[2018-01-21 14:02:50.850603 UTC] Saving snapshot
[2018-01-21 14:02:50.850851 UTC] Starting iteration 578
[2018-01-21 14:02:50.851020 UTC] Start collecting samples
[2018-01-21 14:02:55.319014 UTC] Computing input variables for policy optimization
[2018-01-21 14:02:55.473705 UTC] Performing policy update
[2018-01-21 14:02:55.474765 UTC] Computing gradient in Euclidean space
[2018-01-21 14:02:55.601642 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:02:57.050416 UTC] Performing line search
[2018-01-21 14:02:57.241204 UTC] Updating baseline
[2018-01-21 14:02:59.458140 UTC] Computing logging information
-------------------------------------
| Iteration            | 578        |
| ExpectedImprovement  | 0.017699   |
| ActualImprovement    | 0.016829   |
| ImprovementRatio     | 0.95084    |
| MeanKL               | 0.0080814  |
| Entropy              | -0.23213   |
| Perplexity           | 0.79285    |
| AveragePolicyStd     | 0.23573    |
| AveragePolicyStd[0]  | 0.24675    |
| AveragePolicyStd[1]  | 0.29203    |
| AveragePolicyStd[2]  | 0.17182    |
| AveragePolicyStd[3]  | 0.24094    |
| AveragePolicyStd[4]  | 0.21681    |
| AveragePolicyStd[5]  | 0.24606    |
| AverageReturn        | 1203.1     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 264.64     |
| AverageEpisodeLength | 931.97     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.6      |
| TotalNEpisodes       | 19619      |
| TotalNSamples        | 2.8919e+06 |
| ExplainedVariance    | 0.078003   |
-------------------------------------
[2018-01-21 14:03:00.122272 UTC] Saving snapshot
[2018-01-21 14:03:00.122525 UTC] Starting iteration 579
[2018-01-21 14:03:00.122677 UTC] Start collecting samples
[2018-01-21 14:03:04.646833 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:04.771322 UTC] Performing policy update
[2018-01-21 14:03:04.771939 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:04.895729 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:06.336837 UTC] Performing line search
[2018-01-21 14:03:06.529043 UTC] Updating baseline
[2018-01-21 14:03:08.342925 UTC] Computing logging information
-------------------------------------
| Iteration            | 579        |
| ExpectedImprovement  | 0.01585    |
| ActualImprovement    | 0.015214   |
| ImprovementRatio     | 0.95983    |
| MeanKL               | 0.0077026  |
| Entropy              | -0.22189   |
| Perplexity           | 0.80101    |
| AveragePolicyStd     | 0.23608    |
| AveragePolicyStd[0]  | 0.2473     |
| AveragePolicyStd[1]  | 0.29147    |
| AveragePolicyStd[2]  | 0.17234    |
| AveragePolicyStd[3]  | 0.24087    |
| AveragePolicyStd[4]  | 0.21787    |
| AveragePolicyStd[5]  | 0.24662    |
| AverageReturn        | 1203       |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 263.84     |
| AverageEpisodeLength | 932.94     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.24     |
| TotalNEpisodes       | 19626      |
| TotalNSamples        | 2.8988e+06 |
| ExplainedVariance    | 0.14912    |
-------------------------------------
[2018-01-21 14:03:09.035934 UTC] Saving snapshot
[2018-01-21 14:03:09.036234 UTC] Starting iteration 580
[2018-01-21 14:03:09.036451 UTC] Start collecting samples
[2018-01-21 14:03:13.548449 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:13.671047 UTC] Performing policy update
[2018-01-21 14:03:13.671675 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:13.800052 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:15.262133 UTC] Performing line search
[2018-01-21 14:03:15.453261 UTC] Updating baseline
[2018-01-21 14:03:17.470717 UTC] Computing logging information
-------------------------------------
| Iteration            | 580        |
| ExpectedImprovement  | 0.014772   |
| ActualImprovement    | 0.014104   |
| ImprovementRatio     | 0.95482    |
| MeanKL               | 0.0075601  |
| Entropy              | -0.23211   |
| Perplexity           | 0.79286    |
| AveragePolicyStd     | 0.23568    |
| AveragePolicyStd[0]  | 0.24644    |
| AveragePolicyStd[1]  | 0.291      |
| AveragePolicyStd[2]  | 0.17228    |
| AveragePolicyStd[3]  | 0.24024    |
| AveragePolicyStd[4]  | 0.21686    |
| AveragePolicyStd[5]  | 0.24724    |
| AverageReturn        | 1203.4     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 263.99     |
| AverageEpisodeLength | 932.94     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.24     |
| TotalNEpisodes       | 19629      |
| TotalNSamples        | 2.9018e+06 |
| ExplainedVariance    | 0.057656   |
-------------------------------------
[2018-01-21 14:03:18.119763 UTC] Saving snapshot
[2018-01-21 14:03:18.129263 UTC] Starting iteration 581
[2018-01-21 14:03:18.129440 UTC] Start collecting samples
[2018-01-21 14:03:22.553990 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:22.708445 UTC] Performing policy update
[2018-01-21 14:03:22.709061 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:22.837440 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:24.296173 UTC] Performing line search
[2018-01-21 14:03:24.492615 UTC] Updating baseline
[2018-01-21 14:03:26.490909 UTC] Computing logging information
-------------------------------------
| Iteration            | 581        |
| ExpectedImprovement  | 0.018257   |
| ActualImprovement    | 0.017079   |
| ImprovementRatio     | 0.93548    |
| MeanKL               | 0.0072025  |
| Entropy              | -0.23299   |
| Perplexity           | 0.79217    |
| AveragePolicyStd     | 0.23563    |
| AveragePolicyStd[0]  | 0.24641    |
| AveragePolicyStd[1]  | 0.29017    |
| AveragePolicyStd[2]  | 0.17226    |
| AveragePolicyStd[3]  | 0.24137    |
| AveragePolicyStd[4]  | 0.21636    |
| AveragePolicyStd[5]  | 0.2472     |
| AverageReturn        | 1217.7     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 240.88     |
| AverageEpisodeLength | 942.47     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.79     |
| TotalNEpisodes       | 19633      |
| TotalNSamples        | 2.9058e+06 |
| ExplainedVariance    | 0.02805    |
-------------------------------------
[2018-01-21 14:03:27.170751 UTC] Saving snapshot
[2018-01-21 14:03:27.171054 UTC] Starting iteration 582
[2018-01-21 14:03:27.171265 UTC] Start collecting samples
[2018-01-21 14:03:31.592979 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:31.721846 UTC] Performing policy update
[2018-01-21 14:03:31.722841 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:31.847933 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:33.297271 UTC] Performing line search
[2018-01-21 14:03:33.486848 UTC] Updating baseline
[2018-01-21 14:03:35.518209 UTC] Computing logging information
-------------------------------------
| Iteration            | 582        |
| ExpectedImprovement  | 0.016961   |
| ActualImprovement    | 0.016384   |
| ImprovementRatio     | 0.96595    |
| MeanKL               | 0.0079433  |
| Entropy              | -0.24017   |
| Perplexity           | 0.78649    |
| AveragePolicyStd     | 0.23536    |
| AveragePolicyStd[0]  | 0.24539    |
| AveragePolicyStd[1]  | 0.29018    |
| AveragePolicyStd[2]  | 0.17202    |
| AveragePolicyStd[3]  | 0.24163    |
| AveragePolicyStd[4]  | 0.216      |
| AveragePolicyStd[5]  | 0.24692    |
| AverageReturn        | 1208.5     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 266.35     |
| AverageEpisodeLength | 932.78     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.02     |
| TotalNEpisodes       | 19641      |
| TotalNSamples        | 2.9126e+06 |
| ExplainedVariance    | 0.215      |
-------------------------------------
[2018-01-21 14:03:36.200920 UTC] Saving snapshot
[2018-01-21 14:03:36.201098 UTC] Starting iteration 583
[2018-01-21 14:03:36.201228 UTC] Start collecting samples
[2018-01-21 14:03:40.641852 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:40.764310 UTC] Performing policy update
[2018-01-21 14:03:40.764921 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:40.899920 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:42.318211 UTC] Performing line search
[2018-01-21 14:03:42.513121 UTC] Updating baseline
[2018-01-21 14:03:44.362108 UTC] Computing logging information
-------------------------------------
| Iteration            | 583        |
| ExpectedImprovement  | 0.017031   |
| ActualImprovement    | 0.015708   |
| ImprovementRatio     | 0.92231    |
| MeanKL               | 0.0071895  |
| Entropy              | -0.24104   |
| Perplexity           | 0.78581    |
| AveragePolicyStd     | 0.23536    |
| AveragePolicyStd[0]  | 0.24458    |
| AveragePolicyStd[1]  | 0.29058    |
| AveragePolicyStd[2]  | 0.1716     |
| AveragePolicyStd[3]  | 0.24165    |
| AveragePolicyStd[4]  | 0.21625    |
| AveragePolicyStd[5]  | 0.24748    |
| AverageReturn        | 1199.9     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 279.85     |
| AverageEpisodeLength | 925.58     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.27     |
| TotalNEpisodes       | 19646      |
| TotalNSamples        | 2.9169e+06 |
| ExplainedVariance    | 0.1924     |
-------------------------------------
[2018-01-21 14:03:45.135309 UTC] Saving snapshot
[2018-01-21 14:03:45.135548 UTC] Starting iteration 584
[2018-01-21 14:03:45.135698 UTC] Start collecting samples
[2018-01-21 14:03:49.644492 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:49.792962 UTC] Performing policy update
[2018-01-21 14:03:49.793688 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:49.926221 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:03:51.358936 UTC] Performing line search
[2018-01-21 14:03:51.558002 UTC] Updating baseline
[2018-01-21 14:03:53.739618 UTC] Computing logging information
-------------------------------------
| Iteration            | 584        |
| ExpectedImprovement  | 0.016941   |
| ActualImprovement    | 0.016203   |
| ImprovementRatio     | 0.95642    |
| MeanKL               | 0.0080387  |
| Entropy              | -0.24362   |
| Perplexity           | 0.78378    |
| AveragePolicyStd     | 0.23527    |
| AveragePolicyStd[0]  | 0.24452    |
| AveragePolicyStd[1]  | 0.29074    |
| AveragePolicyStd[2]  | 0.17165    |
| AveragePolicyStd[3]  | 0.24125    |
| AveragePolicyStd[4]  | 0.21565    |
| AveragePolicyStd[5]  | 0.24779    |
| AverageReturn        | 1202       |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 280.47     |
| AverageEpisodeLength | 925.58     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.27     |
| TotalNEpisodes       | 19650      |
| TotalNSamples        | 2.9209e+06 |
| ExplainedVariance    | -0.0043938 |
-------------------------------------
[2018-01-21 14:03:54.481356 UTC] Saving snapshot
[2018-01-21 14:03:54.481587 UTC] Starting iteration 585
[2018-01-21 14:03:54.481735 UTC] Start collecting samples
[2018-01-21 14:03:58.877607 UTC] Computing input variables for policy optimization
[2018-01-21 14:03:59.024824 UTC] Performing policy update
[2018-01-21 14:03:59.025469 UTC] Computing gradient in Euclidean space
[2018-01-21 14:03:59.149046 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:00.576679 UTC] Performing line search
[2018-01-21 14:04:00.766464 UTC] Updating baseline
[2018-01-21 14:04:02.692200 UTC] Computing logging information
-------------------------------------
| Iteration            | 585        |
| ExpectedImprovement  | 0.018392   |
| ActualImprovement    | 0.017102   |
| ImprovementRatio     | 0.92988    |
| MeanKL               | 0.0070866  |
| Entropy              | -0.24855   |
| Perplexity           | 0.77993    |
| AveragePolicyStd     | 0.23502    |
| AveragePolicyStd[0]  | 0.24469    |
| AveragePolicyStd[1]  | 0.28947    |
| AveragePolicyStd[2]  | 0.1718     |
| AveragePolicyStd[3]  | 0.24079    |
| AveragePolicyStd[4]  | 0.21587    |
| AveragePolicyStd[5]  | 0.2475     |
| AverageReturn        | 1207.9     |
| MinReturn            | 12.297     |
| MaxReturn            | 1365.5     |
| StdReturn            | 278.47     |
| AverageEpisodeLength | 928.27     |
| MinEpisodeLength     | 26         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.59     |
| TotalNEpisodes       | 19655      |
| TotalNSamples        | 2.9256e+06 |
| ExplainedVariance    | 0.14879    |
-------------------------------------
[2018-01-21 14:04:03.347992 UTC] Saving snapshot
[2018-01-21 14:04:03.348184 UTC] Starting iteration 586
[2018-01-21 14:04:03.348295 UTC] Start collecting samples
[2018-01-21 14:04:07.892336 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:08.016564 UTC] Performing policy update
[2018-01-21 14:04:08.017162 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:08.141140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:09.591894 UTC] Performing line search
[2018-01-21 14:04:09.776321 UTC] Updating baseline
[2018-01-21 14:04:12.821389 UTC] Computing logging information
-------------------------------------
| Iteration            | 586        |
| ExpectedImprovement  | 0.015423   |
| ActualImprovement    | 0.014504   |
| ImprovementRatio     | 0.94039    |
| MeanKL               | 0.0075699  |
| Entropy              | -0.25557   |
| Perplexity           | 0.77447    |
| AveragePolicyStd     | 0.23471    |
| AveragePolicyStd[0]  | 0.24437    |
| AveragePolicyStd[1]  | 0.28863    |
| AveragePolicyStd[2]  | 0.17188    |
| AveragePolicyStd[3]  | 0.24077    |
| AveragePolicyStd[4]  | 0.21554    |
| AveragePolicyStd[5]  | 0.24709    |
| AverageReturn        | 1219.6     |
| MinReturn            | 113.22     |
| MaxReturn            | 1365.5     |
| StdReturn            | 251.57     |
| AverageEpisodeLength | 938.01     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.73     |
| TotalNEpisodes       | 19662      |
| TotalNSamples        | 2.9326e+06 |
| ExplainedVariance    | -0.0056403 |
-------------------------------------
[2018-01-21 14:04:13.503804 UTC] Saving snapshot
[2018-01-21 14:04:13.504089 UTC] Starting iteration 587
[2018-01-21 14:04:13.504270 UTC] Start collecting samples
[2018-01-21 14:04:17.860700 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:17.990766 UTC] Performing policy update
[2018-01-21 14:04:17.991926 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:18.118921 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:19.582471 UTC] Performing line search
[2018-01-21 14:04:19.768137 UTC] Updating baseline
[2018-01-21 14:04:22.279966 UTC] Computing logging information
-------------------------------------
| Iteration            | 587        |
| ExpectedImprovement  | 0.017447   |
| ActualImprovement    | 0.016497   |
| ImprovementRatio     | 0.94559    |
| MeanKL               | 0.0073762  |
| Entropy              | -0.25957   |
| Perplexity           | 0.77138    |
| AveragePolicyStd     | 0.2346     |
| AveragePolicyStd[0]  | 0.24394    |
| AveragePolicyStd[1]  | 0.28802    |
| AveragePolicyStd[2]  | 0.17112    |
| AveragePolicyStd[3]  | 0.24186    |
| AveragePolicyStd[4]  | 0.21502    |
| AveragePolicyStd[5]  | 0.24761    |
| AverageReturn        | 1223       |
| MinReturn            | 113.22     |
| MaxReturn            | 1365.5     |
| StdReturn            | 251.17     |
| AverageEpisodeLength | 939.22     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.39     |
| TotalNEpisodes       | 19665      |
| TotalNSamples        | 2.9355e+06 |
| ExplainedVariance    | 0.1861     |
-------------------------------------
[2018-01-21 14:04:22.958933 UTC] Saving snapshot
[2018-01-21 14:04:22.959171 UTC] Starting iteration 588
[2018-01-21 14:04:22.959320 UTC] Start collecting samples
[2018-01-21 14:04:27.321594 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:27.475606 UTC] Performing policy update
[2018-01-21 14:04:27.476240 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:27.591601 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:29.042526 UTC] Performing line search
[2018-01-21 14:04:29.243938 UTC] Updating baseline
[2018-01-21 14:04:31.063387 UTC] Computing logging information
-------------------------------------
| Iteration            | 588        |
| ExpectedImprovement  | 0.019118   |
| ActualImprovement    | 0.018003   |
| ImprovementRatio     | 0.94166    |
| MeanKL               | 0.0072829  |
| Entropy              | -0.25927   |
| Perplexity           | 0.77161    |
| AveragePolicyStd     | 0.2346     |
| AveragePolicyStd[0]  | 0.24464    |
| AveragePolicyStd[1]  | 0.28832    |
| AveragePolicyStd[2]  | 0.1712     |
| AveragePolicyStd[3]  | 0.24129    |
| AveragePolicyStd[4]  | 0.21556    |
| AveragePolicyStd[5]  | 0.2466     |
| AverageReturn        | 1225.9     |
| MinReturn            | 113.22     |
| MaxReturn            | 1365.5     |
| StdReturn            | 252.23     |
| AverageEpisodeLength | 939.22     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.39     |
| TotalNEpisodes       | 19671      |
| TotalNSamples        | 2.9415e+06 |
| ExplainedVariance    | -0.017654  |
-------------------------------------
[2018-01-21 14:04:31.730056 UTC] Saving snapshot
[2018-01-21 14:04:31.730295 UTC] Starting iteration 589
[2018-01-21 14:04:31.730457 UTC] Start collecting samples
[2018-01-21 14:04:36.559473 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:36.708006 UTC] Performing policy update
[2018-01-21 14:04:36.709229 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:36.831728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:38.344999 UTC] Performing line search
[2018-01-21 14:04:38.536072 UTC] Updating baseline
[2018-01-21 14:04:40.585627 UTC] Computing logging information
-------------------------------------
| Iteration            | 589        |
| ExpectedImprovement  | 0.016115   |
| ActualImprovement    | 0.014636   |
| ImprovementRatio     | 0.90823    |
| MeanKL               | 0.0077428  |
| Entropy              | -0.26171   |
| Perplexity           | 0.76974    |
| AveragePolicyStd     | 0.23446    |
| AveragePolicyStd[0]  | 0.24432    |
| AveragePolicyStd[1]  | 0.28774    |
| AveragePolicyStd[2]  | 0.17137    |
| AveragePolicyStd[3]  | 0.2412     |
| AveragePolicyStd[4]  | 0.21607    |
| AveragePolicyStd[5]  | 0.24607    |
| AverageReturn        | 1231.9     |
| MinReturn            | 113.22     |
| MaxReturn            | 1365.2     |
| StdReturn            | 234.07     |
| AverageEpisodeLength | 943.49     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.57     |
| TotalNEpisodes       | 19679      |
| TotalNSamples        | 2.9485e+06 |
| ExplainedVariance    | 0.19379    |
-------------------------------------
[2018-01-21 14:04:41.284738 UTC] Saving snapshot
[2018-01-21 14:04:41.285000 UTC] Starting iteration 590
[2018-01-21 14:04:41.285182 UTC] Start collecting samples
[2018-01-21 14:04:45.698567 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:45.828979 UTC] Performing policy update
[2018-01-21 14:04:45.829709 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:45.948312 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:47.357659 UTC] Performing line search
[2018-01-21 14:04:47.558934 UTC] Updating baseline
[2018-01-21 14:04:49.473502 UTC] Computing logging information
-------------------------------------
| Iteration            | 590        |
| ExpectedImprovement  | 0.019266   |
| ActualImprovement    | 0.017863   |
| ImprovementRatio     | 0.92716    |
| MeanKL               | 0.0077999  |
| Entropy              | -0.27135   |
| Perplexity           | 0.76235    |
| AveragePolicyStd     | 0.23409    |
| AveragePolicyStd[0]  | 0.24395    |
| AveragePolicyStd[1]  | 0.28755    |
| AveragePolicyStd[2]  | 0.17115    |
| AveragePolicyStd[3]  | 0.24066    |
| AveragePolicyStd[4]  | 0.21551    |
| AveragePolicyStd[5]  | 0.24574    |
| AverageReturn        | 1220.7     |
| MinReturn            | 113.22     |
| MaxReturn            | 1365.2     |
| StdReturn            | 253.59     |
| AverageEpisodeLength | 935.06     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.92     |
| TotalNEpisodes       | 19684      |
| TotalNSamples        | 2.9524e+06 |
| ExplainedVariance    | 0.34192    |
-------------------------------------
[2018-01-21 14:04:50.224948 UTC] Saving snapshot
[2018-01-21 14:04:50.234912 UTC] Starting iteration 591
[2018-01-21 14:04:50.235150 UTC] Start collecting samples
[2018-01-21 14:04:54.593525 UTC] Computing input variables for policy optimization
[2018-01-21 14:04:54.712338 UTC] Performing policy update
[2018-01-21 14:04:54.712970 UTC] Computing gradient in Euclidean space
[2018-01-21 14:04:54.840435 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:04:56.296199 UTC] Performing line search
[2018-01-21 14:04:56.495211 UTC] Updating baseline
[2018-01-21 14:04:58.745871 UTC] Computing logging information
-------------------------------------
| Iteration            | 591        |
| ExpectedImprovement  | 0.016322   |
| ActualImprovement    | 0.01602    |
| ImprovementRatio     | 0.98145    |
| MeanKL               | 0.0071547  |
| Entropy              | -0.2858    |
| Perplexity           | 0.75141    |
| AveragePolicyStd     | 0.23352    |
| AveragePolicyStd[0]  | 0.24249    |
| AveragePolicyStd[1]  | 0.28733    |
| AveragePolicyStd[2]  | 0.1709     |
| AveragePolicyStd[3]  | 0.2399     |
| AveragePolicyStd[4]  | 0.21529    |
| AveragePolicyStd[5]  | 0.24523    |
| AverageReturn        | 1221.3     |
| MinReturn            | 113.22     |
| MaxReturn            | 1368.5     |
| StdReturn            | 253.88     |
| AverageEpisodeLength | 935.06     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.92     |
| TotalNEpisodes       | 19687      |
| TotalNSamples        | 2.9554e+06 |
| ExplainedVariance    | -0.065674  |
-------------------------------------
[2018-01-21 14:04:59.510324 UTC] Saving snapshot
[2018-01-21 14:04:59.510619 UTC] Starting iteration 592
[2018-01-21 14:04:59.510811 UTC] Start collecting samples
[2018-01-21 14:05:04.123052 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:04.248092 UTC] Performing policy update
[2018-01-21 14:05:04.249395 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:04.367256 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:05.765653 UTC] Performing line search
[2018-01-21 14:05:05.955364 UTC] Updating baseline
[2018-01-21 14:05:08.100667 UTC] Computing logging information
-------------------------------------
| Iteration            | 592        |
| ExpectedImprovement  | 0.018647   |
| ActualImprovement    | 0.017605   |
| ImprovementRatio     | 0.9441     |
| MeanKL               | 0.0073291  |
| Entropy              | -0.28419   |
| Perplexity           | 0.75262    |
| AveragePolicyStd     | 0.23362    |
| AveragePolicyStd[0]  | 0.24278    |
| AveragePolicyStd[1]  | 0.28741    |
| AveragePolicyStd[2]  | 0.17045    |
| AveragePolicyStd[3]  | 0.23975    |
| AveragePolicyStd[4]  | 0.21539    |
| AveragePolicyStd[5]  | 0.24596    |
| AverageReturn        | 1224       |
| MinReturn            | 113.22     |
| MaxReturn            | 1372.9     |
| StdReturn            | 254.77     |
| AverageEpisodeLength | 935.06     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.92     |
| TotalNEpisodes       | 19695      |
| TotalNSamples        | 2.9634e+06 |
| ExplainedVariance    | -0.009905  |
-------------------------------------
[2018-01-21 14:05:08.770934 UTC] Saving snapshot
[2018-01-21 14:05:08.771140 UTC] Starting iteration 593
[2018-01-21 14:05:08.771305 UTC] Start collecting samples
[2018-01-21 14:05:13.301541 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:13.421433 UTC] Performing policy update
[2018-01-21 14:05:13.422048 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:13.540038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:15.002902 UTC] Performing line search
[2018-01-21 14:05:15.203696 UTC] Updating baseline
[2018-01-21 14:05:17.472349 UTC] Computing logging information
--------------------------------------
| Iteration            | 593         |
| ExpectedImprovement  | 0.01549     |
| ActualImprovement    | 0.014499    |
| ImprovementRatio     | 0.93606     |
| MeanKL               | 0.0079481   |
| Entropy              | -0.2821     |
| Perplexity           | 0.7542      |
| AveragePolicyStd     | 0.23372     |
| AveragePolicyStd[0]  | 0.24364     |
| AveragePolicyStd[1]  | 0.28802     |
| AveragePolicyStd[2]  | 0.17056     |
| AveragePolicyStd[3]  | 0.23882     |
| AveragePolicyStd[4]  | 0.21514     |
| AveragePolicyStd[5]  | 0.24617     |
| AverageReturn        | 1224.7      |
| MinReturn            | 113.22      |
| MaxReturn            | 1372.9      |
| StdReturn            | 255.07      |
| AverageEpisodeLength | 935.06      |
| MinEpisodeLength     | 106         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 185.92      |
| TotalNEpisodes       | 19699       |
| TotalNSamples        | 2.9674e+06  |
| ExplainedVariance    | -0.00044952 |
--------------------------------------
[2018-01-21 14:05:18.156551 UTC] Saving snapshot
[2018-01-21 14:05:18.156880 UTC] Starting iteration 594
[2018-01-21 14:05:18.157201 UTC] Start collecting samples
[2018-01-21 14:05:22.684937 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:22.825808 UTC] Performing policy update
[2018-01-21 14:05:22.826478 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:22.951380 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:24.374042 UTC] Performing line search
[2018-01-21 14:05:24.558350 UTC] Updating baseline
[2018-01-21 14:05:27.057572 UTC] Computing logging information
------------------------------------
| Iteration            | 594       |
| ExpectedImprovement  | 0.0177    |
| ActualImprovement    | 0.016328  |
| ImprovementRatio     | 0.92248   |
| MeanKL               | 0.0074937 |
| Entropy              | -0.2831   |
| Perplexity           | 0.75345   |
| AveragePolicyStd     | 0.23368   |
| AveragePolicyStd[0]  | 0.24427   |
| AveragePolicyStd[1]  | 0.28811   |
| AveragePolicyStd[2]  | 0.17059   |
| AveragePolicyStd[3]  | 0.23929   |
| AveragePolicyStd[4]  | 0.21533   |
| AveragePolicyStd[5]  | 0.24448   |
| AverageReturn        | 1219.4    |
| MinReturn            | 113.22    |
| MaxReturn            | 1374.8    |
| StdReturn            | 259.79    |
| AverageEpisodeLength | 930.7     |
| MinEpisodeLength     | 106       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 189.43    |
| TotalNEpisodes       | 19703     |
| TotalNSamples        | 2.971e+06 |
| ExplainedVariance    | 0.23241   |
------------------------------------
[2018-01-21 14:05:27.737343 UTC] Saving snapshot
[2018-01-21 14:05:27.737586 UTC] Starting iteration 595
[2018-01-21 14:05:27.737758 UTC] Start collecting samples
[2018-01-21 14:05:32.529529 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:32.675563 UTC] Performing policy update
[2018-01-21 14:05:32.676247 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:32.809888 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:34.248526 UTC] Performing line search
[2018-01-21 14:05:34.437015 UTC] Updating baseline
[2018-01-21 14:05:36.437520 UTC] Computing logging information
-------------------------------------
| Iteration            | 595        |
| ExpectedImprovement  | 0.015414   |
| ActualImprovement    | 0.014774   |
| ImprovementRatio     | 0.95848    |
| MeanKL               | 0.0075718  |
| Entropy              | -0.29821   |
| Perplexity           | 0.74215    |
| AveragePolicyStd     | 0.23305    |
| AveragePolicyStd[0]  | 0.24295    |
| AveragePolicyStd[1]  | 0.28695    |
| AveragePolicyStd[2]  | 0.17058    |
| AveragePolicyStd[3]  | 0.23925    |
| AveragePolicyStd[4]  | 0.21481    |
| AveragePolicyStd[5]  | 0.24374    |
| AverageReturn        | 1235.3     |
| MinReturn            | 113.22     |
| MaxReturn            | 1374.8     |
| StdReturn            | 236.37     |
| AverageEpisodeLength | 941.99     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.91     |
| TotalNEpisodes       | 19711      |
| TotalNSamples        | 2.9789e+06 |
| ExplainedVariance    | 0.17275    |
-------------------------------------
[2018-01-21 14:05:37.132642 UTC] Saving snapshot
[2018-01-21 14:05:37.132846 UTC] Starting iteration 596
[2018-01-21 14:05:37.132985 UTC] Start collecting samples
[2018-01-21 14:05:41.641334 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:41.791229 UTC] Performing policy update
[2018-01-21 14:05:41.791834 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:41.920736 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:43.365538 UTC] Performing line search
[2018-01-21 14:05:43.563895 UTC] Updating baseline
[2018-01-21 14:05:46.073845 UTC] Computing logging information
-------------------------------------
| Iteration            | 596        |
| ExpectedImprovement  | 0.016987   |
| ActualImprovement    | 0.015845   |
| ImprovementRatio     | 0.93279    |
| MeanKL               | 0.0074363  |
| Entropy              | -0.30834   |
| Perplexity           | 0.73467    |
| AveragePolicyStd     | 0.23265    |
| AveragePolicyStd[0]  | 0.24273    |
| AveragePolicyStd[1]  | 0.28637    |
| AveragePolicyStd[2]  | 0.17038    |
| AveragePolicyStd[3]  | 0.23889    |
| AveragePolicyStd[4]  | 0.214      |
| AveragePolicyStd[5]  | 0.24355    |
| AverageReturn        | 1246.8     |
| MinReturn            | 113.22     |
| MaxReturn            | 1374.8     |
| StdReturn            | 226.17     |
| AverageEpisodeLength | 949.6      |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.95     |
| TotalNEpisodes       | 19714      |
| TotalNSamples        | 2.9819e+06 |
| ExplainedVariance    | 0.019904   |
-------------------------------------
[2018-01-21 14:05:46.734239 UTC] Saving snapshot
[2018-01-21 14:05:46.734492 UTC] Starting iteration 597
[2018-01-21 14:05:46.734656 UTC] Start collecting samples
[2018-01-21 14:05:51.199394 UTC] Computing input variables for policy optimization
[2018-01-21 14:05:51.330541 UTC] Performing policy update
[2018-01-21 14:05:51.331783 UTC] Computing gradient in Euclidean space
[2018-01-21 14:05:51.451494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:05:52.894156 UTC] Performing line search
[2018-01-21 14:05:53.094132 UTC] Updating baseline
[2018-01-21 14:05:54.878548 UTC] Computing logging information
-------------------------------------
| Iteration            | 597        |
| ExpectedImprovement  | 0.016652   |
| ActualImprovement    | 0.015765   |
| ImprovementRatio     | 0.94672    |
| MeanKL               | 0.0076985  |
| Entropy              | -0.30638   |
| Perplexity           | 0.73611    |
| AveragePolicyStd     | 0.23281    |
| AveragePolicyStd[0]  | 0.24347    |
| AveragePolicyStd[1]  | 0.2877     |
| AveragePolicyStd[2]  | 0.17011    |
| AveragePolicyStd[3]  | 0.23897    |
| AveragePolicyStd[4]  | 0.2128     |
| AveragePolicyStd[5]  | 0.24383    |
| AverageReturn        | 1247.9     |
| MinReturn            | 113.22     |
| MaxReturn            | 1374.8     |
| StdReturn            | 226.6      |
| AverageEpisodeLength | 949.6      |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.95     |
| TotalNEpisodes       | 19717      |
| TotalNSamples        | 2.9849e+06 |
| ExplainedVariance    | 0.029845   |
-------------------------------------
[2018-01-21 14:05:55.552900 UTC] Saving snapshot
[2018-01-21 14:05:55.553081 UTC] Starting iteration 598
[2018-01-21 14:05:55.553184 UTC] Start collecting samples
[2018-01-21 14:06:00.072050 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:00.221720 UTC] Performing policy update
[2018-01-21 14:06:00.222853 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:00.346275 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:01.815354 UTC] Performing line search
[2018-01-21 14:06:02.015347 UTC] Updating baseline
[2018-01-21 14:06:04.007757 UTC] Computing logging information
-------------------------------------
| Iteration            | 598        |
| ExpectedImprovement  | 0.017358   |
| ActualImprovement    | 0.016396   |
| ImprovementRatio     | 0.94455    |
| MeanKL               | 0.0077882  |
| Entropy              | -0.30047   |
| Perplexity           | 0.74047    |
| AveragePolicyStd     | 0.23307    |
| AveragePolicyStd[0]  | 0.2425     |
| AveragePolicyStd[1]  | 0.28849    |
| AveragePolicyStd[2]  | 0.16981    |
| AveragePolicyStd[3]  | 0.23948    |
| AveragePolicyStd[4]  | 0.21396    |
| AveragePolicyStd[5]  | 0.24417    |
| AverageReturn        | 1251       |
| MinReturn            | 113.22     |
| MaxReturn            | 1374.8     |
| StdReturn            | 227.38     |
| AverageEpisodeLength | 949.81     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.91     |
| TotalNEpisodes       | 19726      |
| TotalNSamples        | 2.9938e+06 |
| ExplainedVariance    | 0.082403   |
-------------------------------------
[2018-01-21 14:06:04.724497 UTC] Saving snapshot
[2018-01-21 14:06:04.724784 UTC] Starting iteration 599
[2018-01-21 14:06:04.724962 UTC] Start collecting samples
[2018-01-21 14:06:09.113696 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:09.234747 UTC] Performing policy update
[2018-01-21 14:06:09.235916 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:09.356431 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:10.782836 UTC] Performing line search
[2018-01-21 14:06:10.984391 UTC] Updating baseline
[2018-01-21 14:06:13.005040 UTC] Computing logging information
-------------------------------------
| Iteration            | 599        |
| ExpectedImprovement  | 0.016238   |
| ActualImprovement    | 0.015777   |
| ImprovementRatio     | 0.97161    |
| MeanKL               | 0.0087751  |
| Entropy              | -0.31656   |
| Perplexity           | 0.72865    |
| AveragePolicyStd     | 0.23241    |
| AveragePolicyStd[0]  | 0.24203    |
| AveragePolicyStd[1]  | 0.2877     |
| AveragePolicyStd[2]  | 0.17011    |
| AveragePolicyStd[3]  | 0.23946    |
| AveragePolicyStd[4]  | 0.21254    |
| AveragePolicyStd[5]  | 0.2426     |
| AverageReturn        | 1243.5     |
| MinReturn            | 113.22     |
| MaxReturn            | 1374.8     |
| StdReturn            | 241.43     |
| AverageEpisodeLength | 943.48     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.77     |
| TotalNEpisodes       | 19730      |
| TotalNSamples        | 2.9972e+06 |
| ExplainedVariance    | 0.22243    |
-------------------------------------
[2018-01-21 14:06:13.747630 UTC] Saving snapshot
[2018-01-21 14:06:13.748232 UTC] Starting iteration 600
[2018-01-21 14:06:13.748678 UTC] Start collecting samples
[2018-01-21 14:06:18.304053 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:18.439423 UTC] Performing policy update
[2018-01-21 14:06:18.440047 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:18.560502 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:20.011228 UTC] Performing line search
[2018-01-21 14:06:20.222828 UTC] Updating baseline
[2018-01-21 14:06:22.758724 UTC] Computing logging information
-------------------------------------
| Iteration            | 600        |
| ExpectedImprovement  | 0.01683    |
| ActualImprovement    | 0.016662   |
| ImprovementRatio     | 0.99       |
| MeanKL               | 0.0076977  |
| Entropy              | -0.31585   |
| Perplexity           | 0.72917    |
| AveragePolicyStd     | 0.23244    |
| AveragePolicyStd[0]  | 0.24242    |
| AveragePolicyStd[1]  | 0.28771    |
| AveragePolicyStd[2]  | 0.17015    |
| AveragePolicyStd[3]  | 0.23961    |
| AveragePolicyStd[4]  | 0.21214    |
| AveragePolicyStd[5]  | 0.24261    |
| AverageReturn        | 1244.8     |
| MinReturn            | 113.22     |
| MaxReturn            | 1380.2     |
| StdReturn            | 241.94     |
| AverageEpisodeLength | 943.48     |
| MinEpisodeLength     | 106        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.77     |
| TotalNEpisodes       | 19733      |
| TotalNSamples        | 3.0002e+06 |
| ExplainedVariance    | -0.14007   |
-------------------------------------
[2018-01-21 14:06:23.497437 UTC] Saving snapshot
[2018-01-21 14:06:23.507533 UTC] Starting iteration 601
[2018-01-21 14:06:23.507772 UTC] Start collecting samples
[2018-01-21 14:06:28.105240 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:28.235230 UTC] Performing policy update
[2018-01-21 14:06:28.235831 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:28.356399 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:29.836860 UTC] Performing line search
[2018-01-21 14:06:30.057484 UTC] Updating baseline
[2018-01-21 14:06:32.049512 UTC] Computing logging information
-------------------------------------
| Iteration            | 601        |
| ExpectedImprovement  | 0.016079   |
| ActualImprovement    | 0.015563   |
| ImprovementRatio     | 0.96788    |
| MeanKL               | 0.0077948  |
| Entropy              | -0.31774   |
| Perplexity           | 0.72779    |
| AveragePolicyStd     | 0.23231    |
| AveragePolicyStd[0]  | 0.24229    |
| AveragePolicyStd[1]  | 0.28692    |
| AveragePolicyStd[2]  | 0.17037    |
| AveragePolicyStd[3]  | 0.23936    |
| AveragePolicyStd[4]  | 0.21276    |
| AveragePolicyStd[5]  | 0.24218    |
| AverageReturn        | 1263.4     |
| MinReturn            | 275.62     |
| MaxReturn            | 1380.2     |
| StdReturn            | 209.51     |
| AverageEpisodeLength | 955.87     |
| MinEpisodeLength     | 241        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.2      |
| TotalNEpisodes       | 19741      |
| TotalNSamples        | 3.0082e+06 |
| ExplainedVariance    | -0.0004205 |
-------------------------------------
[2018-01-21 14:06:32.752077 UTC] Saving snapshot
[2018-01-21 14:06:32.752356 UTC] Starting iteration 602
[2018-01-21 14:06:32.752540 UTC] Start collecting samples
[2018-01-21 14:06:37.145133 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:37.282886 UTC] Performing policy update
[2018-01-21 14:06:37.283420 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:37.404729 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:38.868143 UTC] Performing line search
[2018-01-21 14:06:39.072776 UTC] Updating baseline
[2018-01-21 14:06:40.988607 UTC] Computing logging information
-------------------------------------
| Iteration            | 602        |
| ExpectedImprovement  | 0.017668   |
| ActualImprovement    | 0.016466   |
| ImprovementRatio     | 0.93198    |
| MeanKL               | 0.007326   |
| Entropy              | -0.31827   |
| Perplexity           | 0.72741    |
| AveragePolicyStd     | 0.23226    |
| AveragePolicyStd[0]  | 0.24247    |
| AveragePolicyStd[1]  | 0.28706    |
| AveragePolicyStd[2]  | 0.17074    |
| AveragePolicyStd[3]  | 0.23857    |
| AveragePolicyStd[4]  | 0.21355    |
| AveragePolicyStd[5]  | 0.24115    |
| AverageReturn        | 1264.2     |
| MinReturn            | 275.62     |
| MaxReturn            | 1380.2     |
| StdReturn            | 209.68     |
| AverageEpisodeLength | 955.87     |
| MinEpisodeLength     | 241        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.2      |
| TotalNEpisodes       | 19745      |
| TotalNSamples        | 3.0122e+06 |
| ExplainedVariance    | -0.0018624 |
-------------------------------------
[2018-01-21 14:06:41.688441 UTC] Saving snapshot
[2018-01-21 14:06:41.688690 UTC] Starting iteration 603
[2018-01-21 14:06:41.688876 UTC] Start collecting samples
[2018-01-21 14:06:46.339087 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:46.462117 UTC] Performing policy update
[2018-01-21 14:06:46.462783 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:46.581676 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:47.971277 UTC] Performing line search
[2018-01-21 14:06:48.161259 UTC] Updating baseline
[2018-01-21 14:06:50.180387 UTC] Computing logging information
-------------------------------------
| Iteration            | 603        |
| ExpectedImprovement  | 0.017047   |
| ActualImprovement    | 0.015728   |
| ImprovementRatio     | 0.92261    |
| MeanKL               | 0.0073956  |
| Entropy              | -0.32356   |
| Perplexity           | 0.72357    |
| AveragePolicyStd     | 0.23202    |
| AveragePolicyStd[0]  | 0.2414     |
| AveragePolicyStd[1]  | 0.28548    |
| AveragePolicyStd[2]  | 0.17049    |
| AveragePolicyStd[3]  | 0.23961    |
| AveragePolicyStd[4]  | 0.21362    |
| AveragePolicyStd[5]  | 0.24149    |
| AverageReturn        | 1273.4     |
| MinReturn            | 275.62     |
| MaxReturn            | 1380.2     |
| StdReturn            | 188.37     |
| AverageEpisodeLength | 963.07     |
| MinEpisodeLength     | 241        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.89     |
| TotalNEpisodes       | 19749      |
| TotalNSamples        | 3.0162e+06 |
| ExplainedVariance    | -0.0023519 |
-------------------------------------
[2018-01-21 14:06:50.939641 UTC] Saving snapshot
[2018-01-21 14:06:50.939916 UTC] Starting iteration 604
[2018-01-21 14:06:50.940101 UTC] Start collecting samples
[2018-01-21 14:06:55.463043 UTC] Computing input variables for policy optimization
[2018-01-21 14:06:55.593093 UTC] Performing policy update
[2018-01-21 14:06:55.593830 UTC] Computing gradient in Euclidean space
[2018-01-21 14:06:55.712531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:06:57.180078 UTC] Performing line search
[2018-01-21 14:06:57.379647 UTC] Updating baseline
[2018-01-21 14:06:59.455845 UTC] Computing logging information
-------------------------------------
| Iteration            | 604        |
| ExpectedImprovement  | 0.016534   |
| ActualImprovement    | 0.015714   |
| ImprovementRatio     | 0.95037    |
| MeanKL               | 0.0077949  |
| Entropy              | -0.33622   |
| Perplexity           | 0.71447    |
| AveragePolicyStd     | 0.23153    |
| AveragePolicyStd[0]  | 0.24088    |
| AveragePolicyStd[1]  | 0.28461    |
| AveragePolicyStd[2]  | 0.17005    |
| AveragePolicyStd[3]  | 0.23895    |
| AveragePolicyStd[4]  | 0.21313    |
| AveragePolicyStd[5]  | 0.24154    |
| AverageReturn        | 1267       |
| MinReturn            | 275.62     |
| MaxReturn            | 1380.2     |
| StdReturn            | 206.4      |
| AverageEpisodeLength | 959.16     |
| MinEpisodeLength     | 241        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.28     |
| TotalNEpisodes       | 19755      |
| TotalNSamples        | 3.0215e+06 |
| ExplainedVariance    | 0.17862    |
-------------------------------------
[2018-01-21 14:07:00.194824 UTC] Saving snapshot
[2018-01-21 14:07:00.195067 UTC] Starting iteration 605
[2018-01-21 14:07:00.195237 UTC] Start collecting samples
[2018-01-21 14:07:04.769454 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:04.908142 UTC] Performing policy update
[2018-01-21 14:07:04.909172 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:05.027532 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:06.438555 UTC] Performing line search
[2018-01-21 14:07:06.631481 UTC] Updating baseline
[2018-01-21 14:07:08.582075 UTC] Computing logging information
-------------------------------------
| Iteration            | 605        |
| ExpectedImprovement  | 0.017819   |
| ActualImprovement    | 0.017192   |
| ImprovementRatio     | 0.96486    |
| MeanKL               | 0.0072687  |
| Entropy              | -0.34093   |
| Perplexity           | 0.71111    |
| AveragePolicyStd     | 0.23132    |
| AveragePolicyStd[0]  | 0.24122    |
| AveragePolicyStd[1]  | 0.2835     |
| AveragePolicyStd[2]  | 0.16988    |
| AveragePolicyStd[3]  | 0.23946    |
| AveragePolicyStd[4]  | 0.21322    |
| AveragePolicyStd[5]  | 0.24063    |
| AverageReturn        | 1249.7     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 253.39     |
| AverageEpisodeLength | 944.04     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.58     |
| TotalNEpisodes       | 19763      |
| TotalNSamples        | 3.0279e+06 |
| ExplainedVariance    | 0.27896    |
-------------------------------------
[2018-01-21 14:07:09.260817 UTC] Saving snapshot
[2018-01-21 14:07:09.261060 UTC] Starting iteration 606
[2018-01-21 14:07:09.261216 UTC] Start collecting samples
[2018-01-21 14:07:13.724428 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:13.868951 UTC] Performing policy update
[2018-01-21 14:07:13.869562 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:13.991182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:15.400716 UTC] Performing line search
[2018-01-21 14:07:15.590722 UTC] Updating baseline
[2018-01-21 14:07:17.601982 UTC] Computing logging information
-------------------------------------
| Iteration            | 606        |
| ExpectedImprovement  | 0.017474   |
| ActualImprovement    | 0.015283   |
| ImprovementRatio     | 0.87459    |
| MeanKL               | 0.0073924  |
| Entropy              | -0.335     |
| Perplexity           | 0.71533    |
| AveragePolicyStd     | 0.2315     |
| AveragePolicyStd[0]  | 0.24112    |
| AveragePolicyStd[1]  | 0.28391    |
| AveragePolicyStd[2]  | 0.17093    |
| AveragePolicyStd[3]  | 0.23936    |
| AveragePolicyStd[4]  | 0.21274    |
| AveragePolicyStd[5]  | 0.24096    |
| AverageReturn        | 1240.7     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 265.74     |
| AverageEpisodeLength | 937.58     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.88     |
| TotalNEpisodes       | 19768      |
| TotalNSamples        | 3.0323e+06 |
| ExplainedVariance    | 0.16901    |
-------------------------------------
[2018-01-21 14:07:18.350659 UTC] Saving snapshot
[2018-01-21 14:07:18.350886 UTC] Starting iteration 607
[2018-01-21 14:07:18.351068 UTC] Start collecting samples
[2018-01-21 14:07:22.811876 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:22.949152 UTC] Performing policy update
[2018-01-21 14:07:22.949945 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:23.072179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:24.520819 UTC] Performing line search
[2018-01-21 14:07:24.711493 UTC] Updating baseline
[2018-01-21 14:07:26.558862 UTC] Computing logging information
-------------------------------------
| Iteration            | 607        |
| ExpectedImprovement  | 0.017818   |
| ActualImprovement    | 0.016696   |
| ImprovementRatio     | 0.93704    |
| MeanKL               | 0.0076155  |
| Entropy              | -0.33725   |
| Perplexity           | 0.71373    |
| AveragePolicyStd     | 0.23142    |
| AveragePolicyStd[0]  | 0.24118    |
| AveragePolicyStd[1]  | 0.28295    |
| AveragePolicyStd[2]  | 0.17059    |
| AveragePolicyStd[3]  | 0.23996    |
| AveragePolicyStd[4]  | 0.21226    |
| AveragePolicyStd[5]  | 0.24161    |
| AverageReturn        | 1238.7     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 271.98     |
| AverageEpisodeLength | 935.84     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.16     |
| TotalNEpisodes       | 19773      |
| TotalNSamples        | 3.0366e+06 |
| ExplainedVariance    | 0.10213    |
-------------------------------------
[2018-01-21 14:07:27.237429 UTC] Saving snapshot
[2018-01-21 14:07:27.237677 UTC] Starting iteration 608
[2018-01-21 14:07:27.237854 UTC] Start collecting samples
[2018-01-21 14:07:31.896901 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:32.022462 UTC] Performing policy update
[2018-01-21 14:07:32.023057 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:32.152090 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:33.770037 UTC] Performing line search
[2018-01-21 14:07:33.968002 UTC] Updating baseline
[2018-01-21 14:07:36.196017 UTC] Computing logging information
-------------------------------------
| Iteration            | 608        |
| ExpectedImprovement  | 0.016457   |
| ActualImprovement    | 0.016034   |
| ImprovementRatio     | 0.97428    |
| MeanKL               | 0.0078051  |
| Entropy              | -0.34071   |
| Perplexity           | 0.71126    |
| AveragePolicyStd     | 0.23133    |
| AveragePolicyStd[0]  | 0.24204    |
| AveragePolicyStd[1]  | 0.28356    |
| AveragePolicyStd[2]  | 0.17032    |
| AveragePolicyStd[3]  | 0.23971    |
| AveragePolicyStd[4]  | 0.21184    |
| AveragePolicyStd[5]  | 0.24051    |
| AverageReturn        | 1246.2     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 265.44     |
| AverageEpisodeLength | 940.8      |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.32     |
| TotalNEpisodes       | 19779      |
| TotalNSamples        | 3.0426e+06 |
| ExplainedVariance    | 0.019174   |
-------------------------------------
[2018-01-21 14:07:36.870447 UTC] Saving snapshot
[2018-01-21 14:07:36.870685 UTC] Starting iteration 609
[2018-01-21 14:07:36.870836 UTC] Start collecting samples
[2018-01-21 14:07:41.271535 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:41.420089 UTC] Performing policy update
[2018-01-21 14:07:41.420891 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:41.538919 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:43.000967 UTC] Performing line search
[2018-01-21 14:07:43.197288 UTC] Updating baseline
[2018-01-21 14:07:45.008943 UTC] Computing logging information
-------------------------------------
| Iteration            | 609        |
| ExpectedImprovement  | 0.017745   |
| ActualImprovement    | 0.017424   |
| ImprovementRatio     | 0.98193    |
| MeanKL               | 0.0074926  |
| Entropy              | -0.34101   |
| Perplexity           | 0.71105    |
| AveragePolicyStd     | 0.23134    |
| AveragePolicyStd[0]  | 0.24294    |
| AveragePolicyStd[1]  | 0.28284    |
| AveragePolicyStd[2]  | 0.16957    |
| AveragePolicyStd[3]  | 0.24005    |
| AveragePolicyStd[4]  | 0.2122     |
| AveragePolicyStd[5]  | 0.24045    |
| AverageReturn        | 1241.5     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 265.83     |
| AverageEpisodeLength | 936.65     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.55     |
| TotalNEpisodes       | 19786      |
| TotalNSamples        | 3.0481e+06 |
| ExplainedVariance    | 0.29825    |
-------------------------------------
[2018-01-21 14:07:45.719390 UTC] Saving snapshot
[2018-01-21 14:07:45.719624 UTC] Starting iteration 610
[2018-01-21 14:07:45.719770 UTC] Start collecting samples
[2018-01-21 14:07:50.247628 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:50.372628 UTC] Performing policy update
[2018-01-21 14:07:50.373866 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:50.497852 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:07:51.912668 UTC] Performing line search
[2018-01-21 14:07:52.128644 UTC] Updating baseline
[2018-01-21 14:07:54.207990 UTC] Computing logging information
-------------------------------------
| Iteration            | 610        |
| ExpectedImprovement  | 0.015105   |
| ActualImprovement    | 0.014545   |
| ImprovementRatio     | 0.96292    |
| MeanKL               | 0.0077206  |
| Entropy              | -0.34187   |
| Perplexity           | 0.71044    |
| AveragePolicyStd     | 0.23129    |
| AveragePolicyStd[0]  | 0.24203    |
| AveragePolicyStd[1]  | 0.28253    |
| AveragePolicyStd[2]  | 0.16976    |
| AveragePolicyStd[3]  | 0.24064    |
| AveragePolicyStd[4]  | 0.2121     |
| AveragePolicyStd[5]  | 0.24068    |
| AverageReturn        | 1241.6     |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 265.84     |
| AverageEpisodeLength | 936.65     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.55     |
| TotalNEpisodes       | 19789      |
| TotalNSamples        | 3.0511e+06 |
| ExplainedVariance    | 0.048577   |
-------------------------------------
[2018-01-21 14:07:54.951188 UTC] Saving snapshot
[2018-01-21 14:07:54.961151 UTC] Starting iteration 611
[2018-01-21 14:07:54.961380 UTC] Start collecting samples
[2018-01-21 14:07:59.396079 UTC] Computing input variables for policy optimization
[2018-01-21 14:07:59.542450 UTC] Performing policy update
[2018-01-21 14:07:59.543040 UTC] Computing gradient in Euclidean space
[2018-01-21 14:07:59.660485 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:01.077569 UTC] Performing line search
[2018-01-21 14:08:01.281404 UTC] Updating baseline
[2018-01-21 14:08:03.509229 UTC] Computing logging information
-------------------------------------
| Iteration            | 611        |
| ExpectedImprovement  | 0.016895   |
| ActualImprovement    | 0.016315   |
| ImprovementRatio     | 0.96563    |
| MeanKL               | 0.0075408  |
| Entropy              | -0.3522    |
| Perplexity           | 0.70314    |
| AveragePolicyStd     | 0.23091    |
| AveragePolicyStd[0]  | 0.24114    |
| AveragePolicyStd[1]  | 0.28219    |
| AveragePolicyStd[2]  | 0.16908    |
| AveragePolicyStd[3]  | 0.24045    |
| AveragePolicyStd[4]  | 0.21222    |
| AveragePolicyStd[5]  | 0.24038    |
| AverageReturn        | 1243       |
| MinReturn            | 151.83     |
| MaxReturn            | 1380.2     |
| StdReturn            | 266.32     |
| AverageEpisodeLength | 936.65     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.55     |
| TotalNEpisodes       | 19795      |
| TotalNSamples        | 3.0571e+06 |
| ExplainedVariance    | -0.055577  |
-------------------------------------
[2018-01-21 14:08:04.203682 UTC] Saving snapshot
[2018-01-21 14:08:04.203891 UTC] Starting iteration 612
[2018-01-21 14:08:04.204031 UTC] Start collecting samples
[2018-01-21 14:08:08.745277 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:08.873843 UTC] Performing policy update
[2018-01-21 14:08:08.874478 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:09.019857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:10.496444 UTC] Performing line search
[2018-01-21 14:08:10.684299 UTC] Updating baseline
[2018-01-21 14:08:12.460299 UTC] Computing logging information
------------------------------------
| Iteration            | 612       |
| ExpectedImprovement  | 0.016442  |
| ActualImprovement    | 0.01598   |
| ImprovementRatio     | 0.97194   |
| MeanKL               | 0.0076745 |
| Entropy              | -0.35584  |
| Perplexity           | 0.70059   |
| AveragePolicyStd     | 0.23076   |
| AveragePolicyStd[0]  | 0.24051   |
| AveragePolicyStd[1]  | 0.2818    |
| AveragePolicyStd[2]  | 0.1691    |
| AveragePolicyStd[3]  | 0.24049   |
| AveragePolicyStd[4]  | 0.21187   |
| AveragePolicyStd[5]  | 0.2408    |
| AverageReturn        | 1248      |
| MinReturn            | 151.83    |
| MaxReturn            | 1387      |
| StdReturn            | 261.88    |
| AverageEpisodeLength | 939.56    |
| MinEpisodeLength     | 180       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186       |
| TotalNEpisodes       | 19802     |
| TotalNSamples        | 3.064e+06 |
| ExplainedVariance    | 0.098538  |
------------------------------------
[2018-01-21 14:08:13.116394 UTC] Saving snapshot
[2018-01-21 14:08:13.116631 UTC] Starting iteration 613
[2018-01-21 14:08:13.116780 UTC] Start collecting samples
[2018-01-21 14:08:17.741177 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:17.868797 UTC] Performing policy update
[2018-01-21 14:08:17.869553 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:17.996059 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:19.420500 UTC] Performing line search
[2018-01-21 14:08:19.606596 UTC] Updating baseline
[2018-01-21 14:08:21.912034 UTC] Computing logging information
------------------------------------
| Iteration            | 613       |
| ExpectedImprovement  | 0.019026  |
| ActualImprovement    | 0.018333  |
| ImprovementRatio     | 0.96356   |
| MeanKL               | 0.007557  |
| Entropy              | -0.3528   |
| Perplexity           | 0.70272   |
| AveragePolicyStd     | 0.23089   |
| AveragePolicyStd[0]  | 0.24011   |
| AveragePolicyStd[1]  | 0.28176   |
| AveragePolicyStd[2]  | 0.16897   |
| AveragePolicyStd[3]  | 0.2406    |
| AveragePolicyStd[4]  | 0.21191   |
| AveragePolicyStd[5]  | 0.24199   |
| AverageReturn        | 1247.6    |
| MinReturn            | 151.83    |
| MaxReturn            | 1387      |
| StdReturn            | 261.74    |
| AverageEpisodeLength | 939.56    |
| MinEpisodeLength     | 180       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186       |
| TotalNEpisodes       | 19804     |
| TotalNSamples        | 3.066e+06 |
| ExplainedVariance    | -0.030924 |
------------------------------------
[2018-01-21 14:08:22.662193 UTC] Saving snapshot
[2018-01-21 14:08:22.662605 UTC] Starting iteration 614
[2018-01-21 14:08:22.663060 UTC] Start collecting samples
[2018-01-21 14:08:27.238034 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:27.360360 UTC] Performing policy update
[2018-01-21 14:08:27.361108 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:27.490010 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:28.944242 UTC] Performing line search
[2018-01-21 14:08:29.139902 UTC] Updating baseline
[2018-01-21 14:08:30.967851 UTC] Computing logging information
------------------------------------
| Iteration            | 614       |
| ExpectedImprovement  | 0.017589  |
| ActualImprovement    | 0.016631  |
| ImprovementRatio     | 0.94556   |
| MeanKL               | 0.0069257 |
| Entropy              | -0.35433  |
| Perplexity           | 0.70165   |
| AveragePolicyStd     | 0.23087   |
| AveragePolicyStd[0]  | 0.2402    |
| AveragePolicyStd[1]  | 0.28154   |
| AveragePolicyStd[2]  | 0.16831   |
| AveragePolicyStd[3]  | 0.2411    |
| AveragePolicyStd[4]  | 0.21183   |
| AveragePolicyStd[5]  | 0.24225   |
| AverageReturn        | 1251      |
| MinReturn            | 151.83    |
| MaxReturn            | 1387      |
| StdReturn            | 261.82    |
| AverageEpisodeLength | 940.86    |
| MinEpisodeLength     | 180       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.97    |
| TotalNEpisodes       | 19810     |
| TotalNSamples        | 3.072e+06 |
| ExplainedVariance    | 0.03028   |
------------------------------------
[2018-01-21 14:08:31.764182 UTC] Saving snapshot
[2018-01-21 14:08:31.764730 UTC] Starting iteration 615
[2018-01-21 14:08:31.765168 UTC] Start collecting samples
[2018-01-21 14:08:37.182611 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:37.315773 UTC] Performing policy update
[2018-01-21 14:08:37.316222 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:37.429922 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:38.889691 UTC] Performing line search
[2018-01-21 14:08:39.098228 UTC] Updating baseline
[2018-01-21 14:08:41.326423 UTC] Computing logging information
------------------------------------
| Iteration            | 615       |
| ExpectedImprovement  | 0.017766  |
| ActualImprovement    | 0.017104  |
| ImprovementRatio     | 0.96275   |
| MeanKL               | 0.0073206 |
| Entropy              | -0.35585  |
| Perplexity           | 0.70057   |
| AveragePolicyStd     | 0.23081   |
| AveragePolicyStd[0]  | 0.24041   |
| AveragePolicyStd[1]  | 0.28164   |
| AveragePolicyStd[2]  | 0.1684    |
| AveragePolicyStd[3]  | 0.24029   |
| AveragePolicyStd[4]  | 0.21191   |
| AveragePolicyStd[5]  | 0.24219   |
| AverageReturn        | 1250.2    |
| MinReturn            | 151.83    |
| MaxReturn            | 1387      |
| StdReturn            | 261.58    |
| AverageEpisodeLength | 940.86    |
| MinEpisodeLength     | 180       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.97    |
| TotalNEpisodes       | 19816     |
| TotalNSamples        | 3.078e+06 |
| ExplainedVariance    | 0.087     |
------------------------------------
[2018-01-21 14:08:42.028852 UTC] Saving snapshot
[2018-01-21 14:08:42.029089 UTC] Starting iteration 616
[2018-01-21 14:08:42.029271 UTC] Start collecting samples
[2018-01-21 14:08:46.565071 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:46.696725 UTC] Performing policy update
[2018-01-21 14:08:46.697883 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:46.831054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:48.296850 UTC] Performing line search
[2018-01-21 14:08:48.496478 UTC] Updating baseline
[2018-01-21 14:08:50.537219 UTC] Computing logging information
-------------------------------------
| Iteration            | 616        |
| ExpectedImprovement  | 0.018935   |
| ActualImprovement    | 0.017429   |
| ImprovementRatio     | 0.92043    |
| MeanKL               | 0.0072745  |
| Entropy              | -0.35405   |
| Perplexity           | 0.70184    |
| AveragePolicyStd     | 0.23085    |
| AveragePolicyStd[0]  | 0.24008    |
| AveragePolicyStd[1]  | 0.28205    |
| AveragePolicyStd[2]  | 0.16904    |
| AveragePolicyStd[3]  | 0.24032    |
| AveragePolicyStd[4]  | 0.21174    |
| AveragePolicyStd[5]  | 0.24186    |
| AverageReturn        | 1240.1     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 274.15     |
| AverageEpisodeLength | 932.91     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.46     |
| TotalNEpisodes       | 19822      |
| TotalNSamples        | 3.0831e+06 |
| ExplainedVariance    | 0.21551    |
-------------------------------------
[2018-01-21 14:08:51.209875 UTC] Saving snapshot
[2018-01-21 14:08:51.210113 UTC] Starting iteration 617
[2018-01-21 14:08:51.210273 UTC] Start collecting samples
[2018-01-21 14:08:55.712919 UTC] Computing input variables for policy optimization
[2018-01-21 14:08:55.847255 UTC] Performing policy update
[2018-01-21 14:08:55.848287 UTC] Computing gradient in Euclidean space
[2018-01-21 14:08:55.970316 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:08:57.384674 UTC] Performing line search
[2018-01-21 14:08:57.577103 UTC] Updating baseline
[2018-01-21 14:08:59.448896 UTC] Computing logging information
-------------------------------------
| Iteration            | 617        |
| ExpectedImprovement  | 0.015999   |
| ActualImprovement    | 0.015535   |
| ImprovementRatio     | 0.97104    |
| MeanKL               | 0.007065   |
| Entropy              | -0.35868   |
| Perplexity           | 0.6986     |
| AveragePolicyStd     | 0.23067    |
| AveragePolicyStd[0]  | 0.23984    |
| AveragePolicyStd[1]  | 0.28087    |
| AveragePolicyStd[2]  | 0.16842    |
| AveragePolicyStd[3]  | 0.24111    |
| AveragePolicyStd[4]  | 0.21199    |
| AveragePolicyStd[5]  | 0.24178    |
| AverageReturn        | 1236.9     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 273.89     |
| AverageEpisodeLength | 929.96     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.19     |
| TotalNEpisodes       | 19828      |
| TotalNSamples        | 3.0882e+06 |
| ExplainedVariance    | 0.3468     |
-------------------------------------
[2018-01-21 14:09:00.098536 UTC] Saving snapshot
[2018-01-21 14:09:00.098779 UTC] Starting iteration 618
[2018-01-21 14:09:00.098952 UTC] Start collecting samples
[2018-01-21 14:09:04.474147 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:04.605807 UTC] Performing policy update
[2018-01-21 14:09:04.606517 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:04.725017 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:06.115928 UTC] Performing line search
[2018-01-21 14:09:06.306112 UTC] Updating baseline
[2018-01-21 14:09:08.546024 UTC] Computing logging information
-------------------------------------
| Iteration            | 618        |
| ExpectedImprovement  | 0.016796   |
| ActualImprovement    | 0.015606   |
| ImprovementRatio     | 0.92914    |
| MeanKL               | 0.0071415  |
| Entropy              | -0.37527   |
| Perplexity           | 0.6871     |
| AveragePolicyStd     | 0.22995    |
| AveragePolicyStd[0]  | 0.23905    |
| AveragePolicyStd[1]  | 0.27928    |
| AveragePolicyStd[2]  | 0.16907    |
| AveragePolicyStd[3]  | 0.23994    |
| AveragePolicyStd[4]  | 0.21087    |
| AveragePolicyStd[5]  | 0.24148    |
| AverageReturn        | 1237.5     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 274.1      |
| AverageEpisodeLength | 929.96     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.19     |
| TotalNEpisodes       | 19831      |
| TotalNSamples        | 3.0912e+06 |
| ExplainedVariance    | 0.097478   |
-------------------------------------
[2018-01-21 14:09:09.296853 UTC] Saving snapshot
[2018-01-21 14:09:09.297136 UTC] Starting iteration 619
[2018-01-21 14:09:09.297324 UTC] Start collecting samples
[2018-01-21 14:09:13.991725 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:14.111174 UTC] Performing policy update
[2018-01-21 14:09:14.111748 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:14.230303 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:15.652424 UTC] Performing line search
[2018-01-21 14:09:15.855294 UTC] Updating baseline
[2018-01-21 14:09:17.924318 UTC] Computing logging information
-------------------------------------
| Iteration            | 619        |
| ExpectedImprovement  | 0.017473   |
| ActualImprovement    | 0.015949   |
| ImprovementRatio     | 0.9128     |
| MeanKL               | 0.0077241  |
| Entropy              | -0.37491   |
| Perplexity           | 0.68735    |
| AveragePolicyStd     | 0.22988    |
| AveragePolicyStd[0]  | 0.23953    |
| AveragePolicyStd[1]  | 0.27713    |
| AveragePolicyStd[2]  | 0.16936    |
| AveragePolicyStd[3]  | 0.23904    |
| AveragePolicyStd[4]  | 0.21149    |
| AveragePolicyStd[5]  | 0.24274    |
| AverageReturn        | 1240       |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 274.98     |
| AverageEpisodeLength | 929.96     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.19     |
| TotalNEpisodes       | 19838      |
| TotalNSamples        | 3.0982e+06 |
| ExplainedVariance    | 0.023153   |
-------------------------------------
[2018-01-21 14:09:18.606704 UTC] Saving snapshot
[2018-01-21 14:09:18.606934 UTC] Starting iteration 620
[2018-01-21 14:09:18.607099 UTC] Start collecting samples
[2018-01-21 14:09:23.108791 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:23.233426 UTC] Performing policy update
[2018-01-21 14:09:23.234014 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:23.356359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:24.805772 UTC] Performing line search
[2018-01-21 14:09:24.999955 UTC] Updating baseline
[2018-01-21 14:09:27.069258 UTC] Computing logging information
-------------------------------------
| Iteration            | 620        |
| ExpectedImprovement  | 0.017821   |
| ActualImprovement    | 0.017382   |
| ImprovementRatio     | 0.97535    |
| MeanKL               | 0.007114   |
| Entropy              | -0.38069   |
| Perplexity           | 0.68339    |
| AveragePolicyStd     | 0.22974    |
| AveragePolicyStd[0]  | 0.23892    |
| AveragePolicyStd[1]  | 0.2785     |
| AveragePolicyStd[2]  | 0.16917    |
| AveragePolicyStd[3]  | 0.23912    |
| AveragePolicyStd[4]  | 0.20955    |
| AveragePolicyStd[5]  | 0.24319    |
| AverageReturn        | 1240.5     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 275.14     |
| AverageEpisodeLength | 929.96     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.19     |
| TotalNEpisodes       | 19842      |
| TotalNSamples        | 3.1022e+06 |
| ExplainedVariance    | 0.0047116  |
-------------------------------------
[2018-01-21 14:09:27.731256 UTC] Saving snapshot
[2018-01-21 14:09:27.737069 UTC] Starting iteration 621
[2018-01-21 14:09:27.737243 UTC] Start collecting samples
[2018-01-21 14:09:32.209474 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:32.356888 UTC] Performing policy update
[2018-01-21 14:09:32.357646 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:32.512859 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:34.014790 UTC] Performing line search
[2018-01-21 14:09:34.238930 UTC] Updating baseline
[2018-01-21 14:09:36.221085 UTC] Computing logging information
-------------------------------------
| Iteration            | 621        |
| ExpectedImprovement  | 0.018479   |
| ActualImprovement    | 0.017448   |
| ImprovementRatio     | 0.94418    |
| MeanKL               | 0.0073828  |
| Entropy              | -0.37871   |
| Perplexity           | 0.68474    |
| AveragePolicyStd     | 0.22979    |
| AveragePolicyStd[0]  | 0.23922    |
| AveragePolicyStd[1]  | 0.27874    |
| AveragePolicyStd[2]  | 0.16974    |
| AveragePolicyStd[3]  | 0.23963    |
| AveragePolicyStd[4]  | 0.20931    |
| AveragePolicyStd[5]  | 0.24209    |
| AverageReturn        | 1231.9     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 290.95     |
| AverageEpisodeLength | 922.5      |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.31     |
| TotalNEpisodes       | 19847      |
| TotalNSamples        | 3.1064e+06 |
| ExplainedVariance    | 0.18474    |
-------------------------------------
[2018-01-21 14:09:36.958484 UTC] Saving snapshot
[2018-01-21 14:09:36.958726 UTC] Starting iteration 622
[2018-01-21 14:09:36.958876 UTC] Start collecting samples
[2018-01-21 14:09:41.673110 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:41.818473 UTC] Performing policy update
[2018-01-21 14:09:41.819075 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:41.944475 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:43.400857 UTC] Performing line search
[2018-01-21 14:09:43.603821 UTC] Updating baseline
[2018-01-21 14:09:46.536593 UTC] Computing logging information
-------------------------------------
| Iteration            | 622        |
| ExpectedImprovement  | 0.017943   |
| ActualImprovement    | 0.017215   |
| ImprovementRatio     | 0.95945    |
| MeanKL               | 0.00771    |
| Entropy              | -0.37968   |
| Perplexity           | 0.68408    |
| AveragePolicyStd     | 0.22973    |
| AveragePolicyStd[0]  | 0.23967    |
| AveragePolicyStd[1]  | 0.27857    |
| AveragePolicyStd[2]  | 0.17003    |
| AveragePolicyStd[3]  | 0.23899    |
| AveragePolicyStd[4]  | 0.20914    |
| AveragePolicyStd[5]  | 0.242      |
| AverageReturn        | 1244.3     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 278.3      |
| AverageEpisodeLength | 929.25     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.5      |
| TotalNEpisodes       | 19853      |
| TotalNSamples        | 3.1124e+06 |
| ExplainedVariance    | -0.020165  |
-------------------------------------
[2018-01-21 14:09:47.299717 UTC] Saving snapshot
[2018-01-21 14:09:47.299960 UTC] Starting iteration 623
[2018-01-21 14:09:47.300186 UTC] Start collecting samples
[2018-01-21 14:09:51.678797 UTC] Computing input variables for policy optimization
[2018-01-21 14:09:51.798454 UTC] Performing policy update
[2018-01-21 14:09:51.799052 UTC] Computing gradient in Euclidean space
[2018-01-21 14:09:51.936934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:09:53.401786 UTC] Performing line search
[2018-01-21 14:09:53.595149 UTC] Updating baseline
[2018-01-21 14:09:55.432083 UTC] Computing logging information
-------------------------------------
| Iteration            | 623        |
| ExpectedImprovement  | 0.015948   |
| ActualImprovement    | 0.015005   |
| ImprovementRatio     | 0.94086    |
| MeanKL               | 0.0076094  |
| Entropy              | -0.3876    |
| Perplexity           | 0.67869    |
| AveragePolicyStd     | 0.22942    |
| AveragePolicyStd[0]  | 0.24019    |
| AveragePolicyStd[1]  | 0.27803    |
| AveragePolicyStd[2]  | 0.17013    |
| AveragePolicyStd[3]  | 0.23888    |
| AveragePolicyStd[4]  | 0.20807    |
| AveragePolicyStd[5]  | 0.24124    |
| AverageReturn        | 1245.5     |
| MinReturn            | 151.83     |
| MaxReturn            | 1392.2     |
| StdReturn            | 278.72     |
| AverageEpisodeLength | 929.25     |
| MinEpisodeLength     | 180        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.5      |
| TotalNEpisodes       | 19856      |
| TotalNSamples        | 3.1154e+06 |
| ExplainedVariance    | 0.033798   |
-------------------------------------
[2018-01-21 14:09:56.118352 UTC] Saving snapshot
[2018-01-21 14:09:56.118601 UTC] Starting iteration 624
[2018-01-21 14:09:56.118777 UTC] Start collecting samples
[2018-01-21 14:10:00.601799 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:00.731148 UTC] Performing policy update
[2018-01-21 14:10:00.732089 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:00.850079 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:02.251496 UTC] Performing line search
[2018-01-21 14:10:02.441064 UTC] Updating baseline
[2018-01-21 14:10:04.263166 UTC] Computing logging information
-------------------------------------
| Iteration            | 624        |
| ExpectedImprovement  | 0.015126   |
| ActualImprovement    | 0.014458   |
| ImprovementRatio     | 0.95588    |
| MeanKL               | 0.0075149  |
| Entropy              | -0.38508   |
| Perplexity           | 0.6804     |
| AveragePolicyStd     | 0.22951    |
| AveragePolicyStd[0]  | 0.24129    |
| AveragePolicyStd[1]  | 0.27704    |
| AveragePolicyStd[2]  | 0.16985    |
| AveragePolicyStd[3]  | 0.23814    |
| AveragePolicyStd[4]  | 0.2085     |
| AveragePolicyStd[5]  | 0.24225    |
| AverageReturn        | 1261.5     |
| MinReturn            | 301.39     |
| MaxReturn            | 1392.2     |
| StdReturn            | 245.41     |
| AverageEpisodeLength | 939.99     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.78     |
| TotalNEpisodes       | 19863      |
| TotalNSamples        | 3.1219e+06 |
| ExplainedVariance    | 0.10021    |
-------------------------------------
[2018-01-21 14:10:04.974831 UTC] Saving snapshot
[2018-01-21 14:10:04.975137 UTC] Starting iteration 625
[2018-01-21 14:10:04.975387 UTC] Start collecting samples
[2018-01-21 14:10:09.489407 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:09.617530 UTC] Performing policy update
[2018-01-21 14:10:09.618136 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:09.735227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:11.158757 UTC] Performing line search
[2018-01-21 14:10:11.345317 UTC] Updating baseline
[2018-01-21 14:10:13.510628 UTC] Computing logging information
-------------------------------------
| Iteration            | 625        |
| ExpectedImprovement  | 0.018247   |
| ActualImprovement    | 0.017048   |
| ImprovementRatio     | 0.93431    |
| MeanKL               | 0.0077516  |
| Entropy              | -0.38043   |
| Perplexity           | 0.68357    |
| AveragePolicyStd     | 0.22966    |
| AveragePolicyStd[0]  | 0.2416     |
| AveragePolicyStd[1]  | 0.27731    |
| AveragePolicyStd[2]  | 0.17042    |
| AveragePolicyStd[3]  | 0.23704    |
| AveragePolicyStd[4]  | 0.20876    |
| AveragePolicyStd[5]  | 0.24283    |
| AverageReturn        | 1273.8     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 222.07     |
| AverageEpisodeLength | 948.67     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.84     |
| TotalNEpisodes       | 19870      |
| TotalNSamples        | 3.1284e+06 |
| ExplainedVariance    | 0.15261    |
-------------------------------------
[2018-01-21 14:10:14.221159 UTC] Saving snapshot
[2018-01-21 14:10:14.221494 UTC] Starting iteration 626
[2018-01-21 14:10:14.221718 UTC] Start collecting samples
[2018-01-21 14:10:18.730467 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:18.857106 UTC] Performing policy update
[2018-01-21 14:10:18.857732 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:18.974229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:20.387861 UTC] Performing line search
[2018-01-21 14:10:20.581743 UTC] Updating baseline
[2018-01-21 14:10:23.143438 UTC] Computing logging information
-------------------------------------
| Iteration            | 626        |
| ExpectedImprovement  | 0.019104   |
| ActualImprovement    | 0.017906   |
| ImprovementRatio     | 0.93726    |
| MeanKL               | 0.0072022  |
| Entropy              | -0.38188   |
| Perplexity           | 0.68257    |
| AveragePolicyStd     | 0.22957    |
| AveragePolicyStd[0]  | 0.24109    |
| AveragePolicyStd[1]  | 0.27703    |
| AveragePolicyStd[2]  | 0.1707     |
| AveragePolicyStd[3]  | 0.23679    |
| AveragePolicyStd[4]  | 0.20917    |
| AveragePolicyStd[5]  | 0.24263    |
| AverageReturn        | 1274.4     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 222.25     |
| AverageEpisodeLength | 948.67     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.84     |
| TotalNEpisodes       | 19873      |
| TotalNSamples        | 3.1314e+06 |
| ExplainedVariance    | -0.018858  |
-------------------------------------
[2018-01-21 14:10:23.831191 UTC] Saving snapshot
[2018-01-21 14:10:23.831428 UTC] Starting iteration 627
[2018-01-21 14:10:23.831584 UTC] Start collecting samples
[2018-01-21 14:10:28.354280 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:28.480457 UTC] Performing policy update
[2018-01-21 14:10:28.481099 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:28.617746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:30.029055 UTC] Performing line search
[2018-01-21 14:10:30.214519 UTC] Updating baseline
[2018-01-21 14:10:32.471367 UTC] Computing logging information
-------------------------------------
| Iteration            | 627        |
| ExpectedImprovement  | 0.017936   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.93644    |
| MeanKL               | 0.0075242  |
| Entropy              | -0.37573   |
| Perplexity           | 0.68679    |
| AveragePolicyStd     | 0.22979    |
| AveragePolicyStd[0]  | 0.24139    |
| AveragePolicyStd[1]  | 0.2765     |
| AveragePolicyStd[2]  | 0.17075    |
| AveragePolicyStd[3]  | 0.23756    |
| AveragePolicyStd[4]  | 0.2096     |
| AveragePolicyStd[5]  | 0.24292    |
| AverageReturn        | 1274.1     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 222.21     |
| AverageEpisodeLength | 948.35     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.77     |
| TotalNEpisodes       | 19879      |
| TotalNSamples        | 3.1374e+06 |
| ExplainedVariance    | 0.19704    |
-------------------------------------
[2018-01-21 14:10:33.157790 UTC] Saving snapshot
[2018-01-21 14:10:33.158051 UTC] Starting iteration 628
[2018-01-21 14:10:33.158235 UTC] Start collecting samples
[2018-01-21 14:10:37.970373 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:38.126243 UTC] Performing policy update
[2018-01-21 14:10:38.126852 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:38.240983 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:39.643594 UTC] Performing line search
[2018-01-21 14:10:39.830643 UTC] Updating baseline
[2018-01-21 14:10:41.968092 UTC] Computing logging information
-------------------------------------
| Iteration            | 628        |
| ExpectedImprovement  | 0.017267   |
| ActualImprovement    | 0.016686   |
| ImprovementRatio     | 0.96633    |
| MeanKL               | 0.0074238  |
| Entropy              | -0.3854    |
| Perplexity           | 0.68018    |
| AveragePolicyStd     | 0.22943    |
| AveragePolicyStd[0]  | 0.24125    |
| AveragePolicyStd[1]  | 0.27683    |
| AveragePolicyStd[2]  | 0.17047    |
| AveragePolicyStd[3]  | 0.23642    |
| AveragePolicyStd[4]  | 0.20954    |
| AveragePolicyStd[5]  | 0.24206    |
| AverageReturn        | 1295.3     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 193.99     |
| AverageEpisodeLength | 962.85     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.53     |
| TotalNEpisodes       | 19886      |
| TotalNSamples        | 3.1444e+06 |
| ExplainedVariance    | -0.022996  |
-------------------------------------
[2018-01-21 14:10:42.673414 UTC] Saving snapshot
[2018-01-21 14:10:42.673692 UTC] Starting iteration 629
[2018-01-21 14:10:42.673910 UTC] Start collecting samples
[2018-01-21 14:10:47.199361 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:47.322538 UTC] Performing policy update
[2018-01-21 14:10:47.323177 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:47.439376 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:48.857315 UTC] Performing line search
[2018-01-21 14:10:49.055900 UTC] Updating baseline
[2018-01-21 14:10:52.054171 UTC] Computing logging information
-------------------------------------
| Iteration            | 629        |
| ExpectedImprovement  | 0.016369   |
| ActualImprovement    | 0.015762   |
| ImprovementRatio     | 0.96294    |
| MeanKL               | 0.0075748  |
| Entropy              | -0.38965   |
| Perplexity           | 0.67729    |
| AveragePolicyStd     | 0.22928    |
| AveragePolicyStd[0]  | 0.24168    |
| AveragePolicyStd[1]  | 0.27644    |
| AveragePolicyStd[2]  | 0.1701     |
| AveragePolicyStd[3]  | 0.23628    |
| AveragePolicyStd[4]  | 0.20922    |
| AveragePolicyStd[5]  | 0.24197    |
| AverageReturn        | 1296.4     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 194.36     |
| AverageEpisodeLength | 962.85     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.53     |
| TotalNEpisodes       | 19888      |
| TotalNSamples        | 3.1464e+06 |
| ExplainedVariance    | -0.023336  |
-------------------------------------
[2018-01-21 14:10:52.710831 UTC] Saving snapshot
[2018-01-21 14:10:52.711011 UTC] Starting iteration 630
[2018-01-21 14:10:52.711116 UTC] Start collecting samples
[2018-01-21 14:10:57.290241 UTC] Computing input variables for policy optimization
[2018-01-21 14:10:57.416480 UTC] Performing policy update
[2018-01-21 14:10:57.417254 UTC] Computing gradient in Euclidean space
[2018-01-21 14:10:57.534490 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:10:58.945979 UTC] Performing line search
[2018-01-21 14:10:59.131459 UTC] Updating baseline
[2018-01-21 14:11:01.458485 UTC] Computing logging information
-------------------------------------
| Iteration            | 630        |
| ExpectedImprovement  | 0.017646   |
| ActualImprovement    | 0.016452   |
| ImprovementRatio     | 0.93233    |
| MeanKL               | 0.0075553  |
| Entropy              | -0.39511   |
| Perplexity           | 0.6736     |
| AveragePolicyStd     | 0.22899    |
| AveragePolicyStd[0]  | 0.24148    |
| AveragePolicyStd[1]  | 0.27565    |
| AveragePolicyStd[2]  | 0.17092    |
| AveragePolicyStd[3]  | 0.23567    |
| AveragePolicyStd[4]  | 0.20916    |
| AveragePolicyStd[5]  | 0.24109    |
| AverageReturn        | 1296.8     |
| MinReturn            | 301.39     |
| MaxReturn            | 1403.6     |
| StdReturn            | 194.53     |
| AverageEpisodeLength | 962.85     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.53     |
| TotalNEpisodes       | 19893      |
| TotalNSamples        | 3.1514e+06 |
| ExplainedVariance    | 0.016168   |
-------------------------------------
[2018-01-21 14:11:02.164483 UTC] Saving snapshot
[2018-01-21 14:11:02.170548 UTC] Starting iteration 631
[2018-01-21 14:11:02.170739 UTC] Start collecting samples
[2018-01-21 14:11:06.685585 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:06.813156 UTC] Performing policy update
[2018-01-21 14:11:06.814290 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:06.935842 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:08.354554 UTC] Performing line search
[2018-01-21 14:11:08.545063 UTC] Updating baseline
[2018-01-21 14:11:10.731533 UTC] Computing logging information
-------------------------------------
| Iteration            | 631        |
| ExpectedImprovement  | 0.017248   |
| ActualImprovement    | 0.015966   |
| ImprovementRatio     | 0.9257     |
| MeanKL               | 0.0072398  |
| Entropy              | -0.40734   |
| Perplexity           | 0.66542    |
| AveragePolicyStd     | 0.22851    |
| AveragePolicyStd[0]  | 0.24108    |
| AveragePolicyStd[1]  | 0.27503    |
| AveragePolicyStd[2]  | 0.17095    |
| AveragePolicyStd[3]  | 0.23583    |
| AveragePolicyStd[4]  | 0.20839    |
| AveragePolicyStd[5]  | 0.23976    |
| AverageReturn        | 1297.3     |
| MinReturn            | 301.39     |
| MaxReturn            | 1419.9     |
| StdReturn            | 196.4      |
| AverageEpisodeLength | 962        |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.46     |
| TotalNEpisodes       | 19900      |
| TotalNSamples        | 3.1582e+06 |
| ExplainedVariance    | 0.096484   |
-------------------------------------
[2018-01-21 14:11:11.417592 UTC] Saving snapshot
[2018-01-21 14:11:11.417874 UTC] Starting iteration 632
[2018-01-21 14:11:11.418066 UTC] Start collecting samples
[2018-01-21 14:11:15.939793 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:16.071734 UTC] Performing policy update
[2018-01-21 14:11:16.072800 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:16.187531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:17.687534 UTC] Performing line search
[2018-01-21 14:11:17.887466 UTC] Updating baseline
[2018-01-21 14:11:20.261597 UTC] Computing logging information
-------------------------------------
| Iteration            | 632        |
| ExpectedImprovement  | 0.017217   |
| ActualImprovement    | 0.01635    |
| ImprovementRatio     | 0.94963    |
| MeanKL               | 0.0076623  |
| Entropy              | -0.41777   |
| Perplexity           | 0.65851    |
| AveragePolicyStd     | 0.22808    |
| AveragePolicyStd[0]  | 0.23975    |
| AveragePolicyStd[1]  | 0.27386    |
| AveragePolicyStd[2]  | 0.17058    |
| AveragePolicyStd[3]  | 0.23559    |
| AveragePolicyStd[4]  | 0.20881    |
| AveragePolicyStd[5]  | 0.2399     |
| AverageReturn        | 1298.6     |
| MinReturn            | 301.39     |
| MaxReturn            | 1419.9     |
| StdReturn            | 196.81     |
| AverageEpisodeLength | 962        |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.46     |
| TotalNEpisodes       | 19904      |
| TotalNSamples        | 3.1622e+06 |
| ExplainedVariance    | -0.027865  |
-------------------------------------
[2018-01-21 14:11:20.955421 UTC] Saving snapshot
[2018-01-21 14:11:20.955671 UTC] Starting iteration 633
[2018-01-21 14:11:20.955854 UTC] Start collecting samples
[2018-01-21 14:11:25.423807 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:25.573268 UTC] Performing policy update
[2018-01-21 14:11:25.573957 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:25.695740 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:27.104040 UTC] Performing line search
[2018-01-21 14:11:27.293888 UTC] Updating baseline
[2018-01-21 14:11:29.453008 UTC] Computing logging information
-------------------------------------
| Iteration            | 633        |
| ExpectedImprovement  | 0.018059   |
| ActualImprovement    | 0.016651   |
| ImprovementRatio     | 0.92202    |
| MeanKL               | 0.007388   |
| Entropy              | -0.42215   |
| Perplexity           | 0.65563    |
| AveragePolicyStd     | 0.2279     |
| AveragePolicyStd[0]  | 0.24017    |
| AveragePolicyStd[1]  | 0.2736     |
| AveragePolicyStd[2]  | 0.17067    |
| AveragePolicyStd[3]  | 0.23528    |
| AveragePolicyStd[4]  | 0.20859    |
| AveragePolicyStd[5]  | 0.23908    |
| AverageReturn        | 1297.4     |
| MinReturn            | 301.39     |
| MaxReturn            | 1419.9     |
| StdReturn            | 199.86     |
| AverageEpisodeLength | 959.49     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.04     |
| TotalNEpisodes       | 19909      |
| TotalNSamples        | 3.1669e+06 |
| ExplainedVariance    | 0.14701    |
-------------------------------------
[2018-01-21 14:11:30.200999 UTC] Saving snapshot
[2018-01-21 14:11:30.201277 UTC] Starting iteration 634
[2018-01-21 14:11:30.201489 UTC] Start collecting samples
[2018-01-21 14:11:35.001877 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:35.124172 UTC] Performing policy update
[2018-01-21 14:11:35.124790 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:35.264298 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:36.702663 UTC] Performing line search
[2018-01-21 14:11:36.911846 UTC] Updating baseline
[2018-01-21 14:11:39.070395 UTC] Computing logging information
-------------------------------------
| Iteration            | 634        |
| ExpectedImprovement  | 0.018192   |
| ActualImprovement    | 0.017536   |
| ImprovementRatio     | 0.96395    |
| MeanKL               | 0.0070106  |
| Entropy              | -0.41984   |
| Perplexity           | 0.65715    |
| AveragePolicyStd     | 0.228      |
| AveragePolicyStd[0]  | 0.23959    |
| AveragePolicyStd[1]  | 0.27375    |
| AveragePolicyStd[2]  | 0.17038    |
| AveragePolicyStd[3]  | 0.23541    |
| AveragePolicyStd[4]  | 0.20902    |
| AveragePolicyStd[5]  | 0.23987    |
| AverageReturn        | 1294.8     |
| MinReturn            | 163.71     |
| MaxReturn            | 1419.9     |
| StdReturn            | 214.35     |
| AverageEpisodeLength | 955.42     |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.17     |
| TotalNEpisodes       | 19918      |
| TotalNSamples        | 3.1749e+06 |
| ExplainedVariance    | 0.23208    |
-------------------------------------
[2018-01-21 14:11:39.784303 UTC] Saving snapshot
[2018-01-21 14:11:39.784527 UTC] Starting iteration 635
[2018-01-21 14:11:39.784706 UTC] Start collecting samples
[2018-01-21 14:11:44.364071 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:44.497629 UTC] Performing policy update
[2018-01-21 14:11:44.498690 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:44.620903 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:46.016226 UTC] Performing line search
[2018-01-21 14:11:46.202943 UTC] Updating baseline
[2018-01-21 14:11:48.115714 UTC] Computing logging information
-------------------------------------
| Iteration            | 635        |
| ExpectedImprovement  | 0.018807   |
| ActualImprovement    | 0.017959   |
| ImprovementRatio     | 0.9549     |
| MeanKL               | 0.007782   |
| Entropy              | -0.42941   |
| Perplexity           | 0.6509     |
| AveragePolicyStd     | 0.22766    |
| AveragePolicyStd[0]  | 0.23901    |
| AveragePolicyStd[1]  | 0.27329    |
| AveragePolicyStd[2]  | 0.16964    |
| AveragePolicyStd[3]  | 0.23563    |
| AveragePolicyStd[4]  | 0.20906    |
| AveragePolicyStd[5]  | 0.23934    |
| AverageReturn        | 1292.9     |
| MinReturn            | 163.71     |
| MaxReturn            | 1419.9     |
| StdReturn            | 217.49     |
| AverageEpisodeLength | 954.12     |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.24     |
| TotalNEpisodes       | 19922      |
| TotalNSamples        | 3.1785e+06 |
| ExplainedVariance    | 0.19555    |
-------------------------------------
[2018-01-21 14:11:48.882375 UTC] Saving snapshot
[2018-01-21 14:11:48.882623 UTC] Starting iteration 636
[2018-01-21 14:11:48.882801 UTC] Start collecting samples
[2018-01-21 14:11:53.345601 UTC] Computing input variables for policy optimization
[2018-01-21 14:11:53.469743 UTC] Performing policy update
[2018-01-21 14:11:53.470682 UTC] Computing gradient in Euclidean space
[2018-01-21 14:11:53.591768 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:11:55.051745 UTC] Performing line search
[2018-01-21 14:11:55.248374 UTC] Updating baseline
[2018-01-21 14:11:57.286145 UTC] Computing logging information
-------------------------------------
| Iteration            | 636        |
| ExpectedImprovement  | 0.017875   |
| ActualImprovement    | 0.015827   |
| ImprovementRatio     | 0.88545    |
| MeanKL               | 0.0077003  |
| Entropy              | -0.43616   |
| Perplexity           | 0.64651    |
| AveragePolicyStd     | 0.2274     |
| AveragePolicyStd[0]  | 0.23807    |
| AveragePolicyStd[1]  | 0.27283    |
| AveragePolicyStd[2]  | 0.16933    |
| AveragePolicyStd[3]  | 0.23576    |
| AveragePolicyStd[4]  | 0.20924    |
| AveragePolicyStd[5]  | 0.23917    |
| AverageReturn        | 1300.3     |
| MinReturn            | 163.71     |
| MaxReturn            | 1419.9     |
| StdReturn            | 209.46     |
| AverageEpisodeLength | 958.73     |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.39     |
| TotalNEpisodes       | 19924      |
| TotalNSamples        | 3.1805e+06 |
| ExplainedVariance    | 0.017836   |
-------------------------------------
[2018-01-21 14:11:58.044147 UTC] Saving snapshot
[2018-01-21 14:11:58.044395 UTC] Starting iteration 637
[2018-01-21 14:11:58.044568 UTC] Start collecting samples
[2018-01-21 14:12:02.657168 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:02.786196 UTC] Performing policy update
[2018-01-21 14:12:02.786931 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:02.918005 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:04.313952 UTC] Performing line search
[2018-01-21 14:12:04.514764 UTC] Updating baseline
[2018-01-21 14:12:06.432231 UTC] Computing logging information
--------------------------------------
| Iteration            | 637         |
| ExpectedImprovement  | 0.016643    |
| ActualImprovement    | 0.015946    |
| ImprovementRatio     | 0.95813     |
| MeanKL               | 0.0075978   |
| Entropy              | -0.44142    |
| Perplexity           | 0.64313     |
| AveragePolicyStd     | 0.22718     |
| AveragePolicyStd[0]  | 0.23756     |
| AveragePolicyStd[1]  | 0.2726      |
| AveragePolicyStd[2]  | 0.16943     |
| AveragePolicyStd[3]  | 0.23507     |
| AveragePolicyStd[4]  | 0.20922     |
| AveragePolicyStd[5]  | 0.23921     |
| AverageReturn        | 1306.6      |
| MinReturn            | 163.71      |
| MaxReturn            | 1419.9      |
| StdReturn            | 199.75      |
| AverageEpisodeLength | 963.4       |
| MinEpisodeLength     | 151         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 137.96      |
| TotalNEpisodes       | 19930       |
| TotalNSamples        | 3.1865e+06  |
| ExplainedVariance    | -7.8435e-05 |
--------------------------------------
[2018-01-21 14:12:07.183591 UTC] Saving snapshot
[2018-01-21 14:12:07.183891 UTC] Starting iteration 638
[2018-01-21 14:12:07.184107 UTC] Start collecting samples
[2018-01-21 14:12:11.666718 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:11.816046 UTC] Performing policy update
[2018-01-21 14:12:11.816905 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:11.942785 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:13.433763 UTC] Performing line search
[2018-01-21 14:12:13.637346 UTC] Updating baseline
[2018-01-21 14:12:15.421957 UTC] Computing logging information
-------------------------------------
| Iteration            | 638        |
| ExpectedImprovement  | 0.018948   |
| ActualImprovement    | 0.018375   |
| ImprovementRatio     | 0.96979    |
| MeanKL               | 0.0083106  |
| Entropy              | -0.44413   |
| Perplexity           | 0.64138    |
| AveragePolicyStd     | 0.22708    |
| AveragePolicyStd[0]  | 0.23711    |
| AveragePolicyStd[1]  | 0.2725     |
| AveragePolicyStd[2]  | 0.16932    |
| AveragePolicyStd[3]  | 0.23524    |
| AveragePolicyStd[4]  | 0.20913    |
| AveragePolicyStd[5]  | 0.23919    |
| AverageReturn        | 1308.4     |
| MinReturn            | 163.71     |
| MaxReturn            | 1419.9     |
| StdReturn            | 200.38     |
| AverageEpisodeLength | 963.4      |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.96     |
| TotalNEpisodes       | 19936      |
| TotalNSamples        | 3.1925e+06 |
| ExplainedVariance    | -0.0084052 |
-------------------------------------
[2018-01-21 14:12:16.111508 UTC] Saving snapshot
[2018-01-21 14:12:16.111725 UTC] Starting iteration 639
[2018-01-21 14:12:16.111895 UTC] Start collecting samples
[2018-01-21 14:12:20.640900 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:20.763823 UTC] Performing policy update
[2018-01-21 14:12:20.764977 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:20.888489 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:22.306156 UTC] Performing line search
[2018-01-21 14:12:22.492585 UTC] Updating baseline
[2018-01-21 14:12:24.814589 UTC] Computing logging information
------------------------------------
| Iteration            | 639       |
| ExpectedImprovement  | 0.017485  |
| ActualImprovement    | 0.016508  |
| ImprovementRatio     | 0.94413   |
| MeanKL               | 0.0075075 |
| Entropy              | -0.44404  |
| Perplexity           | 0.64144   |
| AveragePolicyStd     | 0.22708   |
| AveragePolicyStd[0]  | 0.238     |
| AveragePolicyStd[1]  | 0.27192   |
| AveragePolicyStd[2]  | 0.16933   |
| AveragePolicyStd[3]  | 0.23534   |
| AveragePolicyStd[4]  | 0.20873   |
| AveragePolicyStd[5]  | 0.23917   |
| AverageReturn        | 1302.8    |
| MinReturn            | 163.71    |
| MaxReturn            | 1419.9    |
| StdReturn            | 207.64    |
| AverageEpisodeLength | 958.53    |
| MinEpisodeLength     | 151       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 143.45    |
| TotalNEpisodes       | 19941     |
| TotalNSamples        | 3.197e+06 |
| ExplainedVariance    | 0.23697   |
------------------------------------
[2018-01-21 14:12:25.504130 UTC] Saving snapshot
[2018-01-21 14:12:25.504390 UTC] Starting iteration 640
[2018-01-21 14:12:25.504575 UTC] Start collecting samples
[2018-01-21 14:12:29.996074 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:30.122139 UTC] Performing policy update
[2018-01-21 14:12:30.122897 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:30.241960 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:31.692950 UTC] Performing line search
[2018-01-21 14:12:31.885405 UTC] Updating baseline
[2018-01-21 14:12:34.984365 UTC] Computing logging information
------------------------------------
| Iteration            | 640       |
| ExpectedImprovement  | 0.017538  |
| ActualImprovement    | 0.016114  |
| ImprovementRatio     | 0.91877   |
| MeanKL               | 0.0075967 |
| Entropy              | -0.44353  |
| Perplexity           | 0.64177   |
| AveragePolicyStd     | 0.22706   |
| AveragePolicyStd[0]  | 0.23736   |
| AveragePolicyStd[1]  | 0.27192   |
| AveragePolicyStd[2]  | 0.16987   |
| AveragePolicyStd[3]  | 0.23548   |
| AveragePolicyStd[4]  | 0.20874   |
| AveragePolicyStd[5]  | 0.23901   |
| AverageReturn        | 1303.3    |
| MinReturn            | 163.71    |
| MaxReturn            | 1419.9    |
| StdReturn            | 207.81    |
| AverageEpisodeLength | 958.53    |
| MinEpisodeLength     | 151       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 143.45    |
| TotalNEpisodes       | 19944     |
| TotalNSamples        | 3.2e+06   |
| ExplainedVariance    | -0.015452 |
------------------------------------
[2018-01-21 14:12:35.715837 UTC] Saving snapshot
[2018-01-21 14:12:35.725572 UTC] Starting iteration 641
[2018-01-21 14:12:35.725801 UTC] Start collecting samples
[2018-01-21 14:12:40.201386 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:40.339379 UTC] Performing policy update
[2018-01-21 14:12:40.340546 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:40.482549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:41.943249 UTC] Performing line search
[2018-01-21 14:12:42.140906 UTC] Updating baseline
[2018-01-21 14:12:43.760570 UTC] Computing logging information
------------------------------------
| Iteration            | 641       |
| ExpectedImprovement  | 0.014919  |
| ActualImprovement    | 0.014222  |
| ImprovementRatio     | 0.95325   |
| MeanKL               | 0.0074629 |
| Entropy              | -0.44544  |
| Perplexity           | 0.64054   |
| AveragePolicyStd     | 0.22696   |
| AveragePolicyStd[0]  | 0.23693   |
| AveragePolicyStd[1]  | 0.2711    |
| AveragePolicyStd[2]  | 0.17008   |
| AveragePolicyStd[3]  | 0.23594   |
| AveragePolicyStd[4]  | 0.20877   |
| AveragePolicyStd[5]  | 0.23891   |
| AverageReturn        | 1315.3    |
| MinReturn            | 163.71    |
| MaxReturn            | 1419.9    |
| StdReturn            | 182.24    |
| AverageEpisodeLength | 965.99    |
| MinEpisodeLength     | 151       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 124.81    |
| TotalNEpisodes       | 19952     |
| TotalNSamples        | 3.208e+06 |
| ExplainedVariance    | 0.040217  |
------------------------------------
[2018-01-21 14:12:44.507227 UTC] Saving snapshot
[2018-01-21 14:12:44.507500 UTC] Starting iteration 642
[2018-01-21 14:12:44.507678 UTC] Start collecting samples
[2018-01-21 14:12:49.146603 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:49.267201 UTC] Performing policy update
[2018-01-21 14:12:49.268163 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:49.390154 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:50.783911 UTC] Performing line search
[2018-01-21 14:12:50.974189 UTC] Updating baseline
[2018-01-21 14:12:52.957780 UTC] Computing logging information
-------------------------------------
| Iteration            | 642        |
| ExpectedImprovement  | 0.015792   |
| ActualImprovement    | 0.015194   |
| ImprovementRatio     | 0.96213    |
| MeanKL               | 0.0079665  |
| Entropy              | -0.44838   |
| Perplexity           | 0.63866    |
| AveragePolicyStd     | 0.2268     |
| AveragePolicyStd[0]  | 0.23605    |
| AveragePolicyStd[1]  | 0.27029    |
| AveragePolicyStd[2]  | 0.17035    |
| AveragePolicyStd[3]  | 0.23596    |
| AveragePolicyStd[4]  | 0.20916    |
| AveragePolicyStd[5]  | 0.23897    |
| AverageReturn        | 1293.9     |
| MinReturn            | 163.71     |
| MaxReturn            | 1419.9     |
| StdReturn            | 224.06     |
| AverageEpisodeLength | 950.19     |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.06     |
| TotalNEpisodes       | 19959      |
| TotalNSamples        | 3.2134e+06 |
| ExplainedVariance    | 0.41387    |
-------------------------------------
[2018-01-21 14:12:53.661128 UTC] Saving snapshot
[2018-01-21 14:12:53.661342 UTC] Starting iteration 643
[2018-01-21 14:12:53.661463 UTC] Start collecting samples
[2018-01-21 14:12:58.174032 UTC] Computing input variables for policy optimization
[2018-01-21 14:12:58.311195 UTC] Performing policy update
[2018-01-21 14:12:58.311842 UTC] Computing gradient in Euclidean space
[2018-01-21 14:12:58.442097 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:12:59.943478 UTC] Performing line search
[2018-01-21 14:13:00.163366 UTC] Updating baseline
[2018-01-21 14:13:01.883454 UTC] Computing logging information
-------------------------------------
| Iteration            | 643        |
| ExpectedImprovement  | 0.01713    |
| ActualImprovement    | 0.015797   |
| ImprovementRatio     | 0.92219    |
| MeanKL               | 0.0075909  |
| Entropy              | -0.44332   |
| Perplexity           | 0.6419     |
| AveragePolicyStd     | 0.22703    |
| AveragePolicyStd[0]  | 0.23512    |
| AveragePolicyStd[1]  | 0.2718     |
| AveragePolicyStd[2]  | 0.17033    |
| AveragePolicyStd[3]  | 0.23616    |
| AveragePolicyStd[4]  | 0.20931    |
| AveragePolicyStd[5]  | 0.23945    |
| AverageReturn        | 1288       |
| MinReturn            | 163.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 230.55     |
| AverageEpisodeLength | 945.15     |
| MinEpisodeLength     | 151        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.85     |
| TotalNEpisodes       | 19964      |
| TotalNSamples        | 3.2174e+06 |
| ExplainedVariance    | 0.51188    |
-------------------------------------
[2018-01-21 14:13:02.602023 UTC] Saving snapshot
[2018-01-21 14:13:02.602331 UTC] Starting iteration 644
[2018-01-21 14:13:02.602564 UTC] Start collecting samples
[2018-01-21 14:13:07.045898 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:07.179725 UTC] Performing policy update
[2018-01-21 14:13:07.180708 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:07.302763 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:08.734515 UTC] Performing line search
[2018-01-21 14:13:08.930165 UTC] Updating baseline
[2018-01-21 14:13:11.141235 UTC] Computing logging information
-------------------------------------
| Iteration            | 644        |
| ExpectedImprovement  | 0.016529   |
| ActualImprovement    | 0.015794   |
| ImprovementRatio     | 0.95553    |
| MeanKL               | 0.0074562  |
| Entropy              | -0.44727   |
| Perplexity           | 0.63937    |
| AveragePolicyStd     | 0.22694    |
| AveragePolicyStd[0]  | 0.23473    |
| AveragePolicyStd[1]  | 0.27213    |
| AveragePolicyStd[2]  | 0.16915    |
| AveragePolicyStd[3]  | 0.23544    |
| AveragePolicyStd[4]  | 0.21017    |
| AveragePolicyStd[5]  | 0.24003    |
| AverageReturn        | 1282       |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 249.87     |
| AverageEpisodeLength | 939.23     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.71     |
| TotalNEpisodes       | 19970      |
| TotalNSamples        | 3.2223e+06 |
| ExplainedVariance    | 0.1568     |
-------------------------------------
[2018-01-21 14:13:11.885013 UTC] Saving snapshot
[2018-01-21 14:13:11.885529 UTC] Starting iteration 645
[2018-01-21 14:13:11.885930 UTC] Start collecting samples
[2018-01-21 14:13:16.541741 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:16.672801 UTC] Performing policy update
[2018-01-21 14:13:16.673453 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:16.798947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:18.242180 UTC] Performing line search
[2018-01-21 14:13:18.441976 UTC] Updating baseline
[2018-01-21 14:13:20.511404 UTC] Computing logging information
-------------------------------------
| Iteration            | 645        |
| ExpectedImprovement  | 0.017296   |
| ActualImprovement    | 0.016202   |
| ImprovementRatio     | 0.93678    |
| MeanKL               | 0.0074672  |
| Entropy              | -0.45288   |
| Perplexity           | 0.63579    |
| AveragePolicyStd     | 0.22667    |
| AveragePolicyStd[0]  | 0.23377    |
| AveragePolicyStd[1]  | 0.2711     |
| AveragePolicyStd[2]  | 0.16942    |
| AveragePolicyStd[3]  | 0.23496    |
| AveragePolicyStd[4]  | 0.21072    |
| AveragePolicyStd[5]  | 0.24006    |
| AverageReturn        | 1283.1     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 251.1      |
| AverageEpisodeLength | 938.1      |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.68     |
| TotalNEpisodes       | 19976      |
| TotalNSamples        | 3.2282e+06 |
| ExplainedVariance    | 0.18353    |
-------------------------------------
[2018-01-21 14:13:21.294701 UTC] Saving snapshot
[2018-01-21 14:13:21.294992 UTC] Starting iteration 646
[2018-01-21 14:13:21.295238 UTC] Start collecting samples
[2018-01-21 14:13:25.802199 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:25.954360 UTC] Performing policy update
[2018-01-21 14:13:25.955102 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:26.077271 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:27.513294 UTC] Performing line search
[2018-01-21 14:13:27.701611 UTC] Updating baseline
[2018-01-21 14:13:29.600403 UTC] Computing logging information
-------------------------------------
| Iteration            | 646        |
| ExpectedImprovement  | 0.019169   |
| ActualImprovement    | 0.017385   |
| ImprovementRatio     | 0.90694    |
| MeanKL               | 0.0076261  |
| Entropy              | -0.46441   |
| Perplexity           | 0.62851    |
| AveragePolicyStd     | 0.22623    |
| AveragePolicyStd[0]  | 0.23298    |
| AveragePolicyStd[1]  | 0.27073    |
| AveragePolicyStd[2]  | 0.16923    |
| AveragePolicyStd[3]  | 0.23405    |
| AveragePolicyStd[4]  | 0.21024    |
| AveragePolicyStd[5]  | 0.24018    |
| AverageReturn        | 1277.7     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 261.38     |
| AverageEpisodeLength | 933.01     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.07     |
| TotalNEpisodes       | 19981      |
| TotalNSamples        | 3.2327e+06 |
| ExplainedVariance    | 0.076486   |
-------------------------------------
[2018-01-21 14:13:30.320517 UTC] Saving snapshot
[2018-01-21 14:13:30.320744 UTC] Starting iteration 647
[2018-01-21 14:13:30.320887 UTC] Start collecting samples
[2018-01-21 14:13:34.877158 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:35.017373 UTC] Performing policy update
[2018-01-21 14:13:35.018009 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:35.141035 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:36.581334 UTC] Performing line search
[2018-01-21 14:13:36.777452 UTC] Updating baseline
[2018-01-21 14:13:38.581281 UTC] Computing logging information
-------------------------------------
| Iteration            | 647        |
| ExpectedImprovement  | 0.01804    |
| ActualImprovement    | 0.01698    |
| ImprovementRatio     | 0.94126    |
| MeanKL               | 0.0072823  |
| Entropy              | -0.47146   |
| Perplexity           | 0.62409    |
| AveragePolicyStd     | 0.22594    |
| AveragePolicyStd[0]  | 0.23199    |
| AveragePolicyStd[1]  | 0.27005    |
| AveragePolicyStd[2]  | 0.16937    |
| AveragePolicyStd[3]  | 0.23387    |
| AveragePolicyStd[4]  | 0.21       |
| AveragePolicyStd[5]  | 0.24037    |
| AverageReturn        | 1276       |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 263.15     |
| AverageEpisodeLength | 930.48     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.89     |
| TotalNEpisodes       | 19987      |
| TotalNSamples        | 3.2384e+06 |
| ExplainedVariance    | 0.12231    |
-------------------------------------
[2018-01-21 14:13:39.245563 UTC] Saving snapshot
[2018-01-21 14:13:39.245740 UTC] Starting iteration 648
[2018-01-21 14:13:39.245843 UTC] Start collecting samples
[2018-01-21 14:13:44.050219 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:44.177151 UTC] Performing policy update
[2018-01-21 14:13:44.177777 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:44.295345 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:45.694353 UTC] Performing line search
[2018-01-21 14:13:45.885222 UTC] Updating baseline
[2018-01-21 14:13:47.917481 UTC] Computing logging information
-------------------------------------
| Iteration            | 648        |
| ExpectedImprovement  | 0.015987   |
| ActualImprovement    | 0.015328   |
| ImprovementRatio     | 0.95881    |
| MeanKL               | 0.0076734  |
| Entropy              | -0.47855   |
| Perplexity           | 0.61968    |
| AveragePolicyStd     | 0.22568    |
| AveragePolicyStd[0]  | 0.23176    |
| AveragePolicyStd[1]  | 0.26955    |
| AveragePolicyStd[2]  | 0.16894    |
| AveragePolicyStd[3]  | 0.23392    |
| AveragePolicyStd[4]  | 0.21001    |
| AveragePolicyStd[5]  | 0.2399     |
| AverageReturn        | 1275.9     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 263.11     |
| AverageEpisodeLength | 930.48     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.89     |
| TotalNEpisodes       | 19990      |
| TotalNSamples        | 3.2414e+06 |
| ExplainedVariance    | 0.067637   |
-------------------------------------
[2018-01-21 14:13:48.618870 UTC] Saving snapshot
[2018-01-21 14:13:48.619108 UTC] Starting iteration 649
[2018-01-21 14:13:48.619258 UTC] Start collecting samples
[2018-01-21 14:13:53.153849 UTC] Computing input variables for policy optimization
[2018-01-21 14:13:53.284367 UTC] Performing policy update
[2018-01-21 14:13:53.285135 UTC] Computing gradient in Euclidean space
[2018-01-21 14:13:53.406238 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:13:54.858109 UTC] Performing line search
[2018-01-21 14:13:55.058308 UTC] Updating baseline
[2018-01-21 14:13:57.330591 UTC] Computing logging information
-------------------------------------
| Iteration            | 649        |
| ExpectedImprovement  | 0.01721    |
| ActualImprovement    | 0.016513   |
| ImprovementRatio     | 0.9595     |
| MeanKL               | 0.0083587  |
| Entropy              | -0.47537   |
| Perplexity           | 0.62165    |
| AveragePolicyStd     | 0.2258     |
| AveragePolicyStd[0]  | 0.2326     |
| AveragePolicyStd[1]  | 0.26913    |
| AveragePolicyStd[2]  | 0.16869    |
| AveragePolicyStd[3]  | 0.23398    |
| AveragePolicyStd[4]  | 0.21038    |
| AveragePolicyStd[5]  | 0.24004    |
| AverageReturn        | 1272.6     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 273.66     |
| AverageEpisodeLength | 926.94     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.34     |
| TotalNEpisodes       | 19998      |
| TotalNSamples        | 3.2489e+06 |
| ExplainedVariance    | 0.13605    |
-------------------------------------
[2018-01-21 14:13:58.032048 UTC] Saving snapshot
[2018-01-21 14:13:58.032310 UTC] Starting iteration 650
[2018-01-21 14:13:58.032487 UTC] Start collecting samples
[2018-01-21 14:14:02.736776 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:02.864500 UTC] Performing policy update
[2018-01-21 14:14:02.865172 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:02.987069 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:04.432874 UTC] Performing line search
[2018-01-21 14:14:04.621520 UTC] Updating baseline
[2018-01-21 14:14:07.172740 UTC] Computing logging information
-------------------------------------
| Iteration            | 650        |
| ExpectedImprovement  | 0.016121   |
| ActualImprovement    | 0.014827   |
| ImprovementRatio     | 0.91977    |
| MeanKL               | 0.0080082  |
| Entropy              | -0.47471   |
| Perplexity           | 0.62206    |
| AveragePolicyStd     | 0.22579    |
| AveragePolicyStd[0]  | 0.23292    |
| AveragePolicyStd[1]  | 0.26866    |
| AveragePolicyStd[2]  | 0.16904    |
| AveragePolicyStd[3]  | 0.23368    |
| AveragePolicyStd[4]  | 0.21064    |
| AveragePolicyStd[5]  | 0.2398     |
| AverageReturn        | 1252.4     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 293.69     |
| AverageEpisodeLength | 911.68     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.96     |
| TotalNEpisodes       | 20004      |
| TotalNSamples        | 3.2533e+06 |
| ExplainedVariance    | 0.34607    |
-------------------------------------
[2018-01-21 14:14:07.869184 UTC] Saving snapshot
[2018-01-21 14:14:07.878848 UTC] Starting iteration 651
[2018-01-21 14:14:07.879064 UTC] Start collecting samples
[2018-01-21 14:14:12.276749 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:12.405297 UTC] Performing policy update
[2018-01-21 14:14:12.406516 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:12.529349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:13.990384 UTC] Performing line search
[2018-01-21 14:14:14.183817 UTC] Updating baseline
[2018-01-21 14:14:16.154393 UTC] Computing logging information
-------------------------------------
| Iteration            | 651        |
| ExpectedImprovement  | 0.019312   |
| ActualImprovement    | 0.018045   |
| ImprovementRatio     | 0.93435    |
| MeanKL               | 0.0076166  |
| Entropy              | -0.47532   |
| Perplexity           | 0.62169    |
| AveragePolicyStd     | 0.2258     |
| AveragePolicyStd[0]  | 0.23191    |
| AveragePolicyStd[1]  | 0.26882    |
| AveragePolicyStd[2]  | 0.16858    |
| AveragePolicyStd[3]  | 0.23421    |
| AveragePolicyStd[4]  | 0.21088    |
| AveragePolicyStd[5]  | 0.24041    |
| AverageReturn        | 1255.7     |
| MinReturn            | 127.71     |
| MaxReturn            | 1438.1     |
| StdReturn            | 292.61     |
| AverageEpisodeLength | 914.19     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.48     |
| TotalNEpisodes       | 20008      |
| TotalNSamples        | 3.2573e+06 |
| ExplainedVariance    | -0.012466  |
-------------------------------------
[2018-01-21 14:14:16.811607 UTC] Saving snapshot
[2018-01-21 14:14:16.811859 UTC] Starting iteration 652
[2018-01-21 14:14:16.812041 UTC] Start collecting samples
[2018-01-21 14:14:21.163028 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:21.306842 UTC] Performing policy update
[2018-01-21 14:14:21.307806 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:21.438552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:22.909641 UTC] Performing line search
[2018-01-21 14:14:23.115304 UTC] Updating baseline
[2018-01-21 14:14:25.056730 UTC] Computing logging information
-------------------------------------
| Iteration            | 652        |
| ExpectedImprovement  | 0.015027   |
| ActualImprovement    | 0.014484   |
| ImprovementRatio     | 0.96382    |
| MeanKL               | 0.0076353  |
| Entropy              | -0.48321   |
| Perplexity           | 0.6168     |
| AveragePolicyStd     | 0.22552    |
| AveragePolicyStd[0]  | 0.23166    |
| AveragePolicyStd[1]  | 0.26918    |
| AveragePolicyStd[2]  | 0.16834    |
| AveragePolicyStd[3]  | 0.23345    |
| AveragePolicyStd[4]  | 0.21056    |
| AveragePolicyStd[5]  | 0.23991    |
| AverageReturn        | 1256.2     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 293.92     |
| AverageEpisodeLength | 913        |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.72     |
| TotalNEpisodes       | 20013      |
| TotalNSamples        | 3.2621e+06 |
| ExplainedVariance    | 0.15552    |
-------------------------------------
[2018-01-21 14:14:25.767785 UTC] Saving snapshot
[2018-01-21 14:14:25.768040 UTC] Starting iteration 653
[2018-01-21 14:14:25.768221 UTC] Start collecting samples
[2018-01-21 14:14:30.293728 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:30.417152 UTC] Performing policy update
[2018-01-21 14:14:30.417743 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:30.541260 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:31.966489 UTC] Performing line search
[2018-01-21 14:14:32.166252 UTC] Updating baseline
[2018-01-21 14:14:34.370329 UTC] Computing logging information
-------------------------------------
| Iteration            | 653        |
| ExpectedImprovement  | 0.018658   |
| ActualImprovement    | 0.016714   |
| ImprovementRatio     | 0.89585    |
| MeanKL               | 0.0076899  |
| Entropy              | -0.48807   |
| Perplexity           | 0.61381    |
| AveragePolicyStd     | 0.22531    |
| AveragePolicyStd[0]  | 0.23141    |
| AveragePolicyStd[1]  | 0.26847    |
| AveragePolicyStd[2]  | 0.16839    |
| AveragePolicyStd[3]  | 0.23326    |
| AveragePolicyStd[4]  | 0.21043    |
| AveragePolicyStd[5]  | 0.23994    |
| AverageReturn        | 1268.9     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 274.35     |
| AverageEpisodeLength | 920.43     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.42     |
| TotalNEpisodes       | 20019      |
| TotalNSamples        | 3.2679e+06 |
| ExplainedVariance    | 0.16323    |
-------------------------------------
[2018-01-21 14:14:35.071476 UTC] Saving snapshot
[2018-01-21 14:14:35.071679 UTC] Starting iteration 654
[2018-01-21 14:14:35.071838 UTC] Start collecting samples
[2018-01-21 14:14:39.366624 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:39.503809 UTC] Performing policy update
[2018-01-21 14:14:39.510239 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:39.629223 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:41.053365 UTC] Performing line search
[2018-01-21 14:14:41.267842 UTC] Updating baseline
[2018-01-21 14:14:43.198959 UTC] Computing logging information
-------------------------------------
| Iteration            | 654        |
| ExpectedImprovement  | 0.01833    |
| ActualImprovement    | 0.016967   |
| ImprovementRatio     | 0.9256     |
| MeanKL               | 0.0076097  |
| Entropy              | -0.49382   |
| Perplexity           | 0.61029    |
| AveragePolicyStd     | 0.22507    |
| AveragePolicyStd[0]  | 0.23026    |
| AveragePolicyStd[1]  | 0.26778    |
| AveragePolicyStd[2]  | 0.16842    |
| AveragePolicyStd[3]  | 0.23235    |
| AveragePolicyStd[4]  | 0.21079    |
| AveragePolicyStd[5]  | 0.24084    |
| AverageReturn        | 1269.3     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 277.72     |
| AverageEpisodeLength | 918.91     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.19     |
| TotalNEpisodes       | 20025      |
| TotalNSamples        | 3.2734e+06 |
| ExplainedVariance    | 0.25368    |
-------------------------------------
[2018-01-21 14:14:43.930097 UTC] Saving snapshot
[2018-01-21 14:14:43.930309 UTC] Starting iteration 655
[2018-01-21 14:14:43.930416 UTC] Start collecting samples
[2018-01-21 14:14:48.370330 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:48.511512 UTC] Performing policy update
[2018-01-21 14:14:48.512190 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:48.637537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:50.073073 UTC] Performing line search
[2018-01-21 14:14:50.259082 UTC] Updating baseline
[2018-01-21 14:14:52.472950 UTC] Computing logging information
-------------------------------------
| Iteration            | 655        |
| ExpectedImprovement  | 0.01804    |
| ActualImprovement    | 0.016946   |
| ImprovementRatio     | 0.93934    |
| MeanKL               | 0.0072374  |
| Entropy              | -0.50235   |
| Perplexity           | 0.60511    |
| AveragePolicyStd     | 0.22478    |
| AveragePolicyStd[0]  | 0.23027    |
| AveragePolicyStd[1]  | 0.26744    |
| AveragePolicyStd[2]  | 0.16795    |
| AveragePolicyStd[3]  | 0.23227    |
| AveragePolicyStd[4]  | 0.21007    |
| AveragePolicyStd[5]  | 0.24066    |
| AverageReturn        | 1249.3     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 309.66     |
| AverageEpisodeLength | 903.53     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.15     |
| TotalNEpisodes       | 20029      |
| TotalNSamples        | 3.2759e+06 |
| ExplainedVariance    | 0.16223    |
-------------------------------------
[2018-01-21 14:14:53.254660 UTC] Saving snapshot
[2018-01-21 14:14:53.254890 UTC] Starting iteration 656
[2018-01-21 14:14:53.255033 UTC] Start collecting samples
[2018-01-21 14:14:57.723405 UTC] Computing input variables for policy optimization
[2018-01-21 14:14:57.847300 UTC] Performing policy update
[2018-01-21 14:14:57.848441 UTC] Computing gradient in Euclidean space
[2018-01-21 14:14:57.968110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:14:59.420315 UTC] Performing line search
[2018-01-21 14:14:59.629005 UTC] Updating baseline
[2018-01-21 14:15:01.819919 UTC] Computing logging information
-------------------------------------
| Iteration            | 656        |
| ExpectedImprovement  | 0.016747   |
| ActualImprovement    | 0.016357   |
| ImprovementRatio     | 0.97666    |
| MeanKL               | 0.0083168  |
| Entropy              | -0.5076    |
| Perplexity           | 0.60194    |
| AveragePolicyStd     | 0.22455    |
| AveragePolicyStd[0]  | 0.2296     |
| AveragePolicyStd[1]  | 0.26691    |
| AveragePolicyStd[2]  | 0.16823    |
| AveragePolicyStd[3]  | 0.23213    |
| AveragePolicyStd[4]  | 0.20987    |
| AveragePolicyStd[5]  | 0.24056    |
| AverageReturn        | 1245.4     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 312.39     |
| AverageEpisodeLength | 899.68     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.84     |
| TotalNEpisodes       | 20036      |
| TotalNSamples        | 3.2825e+06 |
| ExplainedVariance    | 0.16801    |
-------------------------------------
[2018-01-21 14:15:02.554710 UTC] Saving snapshot
[2018-01-21 14:15:02.554949 UTC] Starting iteration 657
[2018-01-21 14:15:02.555097 UTC] Start collecting samples
[2018-01-21 14:15:06.918558 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:07.068738 UTC] Performing policy update
[2018-01-21 14:15:07.069511 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:07.198128 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:08.603655 UTC] Performing line search
[2018-01-21 14:15:08.798477 UTC] Updating baseline
[2018-01-21 14:15:10.593893 UTC] Computing logging information
-------------------------------------
| Iteration            | 657        |
| ExpectedImprovement  | 0.015762   |
| ActualImprovement    | 0.015317   |
| ImprovementRatio     | 0.97179    |
| MeanKL               | 0.0076711  |
| Entropy              | -0.50204   |
| Perplexity           | 0.60529    |
| AveragePolicyStd     | 0.22479    |
| AveragePolicyStd[0]  | 0.22952    |
| AveragePolicyStd[1]  | 0.26713    |
| AveragePolicyStd[2]  | 0.16775    |
| AveragePolicyStd[3]  | 0.2328     |
| AveragePolicyStd[4]  | 0.2106     |
| AveragePolicyStd[5]  | 0.24092    |
| AverageReturn        | 1247.4     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 316.57     |
| AverageEpisodeLength | 899.13     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.93     |
| TotalNEpisodes       | 20043      |
| TotalNSamples        | 3.2889e+06 |
| ExplainedVariance    | 0.098006   |
-------------------------------------
[2018-01-21 14:15:11.368419 UTC] Saving snapshot
[2018-01-21 14:15:11.368699 UTC] Starting iteration 658
[2018-01-21 14:15:11.368901 UTC] Start collecting samples
[2018-01-21 14:15:16.044384 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:16.175555 UTC] Performing policy update
[2018-01-21 14:15:16.176144 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:16.307665 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:17.710507 UTC] Performing line search
[2018-01-21 14:15:17.894268 UTC] Updating baseline
[2018-01-21 14:15:19.668211 UTC] Computing logging information
-------------------------------------
| Iteration            | 658        |
| ExpectedImprovement  | 0.016775   |
| ActualImprovement    | 0.016193   |
| ImprovementRatio     | 0.96535    |
| MeanKL               | 0.0074801  |
| Entropy              | -0.49281   |
| Perplexity           | 0.61091    |
| AveragePolicyStd     | 0.22515    |
| AveragePolicyStd[0]  | 0.23038    |
| AveragePolicyStd[1]  | 0.26852    |
| AveragePolicyStd[2]  | 0.16813    |
| AveragePolicyStd[3]  | 0.23305    |
| AveragePolicyStd[4]  | 0.21033    |
| AveragePolicyStd[5]  | 0.24052    |
| AverageReturn        | 1215.3     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 353.69     |
| AverageEpisodeLength | 876.76     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 242.38     |
| TotalNEpisodes       | 20049      |
| TotalNSamples        | 3.2927e+06 |
| ExplainedVariance    | 0.3351     |
-------------------------------------
[2018-01-21 14:15:20.411147 UTC] Saving snapshot
[2018-01-21 14:15:20.411396 UTC] Starting iteration 659
[2018-01-21 14:15:20.411573 UTC] Start collecting samples
[2018-01-21 14:15:24.891751 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:25.017922 UTC] Performing policy update
[2018-01-21 14:15:25.018529 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:25.137200 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:26.544689 UTC] Performing line search
[2018-01-21 14:15:26.738014 UTC] Updating baseline
[2018-01-21 14:15:28.666703 UTC] Computing logging information
-------------------------------------
| Iteration            | 659        |
| ExpectedImprovement  | 0.018563   |
| ActualImprovement    | 0.018049   |
| ImprovementRatio     | 0.97233    |
| MeanKL               | 0.0074584  |
| Entropy              | -0.49564   |
| Perplexity           | 0.60918    |
| AveragePolicyStd     | 0.22501    |
| AveragePolicyStd[0]  | 0.2298     |
| AveragePolicyStd[1]  | 0.26788    |
| AveragePolicyStd[2]  | 0.16851    |
| AveragePolicyStd[3]  | 0.23299    |
| AveragePolicyStd[4]  | 0.21013    |
| AveragePolicyStd[5]  | 0.24076    |
| AverageReturn        | 1218.5     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 349.24     |
| AverageEpisodeLength | 877.7      |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 238.73     |
| TotalNEpisodes       | 20055      |
| TotalNSamples        | 3.2979e+06 |
| ExplainedVariance    | 0.19281    |
-------------------------------------
[2018-01-21 14:15:29.368794 UTC] Saving snapshot
[2018-01-21 14:15:29.369003 UTC] Starting iteration 660
[2018-01-21 14:15:29.369185 UTC] Start collecting samples
[2018-01-21 14:15:34.165928 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:34.308607 UTC] Performing policy update
[2018-01-21 14:15:34.309234 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:34.433629 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:35.849025 UTC] Performing line search
[2018-01-21 14:15:36.040674 UTC] Updating baseline
[2018-01-21 14:15:37.947149 UTC] Computing logging information
-------------------------------------
| Iteration            | 660        |
| ExpectedImprovement  | 0.018982   |
| ActualImprovement    | 0.018327   |
| ImprovementRatio     | 0.96548    |
| MeanKL               | 0.0074756  |
| Entropy              | -0.49488   |
| Perplexity           | 0.60964    |
| AveragePolicyStd     | 0.225      |
| AveragePolicyStd[0]  | 0.22949    |
| AveragePolicyStd[1]  | 0.26751    |
| AveragePolicyStd[2]  | 0.16899    |
| AveragePolicyStd[3]  | 0.23239    |
| AveragePolicyStd[4]  | 0.21028    |
| AveragePolicyStd[5]  | 0.24136    |
| AverageReturn        | 1230.5     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 341.88     |
| AverageEpisodeLength | 884.36     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.95     |
| TotalNEpisodes       | 20060      |
| TotalNSamples        | 3.3027e+06 |
| ExplainedVariance    | 0.20333    |
-------------------------------------
[2018-01-21 14:15:38.619667 UTC] Saving snapshot
[2018-01-21 14:15:38.629452 UTC] Starting iteration 661
[2018-01-21 14:15:38.629658 UTC] Start collecting samples
[2018-01-21 14:15:43.199375 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:43.331561 UTC] Performing policy update
[2018-01-21 14:15:43.332178 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:43.460671 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:44.927820 UTC] Performing line search
[2018-01-21 14:15:45.116229 UTC] Updating baseline
[2018-01-21 14:15:47.062013 UTC] Computing logging information
-------------------------------------
| Iteration            | 661        |
| ExpectedImprovement  | 0.017813   |
| ActualImprovement    | 0.017463   |
| ImprovementRatio     | 0.98035    |
| MeanKL               | 0.0076902  |
| Entropy              | -0.50102   |
| Perplexity           | 0.60591    |
| AveragePolicyStd     | 0.22475    |
| AveragePolicyStd[0]  | 0.22934    |
| AveragePolicyStd[1]  | 0.26664    |
| AveragePolicyStd[2]  | 0.16902    |
| AveragePolicyStd[3]  | 0.23181    |
| AveragePolicyStd[4]  | 0.21006    |
| AveragePolicyStd[5]  | 0.24164    |
| AverageReturn        | 1234.4     |
| MinReturn            | 127.71     |
| MaxReturn            | 1451.9     |
| StdReturn            | 339.96     |
| AverageEpisodeLength | 886.98     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.24     |
| TotalNEpisodes       | 20066      |
| TotalNSamples        | 3.3079e+06 |
| ExplainedVariance    | 0.29444    |
-------------------------------------
[2018-01-21 14:15:47.798412 UTC] Saving snapshot
[2018-01-21 14:15:47.798647 UTC] Starting iteration 662
[2018-01-21 14:15:47.798826 UTC] Start collecting samples
[2018-01-21 14:15:52.300203 UTC] Computing input variables for policy optimization
[2018-01-21 14:15:52.427849 UTC] Performing policy update
[2018-01-21 14:15:52.428517 UTC] Computing gradient in Euclidean space
[2018-01-21 14:15:52.546766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:15:53.975465 UTC] Performing line search
[2018-01-21 14:15:54.163449 UTC] Updating baseline
[2018-01-21 14:15:55.979686 UTC] Computing logging information
-------------------------------------
| Iteration            | 662        |
| ExpectedImprovement  | 0.019345   |
| ActualImprovement    | 0.018271   |
| ImprovementRatio     | 0.94446    |
| MeanKL               | 0.0075673  |
| Entropy              | -0.51819   |
| Perplexity           | 0.5956     |
| AveragePolicyStd     | 0.22407    |
| AveragePolicyStd[0]  | 0.22816    |
| AveragePolicyStd[1]  | 0.26588    |
| AveragePolicyStd[2]  | 0.16933    |
| AveragePolicyStd[3]  | 0.2302     |
| AveragePolicyStd[4]  | 0.20917    |
| AveragePolicyStd[5]  | 0.24169    |
| AverageReturn        | 1239.2     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 332.74     |
| AverageEpisodeLength | 888.79     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.92     |
| TotalNEpisodes       | 20072      |
| TotalNSamples        | 3.3132e+06 |
| ExplainedVariance    | 0.11161    |
-------------------------------------
[2018-01-21 14:15:56.682475 UTC] Saving snapshot
[2018-01-21 14:15:56.682742 UTC] Starting iteration 663
[2018-01-21 14:15:56.682925 UTC] Start collecting samples
[2018-01-21 14:16:01.173977 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:01.311949 UTC] Performing policy update
[2018-01-21 14:16:01.312687 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:01.432560 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:02.853019 UTC] Performing line search
[2018-01-21 14:16:03.041726 UTC] Updating baseline
[2018-01-21 14:16:04.991207 UTC] Computing logging information
-------------------------------------
| Iteration            | 663        |
| ExpectedImprovement  | 0.018005   |
| ActualImprovement    | 0.017697   |
| ImprovementRatio     | 0.98291    |
| MeanKL               | 0.0077585  |
| Entropy              | -0.52533   |
| Perplexity           | 0.59136    |
| AveragePolicyStd     | 0.2238     |
| AveragePolicyStd[0]  | 0.22818    |
| AveragePolicyStd[1]  | 0.26519    |
| AveragePolicyStd[2]  | 0.16902    |
| AveragePolicyStd[3]  | 0.22944    |
| AveragePolicyStd[4]  | 0.20912    |
| AveragePolicyStd[5]  | 0.24186    |
| AverageReturn        | 1242.2     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 334.73     |
| AverageEpisodeLength | 889.29     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.46     |
| TotalNEpisodes       | 20077      |
| TotalNSamples        | 3.3176e+06 |
| ExplainedVariance    | 0.18313    |
-------------------------------------
[2018-01-21 14:16:05.674234 UTC] Saving snapshot
[2018-01-21 14:16:05.674479 UTC] Starting iteration 664
[2018-01-21 14:16:05.674629 UTC] Start collecting samples
[2018-01-21 14:16:10.132833 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:10.288361 UTC] Performing policy update
[2018-01-21 14:16:10.289365 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:10.414086 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:11.815240 UTC] Performing line search
[2018-01-21 14:16:12.011736 UTC] Updating baseline
[2018-01-21 14:16:14.071992 UTC] Computing logging information
-------------------------------------
| Iteration            | 664        |
| ExpectedImprovement  | 0.019425   |
| ActualImprovement    | 0.01838    |
| ImprovementRatio     | 0.94619    |
| MeanKL               | 0.0073039  |
| Entropy              | -0.52043   |
| Perplexity           | 0.59426    |
| AveragePolicyStd     | 0.22393    |
| AveragePolicyStd[0]  | 0.22777    |
| AveragePolicyStd[1]  | 0.2652     |
| AveragePolicyStd[2]  | 0.16995    |
| AveragePolicyStd[3]  | 0.22953    |
| AveragePolicyStd[4]  | 0.20933    |
| AveragePolicyStd[5]  | 0.24182    |
| AverageReturn        | 1235.2     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 339.69     |
| AverageEpisodeLength | 884.21     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.57     |
| TotalNEpisodes       | 20082      |
| TotalNSamples        | 3.3221e+06 |
| ExplainedVariance    | 0.23216    |
-------------------------------------
[2018-01-21 14:16:14.855648 UTC] Saving snapshot
[2018-01-21 14:16:14.855880 UTC] Starting iteration 665
[2018-01-21 14:16:14.856029 UTC] Start collecting samples
[2018-01-21 14:16:19.195527 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:19.326025 UTC] Performing policy update
[2018-01-21 14:16:19.326937 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:19.447253 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:20.865838 UTC] Performing line search
[2018-01-21 14:16:21.053241 UTC] Updating baseline
[2018-01-21 14:16:22.666971 UTC] Computing logging information
-------------------------------------
| Iteration            | 665        |
| ExpectedImprovement  | 0.018043   |
| ActualImprovement    | 0.016972   |
| ImprovementRatio     | 0.94066    |
| MeanKL               | 0.0074908  |
| Entropy              | -0.52245   |
| Perplexity           | 0.59307    |
| AveragePolicyStd     | 0.22387    |
| AveragePolicyStd[0]  | 0.22767    |
| AveragePolicyStd[1]  | 0.26531    |
| AveragePolicyStd[2]  | 0.16972    |
| AveragePolicyStd[3]  | 0.23001    |
| AveragePolicyStd[4]  | 0.20915    |
| AveragePolicyStd[5]  | 0.24137    |
| AverageReturn        | 1239.1     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 339.45     |
| AverageEpisodeLength | 886.74     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.44     |
| TotalNEpisodes       | 20087      |
| TotalNSamples        | 3.3271e+06 |
| ExplainedVariance    | 0.031687   |
-------------------------------------
[2018-01-21 14:16:23.343853 UTC] Saving snapshot
[2018-01-21 14:16:23.344099 UTC] Starting iteration 666
[2018-01-21 14:16:23.344323 UTC] Start collecting samples
[2018-01-21 14:16:27.788117 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:27.912560 UTC] Performing policy update
[2018-01-21 14:16:27.913286 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:28.036545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:29.486593 UTC] Performing line search
[2018-01-21 14:16:29.678342 UTC] Updating baseline
[2018-01-21 14:16:32.794111 UTC] Computing logging information
-------------------------------------
| Iteration            | 666        |
| ExpectedImprovement  | 0.016942   |
| ActualImprovement    | 0.01638    |
| ImprovementRatio     | 0.96685    |
| MeanKL               | 0.0077669  |
| Entropy              | -0.53248   |
| Perplexity           | 0.58715    |
| AveragePolicyStd     | 0.22346    |
| AveragePolicyStd[0]  | 0.22721    |
| AveragePolicyStd[1]  | 0.26454    |
| AveragePolicyStd[2]  | 0.16975    |
| AveragePolicyStd[3]  | 0.22957    |
| AveragePolicyStd[4]  | 0.20921    |
| AveragePolicyStd[5]  | 0.24049    |
| AverageReturn        | 1239.6     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 339.7      |
| AverageEpisodeLength | 886.74     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.44     |
| TotalNEpisodes       | 20091      |
| TotalNSamples        | 3.3311e+06 |
| ExplainedVariance    | -0.018042  |
-------------------------------------
[2018-01-21 14:16:33.493820 UTC] Saving snapshot
[2018-01-21 14:16:33.494110 UTC] Starting iteration 667
[2018-01-21 14:16:33.494297 UTC] Start collecting samples
[2018-01-21 14:16:37.975873 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:38.103162 UTC] Performing policy update
[2018-01-21 14:16:38.104158 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:38.227089 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:39.631333 UTC] Performing line search
[2018-01-21 14:16:39.834039 UTC] Updating baseline
[2018-01-21 14:16:42.613066 UTC] Computing logging information
-------------------------------------
| Iteration            | 667        |
| ExpectedImprovement  | 0.016831   |
| ActualImprovement    | 0.015585   |
| ImprovementRatio     | 0.92596    |
| MeanKL               | 0.0073107  |
| Entropy              | -0.53744   |
| Perplexity           | 0.58424    |
| AveragePolicyStd     | 0.22327    |
| AveragePolicyStd[0]  | 0.22644    |
| AveragePolicyStd[1]  | 0.26372    |
| AveragePolicyStd[2]  | 0.16934    |
| AveragePolicyStd[3]  | 0.22966    |
| AveragePolicyStd[4]  | 0.20985    |
| AveragePolicyStd[5]  | 0.24061    |
| AverageReturn        | 1249       |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 331.88     |
| AverageEpisodeLength | 892.58     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.81     |
| TotalNEpisodes       | 20098      |
| TotalNSamples        | 3.3381e+06 |
| ExplainedVariance    | -0.010059  |
-------------------------------------
[2018-01-21 14:16:43.303569 UTC] Saving snapshot
[2018-01-21 14:16:43.303773 UTC] Starting iteration 668
[2018-01-21 14:16:43.303900 UTC] Start collecting samples
[2018-01-21 14:16:47.780191 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:47.909712 UTC] Performing policy update
[2018-01-21 14:16:47.910401 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:48.032254 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:49.441231 UTC] Performing line search
[2018-01-21 14:16:49.635540 UTC] Updating baseline
[2018-01-21 14:16:52.083286 UTC] Computing logging information
-------------------------------------
| Iteration            | 668        |
| ExpectedImprovement  | 0.017041   |
| ActualImprovement    | 0.016204   |
| ImprovementRatio     | 0.95089    |
| MeanKL               | 0.0079041  |
| Entropy              | -0.53273   |
| Perplexity           | 0.587      |
| AveragePolicyStd     | 0.22345    |
| AveragePolicyStd[0]  | 0.22761    |
| AveragePolicyStd[1]  | 0.26318    |
| AveragePolicyStd[2]  | 0.16919    |
| AveragePolicyStd[3]  | 0.23037    |
| AveragePolicyStd[4]  | 0.20972    |
| AveragePolicyStd[5]  | 0.24062    |
| AverageReturn        | 1262.4     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 326.05     |
| AverageEpisodeLength | 901.18     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.49     |
| TotalNEpisodes       | 20102      |
| TotalNSamples        | 3.3421e+06 |
| ExplainedVariance    | 8.8541e-07 |
-------------------------------------
[2018-01-21 14:16:52.753808 UTC] Saving snapshot
[2018-01-21 14:16:52.754022 UTC] Starting iteration 669
[2018-01-21 14:16:52.754193 UTC] Start collecting samples
[2018-01-21 14:16:57.274336 UTC] Computing input variables for policy optimization
[2018-01-21 14:16:57.390884 UTC] Performing policy update
[2018-01-21 14:16:57.391616 UTC] Computing gradient in Euclidean space
[2018-01-21 14:16:57.514938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:16:58.927357 UTC] Performing line search
[2018-01-21 14:16:59.116847 UTC] Updating baseline
[2018-01-21 14:17:01.381325 UTC] Computing logging information
-------------------------------------
| Iteration            | 669        |
| ExpectedImprovement  | 0.017487   |
| ActualImprovement    | 0.016177   |
| ImprovementRatio     | 0.9251     |
| MeanKL               | 0.0073095  |
| Entropy              | -0.53931   |
| Perplexity           | 0.58315    |
| AveragePolicyStd     | 0.2232     |
| AveragePolicyStd[0]  | 0.22714    |
| AveragePolicyStd[1]  | 0.26313    |
| AveragePolicyStd[2]  | 0.16948    |
| AveragePolicyStd[3]  | 0.22945    |
| AveragePolicyStd[4]  | 0.20882    |
| AveragePolicyStd[5]  | 0.24117    |
| AverageReturn        | 1271       |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 316.19     |
| AverageEpisodeLength | 906.04     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.23     |
| TotalNEpisodes       | 20107      |
| TotalNSamples        | 3.3469e+06 |
| ExplainedVariance    | 0.11617    |
-------------------------------------
[2018-01-21 14:17:02.068011 UTC] Saving snapshot
[2018-01-21 14:17:02.068296 UTC] Starting iteration 670
[2018-01-21 14:17:02.068475 UTC] Start collecting samples
[2018-01-21 14:17:06.459883 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:06.579448 UTC] Performing policy update
[2018-01-21 14:17:06.580145 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:06.695744 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:08.118034 UTC] Performing line search
[2018-01-21 14:17:08.312838 UTC] Updating baseline
[2018-01-21 14:17:10.351675 UTC] Computing logging information
--------------------------------------
| Iteration            | 670         |
| ExpectedImprovement  | 0.016646    |
| ActualImprovement    | 0.01618     |
| ImprovementRatio     | 0.97202     |
| MeanKL               | 0.0082891   |
| Entropy              | -0.54163    |
| Perplexity           | 0.5818      |
| AveragePolicyStd     | 0.2231      |
| AveragePolicyStd[0]  | 0.22674     |
| AveragePolicyStd[1]  | 0.26306     |
| AveragePolicyStd[2]  | 0.16952     |
| AveragePolicyStd[3]  | 0.22905     |
| AveragePolicyStd[4]  | 0.20905     |
| AveragePolicyStd[5]  | 0.2412      |
| AverageReturn        | 1274.1      |
| MinReturn            | 168.95      |
| MaxReturn            | 1479.1      |
| StdReturn            | 316.19      |
| AverageEpisodeLength | 907.91      |
| MinEpisodeLength     | 140         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 215.23      |
| TotalNEpisodes       | 20112       |
| TotalNSamples        | 3.3519e+06  |
| ExplainedVariance    | -0.00038894 |
--------------------------------------
[2018-01-21 14:17:11.095523 UTC] Saving snapshot
[2018-01-21 14:17:11.103540 UTC] Starting iteration 671
[2018-01-21 14:17:11.103769 UTC] Start collecting samples
[2018-01-21 14:17:15.587773 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:15.720251 UTC] Performing policy update
[2018-01-21 14:17:15.720862 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:15.836169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:17.266387 UTC] Performing line search
[2018-01-21 14:17:17.447383 UTC] Updating baseline
[2018-01-21 14:17:19.368666 UTC] Computing logging information
--------------------------------------
| Iteration            | 671         |
| ExpectedImprovement  | 0.016866    |
| ActualImprovement    | 0.015539    |
| ImprovementRatio     | 0.92134     |
| MeanKL               | 0.0076107   |
| Entropy              | -0.53931    |
| Perplexity           | 0.58315     |
| AveragePolicyStd     | 0.22319     |
| AveragePolicyStd[0]  | 0.22653     |
| AveragePolicyStd[1]  | 0.26348     |
| AveragePolicyStd[2]  | 0.16964     |
| AveragePolicyStd[3]  | 0.22843     |
| AveragePolicyStd[4]  | 0.20957     |
| AveragePolicyStd[5]  | 0.24149     |
| AverageReturn        | 1277.9      |
| MinReturn            | 168.95      |
| MaxReturn            | 1479.1      |
| StdReturn            | 315.86      |
| AverageEpisodeLength | 910.35      |
| MinEpisodeLength     | 140         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 214.88      |
| TotalNEpisodes       | 20117       |
| TotalNSamples        | 3.3569e+06  |
| ExplainedVariance    | -0.00063845 |
--------------------------------------
[2018-01-21 14:17:20.044384 UTC] Saving snapshot
[2018-01-21 14:17:20.044700 UTC] Starting iteration 672
[2018-01-21 14:17:20.044890 UTC] Start collecting samples
[2018-01-21 14:17:24.464297 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:24.585231 UTC] Performing policy update
[2018-01-21 14:17:24.586137 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:24.704459 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:26.123468 UTC] Performing line search
[2018-01-21 14:17:26.308371 UTC] Updating baseline
[2018-01-21 14:17:28.724738 UTC] Computing logging information
-------------------------------------
| Iteration            | 672        |
| ExpectedImprovement  | 0.017929   |
| ActualImprovement    | 0.017176   |
| ImprovementRatio     | 0.95801    |
| MeanKL               | 0.0077414  |
| Entropy              | -0.53798   |
| Perplexity           | 0.58393    |
| AveragePolicyStd     | 0.22323    |
| AveragePolicyStd[0]  | 0.2275     |
| AveragePolicyStd[1]  | 0.2637     |
| AveragePolicyStd[2]  | 0.17014    |
| AveragePolicyStd[3]  | 0.22812    |
| AveragePolicyStd[4]  | 0.20837    |
| AveragePolicyStd[5]  | 0.24157    |
| AverageReturn        | 1273.6     |
| MinReturn            | 168.95     |
| MaxReturn            | 1479.1     |
| StdReturn            | 329.09     |
| AverageEpisodeLength | 906.94     |
| MinEpisodeLength     | 140        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.88     |
| TotalNEpisodes       | 20124      |
| TotalNSamples        | 3.3631e+06 |
| ExplainedVariance    | 0.10419    |
-------------------------------------
[2018-01-21 14:17:29.427677 UTC] Saving snapshot
[2018-01-21 14:17:29.427919 UTC] Starting iteration 673
[2018-01-21 14:17:29.428074 UTC] Start collecting samples
[2018-01-21 14:17:33.793451 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:33.936518 UTC] Performing policy update
[2018-01-21 14:17:33.937626 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:34.055614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:35.464157 UTC] Performing line search
[2018-01-21 14:17:35.661302 UTC] Updating baseline
[2018-01-21 14:17:38.736815 UTC] Computing logging information
-------------------------------------
| Iteration            | 673        |
| ExpectedImprovement  | 0.018886   |
| ActualImprovement    | 0.017524   |
| ImprovementRatio     | 0.92789    |
| MeanKL               | 0.0072328  |
| Entropy              | -0.53744   |
| Perplexity           | 0.58424    |
| AveragePolicyStd     | 0.22326    |
| AveragePolicyStd[0]  | 0.2272     |
| AveragePolicyStd[1]  | 0.26364    |
| AveragePolicyStd[2]  | 0.16995    |
| AveragePolicyStd[3]  | 0.22829    |
| AveragePolicyStd[4]  | 0.2088     |
| AveragePolicyStd[5]  | 0.24167    |
| AverageReturn        | 1296.3     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 298.41     |
| AverageEpisodeLength | 922.32     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.88     |
| TotalNEpisodes       | 20128      |
| TotalNSamples        | 3.3671e+06 |
| ExplainedVariance    | -0.032818  |
-------------------------------------
[2018-01-21 14:17:39.504449 UTC] Saving snapshot
[2018-01-21 14:17:39.504683 UTC] Starting iteration 674
[2018-01-21 14:17:39.504834 UTC] Start collecting samples
[2018-01-21 14:17:44.057103 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:44.183305 UTC] Performing policy update
[2018-01-21 14:17:44.183948 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:44.301570 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:45.726018 UTC] Performing line search
[2018-01-21 14:17:45.938897 UTC] Updating baseline
[2018-01-21 14:17:48.756350 UTC] Computing logging information
-------------------------------------
| Iteration            | 674        |
| ExpectedImprovement  | 0.018001   |
| ActualImprovement    | 0.016844   |
| ImprovementRatio     | 0.93576    |
| MeanKL               | 0.0072301  |
| Entropy              | -0.53357   |
| Perplexity           | 0.58651    |
| AveragePolicyStd     | 0.2234     |
| AveragePolicyStd[0]  | 0.22791    |
| AveragePolicyStd[1]  | 0.2636     |
| AveragePolicyStd[2]  | 0.17027    |
| AveragePolicyStd[3]  | 0.22832    |
| AveragePolicyStd[4]  | 0.20814    |
| AveragePolicyStd[5]  | 0.24216    |
| AverageReturn        | 1283.7     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 318.29     |
| AverageEpisodeLength | 913.27     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.53     |
| TotalNEpisodes       | 20136      |
| TotalNSamples        | 3.3738e+06 |
| ExplainedVariance    | 0.15475    |
-------------------------------------
[2018-01-21 14:17:49.448626 UTC] Saving snapshot
[2018-01-21 14:17:49.448870 UTC] Starting iteration 675
[2018-01-21 14:17:49.449031 UTC] Start collecting samples
[2018-01-21 14:17:53.741516 UTC] Computing input variables for policy optimization
[2018-01-21 14:17:53.882618 UTC] Performing policy update
[2018-01-21 14:17:53.883232 UTC] Computing gradient in Euclidean space
[2018-01-21 14:17:54.015274 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:17:55.470321 UTC] Performing line search
[2018-01-21 14:17:55.655871 UTC] Updating baseline
[2018-01-21 14:17:58.035052 UTC] Computing logging information
-------------------------------------
| Iteration            | 675        |
| ExpectedImprovement  | 0.018705   |
| ActualImprovement    | 0.016889   |
| ImprovementRatio     | 0.90295    |
| MeanKL               | 0.0070702  |
| Entropy              | -0.54191   |
| Perplexity           | 0.58164    |
| AveragePolicyStd     | 0.22307    |
| AveragePolicyStd[0]  | 0.22747    |
| AveragePolicyStd[1]  | 0.26279    |
| AveragePolicyStd[2]  | 0.17026    |
| AveragePolicyStd[3]  | 0.22793    |
| AveragePolicyStd[4]  | 0.20797    |
| AveragePolicyStd[5]  | 0.24198    |
| AverageReturn        | 1283.9     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 318.36     |
| AverageEpisodeLength | 913.27     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.53     |
| TotalNEpisodes       | 20140      |
| TotalNSamples        | 3.3778e+06 |
| ExplainedVariance    | -0.039157  |
-------------------------------------
[2018-01-21 14:17:58.777551 UTC] Saving snapshot
[2018-01-21 14:17:58.777783 UTC] Starting iteration 676
[2018-01-21 14:17:58.777994 UTC] Start collecting samples
[2018-01-21 14:18:03.092768 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:03.240562 UTC] Performing policy update
[2018-01-21 14:18:03.241578 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:03.373879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:04.807072 UTC] Performing line search
[2018-01-21 14:18:05.024136 UTC] Updating baseline
[2018-01-21 14:18:07.051379 UTC] Computing logging information
-------------------------------------
| Iteration            | 676        |
| ExpectedImprovement  | 0.01791    |
| ActualImprovement    | 0.016945   |
| ImprovementRatio     | 0.94615    |
| MeanKL               | 0.007227   |
| Entropy              | -0.54359   |
| Perplexity           | 0.58066    |
| AveragePolicyStd     | 0.22299    |
| AveragePolicyStd[0]  | 0.22816    |
| AveragePolicyStd[1]  | 0.26216    |
| AveragePolicyStd[2]  | 0.17042    |
| AveragePolicyStd[3]  | 0.22736    |
| AveragePolicyStd[4]  | 0.20765    |
| AveragePolicyStd[5]  | 0.24219    |
| AverageReturn        | 1291.9     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 311.42     |
| AverageEpisodeLength | 918.69     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.78     |
| TotalNEpisodes       | 20143      |
| TotalNSamples        | 3.3808e+06 |
| ExplainedVariance    | 0.00025282 |
-------------------------------------
[2018-01-21 14:18:07.719896 UTC] Saving snapshot
[2018-01-21 14:18:07.720111 UTC] Starting iteration 677
[2018-01-21 14:18:07.720316 UTC] Start collecting samples
[2018-01-21 14:18:12.195171 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:12.324266 UTC] Performing policy update
[2018-01-21 14:18:12.325353 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:12.443575 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:13.830362 UTC] Performing line search
[2018-01-21 14:18:14.021243 UTC] Updating baseline
[2018-01-21 14:18:16.180647 UTC] Computing logging information
--------------------------------------
| Iteration            | 677         |
| ExpectedImprovement  | 0.01534     |
| ActualImprovement    | 0.01502     |
| ImprovementRatio     | 0.97909     |
| MeanKL               | 0.0074628   |
| Entropy              | -0.54764    |
| Perplexity           | 0.57831     |
| AveragePolicyStd     | 0.2228      |
| AveragePolicyStd[0]  | 0.22811     |
| AveragePolicyStd[1]  | 0.26196     |
| AveragePolicyStd[2]  | 0.17045     |
| AveragePolicyStd[3]  | 0.2267      |
| AveragePolicyStd[4]  | 0.20852     |
| AveragePolicyStd[5]  | 0.24106     |
| AverageReturn        | 1338.8      |
| MinReturn            | 187.64      |
| MaxReturn            | 1479.1      |
| StdReturn            | 247.65      |
| AverageEpisodeLength | 949.34      |
| MinEpisodeLength     | 159         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 168.09      |
| TotalNEpisodes       | 20152       |
| TotalNSamples        | 3.3898e+06  |
| ExplainedVariance    | -2.1211e-05 |
--------------------------------------
[2018-01-21 14:18:16.846044 UTC] Saving snapshot
[2018-01-21 14:18:16.846329 UTC] Starting iteration 678
[2018-01-21 14:18:16.846532 UTC] Start collecting samples
[2018-01-21 14:18:21.160738 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:21.271842 UTC] Performing policy update
[2018-01-21 14:18:21.274806 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:21.393943 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:22.821637 UTC] Performing line search
[2018-01-21 14:18:23.024607 UTC] Updating baseline
[2018-01-21 14:18:24.782226 UTC] Computing logging information
-------------------------------------
| Iteration            | 678        |
| ExpectedImprovement  | 0.017366   |
| ActualImprovement    | 0.016974   |
| ImprovementRatio     | 0.97742    |
| MeanKL               | 0.0073315  |
| Entropy              | -0.5337    |
| Perplexity           | 0.58643    |
| AveragePolicyStd     | 0.2233     |
| AveragePolicyStd[0]  | 0.22819    |
| AveragePolicyStd[1]  | 0.26148    |
| AveragePolicyStd[2]  | 0.17094    |
| AveragePolicyStd[3]  | 0.2281     |
| AveragePolicyStd[4]  | 0.20885    |
| AveragePolicyStd[5]  | 0.24225    |
| AverageReturn        | 1339.9     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 247.94     |
| AverageEpisodeLength | 949.34     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.09     |
| TotalNEpisodes       | 20155      |
| TotalNSamples        | 3.3928e+06 |
| ExplainedVariance    | -0.031665  |
-------------------------------------
[2018-01-21 14:18:25.456415 UTC] Saving snapshot
[2018-01-21 14:18:25.456653 UTC] Starting iteration 679
[2018-01-21 14:18:25.456805 UTC] Start collecting samples
[2018-01-21 14:18:29.972447 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:30.092623 UTC] Performing policy update
[2018-01-21 14:18:30.093706 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:30.212833 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:31.596595 UTC] Performing line search
[2018-01-21 14:18:31.784956 UTC] Updating baseline
[2018-01-21 14:18:33.537637 UTC] Computing logging information
-------------------------------------
| Iteration            | 679        |
| ExpectedImprovement  | 0.016165   |
| ActualImprovement    | 0.014974   |
| ImprovementRatio     | 0.9263     |
| MeanKL               | 0.0073105  |
| Entropy              | -0.53097   |
| Perplexity           | 0.58804    |
| AveragePolicyStd     | 0.22341    |
| AveragePolicyStd[0]  | 0.22822    |
| AveragePolicyStd[1]  | 0.26179    |
| AveragePolicyStd[2]  | 0.17103    |
| AveragePolicyStd[3]  | 0.22869    |
| AveragePolicyStd[4]  | 0.20844    |
| AveragePolicyStd[5]  | 0.24232    |
| AverageReturn        | 1339.2     |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 247.71     |
| AverageEpisodeLength | 949.34     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.09     |
| TotalNEpisodes       | 20159      |
| TotalNSamples        | 3.3968e+06 |
| ExplainedVariance    | 0.0058163  |
-------------------------------------
[2018-01-21 14:18:34.293590 UTC] Saving snapshot
[2018-01-21 14:18:34.293892 UTC] Starting iteration 680
[2018-01-21 14:18:34.294112 UTC] Start collecting samples
[2018-01-21 14:18:38.990252 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:39.114422 UTC] Performing policy update
[2018-01-21 14:18:39.115063 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:39.234272 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:40.684509 UTC] Performing line search
[2018-01-21 14:18:40.874387 UTC] Updating baseline
[2018-01-21 14:18:42.996542 UTC] Computing logging information
-------------------------------------
| Iteration            | 680        |
| ExpectedImprovement  | 0.017377   |
| ActualImprovement    | 0.016205   |
| ImprovementRatio     | 0.93257    |
| MeanKL               | 0.0075124  |
| Entropy              | -0.53206   |
| Perplexity           | 0.58739    |
| AveragePolicyStd     | 0.22338    |
| AveragePolicyStd[0]  | 0.2281     |
| AveragePolicyStd[1]  | 0.26171    |
| AveragePolicyStd[2]  | 0.17097    |
| AveragePolicyStd[3]  | 0.22832    |
| AveragePolicyStd[4]  | 0.20851    |
| AveragePolicyStd[5]  | 0.24266    |
| AverageReturn        | 1350       |
| MinReturn            | 187.64     |
| MaxReturn            | 1479.1     |
| StdReturn            | 237.09     |
| AverageEpisodeLength | 956.39     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.72     |
| TotalNEpisodes       | 20166      |
| TotalNSamples        | 3.4036e+06 |
| ExplainedVariance    | 0.086651   |
-------------------------------------
[2018-01-21 14:18:43.746640 UTC] Saving snapshot
[2018-01-21 14:18:43.756942 UTC] Starting iteration 681
[2018-01-21 14:18:43.757180 UTC] Start collecting samples
[2018-01-21 14:18:48.234157 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:48.391503 UTC] Performing policy update
[2018-01-21 14:18:48.392126 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:48.518179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:50.005154 UTC] Performing line search
[2018-01-21 14:18:50.207430 UTC] Updating baseline
[2018-01-21 14:18:52.846142 UTC] Computing logging information
-------------------------------------
| Iteration            | 681        |
| ExpectedImprovement  | 0.016258   |
| ActualImprovement    | 0.015258   |
| ImprovementRatio     | 0.93849    |
| MeanKL               | 0.0076705  |
| Entropy              | -0.53528   |
| Perplexity           | 0.58551    |
| AveragePolicyStd     | 0.22326    |
| AveragePolicyStd[0]  | 0.22757    |
| AveragePolicyStd[1]  | 0.26124    |
| AveragePolicyStd[2]  | 0.17088    |
| AveragePolicyStd[3]  | 0.22857    |
| AveragePolicyStd[4]  | 0.20813    |
| AveragePolicyStd[5]  | 0.24317    |
| AverageReturn        | 1348.2     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 236.54     |
| AverageEpisodeLength | 956.39     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.72     |
| TotalNEpisodes       | 20170      |
| TotalNSamples        | 3.4076e+06 |
| ExplainedVariance    | 0.0049356  |
-------------------------------------
[2018-01-21 14:18:53.587991 UTC] Saving snapshot
[2018-01-21 14:18:53.588231 UTC] Starting iteration 682
[2018-01-21 14:18:53.588400 UTC] Start collecting samples
[2018-01-21 14:18:58.103829 UTC] Computing input variables for policy optimization
[2018-01-21 14:18:58.241052 UTC] Performing policy update
[2018-01-21 14:18:58.241642 UTC] Computing gradient in Euclidean space
[2018-01-21 14:18:58.363807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:18:59.810663 UTC] Performing line search
[2018-01-21 14:19:00.014107 UTC] Updating baseline
[2018-01-21 14:19:02.000245 UTC] Computing logging information
-------------------------------------
| Iteration            | 682        |
| ExpectedImprovement  | 0.017152   |
| ActualImprovement    | 0.016202   |
| ImprovementRatio     | 0.94462    |
| MeanKL               | 0.0077041  |
| Entropy              | -0.54138   |
| Perplexity           | 0.58195    |
| AveragePolicyStd     | 0.22302    |
| AveragePolicyStd[0]  | 0.22828    |
| AveragePolicyStd[1]  | 0.26102    |
| AveragePolicyStd[2]  | 0.1708     |
| AveragePolicyStd[3]  | 0.22904    |
| AveragePolicyStd[4]  | 0.2075     |
| AveragePolicyStd[5]  | 0.2415     |
| AverageReturn        | 1358.1     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 217.07     |
| AverageEpisodeLength | 963.36     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.74     |
| TotalNEpisodes       | 20174      |
| TotalNSamples        | 3.4116e+06 |
| ExplainedVariance    | -0.0062845 |
-------------------------------------
[2018-01-21 14:19:02.693712 UTC] Saving snapshot
[2018-01-21 14:19:02.693948 UTC] Starting iteration 683
[2018-01-21 14:19:02.694106 UTC] Start collecting samples
[2018-01-21 14:19:07.338832 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:07.471430 UTC] Performing policy update
[2018-01-21 14:19:07.472035 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:07.599292 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:09.012231 UTC] Performing line search
[2018-01-21 14:19:09.212095 UTC] Updating baseline
[2018-01-21 14:19:11.221652 UTC] Computing logging information
-------------------------------------
| Iteration            | 683        |
| ExpectedImprovement  | 0.017842   |
| ActualImprovement    | 0.017045   |
| ImprovementRatio     | 0.95532    |
| MeanKL               | 0.0080131  |
| Entropy              | -0.53944   |
| Perplexity           | 0.58307    |
| AveragePolicyStd     | 0.2231     |
| AveragePolicyStd[0]  | 0.22834    |
| AveragePolicyStd[1]  | 0.26082    |
| AveragePolicyStd[2]  | 0.17061    |
| AveragePolicyStd[3]  | 0.22903    |
| AveragePolicyStd[4]  | 0.208      |
| AveragePolicyStd[5]  | 0.24178    |
| AverageReturn        | 1374.8     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 187.11     |
| AverageEpisodeLength | 974.48     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.47     |
| TotalNEpisodes       | 20182      |
| TotalNSamples        | 3.4196e+06 |
| ExplainedVariance    | 0.0058341  |
-------------------------------------
[2018-01-21 14:19:11.960528 UTC] Saving snapshot
[2018-01-21 14:19:11.960744 UTC] Starting iteration 684
[2018-01-21 14:19:11.960924 UTC] Start collecting samples
[2018-01-21 14:19:16.524693 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:16.664664 UTC] Performing policy update
[2018-01-21 14:19:16.665262 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:16.799456 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:18.217751 UTC] Performing line search
[2018-01-21 14:19:18.403863 UTC] Updating baseline
[2018-01-21 14:19:20.734348 UTC] Computing logging information
-------------------------------------
| Iteration            | 684        |
| ExpectedImprovement  | 0.016222   |
| ActualImprovement    | 0.015578   |
| ImprovementRatio     | 0.96031    |
| MeanKL               | 0.0075447  |
| Entropy              | -0.5458    |
| Perplexity           | 0.57938    |
| AveragePolicyStd     | 0.22288    |
| AveragePolicyStd[0]  | 0.22782    |
| AveragePolicyStd[1]  | 0.25995    |
| AveragePolicyStd[2]  | 0.16987    |
| AveragePolicyStd[3]  | 0.22908    |
| AveragePolicyStd[4]  | 0.20828    |
| AveragePolicyStd[5]  | 0.24228    |
| AverageReturn        | 1375.9     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 187.27     |
| AverageEpisodeLength | 974.48     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.47     |
| TotalNEpisodes       | 20185      |
| TotalNSamples        | 3.4226e+06 |
| ExplainedVariance    | 0.0081611  |
-------------------------------------
[2018-01-21 14:19:21.382603 UTC] Saving snapshot
[2018-01-21 14:19:21.382777 UTC] Starting iteration 685
[2018-01-21 14:19:21.382879 UTC] Start collecting samples
[2018-01-21 14:19:25.820364 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:25.958175 UTC] Performing policy update
[2018-01-21 14:19:25.959230 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:26.087143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:27.507713 UTC] Performing line search
[2018-01-21 14:19:27.703590 UTC] Updating baseline
[2018-01-21 14:19:29.898989 UTC] Computing logging information
-------------------------------------
| Iteration            | 685        |
| ExpectedImprovement  | 0.020419   |
| ActualImprovement    | 0.018865   |
| ImprovementRatio     | 0.92392    |
| MeanKL               | 0.0069795  |
| Entropy              | -0.53976   |
| Perplexity           | 0.58289    |
| AveragePolicyStd     | 0.22307    |
| AveragePolicyStd[0]  | 0.22866    |
| AveragePolicyStd[1]  | 0.25941    |
| AveragePolicyStd[2]  | 0.17008    |
| AveragePolicyStd[3]  | 0.22925    |
| AveragePolicyStd[4]  | 0.20905    |
| AveragePolicyStd[5]  | 0.24198    |
| AverageReturn        | 1364.2     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 217.59     |
| AverageEpisodeLength | 966.58     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.55     |
| TotalNEpisodes       | 20190      |
| TotalNSamples        | 3.4268e+06 |
| ExplainedVariance    | 0.074687   |
-------------------------------------
[2018-01-21 14:19:30.625685 UTC] Saving snapshot
[2018-01-21 14:19:30.625863 UTC] Starting iteration 686
[2018-01-21 14:19:30.625991 UTC] Start collecting samples
[2018-01-21 14:19:35.284494 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:35.416824 UTC] Performing policy update
[2018-01-21 14:19:35.417491 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:35.543351 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:37.025267 UTC] Performing line search
[2018-01-21 14:19:37.212042 UTC] Updating baseline
[2018-01-21 14:19:39.022967 UTC] Computing logging information
-------------------------------------
| Iteration            | 686        |
| ExpectedImprovement  | 0.015906   |
| ActualImprovement    | 0.014981   |
| ImprovementRatio     | 0.94188    |
| MeanKL               | 0.0073945  |
| Entropy              | -0.53688   |
| Perplexity           | 0.58457    |
| AveragePolicyStd     | 0.22317    |
| AveragePolicyStd[0]  | 0.22865    |
| AveragePolicyStd[1]  | 0.25924    |
| AveragePolicyStd[2]  | 0.17022    |
| AveragePolicyStd[3]  | 0.22959    |
| AveragePolicyStd[4]  | 0.2091     |
| AveragePolicyStd[5]  | 0.24223    |
| AverageReturn        | 1359.4     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 220.64     |
| AverageEpisodeLength | 963.4      |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.32     |
| TotalNEpisodes       | 20196      |
| TotalNSamples        | 3.4324e+06 |
| ExplainedVariance    | 0.21928    |
-------------------------------------
[2018-01-21 14:19:39.704044 UTC] Saving snapshot
[2018-01-21 14:19:39.704267 UTC] Starting iteration 687
[2018-01-21 14:19:39.704472 UTC] Start collecting samples
[2018-01-21 14:19:44.508693 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:44.629501 UTC] Performing policy update
[2018-01-21 14:19:44.630096 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:44.749099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:46.154439 UTC] Performing line search
[2018-01-21 14:19:46.343469 UTC] Updating baseline
[2018-01-21 14:19:48.513232 UTC] Computing logging information
-------------------------------------
| Iteration            | 687        |
| ExpectedImprovement  | 0.015957   |
| ActualImprovement    | 0.014845   |
| ImprovementRatio     | 0.93035    |
| MeanKL               | 0.0074971  |
| Entropy              | -0.54286   |
| Perplexity           | 0.58108    |
| AveragePolicyStd     | 0.22296    |
| AveragePolicyStd[0]  | 0.22854    |
| AveragePolicyStd[1]  | 0.25734    |
| AveragePolicyStd[2]  | 0.16929    |
| AveragePolicyStd[3]  | 0.22955    |
| AveragePolicyStd[4]  | 0.20995    |
| AveragePolicyStd[5]  | 0.24307    |
| AverageReturn        | 1359.2     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 220.61     |
| AverageEpisodeLength | 963.4      |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.32     |
| TotalNEpisodes       | 20202      |
| TotalNSamples        | 3.4384e+06 |
| ExplainedVariance    | -0.0033085 |
-------------------------------------
[2018-01-21 14:19:49.231185 UTC] Saving snapshot
[2018-01-21 14:19:49.231421 UTC] Starting iteration 688
[2018-01-21 14:19:49.231573 UTC] Start collecting samples
[2018-01-21 14:19:53.906700 UTC] Computing input variables for policy optimization
[2018-01-21 14:19:54.052588 UTC] Performing policy update
[2018-01-21 14:19:54.053227 UTC] Computing gradient in Euclidean space
[2018-01-21 14:19:54.186905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:19:55.599961 UTC] Performing line search
[2018-01-21 14:19:55.800318 UTC] Updating baseline
[2018-01-21 14:19:57.737262 UTC] Computing logging information
-------------------------------------
| Iteration            | 688        |
| ExpectedImprovement  | 0.015772   |
| ActualImprovement    | 0.015485   |
| ImprovementRatio     | 0.98184    |
| MeanKL               | 0.0085542  |
| Entropy              | -0.54078   |
| Perplexity           | 0.58229    |
| AveragePolicyStd     | 0.22304    |
| AveragePolicyStd[0]  | 0.22863    |
| AveragePolicyStd[1]  | 0.258      |
| AveragePolicyStd[2]  | 0.16942    |
| AveragePolicyStd[3]  | 0.22881    |
| AveragePolicyStd[4]  | 0.21003    |
| AveragePolicyStd[5]  | 0.24336    |
| AverageReturn        | 1361.9     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 219.5      |
| AverageEpisodeLength | 965.2      |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.66     |
| TotalNEpisodes       | 20206      |
| TotalNSamples        | 3.4424e+06 |
| ExplainedVariance    | 0.01019    |
-------------------------------------
[2018-01-21 14:19:58.452275 UTC] Saving snapshot
[2018-01-21 14:19:58.452505 UTC] Starting iteration 689
[2018-01-21 14:19:58.452680 UTC] Start collecting samples
[2018-01-21 14:20:02.921842 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:03.049135 UTC] Performing policy update
[2018-01-21 14:20:03.049810 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:03.174673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:04.611970 UTC] Performing line search
[2018-01-21 14:20:04.820477 UTC] Updating baseline
[2018-01-21 14:20:06.879727 UTC] Computing logging information
-------------------------------------
| Iteration            | 689        |
| ExpectedImprovement  | 0.01596    |
| ActualImprovement    | 0.014844   |
| ImprovementRatio     | 0.93009    |
| MeanKL               | 0.0080907  |
| Entropy              | -0.5332    |
| Perplexity           | 0.58673    |
| AveragePolicyStd     | 0.22333    |
| AveragePolicyStd[0]  | 0.22887    |
| AveragePolicyStd[1]  | 0.25819    |
| AveragePolicyStd[2]  | 0.1697     |
| AveragePolicyStd[3]  | 0.22874    |
| AveragePolicyStd[4]  | 0.20986    |
| AveragePolicyStd[5]  | 0.24465    |
| AverageReturn        | 1361.6     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 219.41     |
| AverageEpisodeLength | 965.2      |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.66     |
| TotalNEpisodes       | 20211      |
| TotalNSamples        | 3.4474e+06 |
| ExplainedVariance    | 0.0021874  |
-------------------------------------
[2018-01-21 14:20:07.618343 UTC] Saving snapshot
[2018-01-21 14:20:07.618589 UTC] Starting iteration 690
[2018-01-21 14:20:07.618738 UTC] Start collecting samples
[2018-01-21 14:20:12.202004 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:12.354530 UTC] Performing policy update
[2018-01-21 14:20:12.355640 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:12.477185 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:13.927527 UTC] Performing line search
[2018-01-21 14:20:14.115910 UTC] Updating baseline
[2018-01-21 14:20:16.397974 UTC] Computing logging information
-------------------------------------
| Iteration            | 690        |
| ExpectedImprovement  | 0.016189   |
| ActualImprovement    | 0.015626   |
| ImprovementRatio     | 0.96525    |
| MeanKL               | 0.0076032  |
| Entropy              | -0.53527   |
| Perplexity           | 0.58551    |
| AveragePolicyStd     | 0.2233     |
| AveragePolicyStd[0]  | 0.22935    |
| AveragePolicyStd[1]  | 0.25909    |
| AveragePolicyStd[2]  | 0.16892    |
| AveragePolicyStd[3]  | 0.22848    |
| AveragePolicyStd[4]  | 0.21019    |
| AveragePolicyStd[5]  | 0.24379    |
| AverageReturn        | 1360.7     |
| MinReturn            | 187.64     |
| MaxReturn            | 1471.5     |
| StdReturn            | 219.2      |
| AverageEpisodeLength | 965.11     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.64     |
| TotalNEpisodes       | 20217      |
| TotalNSamples        | 3.4534e+06 |
| ExplainedVariance    | 0.11523    |
-------------------------------------
[2018-01-21 14:20:17.162411 UTC] Saving snapshot
[2018-01-21 14:20:17.172682 UTC] Starting iteration 691
[2018-01-21 14:20:17.172905 UTC] Start collecting samples
[2018-01-21 14:20:21.689478 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:21.833976 UTC] Performing policy update
[2018-01-21 14:20:21.834696 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:21.962145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:23.470024 UTC] Performing line search
[2018-01-21 14:20:23.664732 UTC] Updating baseline
[2018-01-21 14:20:25.580736 UTC] Computing logging information
-------------------------------------
| Iteration            | 691        |
| ExpectedImprovement  | 0.017184   |
| ActualImprovement    | 0.016515   |
| ImprovementRatio     | 0.96109    |
| MeanKL               | 0.0077352  |
| Entropy              | -0.52884   |
| Perplexity           | 0.58929    |
| AveragePolicyStd     | 0.22352    |
| AveragePolicyStd[0]  | 0.22956    |
| AveragePolicyStd[1]  | 0.25893    |
| AveragePolicyStd[2]  | 0.16951    |
| AveragePolicyStd[3]  | 0.22874    |
| AveragePolicyStd[4]  | 0.20984    |
| AveragePolicyStd[5]  | 0.24457    |
| AverageReturn        | 1347       |
| MinReturn            | 261.26     |
| MaxReturn            | 1457.5     |
| StdReturn            | 215.81     |
| AverageEpisodeLength | 957.7      |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.13     |
| TotalNEpisodes       | 20225      |
| TotalNSamples        | 3.4599e+06 |
| ExplainedVariance    | 0.48174    |
-------------------------------------
[2018-01-21 14:20:26.273289 UTC] Saving snapshot
[2018-01-21 14:20:26.273526 UTC] Starting iteration 692
[2018-01-21 14:20:26.273711 UTC] Start collecting samples
[2018-01-21 14:20:30.560095 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:30.692797 UTC] Performing policy update
[2018-01-21 14:20:30.694040 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:30.829051 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:32.225946 UTC] Performing line search
[2018-01-21 14:20:32.411204 UTC] Updating baseline
[2018-01-21 14:20:34.346942 UTC] Computing logging information
-------------------------------------
| Iteration            | 692        |
| ExpectedImprovement  | 0.019387   |
| ActualImprovement    | 0.017045   |
| ImprovementRatio     | 0.87922    |
| MeanKL               | 0.0081692  |
| Entropy              | -0.5339    |
| Perplexity           | 0.58631    |
| AveragePolicyStd     | 0.22332    |
| AveragePolicyStd[0]  | 0.22913    |
| AveragePolicyStd[1]  | 0.2586     |
| AveragePolicyStd[2]  | 0.16966    |
| AveragePolicyStd[3]  | 0.22821    |
| AveragePolicyStd[4]  | 0.20968    |
| AveragePolicyStd[5]  | 0.24465    |
| AverageReturn        | 1346.4     |
| MinReturn            | 261.26     |
| MaxReturn            | 1457.5     |
| StdReturn            | 215.65     |
| AverageEpisodeLength | 957.7      |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.13     |
| TotalNEpisodes       | 20226      |
| TotalNSamples        | 3.4609e+06 |
| ExplainedVariance    | -0.061323  |
-------------------------------------
[2018-01-21 14:20:35.074862 UTC] Saving snapshot
[2018-01-21 14:20:35.075108 UTC] Starting iteration 693
[2018-01-21 14:20:35.075269 UTC] Start collecting samples
[2018-01-21 14:20:39.698629 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:39.847644 UTC] Performing policy update
[2018-01-21 14:20:39.848257 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:39.982930 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:41.417919 UTC] Performing line search
[2018-01-21 14:20:41.608761 UTC] Updating baseline
[2018-01-21 14:20:43.316102 UTC] Computing logging information
--------------------------------------
| Iteration            | 693         |
| ExpectedImprovement  | 0.016585    |
| ActualImprovement    | 0.01567     |
| ImprovementRatio     | 0.94485     |
| MeanKL               | 0.0074927   |
| Entropy              | -0.53419    |
| Perplexity           | 0.58614     |
| AveragePolicyStd     | 0.22336     |
| AveragePolicyStd[0]  | 0.22913     |
| AveragePolicyStd[1]  | 0.25862     |
| AveragePolicyStd[2]  | 0.16888     |
| AveragePolicyStd[3]  | 0.22872     |
| AveragePolicyStd[4]  | 0.20982     |
| AveragePolicyStd[5]  | 0.24497     |
| AverageReturn        | 1364.7      |
| MinReturn            | 261.26      |
| MaxReturn            | 1457.5      |
| StdReturn            | 173.8       |
| AverageEpisodeLength | 970.6       |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 114.15      |
| TotalNEpisodes       | 20233       |
| TotalNSamples        | 3.4679e+06  |
| ExplainedVariance    | -0.00041931 |
--------------------------------------
[2018-01-21 14:20:44.022968 UTC] Saving snapshot
[2018-01-21 14:20:44.023204 UTC] Starting iteration 694
[2018-01-21 14:20:44.023351 UTC] Start collecting samples
[2018-01-21 14:20:48.621859 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:48.750343 UTC] Performing policy update
[2018-01-21 14:20:48.750958 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:48.873732 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:50.271190 UTC] Performing line search
[2018-01-21 14:20:50.457994 UTC] Updating baseline
[2018-01-21 14:20:52.236119 UTC] Computing logging information
--------------------------------------
| Iteration            | 694         |
| ExpectedImprovement  | 0.016851    |
| ActualImprovement    | 0.015904    |
| ImprovementRatio     | 0.94376     |
| MeanKL               | 0.0073946   |
| Entropy              | -0.52642    |
| Perplexity           | 0.59072     |
| AveragePolicyStd     | 0.2236      |
| AveragePolicyStd[0]  | 0.23004     |
| AveragePolicyStd[1]  | 0.25744     |
| AveragePolicyStd[2]  | 0.16923     |
| AveragePolicyStd[3]  | 0.22905     |
| AveragePolicyStd[4]  | 0.21072     |
| AveragePolicyStd[5]  | 0.2451      |
| AverageReturn        | 1364.1      |
| MinReturn            | 261.26      |
| MaxReturn            | 1467.8      |
| StdReturn            | 173.71      |
| AverageEpisodeLength | 970.6       |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 114.15      |
| TotalNEpisodes       | 20239       |
| TotalNSamples        | 3.4739e+06  |
| ExplainedVariance    | -3.8327e-09 |
--------------------------------------
[2018-01-21 14:20:52.933976 UTC] Saving snapshot
[2018-01-21 14:20:52.934495 UTC] Starting iteration 695
[2018-01-21 14:20:52.934910 UTC] Start collecting samples
[2018-01-21 14:20:57.443064 UTC] Computing input variables for policy optimization
[2018-01-21 14:20:57.578761 UTC] Performing policy update
[2018-01-21 14:20:57.579388 UTC] Computing gradient in Euclidean space
[2018-01-21 14:20:57.696172 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:20:59.168757 UTC] Performing line search
[2018-01-21 14:20:59.364481 UTC] Updating baseline
[2018-01-21 14:21:01.521396 UTC] Computing logging information
-------------------------------------
| Iteration            | 695        |
| ExpectedImprovement  | 0.017887   |
| ActualImprovement    | 0.016441   |
| ImprovementRatio     | 0.91914    |
| MeanKL               | 0.007684   |
| Entropy              | -0.52876   |
| Perplexity           | 0.58934    |
| AveragePolicyStd     | 0.22349    |
| AveragePolicyStd[0]  | 0.22949    |
| AveragePolicyStd[1]  | 0.25724    |
| AveragePolicyStd[2]  | 0.16973    |
| AveragePolicyStd[3]  | 0.22927    |
| AveragePolicyStd[4]  | 0.21003    |
| AveragePolicyStd[5]  | 0.24516    |
| AverageReturn        | 1343.7     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 217.93     |
| AverageEpisodeLength | 957        |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.93     |
| TotalNEpisodes       | 20244      |
| TotalNSamples        | 3.4775e+06 |
| ExplainedVariance    | 0.25944    |
-------------------------------------
[2018-01-21 14:21:02.193950 UTC] Saving snapshot
[2018-01-21 14:21:02.194254 UTC] Starting iteration 696
[2018-01-21 14:21:02.194416 UTC] Start collecting samples
[2018-01-21 14:21:06.600823 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:06.755125 UTC] Performing policy update
[2018-01-21 14:21:06.756279 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:06.877446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:08.308305 UTC] Performing line search
[2018-01-21 14:21:08.499568 UTC] Updating baseline
[2018-01-21 14:21:10.436693 UTC] Computing logging information
-------------------------------------
| Iteration            | 696        |
| ExpectedImprovement  | 0.016316   |
| ActualImprovement    | 0.016018   |
| ImprovementRatio     | 0.98171    |
| MeanKL               | 0.0072006  |
| Entropy              | -0.53636   |
| Perplexity           | 0.58488    |
| AveragePolicyStd     | 0.22322    |
| AveragePolicyStd[0]  | 0.22869    |
| AveragePolicyStd[1]  | 0.2567     |
| AveragePolicyStd[2]  | 0.1696     |
| AveragePolicyStd[3]  | 0.2296     |
| AveragePolicyStd[4]  | 0.20889    |
| AveragePolicyStd[5]  | 0.24585    |
| AverageReturn        | 1343.5     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 217.86     |
| AverageEpisodeLength | 957        |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.93     |
| TotalNEpisodes       | 20250      |
| TotalNSamples        | 3.4835e+06 |
| ExplainedVariance    | 0.050801   |
-------------------------------------
[2018-01-21 14:21:11.110733 UTC] Saving snapshot
[2018-01-21 14:21:11.110973 UTC] Starting iteration 697
[2018-01-21 14:21:11.111121 UTC] Start collecting samples
[2018-01-21 14:21:15.810646 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:15.961334 UTC] Performing policy update
[2018-01-21 14:21:15.962467 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:16.083403 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:17.506050 UTC] Performing line search
[2018-01-21 14:21:17.711606 UTC] Updating baseline
[2018-01-21 14:21:19.650787 UTC] Computing logging information
-------------------------------------
| Iteration            | 697        |
| ExpectedImprovement  | 0.01739    |
| ActualImprovement    | 0.016286   |
| ImprovementRatio     | 0.93651    |
| MeanKL               | 0.0076375  |
| Entropy              | -0.53891   |
| Perplexity           | 0.58338    |
| AveragePolicyStd     | 0.22314    |
| AveragePolicyStd[0]  | 0.22859    |
| AveragePolicyStd[1]  | 0.25642    |
| AveragePolicyStd[2]  | 0.16929    |
| AveragePolicyStd[3]  | 0.22919    |
| AveragePolicyStd[4]  | 0.20891    |
| AveragePolicyStd[5]  | 0.24646    |
| AverageReturn        | 1332.1     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 241.08     |
| AverageEpisodeLength | 949.43     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.23     |
| TotalNEpisodes       | 20255      |
| TotalNSamples        | 3.4877e+06 |
| ExplainedVariance    | 0.093559   |
-------------------------------------
[2018-01-21 14:21:20.401703 UTC] Saving snapshot
[2018-01-21 14:21:20.401997 UTC] Starting iteration 698
[2018-01-21 14:21:20.402221 UTC] Start collecting samples
[2018-01-21 14:21:24.944812 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:25.075359 UTC] Performing policy update
[2018-01-21 14:21:25.075983 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:25.197109 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:26.624198 UTC] Performing line search
[2018-01-21 14:21:26.818854 UTC] Updating baseline
[2018-01-21 14:21:30.292898 UTC] Computing logging information
--------------------------------------
| Iteration            | 698         |
| ExpectedImprovement  | 0.01943     |
| ActualImprovement    | 0.017982    |
| ImprovementRatio     | 0.92551     |
| MeanKL               | 0.0077359   |
| Entropy              | -0.53189    |
| Perplexity           | 0.58749     |
| AveragePolicyStd     | 0.22339     |
| AveragePolicyStd[0]  | 0.22865     |
| AveragePolicyStd[1]  | 0.25619     |
| AveragePolicyStd[2]  | 0.16935     |
| AveragePolicyStd[3]  | 0.22993     |
| AveragePolicyStd[4]  | 0.2096      |
| AveragePolicyStd[5]  | 0.24665     |
| AverageReturn        | 1336.3      |
| MinReturn            | 261.26      |
| MaxReturn            | 1467.8      |
| StdReturn            | 238.76      |
| AverageEpisodeLength | 951.84      |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 161.17      |
| TotalNEpisodes       | 20260       |
| TotalNSamples        | 3.4927e+06  |
| ExplainedVariance    | -4.0208e-05 |
--------------------------------------
[2018-01-21 14:21:31.019073 UTC] Saving snapshot
[2018-01-21 14:21:31.019311 UTC] Starting iteration 699
[2018-01-21 14:21:31.019458 UTC] Start collecting samples
[2018-01-21 14:21:35.446305 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:35.575000 UTC] Performing policy update
[2018-01-21 14:21:35.575818 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:35.696077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:37.136573 UTC] Performing line search
[2018-01-21 14:21:37.330792 UTC] Updating baseline
[2018-01-21 14:21:39.735219 UTC] Computing logging information
-------------------------------------
| Iteration            | 699        |
| ExpectedImprovement  | 0.017642   |
| ActualImprovement    | 0.016292   |
| ImprovementRatio     | 0.92351    |
| MeanKL               | 0.0071586  |
| Entropy              | -0.52826   |
| Perplexity           | 0.58963    |
| AveragePolicyStd     | 0.22354    |
| AveragePolicyStd[0]  | 0.22898    |
| AveragePolicyStd[1]  | 0.25629    |
| AveragePolicyStd[2]  | 0.16927    |
| AveragePolicyStd[3]  | 0.23035    |
| AveragePolicyStd[4]  | 0.20977    |
| AveragePolicyStd[5]  | 0.24656    |
| AverageReturn        | 1334.4     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 238.59     |
| AverageEpisodeLength | 951.06     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.12     |
| TotalNEpisodes       | 20265      |
| TotalNSamples        | 3.4977e+06 |
| ExplainedVariance    | 0.16401    |
-------------------------------------
[2018-01-21 14:21:40.501478 UTC] Saving snapshot
[2018-01-21 14:21:40.501747 UTC] Starting iteration 700
[2018-01-21 14:21:40.501918 UTC] Start collecting samples
[2018-01-21 14:21:45.075658 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:45.213865 UTC] Performing policy update
[2018-01-21 14:21:45.214512 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:45.332539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:46.767172 UTC] Performing line search
[2018-01-21 14:21:46.967759 UTC] Updating baseline
[2018-01-21 14:21:48.623833 UTC] Computing logging information
-------------------------------------
| Iteration            | 700        |
| ExpectedImprovement  | 0.017239   |
| ActualImprovement    | 0.016402   |
| ImprovementRatio     | 0.95142    |
| MeanKL               | 0.0077349  |
| Entropy              | -0.53087   |
| Perplexity           | 0.58809    |
| AveragePolicyStd     | 0.2234     |
| AveragePolicyStd[0]  | 0.22787    |
| AveragePolicyStd[1]  | 0.25614    |
| AveragePolicyStd[2]  | 0.16979    |
| AveragePolicyStd[3]  | 0.22956    |
| AveragePolicyStd[4]  | 0.2102     |
| AveragePolicyStd[5]  | 0.24685    |
| AverageReturn        | 1335.1     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 238.82     |
| AverageEpisodeLength | 950.77     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.06     |
| TotalNEpisodes       | 20270      |
| TotalNSamples        | 3.5026e+06 |
| ExplainedVariance    | 0.059426   |
-------------------------------------
[2018-01-21 14:21:49.390410 UTC] Saving snapshot
[2018-01-21 14:21:49.399584 UTC] Starting iteration 701
[2018-01-21 14:21:49.399807 UTC] Start collecting samples
[2018-01-21 14:21:53.997061 UTC] Computing input variables for policy optimization
[2018-01-21 14:21:54.118261 UTC] Performing policy update
[2018-01-21 14:21:54.119242 UTC] Computing gradient in Euclidean space
[2018-01-21 14:21:54.237235 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:21:55.650004 UTC] Performing line search
[2018-01-21 14:21:55.846973 UTC] Updating baseline
[2018-01-21 14:21:58.673122 UTC] Computing logging information
--------------------------------------
| Iteration            | 701         |
| ExpectedImprovement  | 0.017218    |
| ActualImprovement    | 0.015675    |
| ImprovementRatio     | 0.91037     |
| MeanKL               | 0.0080792   |
| Entropy              | -0.5364     |
| Perplexity           | 0.58485     |
| AveragePolicyStd     | 0.22317     |
| AveragePolicyStd[0]  | 0.22827     |
| AveragePolicyStd[1]  | 0.25532     |
| AveragePolicyStd[2]  | 0.16995     |
| AveragePolicyStd[3]  | 0.22814     |
| AveragePolicyStd[4]  | 0.21025     |
| AveragePolicyStd[5]  | 0.24707     |
| AverageReturn        | 1336.2      |
| MinReturn            | 261.26      |
| MaxReturn            | 1467.8      |
| StdReturn            | 239.25      |
| AverageEpisodeLength | 950.77      |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 161.06      |
| TotalNEpisodes       | 20275       |
| TotalNSamples        | 3.5076e+06  |
| ExplainedVariance    | -7.7654e-06 |
--------------------------------------
[2018-01-21 14:21:59.459299 UTC] Saving snapshot
[2018-01-21 14:21:59.459569 UTC] Starting iteration 702
[2018-01-21 14:21:59.459778 UTC] Start collecting samples
[2018-01-21 14:22:04.028213 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:04.150023 UTC] Performing policy update
[2018-01-21 14:22:04.158728 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:04.289530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:05.731209 UTC] Performing line search
[2018-01-21 14:22:05.930287 UTC] Updating baseline
[2018-01-21 14:22:07.785894 UTC] Computing logging information
------------------------------------
| Iteration            | 702       |
| ExpectedImprovement  | 0.018622  |
| ActualImprovement    | 0.017189  |
| ImprovementRatio     | 0.92305   |
| MeanKL               | 0.007554  |
| Entropy              | -0.54503  |
| Perplexity           | 0.57983   |
| AveragePolicyStd     | 0.22289   |
| AveragePolicyStd[0]  | 0.22775   |
| AveragePolicyStd[1]  | 0.25566   |
| AveragePolicyStd[2]  | 0.16898   |
| AveragePolicyStd[3]  | 0.22841   |
| AveragePolicyStd[4]  | 0.21019   |
| AveragePolicyStd[5]  | 0.24638   |
| AverageReturn        | 1326.7    |
| MinReturn            | 261.26    |
| MaxReturn            | 1467.8    |
| StdReturn            | 253.95    |
| AverageEpisodeLength | 944.61    |
| MinEpisodeLength     | 210       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 170.56    |
| TotalNEpisodes       | 20280     |
| TotalNSamples        | 3.512e+06 |
| ExplainedVariance    | 0.12512   |
------------------------------------
[2018-01-21 14:22:08.488111 UTC] Saving snapshot
[2018-01-21 14:22:08.488359 UTC] Starting iteration 703
[2018-01-21 14:22:08.488508 UTC] Start collecting samples
[2018-01-21 14:22:12.746599 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:12.866393 UTC] Performing policy update
[2018-01-21 14:22:12.867703 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:12.993626 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:14.450584 UTC] Performing line search
[2018-01-21 14:22:14.641748 UTC] Updating baseline
[2018-01-21 14:22:16.860845 UTC] Computing logging information
-------------------------------------
| Iteration            | 703        |
| ExpectedImprovement  | 0.019075   |
| ActualImprovement    | 0.01768    |
| ImprovementRatio     | 0.92683    |
| MeanKL               | 0.0075872  |
| Entropy              | -0.55039   |
| Perplexity           | 0.57672    |
| AveragePolicyStd     | 0.22273    |
| AveragePolicyStd[0]  | 0.22731    |
| AveragePolicyStd[1]  | 0.25578    |
| AveragePolicyStd[2]  | 0.16841    |
| AveragePolicyStd[3]  | 0.22795    |
| AveragePolicyStd[4]  | 0.2103     |
| AveragePolicyStd[5]  | 0.24661    |
| AverageReturn        | 1322.2     |
| MinReturn            | 261.26     |
| MaxReturn            | 1467.8     |
| StdReturn            | 254.91     |
| AverageEpisodeLength | 941.99     |
| MinEpisodeLength     | 210        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.69     |
| TotalNEpisodes       | 20286      |
| TotalNSamples        | 3.5178e+06 |
| ExplainedVariance    | 0.10141    |
-------------------------------------
[2018-01-21 14:22:17.565105 UTC] Saving snapshot
[2018-01-21 14:22:17.565334 UTC] Starting iteration 704
[2018-01-21 14:22:17.565454 UTC] Start collecting samples
[2018-01-21 14:22:21.927437 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:22.065539 UTC] Performing policy update
[2018-01-21 14:22:22.066047 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:22.191465 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:23.659290 UTC] Performing line search
[2018-01-21 14:22:23.862506 UTC] Updating baseline
[2018-01-21 14:22:27.928730 UTC] Computing logging information
-------------------------------------
| Iteration            | 704        |
| ExpectedImprovement  | 0.017819   |
| ActualImprovement    | 0.015897   |
| ImprovementRatio     | 0.89217    |
| MeanKL               | 0.0074134  |
| Entropy              | -0.55456   |
| Perplexity           | 0.57432    |
| AveragePolicyStd     | 0.22257    |
| AveragePolicyStd[0]  | 0.22671    |
| AveragePolicyStd[1]  | 0.25613    |
| AveragePolicyStd[2]  | 0.16851    |
| AveragePolicyStd[3]  | 0.22744    |
| AveragePolicyStd[4]  | 0.21018    |
| AveragePolicyStd[5]  | 0.24643    |
| AverageReturn        | 1334.7     |
| MinReturn            | 299.66     |
| MaxReturn            | 1467.8     |
| StdReturn            | 232.01     |
| AverageEpisodeLength | 949.89     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.22     |
| TotalNEpisodes       | 20290      |
| TotalNSamples        | 3.5218e+06 |
| ExplainedVariance    | -0.022051  |
-------------------------------------
[2018-01-21 14:22:28.629816 UTC] Saving snapshot
[2018-01-21 14:22:28.630046 UTC] Starting iteration 705
[2018-01-21 14:22:28.630223 UTC] Start collecting samples
[2018-01-21 14:22:33.204166 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:33.336523 UTC] Performing policy update
[2018-01-21 14:22:33.337163 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:33.454294 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:34.867942 UTC] Performing line search
[2018-01-21 14:22:35.058752 UTC] Updating baseline
[2018-01-21 14:22:38.177063 UTC] Computing logging information
--------------------------------------
| Iteration            | 705         |
| ExpectedImprovement  | 0.017145    |
| ActualImprovement    | 0.015872    |
| ImprovementRatio     | 0.92573     |
| MeanKL               | 0.0090564   |
| Entropy              | -0.56552    |
| Perplexity           | 0.56807     |
| AveragePolicyStd     | 0.22216     |
| AveragePolicyStd[0]  | 0.22622     |
| AveragePolicyStd[1]  | 0.25536     |
| AveragePolicyStd[2]  | 0.16838     |
| AveragePolicyStd[3]  | 0.22684     |
| AveragePolicyStd[4]  | 0.20965     |
| AveragePolicyStd[5]  | 0.24649     |
| AverageReturn        | 1340.2      |
| MinReturn            | 299.66      |
| MaxReturn            | 1467.8      |
| StdReturn            | 229.87      |
| AverageEpisodeLength | 952.58      |
| MinEpisodeLength     | 243         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 153.72      |
| TotalNEpisodes       | 20295       |
| TotalNSamples        | 3.5268e+06  |
| ExplainedVariance    | -2.2405e-06 |
--------------------------------------
[2018-01-21 14:22:38.853015 UTC] Saving snapshot
[2018-01-21 14:22:38.853260 UTC] Starting iteration 706
[2018-01-21 14:22:38.853471 UTC] Start collecting samples
[2018-01-21 14:22:43.459416 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:43.609831 UTC] Performing policy update
[2018-01-21 14:22:43.610912 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:43.737021 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:45.176464 UTC] Performing line search
[2018-01-21 14:22:45.367334 UTC] Updating baseline
[2018-01-21 14:22:47.868179 UTC] Computing logging information
-------------------------------------
| Iteration            | 706        |
| ExpectedImprovement  | 0.01915    |
| ActualImprovement    | 0.018916   |
| ImprovementRatio     | 0.98779    |
| MeanKL               | 0.007367   |
| Entropy              | -0.56746   |
| Perplexity           | 0.56696    |
| AveragePolicyStd     | 0.2221     |
| AveragePolicyStd[0]  | 0.22626    |
| AveragePolicyStd[1]  | 0.25557    |
| AveragePolicyStd[2]  | 0.16809    |
| AveragePolicyStd[3]  | 0.22669    |
| AveragePolicyStd[4]  | 0.20966    |
| AveragePolicyStd[5]  | 0.24632    |
| AverageReturn        | 1321.6     |
| MinReturn            | 299.66     |
| MaxReturn            | 1467.8     |
| StdReturn            | 265.75     |
| AverageEpisodeLength | 939.15     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.54     |
| TotalNEpisodes       | 20304      |
| TotalNSamples        | 3.5344e+06 |
| ExplainedVariance    | 0.095937   |
-------------------------------------
[2018-01-21 14:22:48.569484 UTC] Saving snapshot
[2018-01-21 14:22:48.569709 UTC] Starting iteration 707
[2018-01-21 14:22:48.569890 UTC] Start collecting samples
[2018-01-21 14:22:53.184696 UTC] Computing input variables for policy optimization
[2018-01-21 14:22:53.317386 UTC] Performing policy update
[2018-01-21 14:22:53.318453 UTC] Computing gradient in Euclidean space
[2018-01-21 14:22:53.438357 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:22:54.808055 UTC] Performing line search
[2018-01-21 14:22:55.002137 UTC] Updating baseline
[2018-01-21 14:22:56.931586 UTC] Computing logging information
-------------------------------------
| Iteration            | 707        |
| ExpectedImprovement  | 0.017767   |
| ActualImprovement    | 0.017201   |
| ImprovementRatio     | 0.96818    |
| MeanKL               | 0.0070394  |
| Entropy              | -0.55946   |
| Perplexity           | 0.57152    |
| AveragePolicyStd     | 0.22237    |
| AveragePolicyStd[0]  | 0.22645    |
| AveragePolicyStd[1]  | 0.25565    |
| AveragePolicyStd[2]  | 0.16857    |
| AveragePolicyStd[3]  | 0.22734    |
| AveragePolicyStd[4]  | 0.20991    |
| AveragePolicyStd[5]  | 0.24632    |
| AverageReturn        | 1314.9     |
| MinReturn            | 299.66     |
| MaxReturn            | 1467.8     |
| StdReturn            | 274.56     |
| AverageEpisodeLength | 933.94     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.2      |
| TotalNEpisodes       | 20309      |
| TotalNSamples        | 3.5388e+06 |
| ExplainedVariance    | 0.055732   |
-------------------------------------
[2018-01-21 14:22:57.638576 UTC] Saving snapshot
[2018-01-21 14:22:57.638819 UTC] Starting iteration 708
[2018-01-21 14:22:57.638975 UTC] Start collecting samples
[2018-01-21 14:23:02.241850 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:02.381385 UTC] Performing policy update
[2018-01-21 14:23:02.382003 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:02.498845 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:03.937962 UTC] Performing line search
[2018-01-21 14:23:04.143291 UTC] Updating baseline
[2018-01-21 14:23:06.137530 UTC] Computing logging information
-------------------------------------
| Iteration            | 708        |
| ExpectedImprovement  | 0.01505    |
| ActualImprovement    | 0.014224   |
| ImprovementRatio     | 0.94515    |
| MeanKL               | 0.0077948  |
| Entropy              | -0.56673   |
| Perplexity           | 0.56738    |
| AveragePolicyStd     | 0.22213    |
| AveragePolicyStd[0]  | 0.22579    |
| AveragePolicyStd[1]  | 0.25508    |
| AveragePolicyStd[2]  | 0.16802    |
| AveragePolicyStd[3]  | 0.22768    |
| AveragePolicyStd[4]  | 0.20968    |
| AveragePolicyStd[5]  | 0.24651    |
| AverageReturn        | 1315.4     |
| MinReturn            | 299.66     |
| MaxReturn            | 1467.8     |
| StdReturn            | 274.75     |
| AverageEpisodeLength | 933.94     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.2      |
| TotalNEpisodes       | 20312      |
| TotalNSamples        | 3.5418e+06 |
| ExplainedVariance    | -0.0015341 |
-------------------------------------
[2018-01-21 14:23:06.876220 UTC] Saving snapshot
[2018-01-21 14:23:06.876454 UTC] Starting iteration 709
[2018-01-21 14:23:06.876602 UTC] Start collecting samples
[2018-01-21 14:23:11.607315 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:11.774108 UTC] Performing policy update
[2018-01-21 14:23:11.774740 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:11.892519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:13.280585 UTC] Performing line search
[2018-01-21 14:23:13.470854 UTC] Updating baseline
[2018-01-21 14:23:15.596796 UTC] Computing logging information
-------------------------------------
| Iteration            | 709        |
| ExpectedImprovement  | 0.018309   |
| ActualImprovement    | 0.017646   |
| ImprovementRatio     | 0.96378    |
| MeanKL               | 0.0081165  |
| Entropy              | -0.56984   |
| Perplexity           | 0.56562    |
| AveragePolicyStd     | 0.22201    |
| AveragePolicyStd[0]  | 0.22561    |
| AveragePolicyStd[1]  | 0.25472    |
| AveragePolicyStd[2]  | 0.16791    |
| AveragePolicyStd[3]  | 0.22737    |
| AveragePolicyStd[4]  | 0.20974    |
| AveragePolicyStd[5]  | 0.24671    |
| AverageReturn        | 1325.6     |
| MinReturn            | 299.66     |
| MaxReturn            | 1467.8     |
| StdReturn            | 261.21     |
| AverageEpisodeLength | 939.53     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.93     |
| TotalNEpisodes       | 20320      |
| TotalNSamples        | 3.5494e+06 |
| ExplainedVariance    | 0.13445    |
-------------------------------------
[2018-01-21 14:23:16.273660 UTC] Saving snapshot
[2018-01-21 14:23:16.273894 UTC] Starting iteration 710
[2018-01-21 14:23:16.274039 UTC] Start collecting samples
[2018-01-21 14:23:20.832437 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:20.979596 UTC] Performing policy update
[2018-01-21 14:23:20.981821 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:21.100327 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:22.512446 UTC] Performing line search
[2018-01-21 14:23:22.695446 UTC] Updating baseline
[2018-01-21 14:23:24.598769 UTC] Computing logging information
-------------------------------------
| Iteration            | 710        |
| ExpectedImprovement  | 0.016923   |
| ActualImprovement    | 0.015935   |
| ImprovementRatio     | 0.94162    |
| MeanKL               | 0.0072882  |
| Entropy              | -0.57137   |
| Perplexity           | 0.56475    |
| AveragePolicyStd     | 0.22196    |
| AveragePolicyStd[0]  | 0.22575    |
| AveragePolicyStd[1]  | 0.25454    |
| AveragePolicyStd[2]  | 0.16776    |
| AveragePolicyStd[3]  | 0.22721    |
| AveragePolicyStd[4]  | 0.20962    |
| AveragePolicyStd[5]  | 0.2469     |
| AverageReturn        | 1333.4     |
| MinReturn            | 299.66     |
| MaxReturn            | 1470.5     |
| StdReturn            | 259.47     |
| AverageEpisodeLength | 943.25     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.15     |
| TotalNEpisodes       | 20323      |
| TotalNSamples        | 3.5524e+06 |
| ExplainedVariance    | 0.039613   |
-------------------------------------
[2018-01-21 14:23:25.288633 UTC] Saving snapshot
[2018-01-21 14:23:25.294778 UTC] Starting iteration 711
[2018-01-21 14:23:25.294971 UTC] Start collecting samples
[2018-01-21 14:23:29.903132 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:30.033506 UTC] Performing policy update
[2018-01-21 14:23:30.034139 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:30.150603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:31.590963 UTC] Performing line search
[2018-01-21 14:23:31.784690 UTC] Updating baseline
[2018-01-21 14:23:34.046637 UTC] Computing logging information
-------------------------------------
| Iteration            | 711        |
| ExpectedImprovement  | 0.018692   |
| ActualImprovement    | 0.017291   |
| ImprovementRatio     | 0.92507    |
| MeanKL               | 0.0070849  |
| Entropy              | -0.57204   |
| Perplexity           | 0.56437    |
| AveragePolicyStd     | 0.22196    |
| AveragePolicyStd[0]  | 0.22584    |
| AveragePolicyStd[1]  | 0.25452    |
| AveragePolicyStd[2]  | 0.16746    |
| AveragePolicyStd[3]  | 0.22729    |
| AveragePolicyStd[4]  | 0.20928    |
| AveragePolicyStd[5]  | 0.2474     |
| AverageReturn        | 1337.9     |
| MinReturn            | 299.66     |
| MaxReturn            | 1470.5     |
| StdReturn            | 258.88     |
| AverageEpisodeLength | 945.19     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.7      |
| TotalNEpisodes       | 20328      |
| TotalNSamples        | 3.5574e+06 |
| ExplainedVariance    | -0.002052  |
-------------------------------------
[2018-01-21 14:23:34.706628 UTC] Saving snapshot
[2018-01-21 14:23:34.706864 UTC] Starting iteration 712
[2018-01-21 14:23:34.707016 UTC] Start collecting samples
[2018-01-21 14:23:39.138132 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:39.258768 UTC] Performing policy update
[2018-01-21 14:23:39.259709 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:39.378750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:40.756016 UTC] Performing line search
[2018-01-21 14:23:40.960054 UTC] Updating baseline
[2018-01-21 14:23:44.167653 UTC] Computing logging information
-------------------------------------
| Iteration            | 712        |
| ExpectedImprovement  | 0.015921   |
| ActualImprovement    | 0.01517    |
| ImprovementRatio     | 0.95284    |
| MeanKL               | 0.0072863  |
| Entropy              | -0.56657   |
| Perplexity           | 0.56747    |
| AveragePolicyStd     | 0.22222    |
| AveragePolicyStd[0]  | 0.22669    |
| AveragePolicyStd[1]  | 0.25533    |
| AveragePolicyStd[2]  | 0.16716    |
| AveragePolicyStd[3]  | 0.22732    |
| AveragePolicyStd[4]  | 0.20894    |
| AveragePolicyStd[5]  | 0.24786    |
| AverageReturn        | 1338.8     |
| MinReturn            | 299.66     |
| MaxReturn            | 1470.5     |
| StdReturn            | 259.14     |
| AverageEpisodeLength | 945.19     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.7      |
| TotalNEpisodes       | 20333      |
| TotalNSamples        | 3.5624e+06 |
| ExplainedVariance    | -0.0071468 |
-------------------------------------
[2018-01-21 14:23:44.840290 UTC] Saving snapshot
[2018-01-21 14:23:44.840607 UTC] Starting iteration 713
[2018-01-21 14:23:44.840878 UTC] Start collecting samples
[2018-01-21 14:23:49.318787 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:49.462357 UTC] Performing policy update
[2018-01-21 14:23:49.462965 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:49.581582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:50.979063 UTC] Performing line search
[2018-01-21 14:23:51.170059 UTC] Updating baseline
[2018-01-21 14:23:52.941619 UTC] Computing logging information
-------------------------------------
| Iteration            | 713        |
| ExpectedImprovement  | 0.016662   |
| ActualImprovement    | 0.015692   |
| ImprovementRatio     | 0.94175    |
| MeanKL               | 0.0073835  |
| Entropy              | -0.56443   |
| Perplexity           | 0.56868    |
| AveragePolicyStd     | 0.22231    |
| AveragePolicyStd[0]  | 0.22685    |
| AveragePolicyStd[1]  | 0.25591    |
| AveragePolicyStd[2]  | 0.16729    |
| AveragePolicyStd[3]  | 0.22704    |
| AveragePolicyStd[4]  | 0.20857    |
| AveragePolicyStd[5]  | 0.2482     |
| AverageReturn        | 1337.4     |
| MinReturn            | 299.66     |
| MaxReturn            | 1474.3     |
| StdReturn            | 260.46     |
| AverageEpisodeLength | 943.2      |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.19     |
| TotalNEpisodes       | 20339      |
| TotalNSamples        | 3.5682e+06 |
| ExplainedVariance    | 0.081948   |
-------------------------------------
[2018-01-21 14:23:53.645655 UTC] Saving snapshot
[2018-01-21 14:23:53.645930 UTC] Starting iteration 714
[2018-01-21 14:23:53.646107 UTC] Start collecting samples
[2018-01-21 14:23:58.140517 UTC] Computing input variables for policy optimization
[2018-01-21 14:23:58.271844 UTC] Performing policy update
[2018-01-21 14:23:58.272493 UTC] Computing gradient in Euclidean space
[2018-01-21 14:23:58.394570 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:23:59.852754 UTC] Performing line search
[2018-01-21 14:24:00.047673 UTC] Updating baseline
[2018-01-21 14:24:02.305669 UTC] Computing logging information
-------------------------------------
| Iteration            | 714        |
| ExpectedImprovement  | 0.018769   |
| ActualImprovement    | 0.017424   |
| ImprovementRatio     | 0.92832    |
| MeanKL               | 0.0070508  |
| Entropy              | -0.56517   |
| Perplexity           | 0.56826    |
| AveragePolicyStd     | 0.2223     |
| AveragePolicyStd[0]  | 0.22729    |
| AveragePolicyStd[1]  | 0.25566    |
| AveragePolicyStd[2]  | 0.16688    |
| AveragePolicyStd[3]  | 0.22745    |
| AveragePolicyStd[4]  | 0.20851    |
| AveragePolicyStd[5]  | 0.24799    |
| AverageReturn        | 1347.4     |
| MinReturn            | 299.66     |
| MaxReturn            | 1474.3     |
| StdReturn            | 245.19     |
| AverageEpisodeLength | 949.89     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.11     |
| TotalNEpisodes       | 20343      |
| TotalNSamples        | 3.5722e+06 |
| ExplainedVariance    | -0.024878  |
-------------------------------------
[2018-01-21 14:24:03.000813 UTC] Saving snapshot
[2018-01-21 14:24:03.001116 UTC] Starting iteration 715
[2018-01-21 14:24:03.001331 UTC] Start collecting samples
[2018-01-21 14:24:07.395593 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:07.521210 UTC] Performing policy update
[2018-01-21 14:24:07.521814 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:07.652366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:09.083910 UTC] Performing line search
[2018-01-21 14:24:09.280247 UTC] Updating baseline
[2018-01-21 14:24:11.064102 UTC] Computing logging information
-------------------------------------
| Iteration            | 715        |
| ExpectedImprovement  | 0.017844   |
| ActualImprovement    | 0.017064   |
| ImprovementRatio     | 0.95624    |
| MeanKL               | 0.0078502  |
| Entropy              | -0.57147   |
| Perplexity           | 0.5647     |
| AveragePolicyStd     | 0.22207    |
| AveragePolicyStd[0]  | 0.22693    |
| AveragePolicyStd[1]  | 0.25552    |
| AveragePolicyStd[2]  | 0.16675    |
| AveragePolicyStd[3]  | 0.22699    |
| AveragePolicyStd[4]  | 0.20837    |
| AveragePolicyStd[5]  | 0.24783    |
| AverageReturn        | 1352.9     |
| MinReturn            | 299.66     |
| MaxReturn            | 1474.3     |
| StdReturn            | 232.42     |
| AverageEpisodeLength | 952.33     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.19     |
| TotalNEpisodes       | 20350      |
| TotalNSamples        | 3.5787e+06 |
| ExplainedVariance    | 0.086465   |
-------------------------------------
[2018-01-21 14:24:11.778171 UTC] Saving snapshot
[2018-01-21 14:24:11.778604 UTC] Starting iteration 716
[2018-01-21 14:24:11.778892 UTC] Start collecting samples
[2018-01-21 14:24:16.298552 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:16.450869 UTC] Performing policy update
[2018-01-21 14:24:16.451632 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:16.580842 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:17.967560 UTC] Performing line search
[2018-01-21 14:24:18.159993 UTC] Updating baseline
[2018-01-21 14:24:19.960419 UTC] Computing logging information
-------------------------------------
| Iteration            | 716        |
| ExpectedImprovement  | 0.018278   |
| ActualImprovement    | 0.017539   |
| ImprovementRatio     | 0.95958    |
| MeanKL               | 0.0073632  |
| Entropy              | -0.57322   |
| Perplexity           | 0.56371    |
| AveragePolicyStd     | 0.22199    |
| AveragePolicyStd[0]  | 0.227      |
| AveragePolicyStd[1]  | 0.25501    |
| AveragePolicyStd[2]  | 0.1667     |
| AveragePolicyStd[3]  | 0.22704    |
| AveragePolicyStd[4]  | 0.20852    |
| AveragePolicyStd[5]  | 0.24766    |
| AverageReturn        | 1365.7     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 207.54     |
| AverageEpisodeLength | 959.9      |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.03     |
| TotalNEpisodes       | 20355      |
| TotalNSamples        | 3.5837e+06 |
| ExplainedVariance    | 0.019861   |
-------------------------------------
[2018-01-21 14:24:20.659780 UTC] Saving snapshot
[2018-01-21 14:24:20.660051 UTC] Starting iteration 717
[2018-01-21 14:24:20.660214 UTC] Start collecting samples
[2018-01-21 14:24:25.284440 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:25.418936 UTC] Performing policy update
[2018-01-21 14:24:25.419983 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:25.548113 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:26.955294 UTC] Performing line search
[2018-01-21 14:24:27.175832 UTC] Updating baseline
[2018-01-21 14:24:29.204766 UTC] Computing logging information
-------------------------------------
| Iteration            | 717        |
| ExpectedImprovement  | 0.015616   |
| ActualImprovement    | 0.014636   |
| ImprovementRatio     | 0.93728    |
| MeanKL               | 0.0082156  |
| Entropy              | -0.5709    |
| Perplexity           | 0.56501    |
| AveragePolicyStd     | 0.22207    |
| AveragePolicyStd[0]  | 0.22728    |
| AveragePolicyStd[1]  | 0.25506    |
| AveragePolicyStd[2]  | 0.16676    |
| AveragePolicyStd[3]  | 0.22725    |
| AveragePolicyStd[4]  | 0.20852    |
| AveragePolicyStd[5]  | 0.24756    |
| AverageReturn        | 1366.4     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 207.73     |
| AverageEpisodeLength | 959.9      |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.03     |
| TotalNEpisodes       | 20358      |
| TotalNSamples        | 3.5867e+06 |
| ExplainedVariance    | -0.0061547 |
-------------------------------------
[2018-01-21 14:24:29.910356 UTC] Saving snapshot
[2018-01-21 14:24:29.910670 UTC] Starting iteration 718
[2018-01-21 14:24:29.910826 UTC] Start collecting samples
[2018-01-21 14:24:34.433299 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:34.568259 UTC] Performing policy update
[2018-01-21 14:24:34.569739 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:34.695539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:36.097302 UTC] Performing line search
[2018-01-21 14:24:36.284012 UTC] Updating baseline
[2018-01-21 14:24:38.320395 UTC] Computing logging information
-------------------------------------
| Iteration            | 718        |
| ExpectedImprovement  | 0.017085   |
| ActualImprovement    | 0.015652   |
| ImprovementRatio     | 0.91612    |
| MeanKL               | 0.0076219  |
| Entropy              | -0.56538   |
| Perplexity           | 0.56815    |
| AveragePolicyStd     | 0.22227    |
| AveragePolicyStd[0]  | 0.22735    |
| AveragePolicyStd[1]  | 0.25469    |
| AveragePolicyStd[2]  | 0.16682    |
| AveragePolicyStd[3]  | 0.22754    |
| AveragePolicyStd[4]  | 0.20914    |
| AveragePolicyStd[5]  | 0.24807    |
| AverageReturn        | 1369.5     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 207.91     |
| AverageEpisodeLength | 960.68     |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.03     |
| TotalNEpisodes       | 20363      |
| TotalNSamples        | 3.5917e+06 |
| ExplainedVariance    | 0.0025438  |
-------------------------------------
[2018-01-21 14:24:39.101153 UTC] Saving snapshot
[2018-01-21 14:24:39.101406 UTC] Starting iteration 719
[2018-01-21 14:24:39.101552 UTC] Start collecting samples
[2018-01-21 14:24:43.543395 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:43.698193 UTC] Performing policy update
[2018-01-21 14:24:43.698867 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:43.814404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:45.270518 UTC] Performing line search
[2018-01-21 14:24:45.472147 UTC] Updating baseline
[2018-01-21 14:24:47.407796 UTC] Computing logging information
-------------------------------------
| Iteration            | 719        |
| ExpectedImprovement  | 0.016855   |
| ActualImprovement    | 0.015923   |
| ImprovementRatio     | 0.94467    |
| MeanKL               | 0.007183   |
| Entropy              | -0.57636   |
| Perplexity           | 0.56194    |
| AveragePolicyStd     | 0.22186    |
| AveragePolicyStd[0]  | 0.22684    |
| AveragePolicyStd[1]  | 0.25386    |
| AveragePolicyStd[2]  | 0.1665     |
| AveragePolicyStd[3]  | 0.22693    |
| AveragePolicyStd[4]  | 0.20895    |
| AveragePolicyStd[5]  | 0.24808    |
| AverageReturn        | 1371.3     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 208.3      |
| AverageEpisodeLength | 960.97     |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.08     |
| TotalNEpisodes       | 20370      |
| TotalNSamples        | 3.5987e+06 |
| ExplainedVariance    | -0.0037015 |
-------------------------------------
[2018-01-21 14:24:48.119092 UTC] Saving snapshot
[2018-01-21 14:24:48.119337 UTC] Starting iteration 720
[2018-01-21 14:24:48.119492 UTC] Start collecting samples
[2018-01-21 14:24:52.451302 UTC] Computing input variables for policy optimization
[2018-01-21 14:24:52.575733 UTC] Performing policy update
[2018-01-21 14:24:52.576730 UTC] Computing gradient in Euclidean space
[2018-01-21 14:24:52.702770 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:24:54.158800 UTC] Performing line search
[2018-01-21 14:24:54.345518 UTC] Updating baseline
[2018-01-21 14:24:57.295370 UTC] Computing logging information
-------------------------------------
| Iteration            | 720        |
| ExpectedImprovement  | 0.019539   |
| ActualImprovement    | 0.018469   |
| ImprovementRatio     | 0.94526    |
| MeanKL               | 0.0072277  |
| Entropy              | -0.57592   |
| Perplexity           | 0.56218    |
| AveragePolicyStd     | 0.22185    |
| AveragePolicyStd[0]  | 0.22621    |
| AveragePolicyStd[1]  | 0.25398    |
| AveragePolicyStd[2]  | 0.16704    |
| AveragePolicyStd[3]  | 0.22684    |
| AveragePolicyStd[4]  | 0.2088     |
| AveragePolicyStd[5]  | 0.24824    |
| AverageReturn        | 1363.7     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 217.82     |
| AverageEpisodeLength | 956.14     |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.86     |
| TotalNEpisodes       | 20375      |
| TotalNSamples        | 3.6032e+06 |
| ExplainedVariance    | 0.072526   |
-------------------------------------
[2018-01-21 14:24:57.984449 UTC] Saving snapshot
[2018-01-21 14:24:57.992305 UTC] Starting iteration 721
[2018-01-21 14:24:57.992542 UTC] Start collecting samples
[2018-01-21 14:25:02.416410 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:02.548557 UTC] Performing policy update
[2018-01-21 14:25:02.549195 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:02.682209 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:04.121666 UTC] Performing line search
[2018-01-21 14:25:04.312576 UTC] Updating baseline
[2018-01-21 14:25:06.178833 UTC] Computing logging information
-------------------------------------
| Iteration            | 721        |
| ExpectedImprovement  | 0.019694   |
| ActualImprovement    | 0.018479   |
| ImprovementRatio     | 0.93831    |
| MeanKL               | 0.0073911  |
| Entropy              | -0.57896   |
| Perplexity           | 0.56048    |
| AveragePolicyStd     | 0.22183    |
| AveragePolicyStd[0]  | 0.22626    |
| AveragePolicyStd[1]  | 0.2548     |
| AveragePolicyStd[2]  | 0.16578    |
| AveragePolicyStd[3]  | 0.22772    |
| AveragePolicyStd[4]  | 0.20833    |
| AveragePolicyStd[5]  | 0.24811    |
| AverageReturn        | 1374.1     |
| MinReturn            | 332.74     |
| MaxReturn            | 1474.4     |
| StdReturn            | 198.93     |
| AverageEpisodeLength | 962.3      |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.1      |
| TotalNEpisodes       | 20378      |
| TotalNSamples        | 3.6062e+06 |
| ExplainedVariance    | -0.016152  |
-------------------------------------
[2018-01-21 14:25:06.880574 UTC] Saving snapshot
[2018-01-21 14:25:06.880869 UTC] Starting iteration 722
[2018-01-21 14:25:06.881065 UTC] Start collecting samples
[2018-01-21 14:25:11.445798 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:11.581235 UTC] Performing policy update
[2018-01-21 14:25:11.581849 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:11.703805 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:13.126729 UTC] Performing line search
[2018-01-21 14:25:13.333733 UTC] Updating baseline
[2018-01-21 14:25:15.990009 UTC] Computing logging information
-------------------------------------
| Iteration            | 722        |
| ExpectedImprovement  | 0.016725   |
| ActualImprovement    | 0.016189   |
| ImprovementRatio     | 0.96795    |
| MeanKL               | 0.0073875  |
| Entropy              | -0.58009   |
| Perplexity           | 0.55985    |
| AveragePolicyStd     | 0.22178    |
| AveragePolicyStd[0]  | 0.22558    |
| AveragePolicyStd[1]  | 0.255      |
| AveragePolicyStd[2]  | 0.16577    |
| AveragePolicyStd[3]  | 0.22803    |
| AveragePolicyStd[4]  | 0.20871    |
| AveragePolicyStd[5]  | 0.24762    |
| AverageReturn        | 1379.3     |
| MinReturn            | 332.74     |
| MaxReturn            | 1484       |
| StdReturn            | 196.86     |
| AverageEpisodeLength | 964.92     |
| MinEpisodeLength     | 262        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.24     |
| TotalNEpisodes       | 20386      |
| TotalNSamples        | 3.6142e+06 |
| ExplainedVariance    | 0.026689   |
-------------------------------------
[2018-01-21 14:25:16.671685 UTC] Saving snapshot
[2018-01-21 14:25:16.671938 UTC] Starting iteration 723
[2018-01-21 14:25:16.672122 UTC] Start collecting samples
[2018-01-21 14:25:21.153114 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:21.290489 UTC] Performing policy update
[2018-01-21 14:25:21.291391 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:21.408423 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:22.798783 UTC] Performing line search
[2018-01-21 14:25:22.992951 UTC] Updating baseline
[2018-01-21 14:25:25.226520 UTC] Computing logging information
-------------------------------------
| Iteration            | 723        |
| ExpectedImprovement  | 0.020013   |
| ActualImprovement    | 0.019387   |
| ImprovementRatio     | 0.96868    |
| MeanKL               | 0.0070405  |
| Entropy              | -0.58812   |
| Perplexity           | 0.55537    |
| AveragePolicyStd     | 0.22152    |
| AveragePolicyStd[0]  | 0.22646    |
| AveragePolicyStd[1]  | 0.25459    |
| AveragePolicyStd[2]  | 0.16532    |
| AveragePolicyStd[3]  | 0.22791    |
| AveragePolicyStd[4]  | 0.20713    |
| AveragePolicyStd[5]  | 0.24774    |
| AverageReturn        | 1366.9     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 232.78     |
| AverageEpisodeLength | 956.15     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.48     |
| TotalNEpisodes       | 20391      |
| TotalNSamples        | 3.6184e+06 |
| ExplainedVariance    | 0.069044   |
-------------------------------------
[2018-01-21 14:25:26.008194 UTC] Saving snapshot
[2018-01-21 14:25:26.008432 UTC] Starting iteration 724
[2018-01-21 14:25:26.008586 UTC] Start collecting samples
[2018-01-21 14:25:30.545077 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:30.719346 UTC] Performing policy update
[2018-01-21 14:25:30.719974 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:30.839659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:32.271540 UTC] Performing line search
[2018-01-21 14:25:32.456615 UTC] Updating baseline
[2018-01-21 14:25:35.451879 UTC] Computing logging information
-------------------------------------
| Iteration            | 724        |
| ExpectedImprovement  | 0.018392   |
| ActualImprovement    | 0.016937   |
| ImprovementRatio     | 0.92088    |
| MeanKL               | 0.0075219  |
| Entropy              | -0.59131   |
| Perplexity           | 0.5536     |
| AveragePolicyStd     | 0.22142    |
| AveragePolicyStd[0]  | 0.22681    |
| AveragePolicyStd[1]  | 0.25343    |
| AveragePolicyStd[2]  | 0.16452    |
| AveragePolicyStd[3]  | 0.22828    |
| AveragePolicyStd[4]  | 0.20777    |
| AveragePolicyStd[5]  | 0.24774    |
| AverageReturn        | 1367.6     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 233.01     |
| AverageEpisodeLength | 956.15     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.48     |
| TotalNEpisodes       | 20395      |
| TotalNSamples        | 3.6224e+06 |
| ExplainedVariance    | 0.014222   |
-------------------------------------
[2018-01-21 14:25:36.180953 UTC] Saving snapshot
[2018-01-21 14:25:36.181208 UTC] Starting iteration 725
[2018-01-21 14:25:36.181386 UTC] Start collecting samples
[2018-01-21 14:25:40.771429 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:40.935128 UTC] Performing policy update
[2018-01-21 14:25:40.935945 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:41.076609 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:42.503893 UTC] Performing line search
[2018-01-21 14:25:42.690317 UTC] Updating baseline
[2018-01-21 14:25:44.742359 UTC] Computing logging information
-------------------------------------
| Iteration            | 725        |
| ExpectedImprovement  | 0.015651   |
| ActualImprovement    | 0.014608   |
| ImprovementRatio     | 0.93336    |
| MeanKL               | 0.0076138  |
| Entropy              | -0.59606   |
| Perplexity           | 0.55098    |
| AveragePolicyStd     | 0.22123    |
| AveragePolicyStd[0]  | 0.22733    |
| AveragePolicyStd[1]  | 0.25219    |
| AveragePolicyStd[2]  | 0.16466    |
| AveragePolicyStd[3]  | 0.22807    |
| AveragePolicyStd[4]  | 0.20718    |
| AveragePolicyStd[5]  | 0.24791    |
| AverageReturn        | 1379.2     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 214.57     |
| AverageEpisodeLength | 962.69     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.02     |
| TotalNEpisodes       | 20401      |
| TotalNSamples        | 3.6284e+06 |
| ExplainedVariance    | 0.012484   |
-------------------------------------
[2018-01-21 14:25:45.462307 UTC] Saving snapshot
[2018-01-21 14:25:45.462625 UTC] Starting iteration 726
[2018-01-21 14:25:45.462852 UTC] Start collecting samples
[2018-01-21 14:25:50.115339 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:50.257943 UTC] Performing policy update
[2018-01-21 14:25:50.258622 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:50.371762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:25:51.746389 UTC] Performing line search
[2018-01-21 14:25:51.935594 UTC] Updating baseline
[2018-01-21 14:25:53.818927 UTC] Computing logging information
-------------------------------------
| Iteration            | 726        |
| ExpectedImprovement  | 0.018225   |
| ActualImprovement    | 0.016878   |
| ImprovementRatio     | 0.92609    |
| MeanKL               | 0.0074621  |
| Entropy              | -0.5944    |
| Perplexity           | 0.55189    |
| AveragePolicyStd     | 0.22133    |
| AveragePolicyStd[0]  | 0.22799    |
| AveragePolicyStd[1]  | 0.25255    |
| AveragePolicyStd[2]  | 0.16411    |
| AveragePolicyStd[3]  | 0.22824    |
| AveragePolicyStd[4]  | 0.20733    |
| AveragePolicyStd[5]  | 0.24773    |
| AverageReturn        | 1390.9     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 187.28     |
| AverageEpisodeLength | 970.07     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.66     |
| TotalNEpisodes       | 20405      |
| TotalNSamples        | 3.6324e+06 |
| ExplainedVariance    | -0.014158  |
-------------------------------------
[2018-01-21 14:25:54.589942 UTC] Saving snapshot
[2018-01-21 14:25:54.590240 UTC] Starting iteration 727
[2018-01-21 14:25:54.590447 UTC] Start collecting samples
[2018-01-21 14:25:59.046681 UTC] Computing input variables for policy optimization
[2018-01-21 14:25:59.162816 UTC] Performing policy update
[2018-01-21 14:25:59.163408 UTC] Computing gradient in Euclidean space
[2018-01-21 14:25:59.279429 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:00.690172 UTC] Performing line search
[2018-01-21 14:26:00.878201 UTC] Updating baseline
[2018-01-21 14:26:03.368508 UTC] Computing logging information
-------------------------------------
| Iteration            | 727        |
| ExpectedImprovement  | 0.016737   |
| ActualImprovement    | 0.015933   |
| ImprovementRatio     | 0.95201    |
| MeanKL               | 0.00791    |
| Entropy              | -0.59766   |
| Perplexity           | 0.5501     |
| AveragePolicyStd     | 0.22124    |
| AveragePolicyStd[0]  | 0.22791    |
| AveragePolicyStd[1]  | 0.25275    |
| AveragePolicyStd[2]  | 0.16382    |
| AveragePolicyStd[3]  | 0.2279     |
| AveragePolicyStd[4]  | 0.20686    |
| AveragePolicyStd[5]  | 0.24819    |
| AverageReturn        | 1393.5     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 176.89     |
| AverageEpisodeLength | 972.25     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.83     |
| TotalNEpisodes       | 20411      |
| TotalNSamples        | 3.6381e+06 |
| ExplainedVariance    | 0.093813   |
-------------------------------------
[2018-01-21 14:26:04.098356 UTC] Saving snapshot
[2018-01-21 14:26:04.098642 UTC] Starting iteration 728
[2018-01-21 14:26:04.098861 UTC] Start collecting samples
[2018-01-21 14:26:08.451302 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:08.577819 UTC] Performing policy update
[2018-01-21 14:26:08.578674 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:08.698149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:10.172250 UTC] Performing line search
[2018-01-21 14:26:10.372185 UTC] Updating baseline
[2018-01-21 14:26:12.420442 UTC] Computing logging information
-------------------------------------
| Iteration            | 728        |
| ExpectedImprovement  | 0.015662   |
| ActualImprovement    | 0.015113   |
| ImprovementRatio     | 0.96499    |
| MeanKL               | 0.0075505  |
| Entropy              | -0.60341   |
| Perplexity           | 0.54694    |
| AveragePolicyStd     | 0.22101    |
| AveragePolicyStd[0]  | 0.229      |
| AveragePolicyStd[1]  | 0.252      |
| AveragePolicyStd[2]  | 0.16384    |
| AveragePolicyStd[3]  | 0.22777    |
| AveragePolicyStd[4]  | 0.20616    |
| AveragePolicyStd[5]  | 0.24726    |
| AverageReturn        | 1395.8     |
| MinReturn            | 131.95     |
| MaxReturn            | 1484       |
| StdReturn            | 176.81     |
| AverageEpisodeLength | 973.05     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.74     |
| TotalNEpisodes       | 20415      |
| TotalNSamples        | 3.6421e+06 |
| ExplainedVariance    | -0.003752  |
-------------------------------------
[2018-01-21 14:26:13.196390 UTC] Saving snapshot
[2018-01-21 14:26:13.196622 UTC] Starting iteration 729
[2018-01-21 14:26:13.196767 UTC] Start collecting samples
[2018-01-21 14:26:17.808215 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:17.940754 UTC] Performing policy update
[2018-01-21 14:26:17.941424 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:18.058416 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:19.453553 UTC] Performing line search
[2018-01-21 14:26:19.638233 UTC] Updating baseline
[2018-01-21 14:26:21.662942 UTC] Computing logging information
------------------------------------
| Iteration            | 729       |
| ExpectedImprovement  | 0.018902  |
| ActualImprovement    | 0.017213  |
| ImprovementRatio     | 0.91064   |
| MeanKL               | 0.0074562 |
| Entropy              | -0.60992  |
| Perplexity           | 0.54339   |
| AveragePolicyStd     | 0.22072   |
| AveragePolicyStd[0]  | 0.22931   |
| AveragePolicyStd[1]  | 0.25022   |
| AveragePolicyStd[2]  | 0.16409   |
| AveragePolicyStd[3]  | 0.22751   |
| AveragePolicyStd[4]  | 0.20582   |
| AveragePolicyStd[5]  | 0.24739   |
| AverageReturn        | 1401.9    |
| MinReturn            | 131.95    |
| MaxReturn            | 1506.1    |
| StdReturn            | 170.01    |
| AverageEpisodeLength | 976.67    |
| MinEpisodeLength     | 123       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 113.13    |
| TotalNEpisodes       | 20421     |
| TotalNSamples        | 3.648e+06 |
| ExplainedVariance    | 0.1322    |
------------------------------------
[2018-01-21 14:26:22.424424 UTC] Saving snapshot
[2018-01-21 14:26:22.424732 UTC] Starting iteration 730
[2018-01-21 14:26:22.424952 UTC] Start collecting samples
[2018-01-21 14:26:26.847068 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:26.978950 UTC] Performing policy update
[2018-01-21 14:26:26.979556 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:27.096427 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:28.516176 UTC] Performing line search
[2018-01-21 14:26:28.707945 UTC] Updating baseline
[2018-01-21 14:26:30.761314 UTC] Computing logging information
-------------------------------------
| Iteration            | 730        |
| ExpectedImprovement  | 0.016517   |
| ActualImprovement    | 0.015459   |
| ImprovementRatio     | 0.93591    |
| MeanKL               | 0.0078751  |
| Entropy              | -0.61734   |
| Perplexity           | 0.53938    |
| AveragePolicyStd     | 0.22046    |
| AveragePolicyStd[0]  | 0.22909    |
| AveragePolicyStd[1]  | 0.24976    |
| AveragePolicyStd[2]  | 0.16425    |
| AveragePolicyStd[3]  | 0.2271     |
| AveragePolicyStd[4]  | 0.20448    |
| AveragePolicyStd[5]  | 0.24807    |
| AverageReturn        | 1402.4     |
| MinReturn            | 131.95     |
| MaxReturn            | 1506.1     |
| StdReturn            | 170.16     |
| AverageEpisodeLength | 976.67     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.13     |
| TotalNEpisodes       | 20426      |
| TotalNSamples        | 3.653e+06  |
| ExplainedVariance    | -0.0049081 |
-------------------------------------
[2018-01-21 14:26:31.520411 UTC] Saving snapshot
[2018-01-21 14:26:31.529799 UTC] Starting iteration 731
[2018-01-21 14:26:31.530035 UTC] Start collecting samples
[2018-01-21 14:26:36.098351 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:36.225286 UTC] Performing policy update
[2018-01-21 14:26:36.225907 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:36.350531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:37.750069 UTC] Performing line search
[2018-01-21 14:26:37.956420 UTC] Updating baseline
[2018-01-21 14:26:40.231641 UTC] Computing logging information
-------------------------------------
| Iteration            | 731        |
| ExpectedImprovement  | 0.015512   |
| ActualImprovement    | 0.014625   |
| ImprovementRatio     | 0.94281    |
| MeanKL               | 0.0076039  |
| Entropy              | -0.62632   |
| Perplexity           | 0.53456    |
| AveragePolicyStd     | 0.22015    |
| AveragePolicyStd[0]  | 0.2295     |
| AveragePolicyStd[1]  | 0.24979    |
| AveragePolicyStd[2]  | 0.16389    |
| AveragePolicyStd[3]  | 0.22676    |
| AveragePolicyStd[4]  | 0.2035     |
| AveragePolicyStd[5]  | 0.24748    |
| AverageReturn        | 1396.8     |
| MinReturn            | 131.95     |
| MaxReturn            | 1506.1     |
| StdReturn            | 182.83     |
| AverageEpisodeLength | 972.07     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.15     |
| TotalNEpisodes       | 20432      |
| TotalNSamples        | 3.6586e+06 |
| ExplainedVariance    | 0.10493    |
-------------------------------------
[2018-01-21 14:26:40.925469 UTC] Saving snapshot
[2018-01-21 14:26:40.925690 UTC] Starting iteration 732
[2018-01-21 14:26:40.925877 UTC] Start collecting samples
[2018-01-21 14:26:45.463584 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:45.584025 UTC] Performing policy update
[2018-01-21 14:26:45.584797 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:45.701812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:47.104558 UTC] Performing line search
[2018-01-21 14:26:47.293717 UTC] Updating baseline
[2018-01-21 14:26:48.773692 UTC] Computing logging information
-------------------------------------
| Iteration            | 732        |
| ExpectedImprovement  | 0.018587   |
| ActualImprovement    | 0.01694    |
| ImprovementRatio     | 0.91141    |
| MeanKL               | 0.0075699  |
| Entropy              | -0.62172   |
| Perplexity           | 0.53702    |
| AveragePolicyStd     | 0.22034    |
| AveragePolicyStd[0]  | 0.22945    |
| AveragePolicyStd[1]  | 0.2506     |
| AveragePolicyStd[2]  | 0.16382    |
| AveragePolicyStd[3]  | 0.22625    |
| AveragePolicyStd[4]  | 0.20399    |
| AveragePolicyStd[5]  | 0.24794    |
| AverageReturn        | 1398.5     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 183.38     |
| AverageEpisodeLength | 972.07     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.15     |
| TotalNEpisodes       | 20437      |
| TotalNSamples        | 3.6636e+06 |
| ExplainedVariance    | -0.0077535 |
-------------------------------------
[2018-01-21 14:26:49.482408 UTC] Saving snapshot
[2018-01-21 14:26:49.482639 UTC] Starting iteration 733
[2018-01-21 14:26:49.482797 UTC] Start collecting samples
[2018-01-21 14:26:53.912624 UTC] Computing input variables for policy optimization
[2018-01-21 14:26:54.038399 UTC] Performing policy update
[2018-01-21 14:26:54.038997 UTC] Computing gradient in Euclidean space
[2018-01-21 14:26:54.159425 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:26:55.561201 UTC] Performing line search
[2018-01-21 14:26:55.752487 UTC] Updating baseline
[2018-01-21 14:26:59.257483 UTC] Computing logging information
-------------------------------------
| Iteration            | 733        |
| ExpectedImprovement  | 0.016286   |
| ActualImprovement    | 0.015275   |
| ImprovementRatio     | 0.93797    |
| MeanKL               | 0.0079402  |
| Entropy              | -0.62716   |
| Perplexity           | 0.53411    |
| AveragePolicyStd     | 0.22016    |
| AveragePolicyStd[0]  | 0.22907    |
| AveragePolicyStd[1]  | 0.25037    |
| AveragePolicyStd[2]  | 0.16327    |
| AveragePolicyStd[3]  | 0.22648    |
| AveragePolicyStd[4]  | 0.20402    |
| AveragePolicyStd[5]  | 0.24776    |
| AverageReturn        | 1401.6     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 181.33     |
| AverageEpisodeLength | 974.06     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.96     |
| TotalNEpisodes       | 20440      |
| TotalNSamples        | 3.6666e+06 |
| ExplainedVariance    | 2.0579e-05 |
-------------------------------------
[2018-01-21 14:27:00.003617 UTC] Saving snapshot
[2018-01-21 14:27:00.003834 UTC] Starting iteration 734
[2018-01-21 14:27:00.004033 UTC] Start collecting samples
[2018-01-21 14:27:04.662937 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:04.798165 UTC] Performing policy update
[2018-01-21 14:27:04.799622 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:04.920506 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:06.315194 UTC] Performing line search
[2018-01-21 14:27:06.501175 UTC] Updating baseline
[2018-01-21 14:27:07.258788 UTC] Computing logging information
--------------------------------------
| Iteration            | 734         |
| ExpectedImprovement  | 0.015193    |
| ActualImprovement    | 0.01429     |
| ImprovementRatio     | 0.94055     |
| MeanKL               | 0.0071662   |
| Entropy              | -0.63383    |
| Perplexity           | 0.53056     |
| AveragePolicyStd     | 0.21989     |
| AveragePolicyStd[0]  | 0.22876     |
| AveragePolicyStd[1]  | 0.25028     |
| AveragePolicyStd[2]  | 0.16364     |
| AveragePolicyStd[3]  | 0.22604     |
| AveragePolicyStd[4]  | 0.20346     |
| AveragePolicyStd[5]  | 0.24715     |
| AverageReturn        | 1410.1      |
| MinReturn            | 131.95      |
| MaxReturn            | 1523.9      |
| StdReturn            | 171.52      |
| AverageEpisodeLength | 978.53      |
| MinEpisodeLength     | 123         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 112.26      |
| TotalNEpisodes       | 20446       |
| TotalNSamples        | 3.6726e+06  |
| ExplainedVariance    | -2.9241e-09 |
--------------------------------------
[2018-01-21 14:27:07.948213 UTC] Saving snapshot
[2018-01-21 14:27:07.948446 UTC] Starting iteration 735
[2018-01-21 14:27:07.948591 UTC] Start collecting samples
[2018-01-21 14:27:12.354304 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:12.524198 UTC] Performing policy update
[2018-01-21 14:27:12.524839 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:12.650251 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:14.096897 UTC] Performing line search
[2018-01-21 14:27:14.287110 UTC] Updating baseline
[2018-01-21 14:27:16.797729 UTC] Computing logging information
--------------------------------------
| Iteration            | 735         |
| ExpectedImprovement  | 0.018184    |
| ActualImprovement    | 0.016881    |
| ImprovementRatio     | 0.92836     |
| MeanKL               | 0.0077665   |
| Entropy              | -0.62893    |
| Perplexity           | 0.53316     |
| AveragePolicyStd     | 0.22012     |
| AveragePolicyStd[0]  | 0.22916     |
| AveragePolicyStd[1]  | 0.25067     |
| AveragePolicyStd[2]  | 0.16309     |
| AveragePolicyStd[3]  | 0.22594     |
| AveragePolicyStd[4]  | 0.20395     |
| AveragePolicyStd[5]  | 0.24789     |
| AverageReturn        | 1410.4      |
| MinReturn            | 131.95      |
| MaxReturn            | 1523.9      |
| StdReturn            | 171.54      |
| AverageEpisodeLength | 978.53      |
| MinEpisodeLength     | 123         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 112.26      |
| TotalNEpisodes       | 20452       |
| TotalNSamples        | 3.6786e+06  |
| ExplainedVariance    | -8.6457e-06 |
--------------------------------------
[2018-01-21 14:27:17.471506 UTC] Saving snapshot
[2018-01-21 14:27:17.471804 UTC] Starting iteration 736
[2018-01-21 14:27:17.472022 UTC] Start collecting samples
[2018-01-21 14:27:21.808197 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:21.947639 UTC] Performing policy update
[2018-01-21 14:27:21.948557 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:22.062708 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:23.514573 UTC] Performing line search
[2018-01-21 14:27:23.705959 UTC] Updating baseline
[2018-01-21 14:27:24.116176 UTC] Computing logging information
-------------------------------------
| Iteration            | 736        |
| ExpectedImprovement  | 0.018142   |
| ActualImprovement    | 0.016511   |
| ImprovementRatio     | 0.91011    |
| MeanKL               | 0.0074517  |
| Entropy              | -0.63191   |
| Perplexity           | 0.53157    |
| AveragePolicyStd     | 0.22001    |
| AveragePolicyStd[0]  | 0.22804    |
| AveragePolicyStd[1]  | 0.25065    |
| AveragePolicyStd[2]  | 0.1632     |
| AveragePolicyStd[3]  | 0.22555    |
| AveragePolicyStd[4]  | 0.20391    |
| AveragePolicyStd[5]  | 0.24868    |
| AverageReturn        | 1410.7     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 171.65     |
| AverageEpisodeLength | 978.53     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.26     |
| TotalNEpisodes       | 20456      |
| TotalNSamples        | 3.6826e+06 |
| ExplainedVariance    | 7.2684e-09 |
-------------------------------------
[2018-01-21 14:27:24.822153 UTC] Saving snapshot
[2018-01-21 14:27:24.822391 UTC] Starting iteration 737
[2018-01-21 14:27:24.822584 UTC] Start collecting samples
[2018-01-21 14:27:29.443603 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:29.618097 UTC] Performing policy update
[2018-01-21 14:27:29.618855 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:29.739163 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:31.190992 UTC] Performing line search
[2018-01-21 14:27:31.375873 UTC] Updating baseline
[2018-01-21 14:27:31.794623 UTC] Computing logging information
-------------------------------------
| Iteration            | 737        |
| ExpectedImprovement  | 0.016848   |
| ActualImprovement    | 0.016095   |
| ImprovementRatio     | 0.95531    |
| MeanKL               | 0.0077176  |
| Entropy              | -0.63721   |
| Perplexity           | 0.52877    |
| AveragePolicyStd     | 0.21983    |
| AveragePolicyStd[0]  | 0.22796    |
| AveragePolicyStd[1]  | 0.25024    |
| AveragePolicyStd[2]  | 0.16271    |
| AveragePolicyStd[3]  | 0.22561    |
| AveragePolicyStd[4]  | 0.20389    |
| AveragePolicyStd[5]  | 0.24858    |
| AverageReturn        | 1411.8     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 171.99     |
| AverageEpisodeLength | 978.53     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.26     |
| TotalNEpisodes       | 20461      |
| TotalNSamples        | 3.6876e+06 |
| ExplainedVariance    | 1.898e-09  |
-------------------------------------
[2018-01-21 14:27:32.534581 UTC] Saving snapshot
[2018-01-21 14:27:32.534839 UTC] Starting iteration 738
[2018-01-21 14:27:32.535022 UTC] Start collecting samples
[2018-01-21 14:27:37.278149 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:37.423028 UTC] Performing policy update
[2018-01-21 14:27:37.423667 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:37.542120 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:38.970961 UTC] Performing line search
[2018-01-21 14:27:39.176745 UTC] Updating baseline
[2018-01-21 14:27:42.042761 UTC] Computing logging information
-------------------------------------
| Iteration            | 738        |
| ExpectedImprovement  | 0.017952   |
| ActualImprovement    | 0.016578   |
| ImprovementRatio     | 0.92347    |
| MeanKL               | 0.0075623  |
| Entropy              | -0.63219   |
| Perplexity           | 0.53143    |
| AveragePolicyStd     | 0.22003    |
| AveragePolicyStd[0]  | 0.22816    |
| AveragePolicyStd[1]  | 0.25037    |
| AveragePolicyStd[2]  | 0.16266    |
| AveragePolicyStd[3]  | 0.22605    |
| AveragePolicyStd[4]  | 0.20383    |
| AveragePolicyStd[5]  | 0.24913    |
| AverageReturn        | 1412.7     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 172.13     |
| AverageEpisodeLength | 978.53     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.26     |
| TotalNEpisodes       | 20467      |
| TotalNSamples        | 3.6936e+06 |
| ExplainedVariance    | 2.8477e-06 |
-------------------------------------
[2018-01-21 14:27:42.782769 UTC] Saving snapshot
[2018-01-21 14:27:42.782999 UTC] Starting iteration 739
[2018-01-21 14:27:42.783142 UTC] Start collecting samples
[2018-01-21 14:27:47.353887 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:47.489671 UTC] Performing policy update
[2018-01-21 14:27:47.490761 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:47.615473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:49.035973 UTC] Performing line search
[2018-01-21 14:27:49.230907 UTC] Updating baseline
[2018-01-21 14:27:51.999940 UTC] Computing logging information
-------------------------------------
| Iteration            | 739        |
| ExpectedImprovement  | 0.019464   |
| ActualImprovement    | 0.01755    |
| ImprovementRatio     | 0.90165    |
| MeanKL               | 0.0069092  |
| Entropy              | -0.63337   |
| Perplexity           | 0.5308     |
| AveragePolicyStd     | 0.21996    |
| AveragePolicyStd[0]  | 0.22832    |
| AveragePolicyStd[1]  | 0.25065    |
| AveragePolicyStd[2]  | 0.16323    |
| AveragePolicyStd[3]  | 0.22554    |
| AveragePolicyStd[4]  | 0.20326    |
| AveragePolicyStd[5]  | 0.24877    |
| AverageReturn        | 1414       |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 172.49     |
| AverageEpisodeLength | 978.53     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.26     |
| TotalNEpisodes       | 20471      |
| TotalNSamples        | 3.6976e+06 |
| ExplainedVariance    | 0.0031658  |
-------------------------------------
[2018-01-21 14:27:52.731897 UTC] Saving snapshot
[2018-01-21 14:27:52.732113 UTC] Starting iteration 740
[2018-01-21 14:27:52.732261 UTC] Start collecting samples
[2018-01-21 14:27:57.221610 UTC] Computing input variables for policy optimization
[2018-01-21 14:27:57.365691 UTC] Performing policy update
[2018-01-21 14:27:57.366327 UTC] Computing gradient in Euclidean space
[2018-01-21 14:27:57.492703 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:27:58.916074 UTC] Performing line search
[2018-01-21 14:27:59.112387 UTC] Updating baseline
[2018-01-21 14:28:00.994550 UTC] Computing logging information
-------------------------------------
| Iteration            | 740        |
| ExpectedImprovement  | 0.01804    |
| ActualImprovement    | 0.016975   |
| ImprovementRatio     | 0.94099    |
| MeanKL               | 0.0074719  |
| Entropy              | -0.64093   |
| Perplexity           | 0.5268     |
| AveragePolicyStd     | 0.2197     |
| AveragePolicyStd[0]  | 0.2276     |
| AveragePolicyStd[1]  | 0.25076    |
| AveragePolicyStd[2]  | 0.16278    |
| AveragePolicyStd[3]  | 0.22522    |
| AveragePolicyStd[4]  | 0.20333    |
| AveragePolicyStd[5]  | 0.24854    |
| AverageReturn        | 1421.9     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 157.96     |
| AverageEpisodeLength | 983.36     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.25     |
| TotalNEpisodes       | 20476      |
| TotalNSamples        | 3.7026e+06 |
| ExplainedVariance    | -0.0030505 |
-------------------------------------
[2018-01-21 14:28:01.665656 UTC] Saving snapshot
[2018-01-21 14:28:01.672251 UTC] Starting iteration 741
[2018-01-21 14:28:01.672484 UTC] Start collecting samples
[2018-01-21 14:28:06.089461 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:06.238851 UTC] Performing policy update
[2018-01-21 14:28:06.240064 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:06.363165 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:07.755437 UTC] Performing line search
[2018-01-21 14:28:07.952842 UTC] Updating baseline
[2018-01-21 14:28:11.488546 UTC] Computing logging information
--------------------------------------
| Iteration            | 741         |
| ExpectedImprovement  | 0.015474    |
| ActualImprovement    | 0.014731    |
| ImprovementRatio     | 0.95201     |
| MeanKL               | 0.0077157   |
| Entropy              | -0.64437    |
| Perplexity           | 0.52499     |
| AveragePolicyStd     | 0.21958     |
| AveragePolicyStd[0]  | 0.22747     |
| AveragePolicyStd[1]  | 0.25086     |
| AveragePolicyStd[2]  | 0.16261     |
| AveragePolicyStd[3]  | 0.22568     |
| AveragePolicyStd[4]  | 0.20309     |
| AveragePolicyStd[5]  | 0.24776     |
| AverageReturn        | 1423.9      |
| MinReturn            | 131.95      |
| MaxReturn            | 1523.9      |
| StdReturn            | 158.34      |
| AverageEpisodeLength | 983.36      |
| MinEpisodeLength     | 123         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 102.25      |
| TotalNEpisodes       | 20483       |
| TotalNSamples        | 3.7096e+06  |
| ExplainedVariance    | -0.00011587 |
--------------------------------------
[2018-01-21 14:28:12.186789 UTC] Saving snapshot
[2018-01-21 14:28:12.187052 UTC] Starting iteration 742
[2018-01-21 14:28:12.187264 UTC] Start collecting samples
[2018-01-21 14:28:16.650084 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:16.767285 UTC] Performing policy update
[2018-01-21 14:28:16.768335 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:16.885644 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:18.298280 UTC] Performing line search
[2018-01-21 14:28:18.502768 UTC] Updating baseline
[2018-01-21 14:28:21.400501 UTC] Computing logging information
-------------------------------------
| Iteration            | 742        |
| ExpectedImprovement  | 0.01754    |
| ActualImprovement    | 0.016766   |
| ImprovementRatio     | 0.95584    |
| MeanKL               | 0.0078044  |
| Entropy              | -0.63324   |
| Perplexity           | 0.53087    |
| AveragePolicyStd     | 0.22001    |
| AveragePolicyStd[0]  | 0.22694    |
| AveragePolicyStd[1]  | 0.25238    |
| AveragePolicyStd[2]  | 0.16312    |
| AveragePolicyStd[3]  | 0.22578    |
| AveragePolicyStd[4]  | 0.20287    |
| AveragePolicyStd[5]  | 0.24899    |
| AverageReturn        | 1425.4     |
| MinReturn            | 131.95     |
| MaxReturn            | 1523.9     |
| StdReturn            | 158.41     |
| AverageEpisodeLength | 983.36     |
| MinEpisodeLength     | 123        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.25     |
| TotalNEpisodes       | 20486      |
| TotalNSamples        | 3.7126e+06 |
| ExplainedVariance    | 1.5654e-05 |
-------------------------------------
[2018-01-21 14:28:22.112616 UTC] Saving snapshot
[2018-01-21 14:28:22.112878 UTC] Starting iteration 743
[2018-01-21 14:28:22.113046 UTC] Start collecting samples
[2018-01-21 14:28:26.672791 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:26.804412 UTC] Performing policy update
[2018-01-21 14:28:26.804867 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:26.934569 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:28.395492 UTC] Performing line search
[2018-01-21 14:28:28.598118 UTC] Updating baseline
[2018-01-21 14:28:30.859669 UTC] Computing logging information
-------------------------------------
| Iteration            | 743        |
| ExpectedImprovement  | 0.018841   |
| ActualImprovement    | 0.017225   |
| ImprovementRatio     | 0.91424    |
| MeanKL               | 0.0077285  |
| Entropy              | -0.63713   |
| Perplexity           | 0.52881    |
| AveragePolicyStd     | 0.21987    |
| AveragePolicyStd[0]  | 0.22687    |
| AveragePolicyStd[1]  | 0.25242    |
| AveragePolicyStd[2]  | 0.16296    |
| AveragePolicyStd[3]  | 0.22572    |
| AveragePolicyStd[4]  | 0.20275    |
| AveragePolicyStd[5]  | 0.24853    |
| AverageReturn        | 1434.4     |
| MinReturn            | 738.18     |
| MaxReturn            | 1523.9     |
| StdReturn            | 107.48     |
| AverageEpisodeLength | 988.25     |
| MinEpisodeLength     | 540        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 66.387     |
| TotalNEpisodes       | 20492      |
| TotalNSamples        | 3.7182e+06 |
| ExplainedVariance    | 0.094353   |
-------------------------------------
[2018-01-21 14:28:31.638606 UTC] Saving snapshot
[2018-01-21 14:28:31.638851 UTC] Starting iteration 744
[2018-01-21 14:28:31.638994 UTC] Start collecting samples
[2018-01-21 14:28:36.314859 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:36.433971 UTC] Performing policy update
[2018-01-21 14:28:36.434606 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:36.554954 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:37.971246 UTC] Performing line search
[2018-01-21 14:28:38.164516 UTC] Updating baseline
[2018-01-21 14:28:38.814961 UTC] Computing logging information
-------------------------------------
| Iteration            | 744        |
| ExpectedImprovement  | 0.019734   |
| ActualImprovement    | 0.018521   |
| ImprovementRatio     | 0.93852    |
| MeanKL               | 0.0073664  |
| Entropy              | -0.64295   |
| Perplexity           | 0.52574    |
| AveragePolicyStd     | 0.21965    |
| AveragePolicyStd[0]  | 0.22672    |
| AveragePolicyStd[1]  | 0.25215    |
| AveragePolicyStd[2]  | 0.16298    |
| AveragePolicyStd[3]  | 0.22516    |
| AveragePolicyStd[4]  | 0.20259    |
| AveragePolicyStd[5]  | 0.24828    |
| AverageReturn        | 1436.2     |
| MinReturn            | 738.18     |
| MaxReturn            | 1523.9     |
| StdReturn            | 107.96     |
| AverageEpisodeLength | 988.25     |
| MinEpisodeLength     | 540        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 66.387     |
| TotalNEpisodes       | 20497      |
| TotalNSamples        | 3.7232e+06 |
| ExplainedVariance    | -0.0068032 |
-------------------------------------
[2018-01-21 14:28:39.530185 UTC] Saving snapshot
[2018-01-21 14:28:39.530546 UTC] Starting iteration 745
[2018-01-21 14:28:39.530817 UTC] Start collecting samples
[2018-01-21 14:28:43.780460 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:43.911367 UTC] Performing policy update
[2018-01-21 14:28:43.911954 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:44.044402 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:45.481120 UTC] Performing line search
[2018-01-21 14:28:45.675876 UTC] Updating baseline
[2018-01-21 14:28:47.444316 UTC] Computing logging information
-------------------------------------
| Iteration            | 745        |
| ExpectedImprovement  | 0.019402   |
| ActualImprovement    | 0.018434   |
| ImprovementRatio     | 0.95011    |
| MeanKL               | 0.0087597  |
| Entropy              | -0.65328   |
| Perplexity           | 0.52034    |
| AveragePolicyStd     | 0.21925    |
| AveragePolicyStd[0]  | 0.22562    |
| AveragePolicyStd[1]  | 0.25193    |
| AveragePolicyStd[2]  | 0.16283    |
| AveragePolicyStd[3]  | 0.22434    |
| AveragePolicyStd[4]  | 0.20312    |
| AveragePolicyStd[5]  | 0.24764    |
| AverageReturn        | 1437.1     |
| MinReturn            | 738.18     |
| MaxReturn            | 1523.9     |
| StdReturn            | 108.13     |
| AverageEpisodeLength | 988.25     |
| MinEpisodeLength     | 540        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 66.387     |
| TotalNEpisodes       | 20501      |
| TotalNSamples        | 3.7272e+06 |
| ExplainedVariance    | 2.7451e-09 |
-------------------------------------
[2018-01-21 14:28:48.229724 UTC] Saving snapshot
[2018-01-21 14:28:48.230259 UTC] Starting iteration 746
[2018-01-21 14:28:48.230700 UTC] Start collecting samples
[2018-01-21 14:28:52.865523 UTC] Computing input variables for policy optimization
[2018-01-21 14:28:53.007821 UTC] Performing policy update
[2018-01-21 14:28:53.008433 UTC] Computing gradient in Euclidean space
[2018-01-21 14:28:53.146947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:28:54.544177 UTC] Performing line search
[2018-01-21 14:28:54.734170 UTC] Updating baseline
[2018-01-21 14:28:56.271508 UTC] Computing logging information
--------------------------------------
| Iteration            | 746         |
| ExpectedImprovement  | 0.018131    |
| ActualImprovement    | 0.016871    |
| ImprovementRatio     | 0.93051     |
| MeanKL               | 0.0070363   |
| Entropy              | -0.65965    |
| Perplexity           | 0.51703     |
| AveragePolicyStd     | 0.21901     |
| AveragePolicyStd[0]  | 0.22584     |
| AveragePolicyStd[1]  | 0.25197     |
| AveragePolicyStd[2]  | 0.16261     |
| AveragePolicyStd[3]  | 0.22426     |
| AveragePolicyStd[4]  | 0.20291     |
| AveragePolicyStd[5]  | 0.24647     |
| AverageReturn        | 1439.3      |
| MinReturn            | 738.18      |
| MaxReturn            | 1523.9      |
| StdReturn            | 108.53      |
| AverageEpisodeLength | 988.25      |
| MinEpisodeLength     | 540         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 66.387      |
| TotalNEpisodes       | 20507       |
| TotalNSamples        | 3.7332e+06  |
| ExplainedVariance    | -4.2234e-10 |
--------------------------------------
[2018-01-21 14:28:57.007651 UTC] Saving snapshot
[2018-01-21 14:28:57.007895 UTC] Starting iteration 747
[2018-01-21 14:28:57.008077 UTC] Start collecting samples
[2018-01-21 14:29:01.681446 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:01.827979 UTC] Performing policy update
[2018-01-21 14:29:01.829265 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:01.950796 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:03.372675 UTC] Performing line search
[2018-01-21 14:29:03.565235 UTC] Updating baseline
[2018-01-21 14:29:04.563878 UTC] Computing logging information
--------------------------------------
| Iteration            | 747         |
| ExpectedImprovement  | 0.017209    |
| ActualImprovement    | 0.016393    |
| ImprovementRatio     | 0.9526      |
| MeanKL               | 0.0077233   |
| Entropy              | -0.672      |
| Perplexity           | 0.51068     |
| AveragePolicyStd     | 0.21856     |
| AveragePolicyStd[0]  | 0.22593     |
| AveragePolicyStd[1]  | 0.25118     |
| AveragePolicyStd[2]  | 0.16239     |
| AveragePolicyStd[3]  | 0.22307     |
| AveragePolicyStd[4]  | 0.20222     |
| AveragePolicyStd[5]  | 0.24658     |
| AverageReturn        | 1445.7      |
| MinReturn            | 738.18      |
| MaxReturn            | 1523.9      |
| StdReturn            | 97.107      |
| AverageEpisodeLength | 991.28      |
| MinEpisodeLength     | 540         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 59.592      |
| TotalNEpisodes       | 20512       |
| TotalNSamples        | 3.7382e+06  |
| ExplainedVariance    | -2.4266e-09 |
--------------------------------------
[2018-01-21 14:29:05.330801 UTC] Saving snapshot
[2018-01-21 14:29:05.331074 UTC] Starting iteration 748
[2018-01-21 14:29:05.331310 UTC] Start collecting samples
[2018-01-21 14:29:09.790920 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:09.916229 UTC] Performing policy update
[2018-01-21 14:29:09.916973 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:10.033905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:11.439645 UTC] Performing line search
[2018-01-21 14:29:11.628439 UTC] Updating baseline
[2018-01-21 14:29:13.152417 UTC] Computing logging information
--------------------------------------
| Iteration            | 748         |
| ExpectedImprovement  | 0.01612     |
| ActualImprovement    | 0.015058    |
| ImprovementRatio     | 0.93412     |
| MeanKL               | 0.0077072   |
| Entropy              | -0.67406    |
| Perplexity           | 0.50963     |
| AveragePolicyStd     | 0.21845     |
| AveragePolicyStd[0]  | 0.22588     |
| AveragePolicyStd[1]  | 0.25114     |
| AveragePolicyStd[2]  | 0.16286     |
| AveragePolicyStd[3]  | 0.22289     |
| AveragePolicyStd[4]  | 0.20194     |
| AveragePolicyStd[5]  | 0.24599     |
| AverageReturn        | 1447.8      |
| MinReturn            | 738.18      |
| MaxReturn            | 1523.9      |
| StdReturn            | 97.09       |
| AverageEpisodeLength | 991.28      |
| MinEpisodeLength     | 540         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 59.592      |
| TotalNEpisodes       | 20517       |
| TotalNSamples        | 3.7432e+06  |
| ExplainedVariance    | -1.7581e-09 |
--------------------------------------
[2018-01-21 14:29:13.928993 UTC] Saving snapshot
[2018-01-21 14:29:13.929266 UTC] Starting iteration 749
[2018-01-21 14:29:13.929475 UTC] Start collecting samples
[2018-01-21 14:29:18.400316 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:18.518735 UTC] Performing policy update
[2018-01-21 14:29:18.519325 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:18.638008 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:20.046533 UTC] Performing line search
[2018-01-21 14:29:20.239426 UTC] Updating baseline
[2018-01-21 14:29:22.191343 UTC] Computing logging information
--------------------------------------
| Iteration            | 749         |
| ExpectedImprovement  | 0.01848     |
| ActualImprovement    | 0.017264    |
| ImprovementRatio     | 0.9342      |
| MeanKL               | 0.0071563   |
| Entropy              | -0.66871    |
| Perplexity           | 0.51237     |
| AveragePolicyStd     | 0.21864     |
| AveragePolicyStd[0]  | 0.22565     |
| AveragePolicyStd[1]  | 0.25203     |
| AveragePolicyStd[2]  | 0.16326     |
| AveragePolicyStd[3]  | 0.22256     |
| AveragePolicyStd[4]  | 0.20216     |
| AveragePolicyStd[5]  | 0.24618     |
| AverageReturn        | 1449.7      |
| MinReturn            | 738.18      |
| MaxReturn            | 1526.7      |
| StdReturn            | 97.021      |
| AverageEpisodeLength | 991.52      |
| MinEpisodeLength     | 540         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 59.578      |
| TotalNEpisodes       | 20521       |
| TotalNSamples        | 3.7472e+06  |
| ExplainedVariance    | -1.2624e-09 |
--------------------------------------
[2018-01-21 14:29:22.890179 UTC] Saving snapshot
[2018-01-21 14:29:22.890424 UTC] Starting iteration 750
[2018-01-21 14:29:22.890599 UTC] Start collecting samples
[2018-01-21 14:29:27.285974 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:27.420819 UTC] Performing policy update
[2018-01-21 14:29:27.426743 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:27.546067 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:29.009934 UTC] Performing line search
[2018-01-21 14:29:29.205073 UTC] Updating baseline
[2018-01-21 14:29:30.543019 UTC] Computing logging information
-------------------------------------
| Iteration            | 750        |
| ExpectedImprovement  | 0.017451   |
| ActualImprovement    | 0.016299   |
| ImprovementRatio     | 0.93396    |
| MeanKL               | 0.0074618  |
| Entropy              | -0.66691   |
| Perplexity           | 0.51329    |
| AveragePolicyStd     | 0.21871    |
| AveragePolicyStd[0]  | 0.22534    |
| AveragePolicyStd[1]  | 0.2519     |
| AveragePolicyStd[2]  | 0.16328    |
| AveragePolicyStd[3]  | 0.22259    |
| AveragePolicyStd[4]  | 0.20224    |
| AveragePolicyStd[5]  | 0.24692    |
| AverageReturn        | 1450.9     |
| MinReturn            | 738.18     |
| MaxReturn            | 1526.7     |
| StdReturn            | 97.061     |
| AverageEpisodeLength | 991.52     |
| MinEpisodeLength     | 540        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 59.578     |
| TotalNEpisodes       | 20527      |
| TotalNSamples        | 3.7532e+06 |
| ExplainedVariance    | 9.4138e-09 |
-------------------------------------
[2018-01-21 14:29:31.219130 UTC] Saving snapshot
[2018-01-21 14:29:31.225359 UTC] Starting iteration 751
[2018-01-21 14:29:31.225555 UTC] Start collecting samples
[2018-01-21 14:29:35.692564 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:35.824945 UTC] Performing policy update
[2018-01-21 14:29:35.826111 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:35.957262 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:37.395588 UTC] Performing line search
[2018-01-21 14:29:37.611640 UTC] Updating baseline
[2018-01-21 14:29:39.830449 UTC] Computing logging information
-------------------------------------
| Iteration            | 751        |
| ExpectedImprovement  | 0.01783    |
| ActualImprovement    | 0.016556   |
| ImprovementRatio     | 0.92854    |
| MeanKL               | 0.0073555  |
| Entropy              | -0.66073   |
| Perplexity           | 0.51647    |
| AveragePolicyStd     | 0.21894    |
| AveragePolicyStd[0]  | 0.22554    |
| AveragePolicyStd[1]  | 0.25199    |
| AveragePolicyStd[2]  | 0.16334    |
| AveragePolicyStd[3]  | 0.22293    |
| AveragePolicyStd[4]  | 0.20265    |
| AveragePolicyStd[5]  | 0.24719    |
| AverageReturn        | 1427.9     |
| MinReturn            | 208.74     |
| MaxReturn            | 1526.7     |
| StdReturn            | 175.45     |
| AverageEpisodeLength | 974.79     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.44     |
| TotalNEpisodes       | 20536      |
| TotalNSamples        | 3.7601e+06 |
| ExplainedVariance    | 0.27162    |
-------------------------------------
[2018-01-21 14:29:40.542116 UTC] Saving snapshot
[2018-01-21 14:29:40.542363 UTC] Starting iteration 752
[2018-01-21 14:29:40.542560 UTC] Start collecting samples
[2018-01-21 14:29:44.949146 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:45.075246 UTC] Performing policy update
[2018-01-21 14:29:45.075871 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:45.194191 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:46.584834 UTC] Performing line search
[2018-01-21 14:29:46.783281 UTC] Updating baseline
[2018-01-21 14:29:48.930166 UTC] Computing logging information
-------------------------------------
| Iteration            | 752        |
| ExpectedImprovement  | 0.017968   |
| ActualImprovement    | 0.017678   |
| ImprovementRatio     | 0.98383    |
| MeanKL               | 0.0078285  |
| Entropy              | -0.66063   |
| Perplexity           | 0.51652    |
| AveragePolicyStd     | 0.21893    |
| AveragePolicyStd[0]  | 0.2259     |
| AveragePolicyStd[1]  | 0.25185    |
| AveragePolicyStd[2]  | 0.16358    |
| AveragePolicyStd[3]  | 0.22247    |
| AveragePolicyStd[4]  | 0.2024     |
| AveragePolicyStd[5]  | 0.24741    |
| AverageReturn        | 1408.2     |
| MinReturn            | 208.74     |
| MaxReturn            | 1526.7     |
| StdReturn            | 226.56     |
| AverageEpisodeLength | 960.97     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.99     |
| TotalNEpisodes       | 20541      |
| TotalNSamples        | 3.7637e+06 |
| ExplainedVariance    | 0.32681    |
-------------------------------------
[2018-01-21 14:29:49.708605 UTC] Saving snapshot
[2018-01-21 14:29:49.708859 UTC] Starting iteration 753
[2018-01-21 14:29:49.709082 UTC] Start collecting samples
[2018-01-21 14:29:54.229817 UTC] Computing input variables for policy optimization
[2018-01-21 14:29:54.369798 UTC] Performing policy update
[2018-01-21 14:29:54.370840 UTC] Computing gradient in Euclidean space
[2018-01-21 14:29:54.496166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:29:55.921656 UTC] Performing line search
[2018-01-21 14:29:56.119724 UTC] Updating baseline
[2018-01-21 14:29:57.884210 UTC] Computing logging information
-------------------------------------
| Iteration            | 753        |
| ExpectedImprovement  | 0.017463   |
| ActualImprovement    | 0.016415   |
| ImprovementRatio     | 0.93996    |
| MeanKL               | 0.007752   |
| Entropy              | -0.66423   |
| Perplexity           | 0.51467    |
| AveragePolicyStd     | 0.21879    |
| AveragePolicyStd[0]  | 0.22578    |
| AveragePolicyStd[1]  | 0.25098    |
| AveragePolicyStd[2]  | 0.16354    |
| AveragePolicyStd[3]  | 0.22245    |
| AveragePolicyStd[4]  | 0.20238    |
| AveragePolicyStd[5]  | 0.24761    |
| AverageReturn        | 1401.8     |
| MinReturn            | 208.74     |
| MaxReturn            | 1543.5     |
| StdReturn            | 235.31     |
| AverageEpisodeLength | 956.65     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.97     |
| TotalNEpisodes       | 20546      |
| TotalNSamples        | 3.7682e+06 |
| ExplainedVariance    | 0.14857    |
-------------------------------------
[2018-01-21 14:29:58.684942 UTC] Saving snapshot
[2018-01-21 14:29:58.685413 UTC] Starting iteration 754
[2018-01-21 14:29:58.685794 UTC] Start collecting samples
[2018-01-21 14:30:03.257020 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:03.375395 UTC] Performing policy update
[2018-01-21 14:30:03.376051 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:03.490938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:04.851888 UTC] Performing line search
[2018-01-21 14:30:05.043621 UTC] Updating baseline
[2018-01-21 14:30:06.917520 UTC] Computing logging information
-------------------------------------
| Iteration            | 754        |
| ExpectedImprovement  | 0.015923   |
| ActualImprovement    | 0.015148   |
| ImprovementRatio     | 0.95133    |
| MeanKL               | 0.0079159  |
| Entropy              | -0.673     |
| Perplexity           | 0.51018    |
| AveragePolicyStd     | 0.21846    |
| AveragePolicyStd[0]  | 0.22522    |
| AveragePolicyStd[1]  | 0.25023    |
| AveragePolicyStd[2]  | 0.16326    |
| AveragePolicyStd[3]  | 0.22269    |
| AveragePolicyStd[4]  | 0.2023     |
| AveragePolicyStd[5]  | 0.24706    |
| AverageReturn        | 1390.4     |
| MinReturn            | 208.74     |
| MaxReturn            | 1543.5     |
| StdReturn            | 261.52     |
| AverageEpisodeLength | 948.66     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.27     |
| TotalNEpisodes       | 20551      |
| TotalNSamples        | 3.7724e+06 |
| ExplainedVariance    | 0.14098    |
-------------------------------------
[2018-01-21 14:30:07.612413 UTC] Saving snapshot
[2018-01-21 14:30:07.612651 UTC] Starting iteration 755
[2018-01-21 14:30:07.612798 UTC] Start collecting samples
[2018-01-21 14:30:12.095988 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:12.244757 UTC] Performing policy update
[2018-01-21 14:30:12.245876 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:12.364631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:13.780844 UTC] Performing line search
[2018-01-21 14:30:13.997166 UTC] Updating baseline
[2018-01-21 14:30:15.892500 UTC] Computing logging information
-------------------------------------
| Iteration            | 755        |
| ExpectedImprovement  | 0.016694   |
| ActualImprovement    | 0.01583    |
| ImprovementRatio     | 0.94827    |
| MeanKL               | 0.0080044  |
| Entropy              | -0.67483   |
| Perplexity           | 0.50924    |
| AveragePolicyStd     | 0.21843    |
| AveragePolicyStd[0]  | 0.22559    |
| AveragePolicyStd[1]  | 0.24988    |
| AveragePolicyStd[2]  | 0.16301    |
| AveragePolicyStd[3]  | 0.22209    |
| AveragePolicyStd[4]  | 0.20175    |
| AveragePolicyStd[5]  | 0.24825    |
| AverageReturn        | 1382.3     |
| MinReturn            | 208.74     |
| MaxReturn            | 1543.5     |
| StdReturn            | 274.58     |
| AverageEpisodeLength | 942.67     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.64     |
| TotalNEpisodes       | 20557      |
| TotalNSamples        | 3.7779e+06 |
| ExplainedVariance    | 0.16452    |
-------------------------------------
[2018-01-21 14:30:16.680976 UTC] Saving snapshot
[2018-01-21 14:30:16.681219 UTC] Starting iteration 756
[2018-01-21 14:30:16.681402 UTC] Start collecting samples
[2018-01-21 14:30:20.974080 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:21.110066 UTC] Performing policy update
[2018-01-21 14:30:21.110827 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:21.233351 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:22.652396 UTC] Performing line search
[2018-01-21 14:30:22.837634 UTC] Updating baseline
[2018-01-21 14:30:24.862128 UTC] Computing logging information
-------------------------------------
| Iteration            | 756        |
| ExpectedImprovement  | 0.017533   |
| ActualImprovement    | 0.016306   |
| ImprovementRatio     | 0.93003    |
| MeanKL               | 0.0075226  |
| Entropy              | -0.68174   |
| Perplexity           | 0.50573    |
| AveragePolicyStd     | 0.21813    |
| AveragePolicyStd[0]  | 0.22489    |
| AveragePolicyStd[1]  | 0.24929    |
| AveragePolicyStd[2]  | 0.16326    |
| AveragePolicyStd[3]  | 0.22169    |
| AveragePolicyStd[4]  | 0.20193    |
| AveragePolicyStd[5]  | 0.24774    |
| AverageReturn        | 1383.8     |
| MinReturn            | 208.74     |
| MaxReturn            | 1543.5     |
| StdReturn            | 275.05     |
| AverageEpisodeLength | 942.67     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.64     |
| TotalNEpisodes       | 20562      |
| TotalNSamples        | 3.7829e+06 |
| ExplainedVariance    | -0.0060397 |
-------------------------------------
[2018-01-21 14:30:25.541668 UTC] Saving snapshot
[2018-01-21 14:30:25.541884 UTC] Starting iteration 757
[2018-01-21 14:30:25.542059 UTC] Start collecting samples
[2018-01-21 14:30:30.119472 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:30.261595 UTC] Performing policy update
[2018-01-21 14:30:30.262318 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:30.379461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:31.791692 UTC] Performing line search
[2018-01-21 14:30:31.982042 UTC] Updating baseline
[2018-01-21 14:30:34.619334 UTC] Computing logging information
------------------------------------
| Iteration            | 757       |
| ExpectedImprovement  | 0.018584  |
| ActualImprovement    | 0.0174    |
| ImprovementRatio     | 0.93627   |
| MeanKL               | 0.0072237 |
| Entropy              | -0.68614  |
| Perplexity           | 0.50352   |
| AveragePolicyStd     | 0.21793   |
| AveragePolicyStd[0]  | 0.22457   |
| AveragePolicyStd[1]  | 0.24847   |
| AveragePolicyStd[2]  | 0.16364   |
| AveragePolicyStd[3]  | 0.22141   |
| AveragePolicyStd[4]  | 0.20183   |
| AveragePolicyStd[5]  | 0.24768   |
| AverageReturn        | 1372.3    |
| MinReturn            | 146.63    |
| MaxReturn            | 1543.5    |
| StdReturn            | 301.79    |
| AverageEpisodeLength | 933.84    |
| MinEpisodeLength     | 117       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 197.43    |
| TotalNEpisodes       | 20568     |
| TotalNSamples        | 3.788e+06 |
| ExplainedVariance    | 0.065643  |
------------------------------------
[2018-01-21 14:30:35.371954 UTC] Saving snapshot
[2018-01-21 14:30:35.372190 UTC] Starting iteration 758
[2018-01-21 14:30:35.372371 UTC] Start collecting samples
[2018-01-21 14:30:39.845079 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:39.984938 UTC] Performing policy update
[2018-01-21 14:30:39.985743 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:40.108908 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:41.574493 UTC] Performing line search
[2018-01-21 14:30:41.782184 UTC] Updating baseline
[2018-01-21 14:30:42.678617 UTC] Computing logging information
------------------------------------
| Iteration            | 758       |
| ExpectedImprovement  | 0.016586  |
| ActualImprovement    | 0.015935  |
| ImprovementRatio     | 0.96073   |
| MeanKL               | 0.0077852 |
| Entropy              | -0.69151  |
| Perplexity           | 0.50082   |
| AveragePolicyStd     | 0.21776   |
| AveragePolicyStd[0]  | 0.22361   |
| AveragePolicyStd[1]  | 0.24878   |
| AveragePolicyStd[2]  | 0.16322   |
| AveragePolicyStd[3]  | 0.2216    |
| AveragePolicyStd[4]  | 0.20175   |
| AveragePolicyStd[5]  | 0.24762   |
| AverageReturn        | 1372.8    |
| MinReturn            | 146.63    |
| MaxReturn            | 1543.5    |
| StdReturn            | 301.87    |
| AverageEpisodeLength | 933.84    |
| MinEpisodeLength     | 117       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 197.43    |
| TotalNEpisodes       | 20574     |
| TotalNSamples        | 3.794e+06 |
| ExplainedVariance    | -0.023793 |
------------------------------------
[2018-01-21 14:30:43.406021 UTC] Saving snapshot
[2018-01-21 14:30:43.406403 UTC] Starting iteration 759
[2018-01-21 14:30:43.406597 UTC] Start collecting samples
[2018-01-21 14:30:47.809281 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:47.943446 UTC] Performing policy update
[2018-01-21 14:30:47.944039 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:48.065115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:49.514906 UTC] Performing line search
[2018-01-21 14:30:49.705947 UTC] Updating baseline
[2018-01-21 14:30:51.477878 UTC] Computing logging information
-------------------------------------
| Iteration            | 759        |
| ExpectedImprovement  | 0.016663   |
| ActualImprovement    | 0.015536   |
| ImprovementRatio     | 0.93238    |
| MeanKL               | 0.0076496  |
| Entropy              | -0.68817   |
| Perplexity           | 0.50249    |
| AveragePolicyStd     | 0.21787    |
| AveragePolicyStd[0]  | 0.22348    |
| AveragePolicyStd[1]  | 0.24889    |
| AveragePolicyStd[2]  | 0.1634     |
| AveragePolicyStd[3]  | 0.22179    |
| AveragePolicyStd[4]  | 0.20215    |
| AveragePolicyStd[5]  | 0.2475     |
| AverageReturn        | 1373.6     |
| MinReturn            | 146.63     |
| MaxReturn            | 1543.5     |
| StdReturn            | 302.04     |
| AverageEpisodeLength | 933.84     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.43     |
| TotalNEpisodes       | 20578      |
| TotalNSamples        | 3.798e+06  |
| ExplainedVariance    | 6.4502e-09 |
-------------------------------------
[2018-01-21 14:30:52.185460 UTC] Saving snapshot
[2018-01-21 14:30:52.185727 UTC] Starting iteration 760
[2018-01-21 14:30:52.185848 UTC] Start collecting samples
[2018-01-21 14:30:56.628111 UTC] Computing input variables for policy optimization
[2018-01-21 14:30:56.756844 UTC] Performing policy update
[2018-01-21 14:30:56.757571 UTC] Computing gradient in Euclidean space
[2018-01-21 14:30:56.889496 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:30:58.319813 UTC] Performing line search
[2018-01-21 14:30:58.517301 UTC] Updating baseline
[2018-01-21 14:31:00.200221 UTC] Computing logging information
-------------------------------------
| Iteration            | 760        |
| ExpectedImprovement  | 0.017988   |
| ActualImprovement    | 0.016615   |
| ImprovementRatio     | 0.92367    |
| MeanKL               | 0.007594   |
| Entropy              | -0.69111   |
| Perplexity           | 0.50102    |
| AveragePolicyStd     | 0.21775    |
| AveragePolicyStd[0]  | 0.22398    |
| AveragePolicyStd[1]  | 0.24808    |
| AveragePolicyStd[2]  | 0.16339    |
| AveragePolicyStd[3]  | 0.22226    |
| AveragePolicyStd[4]  | 0.20177    |
| AveragePolicyStd[5]  | 0.24699    |
| AverageReturn        | 1375.1     |
| MinReturn            | 146.63     |
| MaxReturn            | 1543.5     |
| StdReturn            | 302.5      |
| AverageEpisodeLength | 933.84     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.43     |
| TotalNEpisodes       | 20582      |
| TotalNSamples        | 3.802e+06  |
| ExplainedVariance    | 1.4983e-08 |
-------------------------------------
[2018-01-21 14:31:00.960874 UTC] Saving snapshot
[2018-01-21 14:31:00.970904 UTC] Starting iteration 761
[2018-01-21 14:31:00.971131 UTC] Start collecting samples
[2018-01-21 14:31:05.497532 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:05.637139 UTC] Performing policy update
[2018-01-21 14:31:05.637822 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:05.755817 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:07.190698 UTC] Performing line search
[2018-01-21 14:31:07.381193 UTC] Updating baseline
[2018-01-21 14:31:07.791069 UTC] Computing logging information
--------------------------------------
| Iteration            | 761         |
| ExpectedImprovement  | 0.015872    |
| ActualImprovement    | 0.014996    |
| ImprovementRatio     | 0.94486     |
| MeanKL               | 0.0079253   |
| Entropy              | -0.69274    |
| Perplexity           | 0.5002      |
| AveragePolicyStd     | 0.21772     |
| AveragePolicyStd[0]  | 0.22436     |
| AveragePolicyStd[1]  | 0.24744     |
| AveragePolicyStd[2]  | 0.1628      |
| AveragePolicyStd[3]  | 0.22234     |
| AveragePolicyStd[4]  | 0.20178     |
| AveragePolicyStd[5]  | 0.2476      |
| AverageReturn        | 1382.3      |
| MinReturn            | 146.63      |
| MaxReturn            | 1543.5      |
| StdReturn            | 298.6       |
| AverageEpisodeLength | 937.72      |
| MinEpisodeLength     | 117         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 194.86      |
| TotalNEpisodes       | 20589       |
| TotalNSamples        | 3.809e+06   |
| ExplainedVariance    | -1.7575e-09 |
--------------------------------------
[2018-01-21 14:31:08.497327 UTC] Saving snapshot
[2018-01-21 14:31:08.497640 UTC] Starting iteration 762
[2018-01-21 14:31:08.497873 UTC] Start collecting samples
[2018-01-21 14:31:12.907377 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:13.028506 UTC] Performing policy update
[2018-01-21 14:31:13.029120 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:13.151269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:14.581437 UTC] Performing line search
[2018-01-21 14:31:14.771734 UTC] Updating baseline
[2018-01-21 14:31:17.045791 UTC] Computing logging information
-------------------------------------
| Iteration            | 762        |
| ExpectedImprovement  | 0.019642   |
| ActualImprovement    | 0.017827   |
| ImprovementRatio     | 0.9076     |
| MeanKL               | 0.0072155  |
| Entropy              | -0.69937   |
| Perplexity           | 0.4969     |
| AveragePolicyStd     | 0.2175     |
| AveragePolicyStd[0]  | 0.22451    |
| AveragePolicyStd[1]  | 0.24759    |
| AveragePolicyStd[2]  | 0.16292    |
| AveragePolicyStd[3]  | 0.22123    |
| AveragePolicyStd[4]  | 0.20055    |
| AveragePolicyStd[5]  | 0.24822    |
| AverageReturn        | 1382.8     |
| MinReturn            | 146.63     |
| MaxReturn            | 1543.5     |
| StdReturn            | 298.72     |
| AverageEpisodeLength | 937.48     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.8      |
| TotalNEpisodes       | 20594      |
| TotalNSamples        | 3.8139e+06 |
| ExplainedVariance    | 0.096705   |
-------------------------------------
[2018-01-21 14:31:17.737722 UTC] Saving snapshot
[2018-01-21 14:31:17.737968 UTC] Starting iteration 763
[2018-01-21 14:31:17.738186 UTC] Start collecting samples
[2018-01-21 14:31:22.376767 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:22.494793 UTC] Performing policy update
[2018-01-21 14:31:22.495482 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:22.615928 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:24.047532 UTC] Performing line search
[2018-01-21 14:31:24.243207 UTC] Updating baseline
[2018-01-21 14:31:26.219001 UTC] Computing logging information
-------------------------------------
| Iteration            | 763        |
| ExpectedImprovement  | 0.018967   |
| ActualImprovement    | 0.017465   |
| ImprovementRatio     | 0.9208     |
| MeanKL               | 0.0070044  |
| Entropy              | -0.70774   |
| Perplexity           | 0.49276    |
| AveragePolicyStd     | 0.21727    |
| AveragePolicyStd[0]  | 0.22476    |
| AveragePolicyStd[1]  | 0.24748    |
| AveragePolicyStd[2]  | 0.16205    |
| AveragePolicyStd[3]  | 0.22131    |
| AveragePolicyStd[4]  | 0.19955    |
| AveragePolicyStd[5]  | 0.24845    |
| AverageReturn        | 1383.5     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 299.13     |
| AverageEpisodeLength | 937.48     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.8      |
| TotalNEpisodes       | 20597      |
| TotalNSamples        | 3.8169e+06 |
| ExplainedVariance    | -0.0081536 |
-------------------------------------
[2018-01-21 14:31:26.913067 UTC] Saving snapshot
[2018-01-21 14:31:26.913283 UTC] Starting iteration 764
[2018-01-21 14:31:26.913411 UTC] Start collecting samples
[2018-01-21 14:31:31.664142 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:31.794987 UTC] Performing policy update
[2018-01-21 14:31:31.795633 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:31.927557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:33.378709 UTC] Performing line search
[2018-01-21 14:31:33.577524 UTC] Updating baseline
[2018-01-21 14:31:36.852280 UTC] Computing logging information
-------------------------------------
| Iteration            | 764        |
| ExpectedImprovement  | 0.015698   |
| ActualImprovement    | 0.015057   |
| ImprovementRatio     | 0.95915    |
| MeanKL               | 0.0078183  |
| Entropy              | -0.72114   |
| Perplexity           | 0.4862     |
| AveragePolicyStd     | 0.21677    |
| AveragePolicyStd[0]  | 0.2244     |
| AveragePolicyStd[1]  | 0.24611    |
| AveragePolicyStd[2]  | 0.16181    |
| AveragePolicyStd[3]  | 0.22117    |
| AveragePolicyStd[4]  | 0.19898    |
| AveragePolicyStd[5]  | 0.24814    |
| AverageReturn        | 1385.8     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 299.96     |
| AverageEpisodeLength | 937.48     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.8      |
| TotalNEpisodes       | 20604      |
| TotalNSamples        | 3.8239e+06 |
| ExplainedVariance    | 0.00013476 |
-------------------------------------
[2018-01-21 14:31:37.582678 UTC] Saving snapshot
[2018-01-21 14:31:37.582913 UTC] Starting iteration 765
[2018-01-21 14:31:37.583058 UTC] Start collecting samples
[2018-01-21 14:31:42.152993 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:42.284187 UTC] Performing policy update
[2018-01-21 14:31:42.285308 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:42.415501 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:43.815752 UTC] Performing line search
[2018-01-21 14:31:44.009568 UTC] Updating baseline
[2018-01-21 14:31:45.893153 UTC] Computing logging information
-------------------------------------
| Iteration            | 765        |
| ExpectedImprovement  | 0.018382   |
| ActualImprovement    | 0.017458   |
| ImprovementRatio     | 0.94971    |
| MeanKL               | 0.0074989  |
| Entropy              | -0.72036   |
| Perplexity           | 0.48658    |
| AveragePolicyStd     | 0.21678    |
| AveragePolicyStd[0]  | 0.22433    |
| AveragePolicyStd[1]  | 0.24629    |
| AveragePolicyStd[2]  | 0.16201    |
| AveragePolicyStd[3]  | 0.22108    |
| AveragePolicyStd[4]  | 0.19907    |
| AveragePolicyStd[5]  | 0.24791    |
| AverageReturn        | 1386       |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 300.13     |
| AverageEpisodeLength | 937        |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.7      |
| TotalNEpisodes       | 20609      |
| TotalNSamples        | 3.8289e+06 |
| ExplainedVariance    | 0.18208    |
-------------------------------------
[2018-01-21 14:31:46.579338 UTC] Saving snapshot
[2018-01-21 14:31:46.579552 UTC] Starting iteration 766
[2018-01-21 14:31:46.579691 UTC] Start collecting samples
[2018-01-21 14:31:51.059377 UTC] Computing input variables for policy optimization
[2018-01-21 14:31:51.209943 UTC] Performing policy update
[2018-01-21 14:31:51.210545 UTC] Computing gradient in Euclidean space
[2018-01-21 14:31:51.334638 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:31:52.763136 UTC] Performing line search
[2018-01-21 14:31:52.959350 UTC] Updating baseline
[2018-01-21 14:31:55.080084 UTC] Computing logging information
-------------------------------------
| Iteration            | 766        |
| ExpectedImprovement  | 0.018291   |
| ActualImprovement    | 0.016767   |
| ImprovementRatio     | 0.9167     |
| MeanKL               | 0.0074075  |
| Entropy              | -0.72345   |
| Perplexity           | 0.48508    |
| AveragePolicyStd     | 0.21669    |
| AveragePolicyStd[0]  | 0.22398    |
| AveragePolicyStd[1]  | 0.24624    |
| AveragePolicyStd[2]  | 0.16183    |
| AveragePolicyStd[3]  | 0.22102    |
| AveragePolicyStd[4]  | 0.19879    |
| AveragePolicyStd[5]  | 0.24828    |
| AverageReturn        | 1379.3     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 307.41     |
| AverageEpisodeLength | 932.15     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.07     |
| TotalNEpisodes       | 20613      |
| TotalNSamples        | 3.8324e+06 |
| ExplainedVariance    | 0.16143    |
-------------------------------------
[2018-01-21 14:31:55.796567 UTC] Saving snapshot
[2018-01-21 14:31:55.796868 UTC] Starting iteration 767
[2018-01-21 14:31:55.797168 UTC] Start collecting samples
[2018-01-21 14:32:00.150745 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:00.294344 UTC] Performing policy update
[2018-01-21 14:32:00.294975 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:00.424807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:01.872966 UTC] Performing line search
[2018-01-21 14:32:02.076683 UTC] Updating baseline
[2018-01-21 14:32:04.080029 UTC] Computing logging information
-------------------------------------
| Iteration            | 767        |
| ExpectedImprovement  | 0.018323   |
| ActualImprovement    | 0.017243   |
| ImprovementRatio     | 0.94102    |
| MeanKL               | 0.007567   |
| Entropy              | -0.71887   |
| Perplexity           | 0.4873     |
| AveragePolicyStd     | 0.21687    |
| AveragePolicyStd[0]  | 0.22417    |
| AveragePolicyStd[1]  | 0.24653    |
| AveragePolicyStd[2]  | 0.16186    |
| AveragePolicyStd[3]  | 0.22187    |
| AveragePolicyStd[4]  | 0.19855    |
| AveragePolicyStd[5]  | 0.24822    |
| AverageReturn        | 1380.4     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 307.8      |
| AverageEpisodeLength | 932.15     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.07     |
| TotalNEpisodes       | 20618      |
| TotalNSamples        | 3.8374e+06 |
| ExplainedVariance    | -0.0074759 |
-------------------------------------
[2018-01-21 14:32:04.844092 UTC] Saving snapshot
[2018-01-21 14:32:04.844322 UTC] Starting iteration 768
[2018-01-21 14:32:04.844464 UTC] Start collecting samples
[2018-01-21 14:32:09.428062 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:09.576222 UTC] Performing policy update
[2018-01-21 14:32:09.577145 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:09.689169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:11.100632 UTC] Performing line search
[2018-01-21 14:32:11.302577 UTC] Updating baseline
[2018-01-21 14:32:13.339551 UTC] Computing logging information
-------------------------------------
| Iteration            | 768        |
| ExpectedImprovement  | 0.017277   |
| ActualImprovement    | 0.016908   |
| ImprovementRatio     | 0.97862    |
| MeanKL               | 0.0074253  |
| Entropy              | -0.7196    |
| Perplexity           | 0.48695    |
| AveragePolicyStd     | 0.21685    |
| AveragePolicyStd[0]  | 0.22429    |
| AveragePolicyStd[1]  | 0.2462     |
| AveragePolicyStd[2]  | 0.16168    |
| AveragePolicyStd[3]  | 0.22177    |
| AveragePolicyStd[4]  | 0.19856    |
| AveragePolicyStd[5]  | 0.24862    |
| AverageReturn        | 1369.7     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 326.1      |
| AverageEpisodeLength | 924.3      |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.34     |
| TotalNEpisodes       | 20625      |
| TotalNSamples        | 3.8436e+06 |
| ExplainedVariance    | 0.083296   |
-------------------------------------
[2018-01-21 14:32:14.049368 UTC] Saving snapshot
[2018-01-21 14:32:14.049767 UTC] Starting iteration 769
[2018-01-21 14:32:14.050030 UTC] Start collecting samples
[2018-01-21 14:32:18.488534 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:18.627610 UTC] Performing policy update
[2018-01-21 14:32:18.628711 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:18.748858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:20.200649 UTC] Performing line search
[2018-01-21 14:32:20.395418 UTC] Updating baseline
[2018-01-21 14:32:22.698948 UTC] Computing logging information
-------------------------------------
| Iteration            | 769        |
| ExpectedImprovement  | 0.019836   |
| ActualImprovement    | 0.018021   |
| ImprovementRatio     | 0.90848    |
| MeanKL               | 0.0071003  |
| Entropy              | -0.70852   |
| Perplexity           | 0.49237    |
| AveragePolicyStd     | 0.21723    |
| AveragePolicyStd[0]  | 0.22507    |
| AveragePolicyStd[1]  | 0.24628    |
| AveragePolicyStd[2]  | 0.16222    |
| AveragePolicyStd[3]  | 0.22218    |
| AveragePolicyStd[4]  | 0.19882    |
| AveragePolicyStd[5]  | 0.2488     |
| AverageReturn        | 1375.7     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 321.55     |
| AverageEpisodeLength | 928.47     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 208.02     |
| TotalNEpisodes       | 20629      |
| TotalNSamples        | 3.8476e+06 |
| ExplainedVariance    | 0.2046     |
-------------------------------------
[2018-01-21 14:32:23.450044 UTC] Saving snapshot
[2018-01-21 14:32:23.450322 UTC] Starting iteration 770
[2018-01-21 14:32:23.450522 UTC] Start collecting samples
[2018-01-21 14:32:28.015440 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:28.172021 UTC] Performing policy update
[2018-01-21 14:32:28.172649 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:28.294145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:29.695367 UTC] Performing line search
[2018-01-21 14:32:29.887267 UTC] Updating baseline
[2018-01-21 14:32:31.651541 UTC] Computing logging information
-------------------------------------
| Iteration            | 770        |
| ExpectedImprovement  | 0.018412   |
| ActualImprovement    | 0.017729   |
| ImprovementRatio     | 0.9629     |
| MeanKL               | 0.0075971  |
| Entropy              | -0.71517   |
| Perplexity           | 0.48911    |
| AveragePolicyStd     | 0.21704    |
| AveragePolicyStd[0]  | 0.22501    |
| AveragePolicyStd[1]  | 0.24643    |
| AveragePolicyStd[2]  | 0.1614     |
| AveragePolicyStd[3]  | 0.22258    |
| AveragePolicyStd[4]  | 0.19834    |
| AveragePolicyStd[5]  | 0.24848    |
| AverageReturn        | 1391.6     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 294.47     |
| AverageEpisodeLength | 938.14     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.5      |
| TotalNEpisodes       | 20635      |
| TotalNSamples        | 3.8533e+06 |
| ExplainedVariance    | 0.29881    |
-------------------------------------
[2018-01-21 14:32:32.342714 UTC] Saving snapshot
[2018-01-21 14:32:32.348958 UTC] Starting iteration 771
[2018-01-21 14:32:32.349157 UTC] Start collecting samples
[2018-01-21 14:32:37.067020 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:37.206800 UTC] Performing policy update
[2018-01-21 14:32:37.207794 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:37.329497 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:38.741013 UTC] Performing line search
[2018-01-21 14:32:38.944421 UTC] Updating baseline
[2018-01-21 14:32:40.831478 UTC] Computing logging information
-------------------------------------
| Iteration            | 771        |
| ExpectedImprovement  | 0.018579   |
| ActualImprovement    | 0.017852   |
| ImprovementRatio     | 0.96088    |
| MeanKL               | 0.0073979  |
| Entropy              | -0.72273   |
| Perplexity           | 0.48542    |
| AveragePolicyStd     | 0.21677    |
| AveragePolicyStd[0]  | 0.22509    |
| AveragePolicyStd[1]  | 0.24536    |
| AveragePolicyStd[2]  | 0.1611     |
| AveragePolicyStd[3]  | 0.22256    |
| AveragePolicyStd[4]  | 0.19797    |
| AveragePolicyStd[5]  | 0.24855    |
| AverageReturn        | 1410.7     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 265.32     |
| AverageEpisodeLength | 949.6      |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.15     |
| TotalNEpisodes       | 20642      |
| TotalNSamples        | 3.8596e+06 |
| ExplainedVariance    | 0.20658    |
-------------------------------------
[2018-01-21 14:32:41.521545 UTC] Saving snapshot
[2018-01-21 14:32:41.521723 UTC] Starting iteration 772
[2018-01-21 14:32:41.521835 UTC] Start collecting samples
[2018-01-21 14:32:45.971701 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:46.105726 UTC] Performing policy update
[2018-01-21 14:32:46.106318 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:46.217304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:47.615088 UTC] Performing line search
[2018-01-21 14:32:47.803991 UTC] Updating baseline
[2018-01-21 14:32:49.578761 UTC] Computing logging information
-------------------------------------
| Iteration            | 772        |
| ExpectedImprovement  | 0.014921   |
| ActualImprovement    | 0.014445   |
| ImprovementRatio     | 0.96809    |
| MeanKL               | 0.0076471  |
| Entropy              | -0.72751   |
| Perplexity           | 0.48311    |
| AveragePolicyStd     | 0.21662    |
| AveragePolicyStd[0]  | 0.22505    |
| AveragePolicyStd[1]  | 0.24528    |
| AveragePolicyStd[2]  | 0.16037    |
| AveragePolicyStd[3]  | 0.22255    |
| AveragePolicyStd[4]  | 0.19844    |
| AveragePolicyStd[5]  | 0.24804    |
| AverageReturn        | 1418.9     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 257.91     |
| AverageEpisodeLength | 953.92     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.86     |
| TotalNEpisodes       | 20645      |
| TotalNSamples        | 3.8626e+06 |
| ExplainedVariance    | -0.0086627 |
-------------------------------------
[2018-01-21 14:32:50.277207 UTC] Saving snapshot
[2018-01-21 14:32:50.277450 UTC] Starting iteration 773
[2018-01-21 14:32:50.277604 UTC] Start collecting samples
[2018-01-21 14:32:54.870652 UTC] Computing input variables for policy optimization
[2018-01-21 14:32:54.995326 UTC] Performing policy update
[2018-01-21 14:32:54.995933 UTC] Computing gradient in Euclidean space
[2018-01-21 14:32:55.112582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:32:56.499949 UTC] Performing line search
[2018-01-21 14:32:56.692495 UTC] Updating baseline
[2018-01-21 14:32:58.459735 UTC] Computing logging information
-------------------------------------
| Iteration            | 773        |
| ExpectedImprovement  | 0.017522   |
| ActualImprovement    | 0.016742   |
| ImprovementRatio     | 0.95551    |
| MeanKL               | 0.0074775  |
| Entropy              | -0.72681   |
| Perplexity           | 0.48345    |
| AveragePolicyStd     | 0.21665    |
| AveragePolicyStd[0]  | 0.2252     |
| AveragePolicyStd[1]  | 0.24574    |
| AveragePolicyStd[2]  | 0.15994    |
| AveragePolicyStd[3]  | 0.22237    |
| AveragePolicyStd[4]  | 0.19931    |
| AveragePolicyStd[5]  | 0.24735    |
| AverageReturn        | 1422.5     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 243.17     |
| AverageEpisodeLength | 954.34     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157        |
| TotalNEpisodes       | 20654      |
| TotalNSamples        | 3.8703e+06 |
| ExplainedVariance    | 0.19182    |
-------------------------------------
[2018-01-21 14:32:59.245827 UTC] Saving snapshot
[2018-01-21 14:32:59.246084 UTC] Starting iteration 774
[2018-01-21 14:32:59.246266 UTC] Start collecting samples
[2018-01-21 14:33:03.609943 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:03.741639 UTC] Performing policy update
[2018-01-21 14:33:03.742425 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:03.864677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:05.231691 UTC] Performing line search
[2018-01-21 14:33:05.420223 UTC] Updating baseline
[2018-01-21 14:33:07.676848 UTC] Computing logging information
-------------------------------------
| Iteration            | 774        |
| ExpectedImprovement  | 0.015348   |
| ActualImprovement    | 0.014331   |
| ImprovementRatio     | 0.93376    |
| MeanKL               | 0.0082844  |
| Entropy              | -0.72909   |
| Perplexity           | 0.48235    |
| AveragePolicyStd     | 0.21656    |
| AveragePolicyStd[0]  | 0.22545    |
| AveragePolicyStd[1]  | 0.24576    |
| AveragePolicyStd[2]  | 0.16017    |
| AveragePolicyStd[3]  | 0.22229    |
| AveragePolicyStd[4]  | 0.19861    |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1424.8     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 243.76     |
| AverageEpisodeLength | 954.34     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157        |
| TotalNEpisodes       | 20659      |
| TotalNSamples        | 3.8753e+06 |
| ExplainedVariance    | -0.0019861 |
-------------------------------------
[2018-01-21 14:33:08.403558 UTC] Saving snapshot
[2018-01-21 14:33:08.403804 UTC] Starting iteration 775
[2018-01-21 14:33:08.403984 UTC] Start collecting samples
[2018-01-21 14:33:12.790573 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:12.920057 UTC] Performing policy update
[2018-01-21 14:33:12.920665 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:13.042117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:14.446278 UTC] Performing line search
[2018-01-21 14:33:14.637398 UTC] Updating baseline
[2018-01-21 14:33:17.913701 UTC] Computing logging information
-------------------------------------
| Iteration            | 775        |
| ExpectedImprovement  | 0.019155   |
| ActualImprovement    | 0.017297   |
| ImprovementRatio     | 0.90297    |
| MeanKL               | 0.0071172  |
| Entropy              | -0.73271   |
| Perplexity           | 0.4806     |
| AveragePolicyStd     | 0.21648    |
| AveragePolicyStd[0]  | 0.22534    |
| AveragePolicyStd[1]  | 0.24585    |
| AveragePolicyStd[2]  | 0.1598     |
| AveragePolicyStd[3]  | 0.22268    |
| AveragePolicyStd[4]  | 0.19768    |
| AveragePolicyStd[5]  | 0.24753    |
| AverageReturn        | 1423.1     |
| MinReturn            | 146.63     |
| MaxReturn            | 1576       |
| StdReturn            | 243.64     |
| AverageEpisodeLength | 953.43     |
| MinEpisodeLength     | 117        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157        |
| TotalNEpisodes       | 20661      |
| TotalNSamples        | 3.8772e+06 |
| ExplainedVariance    | 0.19154    |
-------------------------------------
[2018-01-21 14:33:18.667448 UTC] Saving snapshot
[2018-01-21 14:33:18.667701 UTC] Starting iteration 776
[2018-01-21 14:33:18.667878 UTC] Start collecting samples
[2018-01-21 14:33:23.204841 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:23.351916 UTC] Performing policy update
[2018-01-21 14:33:23.352675 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:23.477814 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:24.888336 UTC] Performing line search
[2018-01-21 14:33:25.074867 UTC] Updating baseline
[2018-01-21 14:33:26.843717 UTC] Computing logging information
-------------------------------------
| Iteration            | 776        |
| ExpectedImprovement  | 0.016922   |
| ActualImprovement    | 0.016329   |
| ImprovementRatio     | 0.96495    |
| MeanKL               | 0.0074701  |
| Entropy              | -0.73482   |
| Perplexity           | 0.47959    |
| AveragePolicyStd     | 0.21642    |
| AveragePolicyStd[0]  | 0.22552    |
| AveragePolicyStd[1]  | 0.24566    |
| AveragePolicyStd[2]  | 0.15928    |
| AveragePolicyStd[3]  | 0.22303    |
| AveragePolicyStd[4]  | 0.19786    |
| AveragePolicyStd[5]  | 0.24719    |
| AverageReturn        | 1437.8     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 207.61     |
| AverageEpisodeLength | 962.26     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.65     |
| TotalNEpisodes       | 20668      |
| TotalNSamples        | 3.8842e+06 |
| ExplainedVariance    | 0.016492   |
-------------------------------------
[2018-01-21 14:33:27.533183 UTC] Saving snapshot
[2018-01-21 14:33:27.533383 UTC] Starting iteration 777
[2018-01-21 14:33:27.533548 UTC] Start collecting samples
[2018-01-21 14:33:32.014287 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:32.158357 UTC] Performing policy update
[2018-01-21 14:33:32.158996 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:32.275918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:33.753128 UTC] Performing line search
[2018-01-21 14:33:33.948001 UTC] Updating baseline
[2018-01-21 14:33:35.939234 UTC] Computing logging information
-------------------------------------
| Iteration            | 777        |
| ExpectedImprovement  | 0.017549   |
| ActualImprovement    | 0.016727   |
| ImprovementRatio     | 0.95315    |
| MeanKL               | 0.0072988  |
| Entropy              | -0.74486   |
| Perplexity           | 0.4748     |
| AveragePolicyStd     | 0.21608    |
| AveragePolicyStd[0]  | 0.22471    |
| AveragePolicyStd[1]  | 0.2453     |
| AveragePolicyStd[2]  | 0.15894    |
| AveragePolicyStd[3]  | 0.22311    |
| AveragePolicyStd[4]  | 0.19718    |
| AveragePolicyStd[5]  | 0.24727    |
| AverageReturn        | 1438.4     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 208.83     |
| AverageEpisodeLength | 961.25     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.74     |
| TotalNEpisodes       | 20674      |
| TotalNSamples        | 3.8901e+06 |
| ExplainedVariance    | 0.098756   |
-------------------------------------
[2018-01-21 14:33:36.723863 UTC] Saving snapshot
[2018-01-21 14:33:36.724094 UTC] Starting iteration 778
[2018-01-21 14:33:36.724235 UTC] Start collecting samples
[2018-01-21 14:33:41.371084 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:41.499273 UTC] Performing policy update
[2018-01-21 14:33:41.499955 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:41.623480 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:43.057648 UTC] Performing line search
[2018-01-21 14:33:43.244369 UTC] Updating baseline
[2018-01-21 14:33:45.232266 UTC] Computing logging information
-------------------------------------
| Iteration            | 778        |
| ExpectedImprovement  | 0.017093   |
| ActualImprovement    | 0.015966   |
| ImprovementRatio     | 0.93409    |
| MeanKL               | 0.0078309  |
| Entropy              | -0.74551   |
| Perplexity           | 0.47449    |
| AveragePolicyStd     | 0.21604    |
| AveragePolicyStd[0]  | 0.22461    |
| AveragePolicyStd[1]  | 0.24489    |
| AveragePolicyStd[2]  | 0.15922    |
| AveragePolicyStd[3]  | 0.22376    |
| AveragePolicyStd[4]  | 0.19681    |
| AveragePolicyStd[5]  | 0.24694    |
| AverageReturn        | 1439.5     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 209.16     |
| AverageEpisodeLength | 961.25     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.74     |
| TotalNEpisodes       | 20677      |
| TotalNSamples        | 3.8931e+06 |
| ExplainedVariance    | -0.0001541 |
-------------------------------------
[2018-01-21 14:33:45.937238 UTC] Saving snapshot
[2018-01-21 14:33:45.937477 UTC] Starting iteration 779
[2018-01-21 14:33:45.937629 UTC] Start collecting samples
[2018-01-21 14:33:50.266580 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:50.391607 UTC] Performing policy update
[2018-01-21 14:33:50.392750 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:50.519317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:33:51.910352 UTC] Performing line search
[2018-01-21 14:33:52.103522 UTC] Updating baseline
[2018-01-21 14:33:54.288067 UTC] Computing logging information
-------------------------------------
| Iteration            | 779        |
| ExpectedImprovement  | 0.017666   |
| ActualImprovement    | 0.016777   |
| ImprovementRatio     | 0.94965    |
| MeanKL               | 0.0075281  |
| Entropy              | -0.74592   |
| Perplexity           | 0.4743     |
| AveragePolicyStd     | 0.21599    |
| AveragePolicyStd[0]  | 0.22456    |
| AveragePolicyStd[1]  | 0.24469    |
| AveragePolicyStd[2]  | 0.15964    |
| AveragePolicyStd[3]  | 0.22389    |
| AveragePolicyStd[4]  | 0.1965     |
| AveragePolicyStd[5]  | 0.24669    |
| AverageReturn        | 1439.8     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 209.32     |
| AverageEpisodeLength | 960.91     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.68     |
| TotalNEpisodes       | 20682      |
| TotalNSamples        | 3.8981e+06 |
| ExplainedVariance    | 0.12758    |
-------------------------------------
[2018-01-21 14:33:55.016863 UTC] Saving snapshot
[2018-01-21 14:33:55.017099 UTC] Starting iteration 780
[2018-01-21 14:33:55.017250 UTC] Start collecting samples
[2018-01-21 14:33:59.465468 UTC] Computing input variables for policy optimization
[2018-01-21 14:33:59.592793 UTC] Performing policy update
[2018-01-21 14:33:59.594030 UTC] Computing gradient in Euclidean space
[2018-01-21 14:33:59.712252 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:01.209749 UTC] Performing line search
[2018-01-21 14:34:01.400301 UTC] Updating baseline
[2018-01-21 14:34:06.691928 UTC] Computing logging information
-------------------------------------
| Iteration            | 780        |
| ExpectedImprovement  | 0.014642   |
| ActualImprovement    | 0.014338   |
| ImprovementRatio     | 0.97922    |
| MeanKL               | 0.0075652  |
| Entropy              | -0.74705   |
| Perplexity           | 0.47376    |
| AveragePolicyStd     | 0.21595    |
| AveragePolicyStd[0]  | 0.22461    |
| AveragePolicyStd[1]  | 0.24398    |
| AveragePolicyStd[2]  | 0.15963    |
| AveragePolicyStd[3]  | 0.22441    |
| AveragePolicyStd[4]  | 0.19621    |
| AveragePolicyStd[5]  | 0.24688    |
| AverageReturn        | 1441.1     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 209.81     |
| AverageEpisodeLength | 960.91     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.68     |
| TotalNEpisodes       | 20688      |
| TotalNSamples        | 3.9041e+06 |
| ExplainedVariance    | -0.024093  |
-------------------------------------
[2018-01-21 14:34:07.378518 UTC] Saving snapshot
[2018-01-21 14:34:07.384716 UTC] Starting iteration 781
[2018-01-21 14:34:07.384895 UTC] Start collecting samples
[2018-01-21 14:34:11.923110 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:12.041956 UTC] Performing policy update
[2018-01-21 14:34:12.042810 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:12.164012 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:13.543440 UTC] Performing line search
[2018-01-21 14:34:13.743049 UTC] Updating baseline
[2018-01-21 14:34:15.575891 UTC] Computing logging information
-------------------------------------
| Iteration            | 781        |
| ExpectedImprovement  | 0.015716   |
| ActualImprovement    | 0.014661   |
| ImprovementRatio     | 0.93283    |
| MeanKL               | 0.0081136  |
| Entropy              | -0.74524   |
| Perplexity           | 0.47462    |
| AveragePolicyStd     | 0.21605    |
| AveragePolicyStd[0]  | 0.22423    |
| AveragePolicyStd[1]  | 0.24412    |
| AveragePolicyStd[2]  | 0.15941    |
| AveragePolicyStd[3]  | 0.2243     |
| AveragePolicyStd[4]  | 0.19638    |
| AveragePolicyStd[5]  | 0.24784    |
| AverageReturn        | 1442       |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 210.01     |
| AverageEpisodeLength | 961.15     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.73     |
| TotalNEpisodes       | 20692      |
| TotalNSamples        | 3.9081e+06 |
| ExplainedVariance    | 8.2148e-09 |
-------------------------------------
[2018-01-21 14:34:16.344427 UTC] Saving snapshot
[2018-01-21 14:34:16.344662 UTC] Starting iteration 782
[2018-01-21 14:34:16.344843 UTC] Start collecting samples
[2018-01-21 14:34:20.829779 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:20.946722 UTC] Performing policy update
[2018-01-21 14:34:20.947589 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:21.066247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:22.458263 UTC] Performing line search
[2018-01-21 14:34:22.668755 UTC] Updating baseline
[2018-01-21 14:34:25.132897 UTC] Computing logging information
-------------------------------------
| Iteration            | 782        |
| ExpectedImprovement  | 0.019478   |
| ActualImprovement    | 0.017771   |
| ImprovementRatio     | 0.9124     |
| MeanKL               | 0.0078337  |
| Entropy              | -0.74391   |
| Perplexity           | 0.47525    |
| AveragePolicyStd     | 0.21609    |
| AveragePolicyStd[0]  | 0.2249     |
| AveragePolicyStd[1]  | 0.24422    |
| AveragePolicyStd[2]  | 0.15942    |
| AveragePolicyStd[3]  | 0.22355    |
| AveragePolicyStd[4]  | 0.19669    |
| AveragePolicyStd[5]  | 0.24777    |
| AverageReturn        | 1438.8     |
| MinReturn            | 306.53     |
| MaxReturn            | 1576       |
| StdReturn            | 214.49     |
| AverageEpisodeLength | 957.87     |
| MinEpisodeLength     | 215        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.75     |
| TotalNEpisodes       | 20696      |
| TotalNSamples        | 3.9117e+06 |
| ExplainedVariance    | 0.12387    |
-------------------------------------
[2018-01-21 14:34:25.874473 UTC] Saving snapshot
[2018-01-21 14:34:25.874715 UTC] Starting iteration 783
[2018-01-21 14:34:25.874885 UTC] Start collecting samples
[2018-01-21 14:34:30.456245 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:30.585770 UTC] Performing policy update
[2018-01-21 14:34:30.586838 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:30.703817 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:32.081812 UTC] Performing line search
[2018-01-21 14:34:32.279807 UTC] Updating baseline
[2018-01-21 14:34:33.997632 UTC] Computing logging information
-------------------------------------
| Iteration            | 783        |
| ExpectedImprovement  | 0.019407   |
| ActualImprovement    | 0.01847    |
| ImprovementRatio     | 0.95175    |
| MeanKL               | 0.0072964  |
| Entropy              | -0.74798   |
| Perplexity           | 0.47332    |
| AveragePolicyStd     | 0.21596    |
| AveragePolicyStd[0]  | 0.22542    |
| AveragePolicyStd[1]  | 0.24468    |
| AveragePolicyStd[2]  | 0.15923    |
| AveragePolicyStd[3]  | 0.22344    |
| AveragePolicyStd[4]  | 0.19609    |
| AveragePolicyStd[5]  | 0.2469     |
| AverageReturn        | 1403.8     |
| MinReturn            | 262.21     |
| MaxReturn            | 1554.5     |
| StdReturn            | 284.86     |
| AverageEpisodeLength | 935.49     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.07     |
| TotalNEpisodes       | 20706      |
| TotalNSamples        | 3.9195e+06 |
| ExplainedVariance    | 0.22915    |
-------------------------------------
[2018-01-21 14:34:34.717939 UTC] Saving snapshot
[2018-01-21 14:34:34.718153 UTC] Starting iteration 784
[2018-01-21 14:34:34.718336 UTC] Start collecting samples
[2018-01-21 14:34:39.325458 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:39.446702 UTC] Performing policy update
[2018-01-21 14:34:39.447305 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:39.566976 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:40.963129 UTC] Performing line search
[2018-01-21 14:34:41.162411 UTC] Updating baseline
[2018-01-21 14:34:42.999677 UTC] Computing logging information
-------------------------------------
| Iteration            | 784        |
| ExpectedImprovement  | 0.018894   |
| ActualImprovement    | 0.018391   |
| ImprovementRatio     | 0.97337    |
| MeanKL               | 0.0074183  |
| Entropy              | -0.75636   |
| Perplexity           | 0.46937    |
| AveragePolicyStd     | 0.21568    |
| AveragePolicyStd[0]  | 0.22519    |
| AveragePolicyStd[1]  | 0.24384    |
| AveragePolicyStd[2]  | 0.15863    |
| AveragePolicyStd[3]  | 0.22376    |
| AveragePolicyStd[4]  | 0.19574    |
| AveragePolicyStd[5]  | 0.24695    |
| AverageReturn        | 1407.5     |
| MinReturn            | 262.21     |
| MaxReturn            | 1557.6     |
| StdReturn            | 285.92     |
| AverageEpisodeLength | 935.97     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.18     |
| TotalNEpisodes       | 20710      |
| TotalNSamples        | 3.9235e+06 |
| ExplainedVariance    | -0.071553  |
-------------------------------------
[2018-01-21 14:34:43.733323 UTC] Saving snapshot
[2018-01-21 14:34:43.733609 UTC] Starting iteration 785
[2018-01-21 14:34:43.733812 UTC] Start collecting samples
[2018-01-21 14:34:48.178633 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:48.300691 UTC] Performing policy update
[2018-01-21 14:34:48.301290 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:48.421338 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:49.854949 UTC] Performing line search
[2018-01-21 14:34:50.048865 UTC] Updating baseline
[2018-01-21 14:34:52.170781 UTC] Computing logging information
-------------------------------------
| Iteration            | 785        |
| ExpectedImprovement  | 0.019056   |
| ActualImprovement    | 0.017847   |
| ImprovementRatio     | 0.93656    |
| MeanKL               | 0.0071469  |
| Entropy              | -0.76159   |
| Perplexity           | 0.46692    |
| AveragePolicyStd     | 0.2155     |
| AveragePolicyStd[0]  | 0.22473    |
| AveragePolicyStd[1]  | 0.24324    |
| AveragePolicyStd[2]  | 0.15845    |
| AveragePolicyStd[3]  | 0.22397    |
| AveragePolicyStd[4]  | 0.1955     |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1409.8     |
| MinReturn            | 262.21     |
| MaxReturn            | 1557.6     |
| StdReturn            | 281.08     |
| AverageEpisodeLength | 937.31     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.54     |
| TotalNEpisodes       | 20713      |
| TotalNSamples        | 3.9261e+06 |
| ExplainedVariance    | 0.2469     |
-------------------------------------
[2018-01-21 14:34:52.889184 UTC] Saving snapshot
[2018-01-21 14:34:52.889401 UTC] Starting iteration 786
[2018-01-21 14:34:52.889629 UTC] Start collecting samples
[2018-01-21 14:34:57.323996 UTC] Computing input variables for policy optimization
[2018-01-21 14:34:57.449531 UTC] Performing policy update
[2018-01-21 14:34:57.450355 UTC] Computing gradient in Euclidean space
[2018-01-21 14:34:57.567099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:34:58.959720 UTC] Performing line search
[2018-01-21 14:34:59.146342 UTC] Updating baseline
[2018-01-21 14:35:00.785817 UTC] Computing logging information
-------------------------------------
| Iteration            | 786        |
| ExpectedImprovement  | 0.015479   |
| ActualImprovement    | 0.015098   |
| ImprovementRatio     | 0.9754     |
| MeanKL               | 0.0074957  |
| Entropy              | -0.75996   |
| Perplexity           | 0.46768    |
| AveragePolicyStd     | 0.21559    |
| AveragePolicyStd[0]  | 0.22554    |
| AveragePolicyStd[1]  | 0.24345    |
| AveragePolicyStd[2]  | 0.15832    |
| AveragePolicyStd[3]  | 0.22382    |
| AveragePolicyStd[4]  | 0.19494    |
| AveragePolicyStd[5]  | 0.24748    |
| AverageReturn        | 1420.2     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 261.84     |
| AverageEpisodeLength | 942.17     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.9      |
| TotalNEpisodes       | 20722      |
| TotalNSamples        | 3.9348e+06 |
| ExplainedVariance    | 0.044397   |
-------------------------------------
[2018-01-21 14:35:01.523343 UTC] Saving snapshot
[2018-01-21 14:35:01.523664 UTC] Starting iteration 787
[2018-01-21 14:35:01.523863 UTC] Start collecting samples
[2018-01-21 14:35:06.282309 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:06.413721 UTC] Performing policy update
[2018-01-21 14:35:06.414627 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:06.529464 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:07.959189 UTC] Performing line search
[2018-01-21 14:35:08.144834 UTC] Updating baseline
[2018-01-21 14:35:10.118126 UTC] Computing logging information
-------------------------------------
| Iteration            | 787        |
| ExpectedImprovement  | 0.017484   |
| ActualImprovement    | 0.016549   |
| ImprovementRatio     | 0.94653    |
| MeanKL               | 0.0072177  |
| Entropy              | -0.75867   |
| Perplexity           | 0.46829    |
| AveragePolicyStd     | 0.21568    |
| AveragePolicyStd[0]  | 0.22635    |
| AveragePolicyStd[1]  | 0.24481    |
| AveragePolicyStd[2]  | 0.15812    |
| AveragePolicyStd[3]  | 0.22318    |
| AveragePolicyStd[4]  | 0.1945     |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1419       |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 265.36     |
| AverageEpisodeLength | 939.43     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.72     |
| TotalNEpisodes       | 20727      |
| TotalNSamples        | 3.9395e+06 |
| ExplainedVariance    | 0.10495    |
-------------------------------------
[2018-01-21 14:35:10.827304 UTC] Saving snapshot
[2018-01-21 14:35:10.827544 UTC] Starting iteration 788
[2018-01-21 14:35:10.827747 UTC] Start collecting samples
[2018-01-21 14:35:15.264743 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:15.401573 UTC] Performing policy update
[2018-01-21 14:35:15.402339 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:15.521794 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:16.893052 UTC] Performing line search
[2018-01-21 14:35:17.084132 UTC] Updating baseline
[2018-01-21 14:35:18.860021 UTC] Computing logging information
-------------------------------------
| Iteration            | 788        |
| ExpectedImprovement  | 0.017432   |
| ActualImprovement    | 0.01613    |
| ImprovementRatio     | 0.92531    |
| MeanKL               | 0.0078337  |
| Entropy              | -0.75188   |
| Perplexity           | 0.47148    |
| AveragePolicyStd     | 0.2159     |
| AveragePolicyStd[0]  | 0.22578    |
| AveragePolicyStd[1]  | 0.24577    |
| AveragePolicyStd[2]  | 0.15893    |
| AveragePolicyStd[3]  | 0.22301    |
| AveragePolicyStd[4]  | 0.19435    |
| AveragePolicyStd[5]  | 0.24758    |
| AverageReturn        | 1400.8     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 292.06     |
| AverageEpisodeLength | 927.56     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.29     |
| TotalNEpisodes       | 20730      |
| TotalNSamples        | 3.9414e+06 |
| ExplainedVariance    | 0.46428    |
-------------------------------------
[2018-01-21 14:35:19.544514 UTC] Saving snapshot
[2018-01-21 14:35:19.544714 UTC] Starting iteration 789
[2018-01-21 14:35:19.544859 UTC] Start collecting samples
[2018-01-21 14:35:24.192319 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:24.340300 UTC] Performing policy update
[2018-01-21 14:35:24.341386 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:24.470201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:25.879403 UTC] Performing line search
[2018-01-21 14:35:26.075382 UTC] Updating baseline
[2018-01-21 14:35:27.845511 UTC] Computing logging information
-------------------------------------
| Iteration            | 789        |
| ExpectedImprovement  | 0.016708   |
| ActualImprovement    | 0.016335   |
| ImprovementRatio     | 0.97764    |
| MeanKL               | 0.0084317  |
| Entropy              | -0.7578    |
| Perplexity           | 0.4687     |
| AveragePolicyStd     | 0.21568    |
| AveragePolicyStd[0]  | 0.22521    |
| AveragePolicyStd[1]  | 0.24596    |
| AveragePolicyStd[2]  | 0.15865    |
| AveragePolicyStd[3]  | 0.22212    |
| AveragePolicyStd[4]  | 0.19509    |
| AveragePolicyStd[5]  | 0.24702    |
| AverageReturn        | 1409.7     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 288.26     |
| AverageEpisodeLength | 932.55     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.86     |
| TotalNEpisodes       | 20740      |
| TotalNSamples        | 3.9509e+06 |
| ExplainedVariance    | 0.063657   |
-------------------------------------
[2018-01-21 14:35:28.576185 UTC] Saving snapshot
[2018-01-21 14:35:28.576431 UTC] Starting iteration 790
[2018-01-21 14:35:28.576588 UTC] Start collecting samples
[2018-01-21 14:35:33.157056 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:33.293127 UTC] Performing policy update
[2018-01-21 14:35:33.293823 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:33.414564 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:34.869249 UTC] Performing line search
[2018-01-21 14:35:35.063217 UTC] Updating baseline
[2018-01-21 14:35:36.696038 UTC] Computing logging information
-------------------------------------
| Iteration            | 790        |
| ExpectedImprovement  | 0.016263   |
| ActualImprovement    | 0.014366   |
| ImprovementRatio     | 0.88334    |
| MeanKL               | 0.0075935  |
| Entropy              | -0.75519   |
| Perplexity           | 0.46992    |
| AveragePolicyStd     | 0.21571    |
| AveragePolicyStd[0]  | 0.22552    |
| AveragePolicyStd[1]  | 0.24513    |
| AveragePolicyStd[2]  | 0.159      |
| AveragePolicyStd[3]  | 0.22216    |
| AveragePolicyStd[4]  | 0.19594    |
| AveragePolicyStd[5]  | 0.24652    |
| AverageReturn        | 1410.2     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 288.47     |
| AverageEpisodeLength | 932.55     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.86     |
| TotalNEpisodes       | 20743      |
| TotalNSamples        | 3.9539e+06 |
| ExplainedVariance    | -0.012692  |
-------------------------------------
[2018-01-21 14:35:37.374147 UTC] Saving snapshot
[2018-01-21 14:35:37.384556 UTC] Starting iteration 791
[2018-01-21 14:35:37.384792 UTC] Start collecting samples
[2018-01-21 14:35:41.894396 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:42.023359 UTC] Performing policy update
[2018-01-21 14:35:42.023979 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:42.140184 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:43.509720 UTC] Performing line search
[2018-01-21 14:35:43.700173 UTC] Updating baseline
[2018-01-21 14:35:47.106043 UTC] Computing logging information
-------------------------------------
| Iteration            | 791        |
| ExpectedImprovement  | 0.014223   |
| ActualImprovement    | 0.01381    |
| ImprovementRatio     | 0.97093    |
| MeanKL               | 0.0081834  |
| Entropy              | -0.74752   |
| Perplexity           | 0.47354    |
| AveragePolicyStd     | 0.21593    |
| AveragePolicyStd[0]  | 0.22678    |
| AveragePolicyStd[1]  | 0.24466    |
| AveragePolicyStd[2]  | 0.15955    |
| AveragePolicyStd[3]  | 0.22221    |
| AveragePolicyStd[4]  | 0.19646    |
| AveragePolicyStd[5]  | 0.24594    |
| AverageReturn        | 1411.3     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 288.85     |
| AverageEpisodeLength | 932.55     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.86     |
| TotalNEpisodes       | 20746      |
| TotalNSamples        | 3.9569e+06 |
| ExplainedVariance    | 0.0030071  |
-------------------------------------
[2018-01-21 14:35:47.884038 UTC] Saving snapshot
[2018-01-21 14:35:47.884267 UTC] Starting iteration 792
[2018-01-21 14:35:47.884409 UTC] Start collecting samples
[2018-01-21 14:35:52.409891 UTC] Computing input variables for policy optimization
[2018-01-21 14:35:52.565676 UTC] Performing policy update
[2018-01-21 14:35:52.567127 UTC] Computing gradient in Euclidean space
[2018-01-21 14:35:52.685348 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:35:54.046025 UTC] Performing line search
[2018-01-21 14:35:54.231313 UTC] Updating baseline
[2018-01-21 14:35:56.201561 UTC] Computing logging information
-------------------------------------
| Iteration            | 792        |
| ExpectedImprovement  | 0.018012   |
| ActualImprovement    | 0.016744   |
| ImprovementRatio     | 0.92959    |
| MeanKL               | 0.0077967  |
| Entropy              | -0.74284   |
| Perplexity           | 0.47576    |
| AveragePolicyStd     | 0.21606    |
| AveragePolicyStd[0]  | 0.22733    |
| AveragePolicyStd[1]  | 0.24406    |
| AveragePolicyStd[2]  | 0.15994    |
| AveragePolicyStd[3]  | 0.22274    |
| AveragePolicyStd[4]  | 0.19683    |
| AveragePolicyStd[5]  | 0.24546    |
| AverageReturn        | 1434       |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 265.42     |
| AverageEpisodeLength | 946.11     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.18     |
| TotalNEpisodes       | 20753      |
| TotalNSamples        | 3.9639e+06 |
| ExplainedVariance    | 0.016085   |
-------------------------------------
[2018-01-21 14:35:56.959623 UTC] Saving snapshot
[2018-01-21 14:35:56.959866 UTC] Starting iteration 793
[2018-01-21 14:35:56.960025 UTC] Start collecting samples
[2018-01-21 14:36:01.539841 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:01.670206 UTC] Performing policy update
[2018-01-21 14:36:01.670758 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:01.786317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:03.182998 UTC] Performing line search
[2018-01-21 14:36:03.382378 UTC] Updating baseline
[2018-01-21 14:36:05.620892 UTC] Computing logging information
-------------------------------------
| Iteration            | 793        |
| ExpectedImprovement  | 0.017161   |
| ActualImprovement    | 0.016448   |
| ImprovementRatio     | 0.95846    |
| MeanKL               | 0.007512   |
| Entropy              | -0.74401   |
| Perplexity           | 0.4752     |
| AveragePolicyStd     | 0.21601    |
| AveragePolicyStd[0]  | 0.22707    |
| AveragePolicyStd[1]  | 0.24365    |
| AveragePolicyStd[2]  | 0.16011    |
| AveragePolicyStd[3]  | 0.22221    |
| AveragePolicyStd[4]  | 0.19676    |
| AveragePolicyStd[5]  | 0.24626    |
| AverageReturn        | 1434.1     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 265.5      |
| AverageEpisodeLength | 946.11     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.18     |
| TotalNEpisodes       | 20759      |
| TotalNSamples        | 3.9699e+06 |
| ExplainedVariance    | -0.21012   |
-------------------------------------
[2018-01-21 14:36:06.388259 UTC] Saving snapshot
[2018-01-21 14:36:06.388491 UTC] Starting iteration 794
[2018-01-21 14:36:06.388642 UTC] Start collecting samples
[2018-01-21 14:36:10.786244 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:10.917289 UTC] Performing policy update
[2018-01-21 14:36:10.918532 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:11.035332 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:12.396779 UTC] Performing line search
[2018-01-21 14:36:12.581816 UTC] Updating baseline
[2018-01-21 14:36:14.186294 UTC] Computing logging information
-------------------------------------
| Iteration            | 794        |
| ExpectedImprovement  | 0.016809   |
| ActualImprovement    | 0.015634   |
| ImprovementRatio     | 0.93014    |
| MeanKL               | 0.0075153  |
| Entropy              | -0.76112   |
| Perplexity           | 0.46714    |
| AveragePolicyStd     | 0.21541    |
| AveragePolicyStd[0]  | 0.22639    |
| AveragePolicyStd[1]  | 0.24294    |
| AveragePolicyStd[2]  | 0.15966    |
| AveragePolicyStd[3]  | 0.22187    |
| AveragePolicyStd[4]  | 0.1957     |
| AveragePolicyStd[5]  | 0.24591    |
| AverageReturn        | 1435.5     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 265.45     |
| AverageEpisodeLength | 947.02     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.23     |
| TotalNEpisodes       | 20761      |
| TotalNSamples        | 3.9719e+06 |
| ExplainedVariance    | -0.0037735 |
-------------------------------------
[2018-01-21 14:36:14.938221 UTC] Saving snapshot
[2018-01-21 14:36:14.938452 UTC] Starting iteration 795
[2018-01-21 14:36:14.938660 UTC] Start collecting samples
[2018-01-21 14:36:19.502884 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:19.628480 UTC] Performing policy update
[2018-01-21 14:36:19.629538 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:19.746129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:21.107716 UTC] Performing line search
[2018-01-21 14:36:21.293858 UTC] Updating baseline
[2018-01-21 14:36:23.194286 UTC] Computing logging information
-------------------------------------
| Iteration            | 795        |
| ExpectedImprovement  | 0.018666   |
| ActualImprovement    | 0.017232   |
| ImprovementRatio     | 0.9232     |
| MeanKL               | 0.0074407  |
| Entropy              | -0.76841   |
| Perplexity           | 0.46375    |
| AveragePolicyStd     | 0.21518    |
| AveragePolicyStd[0]  | 0.22625    |
| AveragePolicyStd[1]  | 0.24316    |
| AveragePolicyStd[2]  | 0.15922    |
| AveragePolicyStd[3]  | 0.22097    |
| AveragePolicyStd[4]  | 0.19547    |
| AveragePolicyStd[5]  | 0.24602    |
| AverageReturn        | 1428.2     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 274.02     |
| AverageEpisodeLength | 942.37     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.11     |
| TotalNEpisodes       | 20769      |
| TotalNSamples        | 3.9794e+06 |
| ExplainedVariance    | 0.064577   |
-------------------------------------
[2018-01-21 14:36:23.978761 UTC] Saving snapshot
[2018-01-21 14:36:23.978996 UTC] Starting iteration 796
[2018-01-21 14:36:23.979140 UTC] Start collecting samples
[2018-01-21 14:36:28.318414 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:28.471603 UTC] Performing policy update
[2018-01-21 14:36:28.472385 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:28.589367 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:30.012231 UTC] Performing line search
[2018-01-21 14:36:30.208039 UTC] Updating baseline
[2018-01-21 14:36:33.187037 UTC] Computing logging information
-------------------------------------
| Iteration            | 796        |
| ExpectedImprovement  | 0.017249   |
| ActualImprovement    | 0.016421   |
| ImprovementRatio     | 0.95202    |
| MeanKL               | 0.0076833  |
| Entropy              | -0.76958   |
| Perplexity           | 0.46321    |
| AveragePolicyStd     | 0.21513    |
| AveragePolicyStd[0]  | 0.22616    |
| AveragePolicyStd[1]  | 0.24274    |
| AveragePolicyStd[2]  | 0.15939    |
| AveragePolicyStd[3]  | 0.22072    |
| AveragePolicyStd[4]  | 0.19546    |
| AveragePolicyStd[5]  | 0.24629    |
| AverageReturn        | 1431.8     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 274.21     |
| AverageEpisodeLength | 943.38     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.15     |
| TotalNEpisodes       | 20775      |
| TotalNSamples        | 3.9854e+06 |
| ExplainedVariance    | -0.0028934 |
-------------------------------------
[2018-01-21 14:36:33.878286 UTC] Saving snapshot
[2018-01-21 14:36:33.878515 UTC] Starting iteration 797
[2018-01-21 14:36:33.878673 UTC] Start collecting samples
[2018-01-21 14:36:38.249960 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:38.378993 UTC] Performing policy update
[2018-01-21 14:36:38.380089 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:38.507518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:39.890849 UTC] Performing line search
[2018-01-21 14:36:40.092060 UTC] Updating baseline
[2018-01-21 14:36:42.945038 UTC] Computing logging information
-------------------------------------
| Iteration            | 797        |
| ExpectedImprovement  | 0.017165   |
| ActualImprovement    | 0.014716   |
| ImprovementRatio     | 0.85733    |
| MeanKL               | 0.0073191  |
| Entropy              | -0.77461   |
| Perplexity           | 0.46088    |
| AveragePolicyStd     | 0.2149     |
| AveragePolicyStd[0]  | 0.22571    |
| AveragePolicyStd[1]  | 0.24209    |
| AveragePolicyStd[2]  | 0.15951    |
| AveragePolicyStd[3]  | 0.22091    |
| AveragePolicyStd[4]  | 0.19566    |
| AveragePolicyStd[5]  | 0.24556    |
| AverageReturn        | 1432.2     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 274.36     |
| AverageEpisodeLength | 943.38     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.15     |
| TotalNEpisodes       | 20776      |
| TotalNSamples        | 3.9864e+06 |
| ExplainedVariance    | 0.00015911 |
-------------------------------------
[2018-01-21 14:36:43.668630 UTC] Saving snapshot
[2018-01-21 14:36:43.668896 UTC] Starting iteration 798
[2018-01-21 14:36:43.669068 UTC] Start collecting samples
[2018-01-21 14:36:48.269341 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:48.429765 UTC] Performing policy update
[2018-01-21 14:36:48.430871 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:48.534321 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:49.910268 UTC] Performing line search
[2018-01-21 14:36:50.106998 UTC] Updating baseline
[2018-01-21 14:36:52.733896 UTC] Computing logging information
-------------------------------------
| Iteration            | 798        |
| ExpectedImprovement  | 0.016646   |
| ActualImprovement    | 0.015584   |
| ImprovementRatio     | 0.93622    |
| MeanKL               | 0.007915   |
| Entropy              | -0.77941   |
| Perplexity           | 0.45868    |
| AveragePolicyStd     | 0.2147     |
| AveragePolicyStd[0]  | 0.22557    |
| AveragePolicyStd[1]  | 0.24219    |
| AveragePolicyStd[2]  | 0.15973    |
| AveragePolicyStd[3]  | 0.22117    |
| AveragePolicyStd[4]  | 0.19524    |
| AveragePolicyStd[5]  | 0.24431    |
| AverageReturn        | 1433.9     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 274.8      |
| AverageEpisodeLength | 943.72     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.23     |
| TotalNEpisodes       | 20782      |
| TotalNSamples        | 3.9924e+06 |
| ExplainedVariance    | -0.0077236 |
-------------------------------------
[2018-01-21 14:36:53.464637 UTC] Saving snapshot
[2018-01-21 14:36:53.465021 UTC] Starting iteration 799
[2018-01-21 14:36:53.465253 UTC] Start collecting samples
[2018-01-21 14:36:58.174003 UTC] Computing input variables for policy optimization
[2018-01-21 14:36:58.300831 UTC] Performing policy update
[2018-01-21 14:36:58.301426 UTC] Computing gradient in Euclidean space
[2018-01-21 14:36:58.428446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:36:59.836119 UTC] Performing line search
[2018-01-21 14:37:00.042036 UTC] Updating baseline
[2018-01-21 14:37:02.373539 UTC] Computing logging information
-------------------------------------
| Iteration            | 799        |
| ExpectedImprovement  | 0.018415   |
| ActualImprovement    | 0.017564   |
| ImprovementRatio     | 0.95375    |
| MeanKL               | 0.0076337  |
| Entropy              | -0.77554   |
| Perplexity           | 0.46046    |
| AveragePolicyStd     | 0.21487    |
| AveragePolicyStd[0]  | 0.22658    |
| AveragePolicyStd[1]  | 0.24152    |
| AveragePolicyStd[2]  | 0.1593     |
| AveragePolicyStd[3]  | 0.22149    |
| AveragePolicyStd[4]  | 0.19559    |
| AveragePolicyStd[5]  | 0.24474    |
| AverageReturn        | 1427.2     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 286.53     |
| AverageEpisodeLength | 938.09     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.39     |
| TotalNEpisodes       | 20790      |
| TotalNSamples        | 3.9999e+06 |
| ExplainedVariance    | 0.061245   |
-------------------------------------
[2018-01-21 14:37:03.141653 UTC] Saving snapshot
[2018-01-21 14:37:03.141888 UTC] Starting iteration 800
[2018-01-21 14:37:03.142035 UTC] Start collecting samples
[2018-01-21 14:37:07.572938 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:07.698017 UTC] Performing policy update
[2018-01-21 14:37:07.698633 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:07.812351 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:09.178124 UTC] Performing line search
[2018-01-21 14:37:09.372107 UTC] Updating baseline
[2018-01-21 14:37:11.260491 UTC] Computing logging information
-------------------------------------
| Iteration            | 800        |
| ExpectedImprovement  | 0.017211   |
| ActualImprovement    | 0.01643    |
| ImprovementRatio     | 0.95462    |
| MeanKL               | 0.0084057  |
| Entropy              | -0.77286   |
| Perplexity           | 0.46169    |
| AveragePolicyStd     | 0.21498    |
| AveragePolicyStd[0]  | 0.22699    |
| AveragePolicyStd[1]  | 0.24096    |
| AveragePolicyStd[2]  | 0.15933    |
| AveragePolicyStd[3]  | 0.22162    |
| AveragePolicyStd[4]  | 0.19526    |
| AveragePolicyStd[5]  | 0.24571    |
| AverageReturn        | 1418.8     |
| MinReturn            | 262.21     |
| MaxReturn            | 1598.9     |
| StdReturn            | 302.6      |
| AverageEpisodeLength | 934.07     |
| MinEpisodeLength     | 199        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.58     |
| TotalNEpisodes       | 20794      |
| TotalNSamples        | 4.0031e+06 |
| ExplainedVariance    | 0.1922     |
-------------------------------------
[2018-01-21 14:37:12.051561 UTC] Saving snapshot
[2018-01-21 14:37:12.060732 UTC] Starting iteration 801
[2018-01-21 14:37:12.060920 UTC] Start collecting samples
[2018-01-21 14:37:16.729098 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:16.895346 UTC] Performing policy update
[2018-01-21 14:37:16.896018 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:17.012207 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:18.412724 UTC] Performing line search
[2018-01-21 14:37:18.602197 UTC] Updating baseline
[2018-01-21 14:37:20.515158 UTC] Computing logging information
-------------------------------------
| Iteration            | 801        |
| ExpectedImprovement  | 0.018988   |
| ActualImprovement    | 0.018712   |
| ImprovementRatio     | 0.9855     |
| MeanKL               | 0.007398   |
| Entropy              | -0.76964   |
| Perplexity           | 0.46318    |
| AveragePolicyStd     | 0.21511    |
| AveragePolicyStd[0]  | 0.22708    |
| AveragePolicyStd[1]  | 0.24157    |
| AveragePolicyStd[2]  | 0.15929    |
| AveragePolicyStd[3]  | 0.22205    |
| AveragePolicyStd[4]  | 0.19527    |
| AveragePolicyStd[5]  | 0.24538    |
| AverageReturn        | 1414.4     |
| MinReturn            | 348.92     |
| MaxReturn            | 1598.9     |
| StdReturn            | 292.64     |
| AverageEpisodeLength | 931.44     |
| MinEpisodeLength     | 267        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.83     |
| TotalNEpisodes       | 20802      |
| TotalNSamples        | 4.0101e+06 |
| ExplainedVariance    | 0.31305    |
-------------------------------------
[2018-01-21 14:37:21.199047 UTC] Saving snapshot
[2018-01-21 14:37:21.199411 UTC] Starting iteration 802
[2018-01-21 14:37:21.199661 UTC] Start collecting samples
[2018-01-21 14:37:25.785302 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:25.945448 UTC] Performing policy update
[2018-01-21 14:37:25.946647 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:26.068961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:27.454729 UTC] Performing line search
[2018-01-21 14:37:27.640484 UTC] Updating baseline
[2018-01-21 14:37:29.407823 UTC] Computing logging information
-------------------------------------
| Iteration            | 802        |
| ExpectedImprovement  | 0.019344   |
| ActualImprovement    | 0.018586   |
| ImprovementRatio     | 0.96084    |
| MeanKL               | 0.0072475  |
| Entropy              | -0.77438   |
| Perplexity           | 0.46099    |
| AveragePolicyStd     | 0.21491    |
| AveragePolicyStd[0]  | 0.22662    |
| AveragePolicyStd[1]  | 0.24067    |
| AveragePolicyStd[2]  | 0.15938    |
| AveragePolicyStd[3]  | 0.22204    |
| AveragePolicyStd[4]  | 0.19532    |
| AveragePolicyStd[5]  | 0.24543    |
| AverageReturn        | 1407.5     |
| MinReturn            | 261.77     |
| MaxReturn            | 1598.9     |
| StdReturn            | 300.56     |
| AverageEpisodeLength | 927.22     |
| MinEpisodeLength     | 216        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.96     |
| TotalNEpisodes       | 20810      |
| TotalNSamples        | 4.0162e+06 |
| ExplainedVariance    | 0.20984    |
-------------------------------------
[2018-01-21 14:37:30.107669 UTC] Saving snapshot
[2018-01-21 14:37:30.107912 UTC] Starting iteration 803
[2018-01-21 14:37:30.108069 UTC] Start collecting samples
[2018-01-21 14:37:34.559132 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:34.683763 UTC] Performing policy update
[2018-01-21 14:37:34.684831 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:34.805517 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:36.217473 UTC] Performing line search
[2018-01-21 14:37:36.416720 UTC] Updating baseline
[2018-01-21 14:37:38.153766 UTC] Computing logging information
-------------------------------------
| Iteration            | 803        |
| ExpectedImprovement  | 0.021548   |
| ActualImprovement    | 0.018769   |
| ImprovementRatio     | 0.87103    |
| MeanKL               | 0.0071971  |
| Entropy              | -0.78261   |
| Perplexity           | 0.45721    |
| AveragePolicyStd     | 0.21463    |
| AveragePolicyStd[0]  | 0.2264     |
| AveragePolicyStd[1]  | 0.24014    |
| AveragePolicyStd[2]  | 0.15903    |
| AveragePolicyStd[3]  | 0.22257    |
| AveragePolicyStd[4]  | 0.19476    |
| AveragePolicyStd[5]  | 0.24486    |
| AverageReturn        | 1391.5     |
| MinReturn            | 261.77     |
| MaxReturn            | 1598.9     |
| StdReturn            | 328.73     |
| AverageEpisodeLength | 916.64     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.43     |
| TotalNEpisodes       | 20813      |
| TotalNSamples        | 4.0178e+06 |
| ExplainedVariance    | 0.48206    |
-------------------------------------
[2018-01-21 14:37:38.851868 UTC] Saving snapshot
[2018-01-21 14:37:38.852168 UTC] Starting iteration 804
[2018-01-21 14:37:38.852385 UTC] Start collecting samples
[2018-01-21 14:37:43.487600 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:43.620915 UTC] Performing policy update
[2018-01-21 14:37:43.621582 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:43.750496 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:45.228666 UTC] Performing line search
[2018-01-21 14:37:45.420006 UTC] Updating baseline
[2018-01-21 14:37:47.500259 UTC] Computing logging information
-------------------------------------
| Iteration            | 804        |
| ExpectedImprovement  | 0.017872   |
| ActualImprovement    | 0.017044   |
| ImprovementRatio     | 0.95365    |
| MeanKL               | 0.007444   |
| Entropy              | -0.78187   |
| Perplexity           | 0.45755    |
| AveragePolicyStd     | 0.21467    |
| AveragePolicyStd[0]  | 0.22701    |
| AveragePolicyStd[1]  | 0.24042    |
| AveragePolicyStd[2]  | 0.1588     |
| AveragePolicyStd[3]  | 0.22228    |
| AveragePolicyStd[4]  | 0.19472    |
| AveragePolicyStd[5]  | 0.24481    |
| AverageReturn        | 1396.1     |
| MinReturn            | 261.77     |
| MaxReturn            | 1575.7     |
| StdReturn            | 327.29     |
| AverageEpisodeLength | 919.63     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.45     |
| TotalNEpisodes       | 20817      |
| TotalNSamples        | 4.0218e+06 |
| ExplainedVariance    | -0.044346  |
-------------------------------------
[2018-01-21 14:37:48.255804 UTC] Saving snapshot
[2018-01-21 14:37:48.256045 UTC] Starting iteration 805
[2018-01-21 14:37:48.256200 UTC] Start collecting samples
[2018-01-21 14:37:52.776293 UTC] Computing input variables for policy optimization
[2018-01-21 14:37:52.913877 UTC] Performing policy update
[2018-01-21 14:37:52.914620 UTC] Computing gradient in Euclidean space
[2018-01-21 14:37:53.049098 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:37:54.459845 UTC] Performing line search
[2018-01-21 14:37:54.653872 UTC] Updating baseline
[2018-01-21 14:37:57.732246 UTC] Computing logging information
--------------------------------------
| Iteration            | 805         |
| ExpectedImprovement  | 0.017074    |
| ActualImprovement    | 0.016324    |
| ImprovementRatio     | 0.95604     |
| MeanKL               | 0.0076576   |
| Entropy              | -0.78709    |
| Perplexity           | 0.45517     |
| AveragePolicyStd     | 0.21447     |
| AveragePolicyStd[0]  | 0.22679     |
| AveragePolicyStd[1]  | 0.2407      |
| AveragePolicyStd[2]  | 0.15876     |
| AveragePolicyStd[3]  | 0.22215     |
| AveragePolicyStd[4]  | 0.19481     |
| AveragePolicyStd[5]  | 0.24359     |
| AverageReturn        | 1402.7      |
| MinReturn            | 261.77      |
| MaxReturn            | 1589.8      |
| StdReturn            | 326.19      |
| AverageEpisodeLength | 922.69      |
| MinEpisodeLength     | 194         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 203.33      |
| TotalNEpisodes       | 20826       |
| TotalNSamples        | 4.0308e+06  |
| ExplainedVariance    | -3.3272e-05 |
--------------------------------------
[2018-01-21 14:37:58.454858 UTC] Saving snapshot
[2018-01-21 14:37:58.455197 UTC] Starting iteration 806
[2018-01-21 14:37:58.455359 UTC] Start collecting samples
[2018-01-21 14:38:02.877786 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:03.024331 UTC] Performing policy update
[2018-01-21 14:38:03.024990 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:03.144197 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:04.573862 UTC] Performing line search
[2018-01-21 14:38:04.768941 UTC] Updating baseline
[2018-01-21 14:38:07.609392 UTC] Computing logging information
-------------------------------------
| Iteration            | 806        |
| ExpectedImprovement  | 0.016603   |
| ActualImprovement    | 0.015848   |
| ImprovementRatio     | 0.95453    |
| MeanKL               | 0.007647   |
| Entropy              | -0.79302   |
| Perplexity           | 0.45248    |
| AveragePolicyStd     | 0.21429    |
| AveragePolicyStd[0]  | 0.22666    |
| AveragePolicyStd[1]  | 0.2409     |
| AveragePolicyStd[2]  | 0.15844    |
| AveragePolicyStd[3]  | 0.22163    |
| AveragePolicyStd[4]  | 0.19416    |
| AveragePolicyStd[5]  | 0.24395    |
| AverageReturn        | 1409.7     |
| MinReturn            | 261.77     |
| MaxReturn            | 1589.8     |
| StdReturn            | 320.6      |
| AverageEpisodeLength | 927.33     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.72     |
| TotalNEpisodes       | 20829      |
| TotalNSamples        | 4.0338e+06 |
| ExplainedVariance    | 8.2051e-05 |
-------------------------------------
[2018-01-21 14:38:08.303568 UTC] Saving snapshot
[2018-01-21 14:38:08.303816 UTC] Starting iteration 807
[2018-01-21 14:38:08.303971 UTC] Start collecting samples
[2018-01-21 14:38:12.797602 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:12.920822 UTC] Performing policy update
[2018-01-21 14:38:12.921555 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:13.044139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:14.512466 UTC] Performing line search
[2018-01-21 14:38:14.711214 UTC] Updating baseline
[2018-01-21 14:38:16.640138 UTC] Computing logging information
-------------------------------------
| Iteration            | 807        |
| ExpectedImprovement  | 0.015458   |
| ActualImprovement    | 0.014732   |
| ImprovementRatio     | 0.953      |
| MeanKL               | 0.0075922  |
| Entropy              | -0.79181   |
| Perplexity           | 0.45302    |
| AveragePolicyStd     | 0.21433    |
| AveragePolicyStd[0]  | 0.22617    |
| AveragePolicyStd[1]  | 0.24135    |
| AveragePolicyStd[2]  | 0.15833    |
| AveragePolicyStd[3]  | 0.22179    |
| AveragePolicyStd[4]  | 0.19469    |
| AveragePolicyStd[5]  | 0.24364    |
| AverageReturn        | 1428.9     |
| MinReturn            | 261.77     |
| MaxReturn            | 1589.8     |
| StdReturn            | 295.98     |
| AverageEpisodeLength | 939.1      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.85     |
| TotalNEpisodes       | 20832      |
| TotalNSamples        | 4.0368e+06 |
| ExplainedVariance    | -0.027268  |
-------------------------------------
[2018-01-21 14:38:17.421961 UTC] Saving snapshot
[2018-01-21 14:38:17.422206 UTC] Starting iteration 808
[2018-01-21 14:38:17.422351 UTC] Start collecting samples
[2018-01-21 14:38:21.763625 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:21.889737 UTC] Performing policy update
[2018-01-21 14:38:21.890498 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:22.005684 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:23.417339 UTC] Performing line search
[2018-01-21 14:38:23.615403 UTC] Updating baseline
[2018-01-21 14:38:26.787342 UTC] Computing logging information
-------------------------------------
| Iteration            | 808        |
| ExpectedImprovement  | 0.019404   |
| ActualImprovement    | 0.018151   |
| ImprovementRatio     | 0.93545    |
| MeanKL               | 0.0079701  |
| Entropy              | -0.77672   |
| Perplexity           | 0.45991    |
| AveragePolicyStd     | 0.21486    |
| AveragePolicyStd[0]  | 0.22698    |
| AveragePolicyStd[1]  | 0.24202    |
| AveragePolicyStd[2]  | 0.15878    |
| AveragePolicyStd[3]  | 0.22245    |
| AveragePolicyStd[4]  | 0.19531    |
| AveragePolicyStd[5]  | 0.24359    |
| AverageReturn        | 1429.3     |
| MinReturn            | 261.77     |
| MaxReturn            | 1589.8     |
| StdReturn            | 296.18     |
| AverageEpisodeLength | 939.1      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.85     |
| TotalNEpisodes       | 20839      |
| TotalNSamples        | 4.0438e+06 |
| ExplainedVariance    | 8.3583e-05 |
-------------------------------------
[2018-01-21 14:38:27.484886 UTC] Saving snapshot
[2018-01-21 14:38:27.485065 UTC] Starting iteration 809
[2018-01-21 14:38:27.485207 UTC] Start collecting samples
[2018-01-21 14:38:31.997267 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:32.141200 UTC] Performing policy update
[2018-01-21 14:38:32.141879 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:32.257042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:33.673132 UTC] Performing line search
[2018-01-21 14:38:33.858457 UTC] Updating baseline
[2018-01-21 14:38:35.871354 UTC] Computing logging information
------------------------------------
| Iteration            | 809       |
| ExpectedImprovement  | 0.017756  |
| ActualImprovement    | 0.016881  |
| ImprovementRatio     | 0.95069   |
| MeanKL               | 0.0075402 |
| Entropy              | -0.7735   |
| Perplexity           | 0.46139   |
| AveragePolicyStd     | 0.21493   |
| AveragePolicyStd[0]  | 0.22696   |
| AveragePolicyStd[1]  | 0.24212   |
| AveragePolicyStd[2]  | 0.15916   |
| AveragePolicyStd[3]  | 0.22294   |
| AveragePolicyStd[4]  | 0.19564   |
| AveragePolicyStd[5]  | 0.24277   |
| AverageReturn        | 1399.8    |
| MinReturn            | 218.98    |
| MaxReturn            | 1589.8    |
| StdReturn            | 334.82    |
| AverageEpisodeLength | 921.33    |
| MinEpisodeLength     | 173       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 209.73    |
| TotalNEpisodes       | 20848     |
| TotalNSamples        | 4.051e+06 |
| ExplainedVariance    | 0.24329   |
------------------------------------
[2018-01-21 14:38:36.598799 UTC] Saving snapshot
[2018-01-21 14:38:36.599039 UTC] Starting iteration 810
[2018-01-21 14:38:36.599225 UTC] Start collecting samples
[2018-01-21 14:38:40.928002 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:41.048704 UTC] Performing policy update
[2018-01-21 14:38:41.049782 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:41.177632 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:42.645107 UTC] Performing line search
[2018-01-21 14:38:42.844102 UTC] Updating baseline
[2018-01-21 14:38:44.716976 UTC] Computing logging information
-------------------------------------
| Iteration            | 810        |
| ExpectedImprovement  | 0.017475   |
| ActualImprovement    | 0.016865   |
| ImprovementRatio     | 0.96512    |
| MeanKL               | 0.0076264  |
| Entropy              | -0.77173   |
| Perplexity           | 0.46221    |
| AveragePolicyStd     | 0.21499    |
| AveragePolicyStd[0]  | 0.22734    |
| AveragePolicyStd[1]  | 0.242      |
| AveragePolicyStd[2]  | 0.15937    |
| AveragePolicyStd[3]  | 0.22244    |
| AveragePolicyStd[4]  | 0.19564    |
| AveragePolicyStd[5]  | 0.24312    |
| AverageReturn        | 1387.3     |
| MinReturn            | 218.98     |
| MaxReturn            | 1589.8     |
| StdReturn            | 352.28     |
| AverageEpisodeLength | 913.43     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.18     |
| TotalNEpisodes       | 20850      |
| TotalNSamples        | 4.0522e+06 |
| ExplainedVariance    | 0.20872    |
-------------------------------------
[2018-01-21 14:38:45.441745 UTC] Saving snapshot
[2018-01-21 14:38:45.452051 UTC] Starting iteration 811
[2018-01-21 14:38:45.452298 UTC] Start collecting samples
[2018-01-21 14:38:49.883723 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:50.018546 UTC] Performing policy update
[2018-01-21 14:38:50.019714 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:50.143607 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:38:51.539898 UTC] Performing line search
[2018-01-21 14:38:51.734678 UTC] Updating baseline
[2018-01-21 14:38:53.649648 UTC] Computing logging information
-------------------------------------
| Iteration            | 811        |
| ExpectedImprovement  | 0.015688   |
| ActualImprovement    | 0.015498   |
| ImprovementRatio     | 0.98791    |
| MeanKL               | 0.0075976  |
| Entropy              | -0.77983   |
| Perplexity           | 0.45848    |
| AveragePolicyStd     | 0.2147     |
| AveragePolicyStd[0]  | 0.22638    |
| AveragePolicyStd[1]  | 0.24137    |
| AveragePolicyStd[2]  | 0.15919    |
| AveragePolicyStd[3]  | 0.22234    |
| AveragePolicyStd[4]  | 0.19541    |
| AveragePolicyStd[5]  | 0.2435     |
| AverageReturn        | 1384.6     |
| MinReturn            | 218.98     |
| MaxReturn            | 1589.8     |
| StdReturn            | 352.58     |
| AverageEpisodeLength | 911.74     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.16     |
| TotalNEpisodes       | 20856      |
| TotalNSamples        | 4.0581e+06 |
| ExplainedVariance    | 0.12498    |
-------------------------------------
[2018-01-21 14:38:54.416709 UTC] Saving snapshot
[2018-01-21 14:38:54.416953 UTC] Starting iteration 812
[2018-01-21 14:38:54.417137 UTC] Start collecting samples
[2018-01-21 14:38:58.880845 UTC] Computing input variables for policy optimization
[2018-01-21 14:38:59.012712 UTC] Performing policy update
[2018-01-21 14:38:59.013308 UTC] Computing gradient in Euclidean space
[2018-01-21 14:38:59.137623 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:00.537753 UTC] Performing line search
[2018-01-21 14:39:00.724453 UTC] Updating baseline
[2018-01-21 14:39:02.742252 UTC] Computing logging information
-------------------------------------
| Iteration            | 812        |
| ExpectedImprovement  | 0.017657   |
| ActualImprovement    | 0.017073   |
| ImprovementRatio     | 0.96697    |
| MeanKL               | 0.0077239  |
| Entropy              | -0.77207   |
| Perplexity           | 0.46206    |
| AveragePolicyStd     | 0.21499    |
| AveragePolicyStd[0]  | 0.22616    |
| AveragePolicyStd[1]  | 0.2422     |
| AveragePolicyStd[2]  | 0.15924    |
| AveragePolicyStd[3]  | 0.22241    |
| AveragePolicyStd[4]  | 0.19586    |
| AveragePolicyStd[5]  | 0.24408    |
| AverageReturn        | 1381.5     |
| MinReturn            | 218.98     |
| MaxReturn            | 1589.8     |
| StdReturn            | 364.5      |
| AverageEpisodeLength | 907.78     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.62     |
| TotalNEpisodes       | 20865      |
| TotalNSamples        | 4.0662e+06 |
| ExplainedVariance    | 0.14063    |
-------------------------------------
[2018-01-21 14:39:03.518036 UTC] Saving snapshot
[2018-01-21 14:39:03.518322 UTC] Starting iteration 813
[2018-01-21 14:39:03.518557 UTC] Start collecting samples
[2018-01-21 14:39:08.152540 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:08.267777 UTC] Performing policy update
[2018-01-21 14:39:08.268430 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:08.393407 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:09.806606 UTC] Performing line search
[2018-01-21 14:39:10.004989 UTC] Updating baseline
[2018-01-21 14:39:12.016834 UTC] Computing logging information
-------------------------------------
| Iteration            | 813        |
| ExpectedImprovement  | 0.018747   |
| ActualImprovement    | 0.018342   |
| ImprovementRatio     | 0.97837    |
| MeanKL               | 0.0075835  |
| Entropy              | -0.7672    |
| Perplexity           | 0.46431    |
| AveragePolicyStd     | 0.21517    |
| AveragePolicyStd[0]  | 0.22652    |
| AveragePolicyStd[1]  | 0.24221    |
| AveragePolicyStd[2]  | 0.15932    |
| AveragePolicyStd[3]  | 0.22126    |
| AveragePolicyStd[4]  | 0.19648    |
| AveragePolicyStd[5]  | 0.24523    |
| AverageReturn        | 1382.5     |
| MinReturn            | 218.98     |
| MaxReturn            | 1589.8     |
| StdReturn            | 364.83     |
| AverageEpisodeLength | 907.78     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.62     |
| TotalNEpisodes       | 20866      |
| TotalNSamples        | 4.0672e+06 |
| ExplainedVariance    | -0.21101   |
-------------------------------------
[2018-01-21 14:39:12.685232 UTC] Saving snapshot
[2018-01-21 14:39:12.685488 UTC] Starting iteration 814
[2018-01-21 14:39:12.685673 UTC] Start collecting samples
[2018-01-21 14:39:17.237031 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:17.371039 UTC] Performing policy update
[2018-01-21 14:39:17.371983 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:17.497081 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:18.916742 UTC] Performing line search
[2018-01-21 14:39:19.108726 UTC] Updating baseline
[2018-01-21 14:39:21.034211 UTC] Computing logging information
-------------------------------------
| Iteration            | 814        |
| ExpectedImprovement  | 0.02068    |
| ActualImprovement    | 0.019622   |
| ImprovementRatio     | 0.9488     |
| MeanKL               | 0.0078531  |
| Entropy              | -0.77447   |
| Perplexity           | 0.46095    |
| AveragePolicyStd     | 0.21491    |
| AveragePolicyStd[0]  | 0.2263     |
| AveragePolicyStd[1]  | 0.242      |
| AveragePolicyStd[2]  | 0.15918    |
| AveragePolicyStd[3]  | 0.22041    |
| AveragePolicyStd[4]  | 0.19632    |
| AveragePolicyStd[5]  | 0.24527    |
| AverageReturn        | 1376.7     |
| MinReturn            | 218.98     |
| MaxReturn            | 1589.8     |
| StdReturn            | 365.98     |
| AverageEpisodeLength | 904.66     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.47     |
| TotalNEpisodes       | 20872      |
| TotalNSamples        | 4.0729e+06 |
| ExplainedVariance    | 0.10389    |
-------------------------------------
[2018-01-21 14:39:21.740915 UTC] Saving snapshot
[2018-01-21 14:39:21.741135 UTC] Starting iteration 815
[2018-01-21 14:39:21.741270 UTC] Start collecting samples
[2018-01-21 14:39:26.224022 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:26.349343 UTC] Performing policy update
[2018-01-21 14:39:26.350474 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:26.469515 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:27.884909 UTC] Performing line search
[2018-01-21 14:39:28.076178 UTC] Updating baseline
[2018-01-21 14:39:30.939992 UTC] Computing logging information
-------------------------------------
| Iteration            | 815        |
| ExpectedImprovement  | 0.014104   |
| ActualImprovement    | 0.01419    |
| ImprovementRatio     | 1.0061     |
| MeanKL               | 0.0075916  |
| Entropy              | -0.77458   |
| Perplexity           | 0.4609     |
| AveragePolicyStd     | 0.21488    |
| AveragePolicyStd[0]  | 0.22662    |
| AveragePolicyStd[1]  | 0.24193    |
| AveragePolicyStd[2]  | 0.15965    |
| AveragePolicyStd[3]  | 0.22025    |
| AveragePolicyStd[4]  | 0.19606    |
| AveragePolicyStd[5]  | 0.24474    |
| AverageReturn        | 1377.7     |
| MinReturn            | 218.98     |
| MaxReturn            | 1591.2     |
| StdReturn            | 366.37     |
| AverageEpisodeLength | 904.66     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 229.47     |
| TotalNEpisodes       | 20880      |
| TotalNSamples        | 4.0809e+06 |
| ExplainedVariance    | -0.037749  |
-------------------------------------
[2018-01-21 14:39:31.675078 UTC] Saving snapshot
[2018-01-21 14:39:31.675346 UTC] Starting iteration 816
[2018-01-21 14:39:31.675528 UTC] Start collecting samples
[2018-01-21 14:39:35.982228 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:36.118041 UTC] Performing policy update
[2018-01-21 14:39:36.118670 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:36.238995 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:37.644943 UTC] Performing line search
[2018-01-21 14:39:37.836305 UTC] Updating baseline
[2018-01-21 14:39:41.579333 UTC] Computing logging information
--------------------------------------
| Iteration            | 816         |
| ExpectedImprovement  | 0.017917    |
| ActualImprovement    | 0.01584     |
| ImprovementRatio     | 0.88405     |
| MeanKL               | 0.0076888   |
| Entropy              | -0.77234    |
| Perplexity           | 0.46193     |
| AveragePolicyStd     | 0.21492     |
| AveragePolicyStd[0]  | 0.22668     |
| AveragePolicyStd[1]  | 0.24189     |
| AveragePolicyStd[2]  | 0.16011     |
| AveragePolicyStd[3]  | 0.22056     |
| AveragePolicyStd[4]  | 0.19604     |
| AveragePolicyStd[5]  | 0.24426     |
| AverageReturn        | 1378.4      |
| MinReturn            | 218.98      |
| MaxReturn            | 1591.2      |
| StdReturn            | 366.72      |
| AverageEpisodeLength | 904.66      |
| MinEpisodeLength     | 173         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 229.47      |
| TotalNEpisodes       | 20882       |
| TotalNSamples        | 4.0829e+06  |
| ExplainedVariance    | -2.4896e-07 |
--------------------------------------
[2018-01-21 14:39:42.279009 UTC] Saving snapshot
[2018-01-21 14:39:42.279245 UTC] Starting iteration 817
[2018-01-21 14:39:42.279394 UTC] Start collecting samples
[2018-01-21 14:39:46.739156 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:46.883634 UTC] Performing policy update
[2018-01-21 14:39:46.884246 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:47.002244 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:48.406042 UTC] Performing line search
[2018-01-21 14:39:48.597154 UTC] Updating baseline
[2018-01-21 14:39:50.890774 UTC] Computing logging information
-------------------------------------
| Iteration            | 817        |
| ExpectedImprovement  | 0.016846   |
| ActualImprovement    | 0.015853   |
| ImprovementRatio     | 0.94106    |
| MeanKL               | 0.0071451  |
| Entropy              | -0.78129   |
| Perplexity           | 0.45782    |
| AveragePolicyStd     | 0.21467    |
| AveragePolicyStd[0]  | 0.22653    |
| AveragePolicyStd[1]  | 0.24243    |
| AveragePolicyStd[2]  | 0.15866    |
| AveragePolicyStd[3]  | 0.22135    |
| AveragePolicyStd[4]  | 0.19619    |
| AveragePolicyStd[5]  | 0.24284    |
| AverageReturn        | 1374.1     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 370.01     |
| AverageEpisodeLength | 901        |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 230.83     |
| TotalNEpisodes       | 20889      |
| TotalNSamples        | 4.0895e+06 |
| ExplainedVariance    | 0.078014   |
-------------------------------------
[2018-01-21 14:39:51.577652 UTC] Saving snapshot
[2018-01-21 14:39:51.577857 UTC] Starting iteration 818
[2018-01-21 14:39:51.578021 UTC] Start collecting samples
[2018-01-21 14:39:56.373287 UTC] Computing input variables for policy optimization
[2018-01-21 14:39:56.521705 UTC] Performing policy update
[2018-01-21 14:39:56.522331 UTC] Computing gradient in Euclidean space
[2018-01-21 14:39:56.669823 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:39:58.145590 UTC] Performing line search
[2018-01-21 14:39:58.336885 UTC] Updating baseline
[2018-01-21 14:40:00.466893 UTC] Computing logging information
-------------------------------------
| Iteration            | 818        |
| ExpectedImprovement  | 0.015704   |
| ActualImprovement    | 0.014905   |
| ImprovementRatio     | 0.94908    |
| MeanKL               | 0.0075794  |
| Entropy              | -0.78877   |
| Perplexity           | 0.45441    |
| AveragePolicyStd     | 0.21436    |
| AveragePolicyStd[0]  | 0.22587    |
| AveragePolicyStd[1]  | 0.24174    |
| AveragePolicyStd[2]  | 0.1588     |
| AveragePolicyStd[3]  | 0.2207     |
| AveragePolicyStd[4]  | 0.19641    |
| AveragePolicyStd[5]  | 0.24266    |
| AverageReturn        | 1386.5     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 359.88     |
| AverageEpisodeLength | 907.28     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.53     |
| TotalNEpisodes       | 20895      |
| TotalNSamples        | 4.0949e+06 |
| ExplainedVariance    | 0.096638   |
-------------------------------------
[2018-01-21 14:40:01.172328 UTC] Saving snapshot
[2018-01-21 14:40:01.172548 UTC] Starting iteration 819
[2018-01-21 14:40:01.172725 UTC] Start collecting samples
[2018-01-21 14:40:05.801414 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:05.925325 UTC] Performing policy update
[2018-01-21 14:40:05.925921 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:06.041927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:07.434589 UTC] Performing line search
[2018-01-21 14:40:07.623850 UTC] Updating baseline
[2018-01-21 14:40:09.337906 UTC] Computing logging information
-------------------------------------
| Iteration            | 819        |
| ExpectedImprovement  | 0.016642   |
| ActualImprovement    | 0.015029   |
| ImprovementRatio     | 0.90307    |
| MeanKL               | 0.0074982  |
| Entropy              | -0.79198   |
| Perplexity           | 0.45295    |
| AveragePolicyStd     | 0.21423    |
| AveragePolicyStd[0]  | 0.22603    |
| AveragePolicyStd[1]  | 0.24156    |
| AveragePolicyStd[2]  | 0.15902    |
| AveragePolicyStd[3]  | 0.22014    |
| AveragePolicyStd[4]  | 0.19617    |
| AveragePolicyStd[5]  | 0.24246    |
| AverageReturn        | 1403.9     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 350.95     |
| AverageEpisodeLength | 917.64     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.01     |
| TotalNEpisodes       | 20898      |
| TotalNSamples        | 4.0979e+06 |
| ExplainedVariance    | -0.020517  |
-------------------------------------
[2018-01-21 14:40:10.112033 UTC] Saving snapshot
[2018-01-21 14:40:10.112322 UTC] Starting iteration 820
[2018-01-21 14:40:10.112518 UTC] Start collecting samples
[2018-01-21 14:40:14.934840 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:15.075663 UTC] Performing policy update
[2018-01-21 14:40:15.076255 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:15.190070 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:16.644681 UTC] Performing line search
[2018-01-21 14:40:16.843150 UTC] Updating baseline
[2018-01-21 14:40:19.209379 UTC] Computing logging information
-------------------------------------
| Iteration            | 820        |
| ExpectedImprovement  | 0.016235   |
| ActualImprovement    | 0.015361   |
| ImprovementRatio     | 0.94614    |
| MeanKL               | 0.0077673  |
| Entropy              | -0.79964   |
| Perplexity           | 0.44949    |
| AveragePolicyStd     | 0.21393    |
| AveragePolicyStd[0]  | 0.22544    |
| AveragePolicyStd[1]  | 0.24089    |
| AveragePolicyStd[2]  | 0.15916    |
| AveragePolicyStd[3]  | 0.21996    |
| AveragePolicyStd[4]  | 0.19597    |
| AveragePolicyStd[5]  | 0.24215    |
| AverageReturn        | 1402.5     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 350.97     |
| AverageEpisodeLength | 916.2      |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.12     |
| TotalNEpisodes       | 20903      |
| TotalNSamples        | 4.1027e+06 |
| ExplainedVariance    | 0.093972   |
-------------------------------------
[2018-01-21 14:40:19.925352 UTC] Saving snapshot
[2018-01-21 14:40:19.935822 UTC] Starting iteration 821
[2018-01-21 14:40:19.936056 UTC] Start collecting samples
[2018-01-21 14:40:24.556833 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:24.699631 UTC] Performing policy update
[2018-01-21 14:40:24.700536 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:24.820828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:26.233116 UTC] Performing line search
[2018-01-21 14:40:26.422192 UTC] Updating baseline
[2018-01-21 14:40:28.481470 UTC] Computing logging information
-------------------------------------
| Iteration            | 821        |
| ExpectedImprovement  | 0.016699   |
| ActualImprovement    | 0.016087   |
| ImprovementRatio     | 0.96334    |
| MeanKL               | 0.0080204  |
| Entropy              | -0.79724   |
| Perplexity           | 0.45057    |
| AveragePolicyStd     | 0.214      |
| AveragePolicyStd[0]  | 0.2255     |
| AveragePolicyStd[1]  | 0.24026    |
| AveragePolicyStd[2]  | 0.1594     |
| AveragePolicyStd[3]  | 0.22077    |
| AveragePolicyStd[4]  | 0.1958     |
| AveragePolicyStd[5]  | 0.24226    |
| AverageReturn        | 1435.1     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 312.22     |
| AverageEpisodeLength | 934.79     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.85     |
| TotalNEpisodes       | 20910      |
| TotalNSamples        | 4.1097e+06 |
| ExplainedVariance    | 0.056217   |
-------------------------------------
[2018-01-21 14:40:29.187986 UTC] Saving snapshot
[2018-01-21 14:40:29.188328 UTC] Starting iteration 822
[2018-01-21 14:40:29.188553 UTC] Start collecting samples
[2018-01-21 14:40:33.491852 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:33.639232 UTC] Performing policy update
[2018-01-21 14:40:33.640314 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:33.786060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:35.294206 UTC] Performing line search
[2018-01-21 14:40:35.496557 UTC] Updating baseline
[2018-01-21 14:40:37.218422 UTC] Computing logging information
-------------------------------------
| Iteration            | 822        |
| ExpectedImprovement  | 0.016566   |
| ActualImprovement    | 0.015456   |
| ImprovementRatio     | 0.93301    |
| MeanKL               | 0.0072095  |
| Entropy              | -0.79802   |
| Perplexity           | 0.45022    |
| AveragePolicyStd     | 0.21399    |
| AveragePolicyStd[0]  | 0.22551    |
| AveragePolicyStd[1]  | 0.24064    |
| AveragePolicyStd[2]  | 0.15915    |
| AveragePolicyStd[3]  | 0.22089    |
| AveragePolicyStd[4]  | 0.19573    |
| AveragePolicyStd[5]  | 0.24201    |
| AverageReturn        | 1438.6     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 304.87     |
| AverageEpisodeLength | 936.52     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.41     |
| TotalNEpisodes       | 20916      |
| TotalNSamples        | 4.1145e+06 |
| ExplainedVariance    | 0.1813     |
-------------------------------------
[2018-01-21 14:40:37.935468 UTC] Saving snapshot
[2018-01-21 14:40:37.935768 UTC] Starting iteration 823
[2018-01-21 14:40:37.935980 UTC] Start collecting samples
[2018-01-21 14:40:42.395635 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:42.523553 UTC] Performing policy update
[2018-01-21 14:40:42.524214 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:42.647987 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:44.057454 UTC] Performing line search
[2018-01-21 14:40:44.249077 UTC] Updating baseline
[2018-01-21 14:40:46.143487 UTC] Computing logging information
-------------------------------------
| Iteration            | 823        |
| ExpectedImprovement  | 0.016297   |
| ActualImprovement    | 0.013908   |
| ImprovementRatio     | 0.85343    |
| MeanKL               | 0.0072281  |
| Entropy              | -0.80172   |
| Perplexity           | 0.44856    |
| AveragePolicyStd     | 0.21389    |
| AveragePolicyStd[0]  | 0.22573    |
| AveragePolicyStd[1]  | 0.2402     |
| AveragePolicyStd[2]  | 0.15852    |
| AveragePolicyStd[3]  | 0.22062    |
| AveragePolicyStd[4]  | 0.19578    |
| AveragePolicyStd[5]  | 0.24252    |
| AverageReturn        | 1429.4     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 313.13     |
| AverageEpisodeLength | 931.18     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.99     |
| TotalNEpisodes       | 20918      |
| TotalNSamples        | 4.1159e+06 |
| ExplainedVariance    | 0.31607    |
-------------------------------------
[2018-01-21 14:40:46.928472 UTC] Saving snapshot
[2018-01-21 14:40:46.928907 UTC] Starting iteration 824
[2018-01-21 14:40:46.929200 UTC] Start collecting samples
[2018-01-21 14:40:51.554604 UTC] Computing input variables for policy optimization
[2018-01-21 14:40:51.691706 UTC] Performing policy update
[2018-01-21 14:40:51.692315 UTC] Computing gradient in Euclidean space
[2018-01-21 14:40:51.815630 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:40:53.282442 UTC] Performing line search
[2018-01-21 14:40:53.469931 UTC] Updating baseline
[2018-01-21 14:40:55.371537 UTC] Computing logging information
-------------------------------------
| Iteration            | 824        |
| ExpectedImprovement  | 0.017829   |
| ActualImprovement    | 0.016654   |
| ImprovementRatio     | 0.93409    |
| MeanKL               | 0.0071682  |
| Entropy              | -0.80139   |
| Perplexity           | 0.44871    |
| AveragePolicyStd     | 0.21389    |
| AveragePolicyStd[0]  | 0.22595    |
| AveragePolicyStd[1]  | 0.23988    |
| AveragePolicyStd[2]  | 0.15852    |
| AveragePolicyStd[3]  | 0.2205     |
| AveragePolicyStd[4]  | 0.19631    |
| AveragePolicyStd[5]  | 0.24215    |
| AverageReturn        | 1424       |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 316.1      |
| AverageEpisodeLength | 927.69     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.84     |
| TotalNEpisodes       | 20927      |
| TotalNSamples        | 4.1246e+06 |
| ExplainedVariance    | 0.084296   |
-------------------------------------
[2018-01-21 14:40:56.130539 UTC] Saving snapshot
[2018-01-21 14:40:56.130784 UTC] Starting iteration 825
[2018-01-21 14:40:56.130967 UTC] Start collecting samples
[2018-01-21 14:41:00.661968 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:00.805830 UTC] Performing policy update
[2018-01-21 14:41:00.806464 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:00.945422 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:02.387891 UTC] Performing line search
[2018-01-21 14:41:02.574080 UTC] Updating baseline
[2018-01-21 14:41:07.853224 UTC] Computing logging information
--------------------------------------
| Iteration            | 825         |
| ExpectedImprovement  | 0.016681    |
| ActualImprovement    | 0.015829    |
| ImprovementRatio     | 0.94895     |
| MeanKL               | 0.0073762   |
| Entropy              | -0.80901    |
| Perplexity           | 0.4453      |
| AveragePolicyStd     | 0.21363     |
| AveragePolicyStd[0]  | 0.22603     |
| AveragePolicyStd[1]  | 0.23938     |
| AveragePolicyStd[2]  | 0.15791     |
| AveragePolicyStd[3]  | 0.22057     |
| AveragePolicyStd[4]  | 0.19623     |
| AveragePolicyStd[5]  | 0.24169     |
| AverageReturn        | 1424.7      |
| MinReturn            | 218.98      |
| MaxReturn            | 1615.4      |
| StdReturn            | 316.34      |
| AverageEpisodeLength | 927.69      |
| MinEpisodeLength     | 173         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 196.84      |
| TotalNEpisodes       | 20932       |
| TotalNSamples        | 4.1296e+06  |
| ExplainedVariance    | -0.00014322 |
--------------------------------------
[2018-01-21 14:41:08.590838 UTC] Saving snapshot
[2018-01-21 14:41:08.591145 UTC] Starting iteration 826
[2018-01-21 14:41:08.591373 UTC] Start collecting samples
[2018-01-21 14:41:12.915649 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:13.057247 UTC] Performing policy update
[2018-01-21 14:41:13.058418 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:13.184589 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:14.700088 UTC] Performing line search
[2018-01-21 14:41:14.909040 UTC] Updating baseline
[2018-01-21 14:41:17.136815 UTC] Computing logging information
--------------------------------------
| Iteration            | 826         |
| ExpectedImprovement  | 0.016263    |
| ActualImprovement    | 0.014696    |
| ImprovementRatio     | 0.90361     |
| MeanKL               | 0.0075684   |
| Entropy              | -0.80977    |
| Perplexity           | 0.44496     |
| AveragePolicyStd     | 0.21362     |
| AveragePolicyStd[0]  | 0.22653     |
| AveragePolicyStd[1]  | 0.23911     |
| AveragePolicyStd[2]  | 0.15802     |
| AveragePolicyStd[3]  | 0.22033     |
| AveragePolicyStd[4]  | 0.19561     |
| AveragePolicyStd[5]  | 0.24209     |
| AverageReturn        | 1425.9      |
| MinReturn            | 218.98      |
| MaxReturn            | 1615.4      |
| StdReturn            | 316.73      |
| AverageEpisodeLength | 927.69      |
| MinEpisodeLength     | 173         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 196.84      |
| TotalNEpisodes       | 20934       |
| TotalNSamples        | 4.1316e+06  |
| ExplainedVariance    | -5.1757e-07 |
--------------------------------------
[2018-01-21 14:41:17.880069 UTC] Saving snapshot
[2018-01-21 14:41:17.880435 UTC] Starting iteration 827
[2018-01-21 14:41:17.880696 UTC] Start collecting samples
[2018-01-21 14:41:22.463311 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:22.602808 UTC] Performing policy update
[2018-01-21 14:41:22.603429 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:22.726853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:24.192823 UTC] Performing line search
[2018-01-21 14:41:24.381542 UTC] Updating baseline
[2018-01-21 14:41:26.689519 UTC] Computing logging information
-------------------------------------
| Iteration            | 827        |
| ExpectedImprovement  | 0.019595   |
| ActualImprovement    | 0.018352   |
| ImprovementRatio     | 0.93654    |
| MeanKL               | 0.0075004  |
| Entropy              | -0.81995   |
| Perplexity           | 0.44046    |
| AveragePolicyStd     | 0.21321    |
| AveragePolicyStd[0]  | 0.22609    |
| AveragePolicyStd[1]  | 0.23827    |
| AveragePolicyStd[2]  | 0.15819    |
| AveragePolicyStd[3]  | 0.21981    |
| AveragePolicyStd[4]  | 0.19553    |
| AveragePolicyStd[5]  | 0.24137    |
| AverageReturn        | 1421.2     |
| MinReturn            | 218.98     |
| MaxReturn            | 1615.4     |
| StdReturn            | 319.23     |
| AverageEpisodeLength | 924.25     |
| MinEpisodeLength     | 173        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.54     |
| TotalNEpisodes       | 20942      |
| TotalNSamples        | 4.1392e+06 |
| ExplainedVariance    | 0.069467   |
-------------------------------------
[2018-01-21 14:41:27.403377 UTC] Saving snapshot
[2018-01-21 14:41:27.403671 UTC] Starting iteration 828
[2018-01-21 14:41:27.403864 UTC] Start collecting samples
[2018-01-21 14:41:31.864915 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:32.025050 UTC] Performing policy update
[2018-01-21 14:41:32.025720 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:32.148862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:33.602516 UTC] Performing line search
[2018-01-21 14:41:33.818587 UTC] Updating baseline
[2018-01-21 14:41:35.990829 UTC] Computing logging information
--------------------------------------
| Iteration            | 828         |
| ExpectedImprovement  | 0.018021    |
| ActualImprovement    | 0.017468    |
| ImprovementRatio     | 0.96928     |
| MeanKL               | 0.0078849   |
| Entropy              | -0.82108    |
| Perplexity           | 0.43996     |
| AveragePolicyStd     | 0.21317     |
| AveragePolicyStd[0]  | 0.22551     |
| AveragePolicyStd[1]  | 0.23847     |
| AveragePolicyStd[2]  | 0.1585      |
| AveragePolicyStd[3]  | 0.21913     |
| AveragePolicyStd[4]  | 0.19506     |
| AveragePolicyStd[5]  | 0.24237     |
| AverageReturn        | 1452.4      |
| MinReturn            | 288.44      |
| MaxReturn            | 1615.4      |
| StdReturn            | 276.78      |
| AverageEpisodeLength | 942.02      |
| MinEpisodeLength     | 210         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 171.76      |
| TotalNEpisodes       | 20948       |
| TotalNSamples        | 4.1452e+06  |
| ExplainedVariance    | -0.00025863 |
--------------------------------------
[2018-01-21 14:41:36.710920 UTC] Saving snapshot
[2018-01-21 14:41:36.711232 UTC] Starting iteration 829
[2018-01-21 14:41:36.711445 UTC] Start collecting samples
[2018-01-21 14:41:41.162347 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:41.303572 UTC] Performing policy update
[2018-01-21 14:41:41.304166 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:41.423996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:42.865503 UTC] Performing line search
[2018-01-21 14:41:43.059354 UTC] Updating baseline
[2018-01-21 14:41:46.743395 UTC] Computing logging information
-------------------------------------
| Iteration            | 829        |
| ExpectedImprovement  | 0.016086   |
| ActualImprovement    | 0.015289   |
| ImprovementRatio     | 0.95042    |
| MeanKL               | 0.0076465  |
| Entropy              | -0.82077   |
| Perplexity           | 0.44009    |
| AveragePolicyStd     | 0.21317    |
| AveragePolicyStd[0]  | 0.22615    |
| AveragePolicyStd[1]  | 0.2377     |
| AveragePolicyStd[2]  | 0.1588     |
| AveragePolicyStd[3]  | 0.21911    |
| AveragePolicyStd[4]  | 0.19478    |
| AveragePolicyStd[5]  | 0.24245    |
| AverageReturn        | 1465.4     |
| MinReturn            | 294.52     |
| MaxReturn            | 1615.4     |
| StdReturn            | 251.1      |
| AverageEpisodeLength | 949.92     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.29     |
| TotalNEpisodes       | 20950      |
| TotalNSamples        | 4.1472e+06 |
| ExplainedVariance    | 7.2794e-06 |
-------------------------------------
[2018-01-21 14:41:47.532099 UTC] Saving snapshot
[2018-01-21 14:41:47.532342 UTC] Starting iteration 830
[2018-01-21 14:41:47.532523 UTC] Start collecting samples
[2018-01-21 14:41:52.142492 UTC] Computing input variables for policy optimization
[2018-01-21 14:41:52.276482 UTC] Performing policy update
[2018-01-21 14:41:52.277091 UTC] Computing gradient in Euclidean space
[2018-01-21 14:41:52.401581 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:41:53.864241 UTC] Performing line search
[2018-01-21 14:41:54.066583 UTC] Updating baseline
[2018-01-21 14:41:56.287726 UTC] Computing logging information
-------------------------------------
| Iteration            | 830        |
| ExpectedImprovement  | 0.016881   |
| ActualImprovement    | 0.015864   |
| ImprovementRatio     | 0.93979    |
| MeanKL               | 0.0069168  |
| Entropy              | -0.81592   |
| Perplexity           | 0.44223    |
| AveragePolicyStd     | 0.21331    |
| AveragePolicyStd[0]  | 0.22667    |
| AveragePolicyStd[1]  | 0.23751    |
| AveragePolicyStd[2]  | 0.1594     |
| AveragePolicyStd[3]  | 0.21843    |
| AveragePolicyStd[4]  | 0.195      |
| AveragePolicyStd[5]  | 0.24285    |
| AverageReturn        | 1462       |
| MinReturn            | 294.52     |
| MaxReturn            | 1615.4     |
| StdReturn            | 259.93     |
| AverageEpisodeLength | 946.94     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.32     |
| TotalNEpisodes       | 20958      |
| TotalNSamples        | 4.1548e+06 |
| ExplainedVariance    | 0.076631   |
-------------------------------------
[2018-01-21 14:41:57.034549 UTC] Saving snapshot
[2018-01-21 14:41:57.044148 UTC] Starting iteration 831
[2018-01-21 14:41:57.044380 UTC] Start collecting samples
[2018-01-21 14:42:01.425652 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:01.555619 UTC] Performing policy update
[2018-01-21 14:42:01.556249 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:01.676807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:03.119212 UTC] Performing line search
[2018-01-21 14:42:03.306996 UTC] Updating baseline
[2018-01-21 14:42:05.452999 UTC] Computing logging information
------------------------------------
| Iteration            | 831       |
| ExpectedImprovement  | 0.018219  |
| ActualImprovement    | 0.016964  |
| ImprovementRatio     | 0.93108   |
| MeanKL               | 0.0077763 |
| Entropy              | -0.81736  |
| Perplexity           | 0.4416    |
| AveragePolicyStd     | 0.21327   |
| AveragePolicyStd[0]  | 0.2261    |
| AveragePolicyStd[1]  | 0.23757   |
| AveragePolicyStd[2]  | 0.15941   |
| AveragePolicyStd[3]  | 0.21839   |
| AveragePolicyStd[4]  | 0.19458   |
| AveragePolicyStd[5]  | 0.24358   |
| AverageReturn        | 1463.8    |
| MinReturn            | 295.61    |
| MaxReturn            | 1615.4    |
| StdReturn            | 259.87    |
| AverageEpisodeLength | 947.79    |
| MinEpisodeLength     | 224       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 159.94    |
| TotalNEpisodes       | 20965     |
| TotalNSamples        | 4.161e+06 |
| ExplainedVariance    | 0.10246   |
------------------------------------
[2018-01-21 14:42:06.187235 UTC] Saving snapshot
[2018-01-21 14:42:06.187479 UTC] Starting iteration 832
[2018-01-21 14:42:06.187637 UTC] Start collecting samples
[2018-01-21 14:42:10.703090 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:10.823201 UTC] Performing policy update
[2018-01-21 14:42:10.824107 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:10.943482 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:12.399326 UTC] Performing line search
[2018-01-21 14:42:12.591926 UTC] Updating baseline
[2018-01-21 14:42:14.629427 UTC] Computing logging information
-------------------------------------
| Iteration            | 832        |
| ExpectedImprovement  | 0.017867   |
| ActualImprovement    | 0.016732   |
| ImprovementRatio     | 0.93643    |
| MeanKL               | 0.0078462  |
| Entropy              | -0.82429   |
| Perplexity           | 0.43854    |
| AveragePolicyStd     | 0.21301    |
| AveragePolicyStd[0]  | 0.22561    |
| AveragePolicyStd[1]  | 0.23726    |
| AveragePolicyStd[2]  | 0.15937    |
| AveragePolicyStd[3]  | 0.21822    |
| AveragePolicyStd[4]  | 0.19457    |
| AveragePolicyStd[5]  | 0.24301    |
| AverageReturn        | 1457.7     |
| MinReturn            | 295.61     |
| MaxReturn            | 1615.4     |
| StdReturn            | 263.69     |
| AverageEpisodeLength | 944.42     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.33     |
| TotalNEpisodes       | 20968      |
| TotalNSamples        | 4.1637e+06 |
| ExplainedVariance    | 0.18185    |
-------------------------------------
[2018-01-21 14:42:15.339391 UTC] Saving snapshot
[2018-01-21 14:42:15.339664 UTC] Starting iteration 833
[2018-01-21 14:42:15.339849 UTC] Start collecting samples
[2018-01-21 14:42:20.011392 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:20.215954 UTC] Performing policy update
[2018-01-21 14:42:20.216708 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:20.339571 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:21.808359 UTC] Performing line search
[2018-01-21 14:42:22.006191 UTC] Updating baseline
[2018-01-21 14:42:24.140490 UTC] Computing logging information
-------------------------------------
| Iteration            | 833        |
| ExpectedImprovement  | 0.018709   |
| ActualImprovement    | 0.018495   |
| ImprovementRatio     | 0.98857    |
| MeanKL               | 0.0069194  |
| Entropy              | -0.83174   |
| Perplexity           | 0.43529    |
| AveragePolicyStd     | 0.21276    |
| AveragePolicyStd[0]  | 0.2254     |
| AveragePolicyStd[1]  | 0.23762    |
| AveragePolicyStd[2]  | 0.15913    |
| AveragePolicyStd[3]  | 0.2182     |
| AveragePolicyStd[4]  | 0.19384    |
| AveragePolicyStd[5]  | 0.24237    |
| AverageReturn        | 1463.5     |
| MinReturn            | 295.61     |
| MaxReturn            | 1615.4     |
| StdReturn            | 260.26     |
| AverageEpisodeLength | 947.54     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.36     |
| TotalNEpisodes       | 20973      |
| TotalNSamples        | 4.1687e+06 |
| ExplainedVariance    | -0.071134  |
-------------------------------------
[2018-01-21 14:42:24.861431 UTC] Saving snapshot
[2018-01-21 14:42:24.861716 UTC] Starting iteration 834
[2018-01-21 14:42:24.861938 UTC] Start collecting samples
[2018-01-21 14:42:29.472384 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:29.602132 UTC] Performing policy update
[2018-01-21 14:42:29.602767 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:29.720326 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:31.111217 UTC] Performing line search
[2018-01-21 14:42:31.295741 UTC] Updating baseline
[2018-01-21 14:42:33.792058 UTC] Computing logging information
--------------------------------------
| Iteration            | 834         |
| ExpectedImprovement  | 0.016381    |
| ActualImprovement    | 0.015318    |
| ImprovementRatio     | 0.9351      |
| MeanKL               | 0.0079026   |
| Entropy              | -0.83834    |
| Perplexity           | 0.43243     |
| AveragePolicyStd     | 0.21262     |
| AveragePolicyStd[0]  | 0.22581     |
| AveragePolicyStd[1]  | 0.23731     |
| AveragePolicyStd[2]  | 0.15816     |
| AveragePolicyStd[3]  | 0.21813     |
| AveragePolicyStd[4]  | 0.19286     |
| AveragePolicyStd[5]  | 0.24344     |
| AverageReturn        | 1464.3      |
| MinReturn            | 295.61      |
| MaxReturn            | 1615.4      |
| StdReturn            | 260.56      |
| AverageEpisodeLength | 947.54      |
| MinEpisodeLength     | 224         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 160.36      |
| TotalNEpisodes       | 20979       |
| TotalNSamples        | 4.1747e+06  |
| ExplainedVariance    | -2.1061e-06 |
--------------------------------------
[2018-01-21 14:42:34.504900 UTC] Saving snapshot
[2018-01-21 14:42:34.505348 UTC] Starting iteration 835
[2018-01-21 14:42:34.505530 UTC] Start collecting samples
[2018-01-21 14:42:38.961606 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:39.098960 UTC] Performing policy update
[2018-01-21 14:42:39.099725 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:39.216947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:40.661201 UTC] Performing line search
[2018-01-21 14:42:40.879545 UTC] Updating baseline
[2018-01-21 14:42:43.175430 UTC] Computing logging information
-------------------------------------
| Iteration            | 835        |
| ExpectedImprovement  | 0.017255   |
| ActualImprovement    | 0.016105   |
| ImprovementRatio     | 0.93335    |
| MeanKL               | 0.0079551  |
| Entropy              | -0.83906   |
| Perplexity           | 0.43212    |
| AveragePolicyStd     | 0.21263    |
| AveragePolicyStd[0]  | 0.22552    |
| AveragePolicyStd[1]  | 0.23745    |
| AveragePolicyStd[2]  | 0.15761    |
| AveragePolicyStd[3]  | 0.21824    |
| AveragePolicyStd[4]  | 0.193      |
| AveragePolicyStd[5]  | 0.24399    |
| AverageReturn        | 1463.1     |
| MinReturn            | 295.61     |
| MaxReturn            | 1615.4     |
| StdReturn            | 260.4      |
| AverageEpisodeLength | 947.11     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.28     |
| TotalNEpisodes       | 20983      |
| TotalNSamples        | 4.1786e+06 |
| ExplainedVariance    | 0.15361    |
-------------------------------------
[2018-01-21 14:42:43.875613 UTC] Saving snapshot
[2018-01-21 14:42:43.875911 UTC] Starting iteration 836
[2018-01-21 14:42:43.876136 UTC] Start collecting samples
[2018-01-21 14:42:48.200621 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:48.338640 UTC] Performing policy update
[2018-01-21 14:42:48.339871 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:48.462977 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:49.881388 UTC] Performing line search
[2018-01-21 14:42:50.077428 UTC] Updating baseline
[2018-01-21 14:42:52.144650 UTC] Computing logging information
------------------------------------
| Iteration            | 836       |
| ExpectedImprovement  | 0.017844  |
| ActualImprovement    | 0.016783  |
| ImprovementRatio     | 0.94056   |
| MeanKL               | 0.0073505 |
| Entropy              | -0.84493  |
| Perplexity           | 0.42959   |
| AveragePolicyStd     | 0.21239   |
| AveragePolicyStd[0]  | 0.22525   |
| AveragePolicyStd[1]  | 0.23728   |
| AveragePolicyStd[2]  | 0.15808   |
| AveragePolicyStd[3]  | 0.21824   |
| AveragePolicyStd[4]  | 0.19214   |
| AveragePolicyStd[5]  | 0.24338   |
| AverageReturn        | 1458.7    |
| MinReturn            | 295.61    |
| MaxReturn            | 1606.7    |
| StdReturn            | 261.27    |
| AverageEpisodeLength | 944.62    |
| MinEpisodeLength     | 224       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 161.19    |
| TotalNEpisodes       | 20990     |
| TotalNSamples        | 4.185e+06 |
| ExplainedVariance    | 0.19418   |
------------------------------------
[2018-01-21 14:42:52.865775 UTC] Saving snapshot
[2018-01-21 14:42:52.865986 UTC] Starting iteration 837
[2018-01-21 14:42:52.866168 UTC] Start collecting samples
[2018-01-21 14:42:57.505501 UTC] Computing input variables for policy optimization
[2018-01-21 14:42:57.648618 UTC] Performing policy update
[2018-01-21 14:42:57.649288 UTC] Computing gradient in Euclidean space
[2018-01-21 14:42:57.769193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:42:59.207650 UTC] Performing line search
[2018-01-21 14:42:59.393417 UTC] Updating baseline
[2018-01-21 14:43:01.438875 UTC] Computing logging information
------------------------------------
| Iteration            | 837       |
| ExpectedImprovement  | 0.015648  |
| ActualImprovement    | 0.014954  |
| ImprovementRatio     | 0.95565   |
| MeanKL               | 0.0076402 |
| Entropy              | -0.85099  |
| Perplexity           | 0.42699   |
| AveragePolicyStd     | 0.21217   |
| AveragePolicyStd[0]  | 0.2251    |
| AveragePolicyStd[1]  | 0.23728   |
| AveragePolicyStd[2]  | 0.15799   |
| AveragePolicyStd[3]  | 0.21797   |
| AveragePolicyStd[4]  | 0.19197   |
| AveragePolicyStd[5]  | 0.24271   |
| AverageReturn        | 1470      |
| MinReturn            | 295.61    |
| MaxReturn            | 1606.7    |
| StdReturn            | 243.04    |
| AverageEpisodeLength | 951.27    |
| MinEpisodeLength     | 224       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 149.17    |
| TotalNEpisodes       | 20994     |
| TotalNSamples        | 4.189e+06 |
| ExplainedVariance    | -0.020109 |
------------------------------------
[2018-01-21 14:43:02.179908 UTC] Saving snapshot
[2018-01-21 14:43:02.180164 UTC] Starting iteration 838
[2018-01-21 14:43:02.180327 UTC] Start collecting samples
[2018-01-21 14:43:06.545857 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:06.686411 UTC] Performing policy update
[2018-01-21 14:43:06.687042 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:06.830986 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:08.252639 UTC] Performing line search
[2018-01-21 14:43:08.444162 UTC] Updating baseline
[2018-01-21 14:43:10.376811 UTC] Computing logging information
-------------------------------------
| Iteration            | 838        |
| ExpectedImprovement  | 0.019027   |
| ActualImprovement    | 0.017766   |
| ImprovementRatio     | 0.93373    |
| MeanKL               | 0.007222   |
| Entropy              | -0.84308   |
| Perplexity           | 0.43038    |
| AveragePolicyStd     | 0.21244    |
| AveragePolicyStd[0]  | 0.2258     |
| AveragePolicyStd[1]  | 0.23726    |
| AveragePolicyStd[2]  | 0.15841    |
| AveragePolicyStd[3]  | 0.21762    |
| AveragePolicyStd[4]  | 0.19235    |
| AveragePolicyStd[5]  | 0.24316    |
| AverageReturn        | 1469       |
| MinReturn            | 295.61     |
| MaxReturn            | 1606.7     |
| StdReturn            | 242.83     |
| AverageEpisodeLength | 951.27     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.17     |
| TotalNEpisodes       | 20998      |
| TotalNSamples        | 4.193e+06  |
| ExplainedVariance    | -0.0086942 |
-------------------------------------
[2018-01-21 14:43:11.089623 UTC] Saving snapshot
[2018-01-21 14:43:11.089835 UTC] Starting iteration 839
[2018-01-21 14:43:11.090017 UTC] Start collecting samples
[2018-01-21 14:43:15.572981 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:15.703322 UTC] Performing policy update
[2018-01-21 14:43:15.703947 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:15.818682 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:17.270998 UTC] Performing line search
[2018-01-21 14:43:17.458690 UTC] Updating baseline
[2018-01-21 14:43:19.479111 UTC] Computing logging information
-------------------------------------
| Iteration            | 839        |
| ExpectedImprovement  | 0.018748   |
| ActualImprovement    | 0.017326   |
| ImprovementRatio     | 0.92416    |
| MeanKL               | 0.0073237  |
| Entropy              | -0.84421   |
| Perplexity           | 0.4299     |
| AveragePolicyStd     | 0.21242    |
| AveragePolicyStd[0]  | 0.22595    |
| AveragePolicyStd[1]  | 0.23765    |
| AveragePolicyStd[2]  | 0.15837    |
| AveragePolicyStd[3]  | 0.21756    |
| AveragePolicyStd[4]  | 0.19178    |
| AveragePolicyStd[5]  | 0.24319    |
| AverageReturn        | 1467       |
| MinReturn            | 295.61     |
| MaxReturn            | 1606.7     |
| StdReturn            | 247.53     |
| AverageEpisodeLength | 949.44     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.77     |
| TotalNEpisodes       | 21003      |
| TotalNSamples        | 4.1976e+06 |
| ExplainedVariance    | 0.20419    |
-------------------------------------
[2018-01-21 14:43:20.263526 UTC] Saving snapshot
[2018-01-21 14:43:20.263799 UTC] Starting iteration 840
[2018-01-21 14:43:20.263982 UTC] Start collecting samples
[2018-01-21 14:43:24.669401 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:24.802262 UTC] Performing policy update
[2018-01-21 14:43:24.803012 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:24.930412 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:26.319787 UTC] Performing line search
[2018-01-21 14:43:26.508155 UTC] Updating baseline
[2018-01-21 14:43:28.735323 UTC] Computing logging information
-------------------------------------
| Iteration            | 840        |
| ExpectedImprovement  | 0.016457   |
| ActualImprovement    | 0.015282   |
| ImprovementRatio     | 0.9286     |
| MeanKL               | 0.007853   |
| Entropy              | -0.85022   |
| Perplexity           | 0.42732    |
| AveragePolicyStd     | 0.21219    |
| AveragePolicyStd[0]  | 0.22617    |
| AveragePolicyStd[1]  | 0.23666    |
| AveragePolicyStd[2]  | 0.15846    |
| AveragePolicyStd[3]  | 0.21725    |
| AveragePolicyStd[4]  | 0.19139    |
| AveragePolicyStd[5]  | 0.24321    |
| AverageReturn        | 1462.7     |
| MinReturn            | 295.61     |
| MaxReturn            | 1606       |
| StdReturn            | 247.49     |
| AverageEpisodeLength | 948.08     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.92     |
| TotalNEpisodes       | 21009      |
| TotalNSamples        | 4.2035e+06 |
| ExplainedVariance    | 0.21248    |
-------------------------------------
[2018-01-21 14:43:29.461406 UTC] Saving snapshot
[2018-01-21 14:43:29.467626 UTC] Starting iteration 841
[2018-01-21 14:43:29.467820 UTC] Start collecting samples
[2018-01-21 14:43:33.896990 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:34.031130 UTC] Performing policy update
[2018-01-21 14:43:34.031953 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:34.153465 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:35.604939 UTC] Performing line search
[2018-01-21 14:43:35.809165 UTC] Updating baseline
[2018-01-21 14:43:37.885279 UTC] Computing logging information
-------------------------------------
| Iteration            | 841        |
| ExpectedImprovement  | 0.017077   |
| ActualImprovement    | 0.015916   |
| ImprovementRatio     | 0.93204    |
| MeanKL               | 0.0085149  |
| Entropy              | -0.85235   |
| Perplexity           | 0.42641    |
| AveragePolicyStd     | 0.21209    |
| AveragePolicyStd[0]  | 0.22611    |
| AveragePolicyStd[1]  | 0.23689    |
| AveragePolicyStd[2]  | 0.15872    |
| AveragePolicyStd[3]  | 0.21669    |
| AveragePolicyStd[4]  | 0.19135    |
| AveragePolicyStd[5]  | 0.24279    |
| AverageReturn        | 1481.4     |
| MinReturn            | 295.61     |
| MaxReturn            | 1601.7     |
| StdReturn            | 208.74     |
| AverageEpisodeLength | 960.44     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.03     |
| TotalNEpisodes       | 21014      |
| TotalNSamples        | 4.2085e+06 |
| ExplainedVariance    | 0.0077299  |
-------------------------------------
[2018-01-21 14:43:38.600527 UTC] Saving snapshot
[2018-01-21 14:43:38.600797 UTC] Starting iteration 842
[2018-01-21 14:43:38.600979 UTC] Start collecting samples
[2018-01-21 14:43:43.066948 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:43.207975 UTC] Performing policy update
[2018-01-21 14:43:43.208590 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:43.335681 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:44.806491 UTC] Performing line search
[2018-01-21 14:43:44.996377 UTC] Updating baseline
[2018-01-21 14:43:46.917014 UTC] Computing logging information
-------------------------------------
| Iteration            | 842        |
| ExpectedImprovement  | 0.017916   |
| ActualImprovement    | 0.016811   |
| ImprovementRatio     | 0.93834    |
| MeanKL               | 0.0073714  |
| Entropy              | -0.85699   |
| Perplexity           | 0.42444    |
| AveragePolicyStd     | 0.21192    |
| AveragePolicyStd[0]  | 0.22563    |
| AveragePolicyStd[1]  | 0.23651    |
| AveragePolicyStd[2]  | 0.15889    |
| AveragePolicyStd[3]  | 0.21679    |
| AveragePolicyStd[4]  | 0.19078    |
| AveragePolicyStd[5]  | 0.24293    |
| AverageReturn        | 1490.6     |
| MinReturn            | 295.61     |
| MaxReturn            | 1601.7     |
| StdReturn            | 193.64     |
| AverageEpisodeLength | 965.74     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.03     |
| TotalNEpisodes       | 21018      |
| TotalNSamples        | 4.2125e+06 |
| ExplainedVariance    | 0.17601    |
-------------------------------------
[2018-01-21 14:43:47.646872 UTC] Saving snapshot
[2018-01-21 14:43:47.647116 UTC] Starting iteration 843
[2018-01-21 14:43:47.647272 UTC] Start collecting samples
[2018-01-21 14:43:52.091991 UTC] Computing input variables for policy optimization
[2018-01-21 14:43:52.246082 UTC] Performing policy update
[2018-01-21 14:43:52.246755 UTC] Computing gradient in Euclidean space
[2018-01-21 14:43:52.360424 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:43:53.773924 UTC] Performing line search
[2018-01-21 14:43:54.061880 UTC] Updating baseline
[2018-01-21 14:43:56.190543 UTC] Computing logging information
-------------------------------------
| Iteration            | 843        |
| ExpectedImprovement  | 0.015504   |
| ActualImprovement    | 0.015092   |
| ImprovementRatio     | 0.97344    |
| MeanKL               | 0.0083299  |
| Entropy              | -0.86622   |
| Perplexity           | 0.42054    |
| AveragePolicyStd     | 0.21159    |
| AveragePolicyStd[0]  | 0.22521    |
| AveragePolicyStd[1]  | 0.23655    |
| AveragePolicyStd[2]  | 0.15892    |
| AveragePolicyStd[3]  | 0.21578    |
| AveragePolicyStd[4]  | 0.19038    |
| AveragePolicyStd[5]  | 0.2427     |
| AverageReturn        | 1497.3     |
| MinReturn            | 295.61     |
| MaxReturn            | 1601.7     |
| StdReturn            | 187.63     |
| AverageEpisodeLength | 968.73     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.73     |
| TotalNEpisodes       | 21025      |
| TotalNSamples        | 4.2194e+06 |
| ExplainedVariance    | 0.12488    |
-------------------------------------
[2018-01-21 14:43:56.962159 UTC] Saving snapshot
[2018-01-21 14:43:56.962474 UTC] Starting iteration 844
[2018-01-21 14:43:56.962706 UTC] Start collecting samples
[2018-01-21 14:44:01.578907 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:01.707100 UTC] Performing policy update
[2018-01-21 14:44:01.707734 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:01.829604 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:03.281731 UTC] Performing line search
[2018-01-21 14:44:03.472955 UTC] Updating baseline
[2018-01-21 14:44:07.014883 UTC] Computing logging information
-------------------------------------
| Iteration            | 844        |
| ExpectedImprovement  | 0.020603   |
| ActualImprovement    | 0.020248   |
| ImprovementRatio     | 0.98275    |
| MeanKL               | 0.0073384  |
| Entropy              | -0.85877   |
| Perplexity           | 0.42368    |
| AveragePolicyStd     | 0.21182    |
| AveragePolicyStd[0]  | 0.22608    |
| AveragePolicyStd[1]  | 0.23632    |
| AveragePolicyStd[2]  | 0.15932    |
| AveragePolicyStd[3]  | 0.21559    |
| AveragePolicyStd[4]  | 0.19093    |
| AveragePolicyStd[5]  | 0.24272    |
| AverageReturn        | 1498.8     |
| MinReturn            | 295.61     |
| MaxReturn            | 1601.7     |
| StdReturn            | 188.03     |
| AverageEpisodeLength | 968.73     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.73     |
| TotalNEpisodes       | 21030      |
| TotalNSamples        | 4.2244e+06 |
| ExplainedVariance    | -0.080921  |
-------------------------------------
[2018-01-21 14:44:07.741995 UTC] Saving snapshot
[2018-01-21 14:44:07.742226 UTC] Starting iteration 845
[2018-01-21 14:44:07.742405 UTC] Start collecting samples
[2018-01-21 14:44:12.207416 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:12.328767 UTC] Performing policy update
[2018-01-21 14:44:12.329465 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:12.447771 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:13.884114 UTC] Performing line search
[2018-01-21 14:44:14.084111 UTC] Updating baseline
[2018-01-21 14:44:16.474863 UTC] Computing logging information
-------------------------------------
| Iteration            | 845        |
| ExpectedImprovement  | 0.016518   |
| ActualImprovement    | 0.015595   |
| ImprovementRatio     | 0.94412    |
| MeanKL               | 0.0073712  |
| Entropy              | -0.85929   |
| Perplexity           | 0.42346    |
| AveragePolicyStd     | 0.21179    |
| AveragePolicyStd[0]  | 0.22613    |
| AveragePolicyStd[1]  | 0.23593    |
| AveragePolicyStd[2]  | 0.15956    |
| AveragePolicyStd[3]  | 0.21563    |
| AveragePolicyStd[4]  | 0.19072    |
| AveragePolicyStd[5]  | 0.24278    |
| AverageReturn        | 1498.8     |
| MinReturn            | 295.61     |
| MaxReturn            | 1601.7     |
| StdReturn            | 188.14     |
| AverageEpisodeLength | 968.73     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.73     |
| TotalNEpisodes       | 21034      |
| TotalNSamples        | 4.2284e+06 |
| ExplainedVariance    | -0.000776  |
-------------------------------------
[2018-01-21 14:44:17.221080 UTC] Saving snapshot
[2018-01-21 14:44:17.221589 UTC] Starting iteration 846
[2018-01-21 14:44:17.222005 UTC] Start collecting samples
[2018-01-21 14:44:21.696560 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:21.832077 UTC] Performing policy update
[2018-01-21 14:44:21.833122 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:21.962894 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:23.493195 UTC] Performing line search
[2018-01-21 14:44:23.685283 UTC] Updating baseline
[2018-01-21 14:44:26.371791 UTC] Computing logging information
--------------------------------------
| Iteration            | 846         |
| ExpectedImprovement  | 0.018013    |
| ActualImprovement    | 0.017027    |
| ImprovementRatio     | 0.94525     |
| MeanKL               | 0.0074377   |
| Entropy              | -0.85857    |
| Perplexity           | 0.42377     |
| AveragePolicyStd     | 0.21181     |
| AveragePolicyStd[0]  | 0.22588     |
| AveragePolicyStd[1]  | 0.23628     |
| AveragePolicyStd[2]  | 0.15967     |
| AveragePolicyStd[3]  | 0.21557     |
| AveragePolicyStd[4]  | 0.19075     |
| AveragePolicyStd[5]  | 0.24274     |
| AverageReturn        | 1506.7      |
| MinReturn            | 295.61      |
| MaxReturn            | 1617.7      |
| StdReturn            | 182.72      |
| AverageEpisodeLength | 972.17      |
| MinEpisodeLength     | 224         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 109.34      |
| TotalNEpisodes       | 21041       |
| TotalNSamples        | 4.2354e+06  |
| ExplainedVariance    | -0.00099234 |
--------------------------------------
[2018-01-21 14:44:27.186674 UTC] Saving snapshot
[2018-01-21 14:44:27.186912 UTC] Starting iteration 847
[2018-01-21 14:44:27.187091 UTC] Start collecting samples
[2018-01-21 14:44:31.682650 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:31.816779 UTC] Performing policy update
[2018-01-21 14:44:31.817461 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:31.942528 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:33.379027 UTC] Performing line search
[2018-01-21 14:44:33.577296 UTC] Updating baseline
[2018-01-21 14:44:35.222649 UTC] Computing logging information
-------------------------------------
| Iteration            | 847        |
| ExpectedImprovement  | 0.018082   |
| ActualImprovement    | 0.01717    |
| ImprovementRatio     | 0.9496     |
| MeanKL               | 0.0083013  |
| Entropy              | -0.86693   |
| Perplexity           | 0.42024    |
| AveragePolicyStd     | 0.21151    |
| AveragePolicyStd[0]  | 0.22525    |
| AveragePolicyStd[1]  | 0.23531    |
| AveragePolicyStd[2]  | 0.1596     |
| AveragePolicyStd[3]  | 0.2158     |
| AveragePolicyStd[4]  | 0.1902     |
| AveragePolicyStd[5]  | 0.24292    |
| AverageReturn        | 1505.8     |
| MinReturn            | 295.61     |
| MaxReturn            | 1617.7     |
| StdReturn            | 182.56     |
| AverageEpisodeLength | 972.17     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.34     |
| TotalNEpisodes       | 21045      |
| TotalNSamples        | 4.2394e+06 |
| ExplainedVariance    | 2.4595e-07 |
-------------------------------------
[2018-01-21 14:44:35.943535 UTC] Saving snapshot
[2018-01-21 14:44:35.943772 UTC] Starting iteration 848
[2018-01-21 14:44:35.943921 UTC] Start collecting samples
[2018-01-21 14:44:40.352699 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:40.477307 UTC] Performing policy update
[2018-01-21 14:44:40.478384 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:40.600173 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:42.051618 UTC] Performing line search
[2018-01-21 14:44:42.258894 UTC] Updating baseline
[2018-01-21 14:44:45.390697 UTC] Computing logging information
-------------------------------------
| Iteration            | 848        |
| ExpectedImprovement  | 0.015809   |
| ActualImprovement    | 0.01524    |
| ImprovementRatio     | 0.96401    |
| MeanKL               | 0.0073958  |
| Entropy              | -0.86397   |
| Perplexity           | 0.42148    |
| AveragePolicyStd     | 0.21161    |
| AveragePolicyStd[0]  | 0.22518    |
| AveragePolicyStd[1]  | 0.23498    |
| AveragePolicyStd[2]  | 0.15989    |
| AveragePolicyStd[3]  | 0.2163     |
| AveragePolicyStd[4]  | 0.18998    |
| AveragePolicyStd[5]  | 0.24335    |
| AverageReturn        | 1505.3     |
| MinReturn            | 295.61     |
| MaxReturn            | 1617.7     |
| StdReturn            | 182.48     |
| AverageEpisodeLength | 972.17     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.34     |
| TotalNEpisodes       | 21049      |
| TotalNSamples        | 4.2434e+06 |
| ExplainedVariance    | -0.0012825 |
-------------------------------------
[2018-01-21 14:44:46.137753 UTC] Saving snapshot
[2018-01-21 14:44:46.137988 UTC] Starting iteration 849
[2018-01-21 14:44:46.138134 UTC] Start collecting samples
[2018-01-21 14:44:50.732795 UTC] Computing input variables for policy optimization
[2018-01-21 14:44:50.870766 UTC] Performing policy update
[2018-01-21 14:44:50.871385 UTC] Computing gradient in Euclidean space
[2018-01-21 14:44:51.000017 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:44:52.470393 UTC] Performing line search
[2018-01-21 14:44:52.653513 UTC] Updating baseline
[2018-01-21 14:44:54.945214 UTC] Computing logging information
-------------------------------------
| Iteration            | 849        |
| ExpectedImprovement  | 0.017027   |
| ActualImprovement    | 0.016274   |
| ImprovementRatio     | 0.95576    |
| MeanKL               | 0.0076685  |
| Entropy              | -0.85928   |
| Perplexity           | 0.42347    |
| AveragePolicyStd     | 0.2118     |
| AveragePolicyStd[0]  | 0.22543    |
| AveragePolicyStd[1]  | 0.23522    |
| AveragePolicyStd[2]  | 0.1598     |
| AveragePolicyStd[3]  | 0.21706    |
| AveragePolicyStd[4]  | 0.18982    |
| AveragePolicyStd[5]  | 0.24346    |
| AverageReturn        | 1511.1     |
| MinReturn            | 295.61     |
| MaxReturn            | 1617.7     |
| StdReturn            | 167.53     |
| AverageEpisodeLength | 975.44     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.7      |
| TotalNEpisodes       | 21055      |
| TotalNSamples        | 4.2493e+06 |
| ExplainedVariance    | 0.042608   |
-------------------------------------
[2018-01-21 14:44:55.731064 UTC] Saving snapshot
[2018-01-21 14:44:55.731297 UTC] Starting iteration 850
[2018-01-21 14:44:55.731441 UTC] Start collecting samples
[2018-01-21 14:45:00.260380 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:00.390216 UTC] Performing policy update
[2018-01-21 14:45:00.390952 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:00.505269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:01.949473 UTC] Performing line search
[2018-01-21 14:45:02.146183 UTC] Updating baseline
[2018-01-21 14:45:04.428622 UTC] Computing logging information
-------------------------------------
| Iteration            | 850        |
| ExpectedImprovement  | 0.01825    |
| ActualImprovement    | 0.016456   |
| ImprovementRatio     | 0.90167    |
| MeanKL               | 0.0074076  |
| Entropy              | -0.85833   |
| Perplexity           | 0.42387    |
| AveragePolicyStd     | 0.21185    |
| AveragePolicyStd[0]  | 0.22579    |
| AveragePolicyStd[1]  | 0.23572    |
| AveragePolicyStd[2]  | 0.15979    |
| AveragePolicyStd[3]  | 0.21669    |
| AveragePolicyStd[4]  | 0.18946    |
| AveragePolicyStd[5]  | 0.24368    |
| AverageReturn        | 1510.7     |
| MinReturn            | 295.61     |
| MaxReturn            | 1617.7     |
| StdReturn            | 167.49     |
| AverageEpisodeLength | 975.44     |
| MinEpisodeLength     | 224        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.7      |
| TotalNEpisodes       | 21059      |
| TotalNSamples        | 4.2533e+06 |
| ExplainedVariance    | 0.069267   |
-------------------------------------
[2018-01-21 14:45:05.157814 UTC] Saving snapshot
[2018-01-21 14:45:05.164043 UTC] Starting iteration 851
[2018-01-21 14:45:05.164263 UTC] Start collecting samples
[2018-01-21 14:45:09.619684 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:09.754849 UTC] Performing policy update
[2018-01-21 14:45:09.755513 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:09.876483 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:11.286838 UTC] Performing line search
[2018-01-21 14:45:11.490417 UTC] Updating baseline
[2018-01-21 14:45:13.579908 UTC] Computing logging information
-------------------------------------
| Iteration            | 851        |
| ExpectedImprovement  | 0.016407   |
| ActualImprovement    | 0.01626    |
| ImprovementRatio     | 0.99105    |
| MeanKL               | 0.0072255  |
| Entropy              | -0.86317   |
| Perplexity           | 0.42182    |
| AveragePolicyStd     | 0.21167    |
| AveragePolicyStd[0]  | 0.22513    |
| AveragePolicyStd[1]  | 0.23525    |
| AveragePolicyStd[2]  | 0.15959    |
| AveragePolicyStd[3]  | 0.21647    |
| AveragePolicyStd[4]  | 0.18986    |
| AveragePolicyStd[5]  | 0.24373    |
| AverageReturn        | 1523       |
| MinReturn            | 986.02     |
| MaxReturn            | 1617.7     |
| StdReturn            | 114.59     |
| AverageEpisodeLength | 983.2      |
| MinEpisodeLength     | 645        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 66.637     |
| TotalNEpisodes       | 21064      |
| TotalNSamples        | 4.2583e+06 |
| ExplainedVariance    | -0.030624  |
-------------------------------------
[2018-01-21 14:45:14.268823 UTC] Saving snapshot
[2018-01-21 14:45:14.269053 UTC] Starting iteration 852
[2018-01-21 14:45:14.269197 UTC] Start collecting samples
[2018-01-21 14:45:18.727868 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:18.875982 UTC] Performing policy update
[2018-01-21 14:45:18.876594 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:18.993863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:20.418770 UTC] Performing line search
[2018-01-21 14:45:20.605765 UTC] Updating baseline
[2018-01-21 14:45:23.735058 UTC] Computing logging information
-------------------------------------
| Iteration            | 852        |
| ExpectedImprovement  | 0.017513   |
| ActualImprovement    | 0.016531   |
| ImprovementRatio     | 0.94389    |
| MeanKL               | 0.0077824  |
| Entropy              | -0.86241   |
| Perplexity           | 0.42214    |
| AveragePolicyStd     | 0.21169    |
| AveragePolicyStd[0]  | 0.22586    |
| AveragePolicyStd[1]  | 0.23537    |
| AveragePolicyStd[2]  | 0.15966    |
| AveragePolicyStd[3]  | 0.21525    |
| AveragePolicyStd[4]  | 0.19037    |
| AveragePolicyStd[5]  | 0.24362    |
| AverageReturn        | 1531.7     |
| MinReturn            | 986.02     |
| MaxReturn            | 1626.8     |
| StdReturn            | 102.71     |
| AverageEpisodeLength | 986.57     |
| MinEpisodeLength     | 645        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 58.367     |
| TotalNEpisodes       | 21071      |
| TotalNSamples        | 4.2653e+06 |
| ExplainedVariance    | -0.0015196 |
-------------------------------------
[2018-01-21 14:45:24.473007 UTC] Saving snapshot
[2018-01-21 14:45:24.473184 UTC] Starting iteration 853
[2018-01-21 14:45:24.473304 UTC] Start collecting samples
[2018-01-21 14:45:29.012997 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:29.162658 UTC] Performing policy update
[2018-01-21 14:45:29.163346 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:29.283996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:30.676284 UTC] Performing line search
[2018-01-21 14:45:30.864089 UTC] Updating baseline
[2018-01-21 14:45:32.859660 UTC] Computing logging information
-------------------------------------
| Iteration            | 853        |
| ExpectedImprovement  | 0.019792   |
| ActualImprovement    | 0.018412   |
| ImprovementRatio     | 0.93029    |
| MeanKL               | 0.0071926  |
| Entropy              | -0.86513   |
| Perplexity           | 0.421      |
| AveragePolicyStd     | 0.21165    |
| AveragePolicyStd[0]  | 0.2256     |
| AveragePolicyStd[1]  | 0.23596    |
| AveragePolicyStd[2]  | 0.15886    |
| AveragePolicyStd[3]  | 0.21506    |
| AveragePolicyStd[4]  | 0.1904     |
| AveragePolicyStd[5]  | 0.24402    |
| AverageReturn        | 1524.2     |
| MinReturn            | 775.22     |
| MaxReturn            | 1626.8     |
| StdReturn            | 127.5      |
| AverageEpisodeLength | 981.87     |
| MinEpisodeLength     | 530        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 73.941     |
| TotalNEpisodes       | 21075      |
| TotalNSamples        | 4.2688e+06 |
| ExplainedVariance    | 0.13275    |
-------------------------------------
[2018-01-21 14:45:33.611280 UTC] Saving snapshot
[2018-01-21 14:45:33.611531 UTC] Starting iteration 854
[2018-01-21 14:45:33.611717 UTC] Start collecting samples
[2018-01-21 14:45:38.183397 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:38.341007 UTC] Performing policy update
[2018-01-21 14:45:38.341632 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:38.462037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:39.897204 UTC] Performing line search
[2018-01-21 14:45:40.084012 UTC] Updating baseline
[2018-01-21 14:45:42.257236 UTC] Computing logging information
-------------------------------------
| Iteration            | 854        |
| ExpectedImprovement  | 0.019629   |
| ActualImprovement    | 0.019115   |
| ImprovementRatio     | 0.97384    |
| MeanKL               | 0.0074936  |
| Entropy              | -0.86648   |
| Perplexity           | 0.42043    |
| AveragePolicyStd     | 0.21161    |
| AveragePolicyStd[0]  | 0.22551    |
| AveragePolicyStd[1]  | 0.23585    |
| AveragePolicyStd[2]  | 0.15884    |
| AveragePolicyStd[3]  | 0.21528    |
| AveragePolicyStd[4]  | 0.19007    |
| AveragePolicyStd[5]  | 0.24412    |
| AverageReturn        | 1506.7     |
| MinReturn            | 365.28     |
| MaxReturn            | 1626.8     |
| StdReturn            | 185.84     |
| AverageEpisodeLength | 969.94     |
| MinEpisodeLength     | 261        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.42     |
| TotalNEpisodes       | 21081      |
| TotalNSamples        | 4.2736e+06 |
| ExplainedVariance    | 0.22294    |
-------------------------------------
[2018-01-21 14:45:43.030005 UTC] Saving snapshot
[2018-01-21 14:45:43.030252 UTC] Starting iteration 855
[2018-01-21 14:45:43.030458 UTC] Start collecting samples
[2018-01-21 14:45:47.900757 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:48.030999 UTC] Performing policy update
[2018-01-21 14:45:48.032076 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:48.150115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:49.580127 UTC] Performing line search
[2018-01-21 14:45:49.769862 UTC] Updating baseline
[2018-01-21 14:45:51.949741 UTC] Computing logging information
-------------------------------------
| Iteration            | 855        |
| ExpectedImprovement  | 0.018163   |
| ActualImprovement    | 0.017553   |
| ImprovementRatio     | 0.96645    |
| MeanKL               | 0.0078048  |
| Entropy              | -0.86719   |
| Perplexity           | 0.42013    |
| AveragePolicyStd     | 0.21158    |
| AveragePolicyStd[0]  | 0.22531    |
| AveragePolicyStd[1]  | 0.23574    |
| AveragePolicyStd[2]  | 0.15897    |
| AveragePolicyStd[3]  | 0.21555    |
| AveragePolicyStd[4]  | 0.18978    |
| AveragePolicyStd[5]  | 0.24413    |
| AverageReturn        | 1508.5     |
| MinReturn            | 365.28     |
| MaxReturn            | 1626.8     |
| StdReturn            | 194.31     |
| AverageEpisodeLength | 970.58     |
| MinEpisodeLength     | 261        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.18     |
| TotalNEpisodes       | 21087      |
| TotalNSamples        | 4.2791e+06 |
| ExplainedVariance    | 0.19541    |
-------------------------------------
[2018-01-21 14:45:52.665230 UTC] Saving snapshot
[2018-01-21 14:45:52.665445 UTC] Starting iteration 856
[2018-01-21 14:45:52.665626 UTC] Start collecting samples
[2018-01-21 14:45:57.194655 UTC] Computing input variables for policy optimization
[2018-01-21 14:45:57.318620 UTC] Performing policy update
[2018-01-21 14:45:57.319316 UTC] Computing gradient in Euclidean space
[2018-01-21 14:45:57.445503 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:45:58.893515 UTC] Performing line search
[2018-01-21 14:45:59.082415 UTC] Updating baseline
[2018-01-21 14:46:01.007822 UTC] Computing logging information
------------------------------------
| Iteration            | 856       |
| ExpectedImprovement  | 0.017918  |
| ActualImprovement    | 0.016277  |
| ImprovementRatio     | 0.90842   |
| MeanKL               | 0.007395  |
| Entropy              | -0.8652   |
| Perplexity           | 0.42097   |
| AveragePolicyStd     | 0.21162   |
| AveragePolicyStd[0]  | 0.22494   |
| AveragePolicyStd[1]  | 0.23584   |
| AveragePolicyStd[2]  | 0.1592    |
| AveragePolicyStd[3]  | 0.21534   |
| AveragePolicyStd[4]  | 0.1904    |
| AveragePolicyStd[5]  | 0.244     |
| AverageReturn        | 1506.6    |
| MinReturn            | 365.28    |
| MaxReturn            | 1626.8    |
| StdReturn            | 194.29    |
| AverageEpisodeLength | 970.28    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 118.14    |
| TotalNEpisodes       | 21092     |
| TotalNSamples        | 4.284e+06 |
| ExplainedVariance    | 0.23101   |
------------------------------------
[2018-01-21 14:46:01.694724 UTC] Saving snapshot
[2018-01-21 14:46:01.694947 UTC] Starting iteration 857
[2018-01-21 14:46:01.695126 UTC] Start collecting samples
[2018-01-21 14:46:06.135472 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:06.256171 UTC] Performing policy update
[2018-01-21 14:46:06.257262 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:06.386232 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:07.776195 UTC] Performing line search
[2018-01-21 14:46:07.972855 UTC] Updating baseline
[2018-01-21 14:46:10.120160 UTC] Computing logging information
------------------------------------
| Iteration            | 857       |
| ExpectedImprovement  | 0.020249  |
| ActualImprovement    | 0.01984   |
| ImprovementRatio     | 0.97979   |
| MeanKL               | 0.0079708 |
| Entropy              | -0.86787  |
| Perplexity           | 0.41984   |
| AveragePolicyStd     | 0.21158   |
| AveragePolicyStd[0]  | 0.22541   |
| AveragePolicyStd[1]  | 0.23619   |
| AveragePolicyStd[2]  | 0.1586    |
| AveragePolicyStd[3]  | 0.21433   |
| AveragePolicyStd[4]  | 0.19046   |
| AveragePolicyStd[5]  | 0.24448   |
| AverageReturn        | 1507.5    |
| MinReturn            | 365.28    |
| MaxReturn            | 1626.8    |
| StdReturn            | 194.49    |
| AverageEpisodeLength | 970.28    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 118.14    |
| TotalNEpisodes       | 21097     |
| TotalNSamples        | 4.289e+06 |
| ExplainedVariance    | -0.083296 |
------------------------------------
[2018-01-21 14:46:10.823967 UTC] Saving snapshot
[2018-01-21 14:46:10.824207 UTC] Starting iteration 858
[2018-01-21 14:46:10.824356 UTC] Start collecting samples
[2018-01-21 14:46:15.364486 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:15.493137 UTC] Performing policy update
[2018-01-21 14:46:15.493730 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:15.610636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:17.012701 UTC] Performing line search
[2018-01-21 14:46:17.198284 UTC] Updating baseline
[2018-01-21 14:46:19.350819 UTC] Computing logging information
--------------------------------------
| Iteration            | 858         |
| ExpectedImprovement  | 0.017462    |
| ActualImprovement    | 0.016617    |
| ImprovementRatio     | 0.95159     |
| MeanKL               | 0.0082974   |
| Entropy              | -0.86233    |
| Perplexity           | 0.42218     |
| AveragePolicyStd     | 0.21176     |
| AveragePolicyStd[0]  | 0.2255      |
| AveragePolicyStd[1]  | 0.23609     |
| AveragePolicyStd[2]  | 0.1588      |
| AveragePolicyStd[3]  | 0.21467     |
| AveragePolicyStd[4]  | 0.19077     |
| AveragePolicyStd[5]  | 0.24476     |
| AverageReturn        | 1514.1      |
| MinReturn            | 365.28      |
| MaxReturn            | 1626.8      |
| StdReturn            | 187.61      |
| AverageEpisodeLength | 973.83      |
| MinEpisodeLength     | 261         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 113.56      |
| TotalNEpisodes       | 21102       |
| TotalNSamples        | 4.294e+06   |
| ExplainedVariance    | -0.00047987 |
--------------------------------------
[2018-01-21 14:46:20.106163 UTC] Saving snapshot
[2018-01-21 14:46:20.106406 UTC] Starting iteration 859
[2018-01-21 14:46:20.106606 UTC] Start collecting samples
[2018-01-21 14:46:24.650404 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:24.786854 UTC] Performing policy update
[2018-01-21 14:46:24.787508 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:24.916390 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:26.345259 UTC] Performing line search
[2018-01-21 14:46:26.537176 UTC] Updating baseline
[2018-01-21 14:46:29.298880 UTC] Computing logging information
------------------------------------
| Iteration            | 859       |
| ExpectedImprovement  | 0.018208  |
| ActualImprovement    | 0.016776  |
| ImprovementRatio     | 0.92134   |
| MeanKL               | 0.0077531 |
| Entropy              | -0.86734  |
| Perplexity           | 0.42007   |
| AveragePolicyStd     | 0.21154   |
| AveragePolicyStd[0]  | 0.22541   |
| AveragePolicyStd[1]  | 0.23534   |
| AveragePolicyStd[2]  | 0.15907   |
| AveragePolicyStd[3]  | 0.21446   |
| AveragePolicyStd[4]  | 0.19093   |
| AveragePolicyStd[5]  | 0.24402   |
| AverageReturn        | 1513.3    |
| MinReturn            | 365.28    |
| MaxReturn            | 1626.8    |
| StdReturn            | 187.58    |
| AverageEpisodeLength | 973.83    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 113.56    |
| TotalNEpisodes       | 21107     |
| TotalNSamples        | 4.299e+06 |
| ExplainedVariance    | -0.001551 |
------------------------------------
[2018-01-21 14:46:30.028740 UTC] Saving snapshot
[2018-01-21 14:46:30.028963 UTC] Starting iteration 860
[2018-01-21 14:46:30.029133 UTC] Start collecting samples
[2018-01-21 14:46:34.460535 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:34.605688 UTC] Performing policy update
[2018-01-21 14:46:34.606291 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:34.727867 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:36.156671 UTC] Performing line search
[2018-01-21 14:46:36.344643 UTC] Updating baseline
[2018-01-21 14:46:38.957118 UTC] Computing logging information
-------------------------------------
| Iteration            | 860        |
| ExpectedImprovement  | 0.015397   |
| ActualImprovement    | 0.014828   |
| ImprovementRatio     | 0.96302    |
| MeanKL               | 0.007916   |
| Entropy              | -0.86476   |
| Perplexity           | 0.42115    |
| AveragePolicyStd     | 0.21166    |
| AveragePolicyStd[0]  | 0.22588    |
| AveragePolicyStd[1]  | 0.23532    |
| AveragePolicyStd[2]  | 0.15854    |
| AveragePolicyStd[3]  | 0.2149     |
| AveragePolicyStd[4]  | 0.19114    |
| AveragePolicyStd[5]  | 0.24419    |
| AverageReturn        | 1514.9     |
| MinReturn            | 365.28     |
| MaxReturn            | 1626.8     |
| StdReturn            | 185.89     |
| AverageEpisodeLength | 975.19     |
| MinEpisodeLength     | 261        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.05     |
| TotalNEpisodes       | 21113      |
| TotalNSamples        | 4.305e+06  |
| ExplainedVariance    | 0.00025995 |
-------------------------------------
[2018-01-21 14:46:39.671484 UTC] Saving snapshot
[2018-01-21 14:46:39.681077 UTC] Starting iteration 861
[2018-01-21 14:46:39.681329 UTC] Start collecting samples
[2018-01-21 14:46:44.110914 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:44.236264 UTC] Performing policy update
[2018-01-21 14:46:44.236935 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:44.347269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:45.767780 UTC] Performing line search
[2018-01-21 14:46:45.975099 UTC] Updating baseline
[2018-01-21 14:46:48.066558 UTC] Computing logging information
--------------------------------------
| Iteration            | 861         |
| ExpectedImprovement  | 0.016837    |
| ActualImprovement    | 0.016186    |
| ImprovementRatio     | 0.96131     |
| MeanKL               | 0.007745    |
| Entropy              | -0.85415    |
| Perplexity           | 0.42564     |
| AveragePolicyStd     | 0.21203     |
| AveragePolicyStd[0]  | 0.22577     |
| AveragePolicyStd[1]  | 0.23651     |
| AveragePolicyStd[2]  | 0.15878     |
| AveragePolicyStd[3]  | 0.21583     |
| AveragePolicyStd[4]  | 0.19147     |
| AveragePolicyStd[5]  | 0.24384     |
| AverageReturn        | 1514        |
| MinReturn            | 365.28      |
| MaxReturn            | 1626.8      |
| StdReturn            | 185.74      |
| AverageEpisodeLength | 975.23      |
| MinEpisodeLength     | 261         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 113.06      |
| TotalNEpisodes       | 21117       |
| TotalNSamples        | 4.309e+06   |
| ExplainedVariance    | -0.00065533 |
--------------------------------------
[2018-01-21 14:46:48.785236 UTC] Saving snapshot
[2018-01-21 14:46:48.785472 UTC] Starting iteration 862
[2018-01-21 14:46:48.785680 UTC] Start collecting samples
[2018-01-21 14:46:53.339567 UTC] Computing input variables for policy optimization
[2018-01-21 14:46:53.467870 UTC] Performing policy update
[2018-01-21 14:46:53.468878 UTC] Computing gradient in Euclidean space
[2018-01-21 14:46:53.592041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:46:55.083151 UTC] Performing line search
[2018-01-21 14:46:55.281961 UTC] Updating baseline
[2018-01-21 14:46:57.504954 UTC] Computing logging information
-------------------------------------
| Iteration            | 862        |
| ExpectedImprovement  | 0.018295   |
| ActualImprovement    | 0.017464   |
| ImprovementRatio     | 0.95459    |
| MeanKL               | 0.0078731  |
| Entropy              | -0.85397   |
| Perplexity           | 0.42572    |
| AveragePolicyStd     | 0.21202    |
| AveragePolicyStd[0]  | 0.22596    |
| AveragePolicyStd[1]  | 0.23583    |
| AveragePolicyStd[2]  | 0.15895    |
| AveragePolicyStd[3]  | 0.21594    |
| AveragePolicyStd[4]  | 0.19152    |
| AveragePolicyStd[5]  | 0.24394    |
| AverageReturn        | 1513.1     |
| MinReturn            | 365.28     |
| MaxReturn            | 1626.8     |
| StdReturn            | 185.4      |
| AverageEpisodeLength | 975.73     |
| MinEpisodeLength     | 261        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.06     |
| TotalNEpisodes       | 21123      |
| TotalNSamples        | 4.315e+06  |
| ExplainedVariance    | -0.0019759 |
-------------------------------------
[2018-01-21 14:46:58.202748 UTC] Saving snapshot
[2018-01-21 14:46:58.202999 UTC] Starting iteration 863
[2018-01-21 14:46:58.203181 UTC] Start collecting samples
[2018-01-21 14:47:02.680119 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:02.801670 UTC] Performing policy update
[2018-01-21 14:47:02.802674 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:02.928200 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:04.303719 UTC] Performing line search
[2018-01-21 14:47:04.489215 UTC] Updating baseline
[2018-01-21 14:47:06.462773 UTC] Computing logging information
------------------------------------
| Iteration            | 863       |
| ExpectedImprovement  | 0.018381  |
| ActualImprovement    | 0.017283  |
| ImprovementRatio     | 0.94028   |
| MeanKL               | 0.008234  |
| Entropy              | -0.85345  |
| Perplexity           | 0.42594   |
| AveragePolicyStd     | 0.21208   |
| AveragePolicyStd[0]  | 0.22584   |
| AveragePolicyStd[1]  | 0.23647   |
| AveragePolicyStd[2]  | 0.15855   |
| AveragePolicyStd[3]  | 0.21594   |
| AveragePolicyStd[4]  | 0.19141   |
| AveragePolicyStd[5]  | 0.24428   |
| AverageReturn        | 1511.6    |
| MinReturn            | 365.28    |
| MaxReturn            | 1626.8    |
| StdReturn            | 185.1     |
| AverageEpisodeLength | 975.73    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 113.06    |
| TotalNEpisodes       | 21127     |
| TotalNSamples        | 4.319e+06 |
| ExplainedVariance    | 0.0054227 |
------------------------------------
[2018-01-21 14:47:07.188213 UTC] Saving snapshot
[2018-01-21 14:47:07.188476 UTC] Starting iteration 864
[2018-01-21 14:47:07.188665 UTC] Start collecting samples
[2018-01-21 14:47:11.689489 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:11.832632 UTC] Performing policy update
[2018-01-21 14:47:11.833311 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:11.963892 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:13.397790 UTC] Performing line search
[2018-01-21 14:47:13.599286 UTC] Updating baseline
[2018-01-21 14:47:16.564572 UTC] Computing logging information
------------------------------------
| Iteration            | 864       |
| ExpectedImprovement  | 0.019253  |
| ActualImprovement    | 0.018076  |
| ImprovementRatio     | 0.93887   |
| MeanKL               | 0.0076822 |
| Entropy              | -0.86012  |
| Perplexity           | 0.42311   |
| AveragePolicyStd     | 0.21186   |
| AveragePolicyStd[0]  | 0.22517   |
| AveragePolicyStd[1]  | 0.23661   |
| AveragePolicyStd[2]  | 0.15829   |
| AveragePolicyStd[3]  | 0.21579   |
| AveragePolicyStd[4]  | 0.19112   |
| AveragePolicyStd[5]  | 0.24418   |
| AverageReturn        | 1494.1    |
| MinReturn            | 365.24    |
| MaxReturn            | 1626.8    |
| StdReturn            | 222.17    |
| AverageEpisodeLength | 965.17    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 136.44    |
| TotalNEpisodes       | 21133     |
| TotalNSamples        | 4.324e+06 |
| ExplainedVariance    | 0.16451   |
------------------------------------
[2018-01-21 14:47:17.272175 UTC] Saving snapshot
[2018-01-21 14:47:17.272651 UTC] Starting iteration 865
[2018-01-21 14:47:17.273028 UTC] Start collecting samples
[2018-01-21 14:47:21.761203 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:21.896364 UTC] Performing policy update
[2018-01-21 14:47:21.897158 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:22.036688 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:23.461671 UTC] Performing line search
[2018-01-21 14:47:23.656793 UTC] Updating baseline
[2018-01-21 14:47:25.887303 UTC] Computing logging information
------------------------------------
| Iteration            | 865       |
| ExpectedImprovement  | 0.017893  |
| ActualImprovement    | 0.016825  |
| ImprovementRatio     | 0.94032   |
| MeanKL               | 0.007587  |
| Entropy              | -0.86115  |
| Perplexity           | 0.42267   |
| AveragePolicyStd     | 0.21184   |
| AveragePolicyStd[0]  | 0.2254    |
| AveragePolicyStd[1]  | 0.2369    |
| AveragePolicyStd[2]  | 0.15816   |
| AveragePolicyStd[3]  | 0.21551   |
| AveragePolicyStd[4]  | 0.19087   |
| AveragePolicyStd[5]  | 0.24421   |
| AverageReturn        | 1492.8    |
| MinReturn            | 365.24    |
| MaxReturn            | 1626.8    |
| StdReturn            | 221.77    |
| AverageEpisodeLength | 965.17    |
| MinEpisodeLength     | 261       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 136.44    |
| TotalNEpisodes       | 21138     |
| TotalNSamples        | 4.329e+06 |
| ExplainedVariance    | -0.028398 |
------------------------------------
[2018-01-21 14:47:26.683778 UTC] Saving snapshot
[2018-01-21 14:47:26.684071 UTC] Starting iteration 866
[2018-01-21 14:47:26.684266 UTC] Start collecting samples
[2018-01-21 14:47:31.217283 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:31.342698 UTC] Performing policy update
[2018-01-21 14:47:31.343289 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:31.462651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:32.872064 UTC] Performing line search
[2018-01-21 14:47:33.069032 UTC] Updating baseline
[2018-01-21 14:47:35.189142 UTC] Computing logging information
-------------------------------------
| Iteration            | 866        |
| ExpectedImprovement  | 0.018172   |
| ActualImprovement    | 0.017229   |
| ImprovementRatio     | 0.94812    |
| MeanKL               | 0.0074405  |
| Entropy              | -0.86309   |
| Perplexity           | 0.42186    |
| AveragePolicyStd     | 0.21181    |
| AveragePolicyStd[0]  | 0.2252     |
| AveragePolicyStd[1]  | 0.23699    |
| AveragePolicyStd[2]  | 0.15748    |
| AveragePolicyStd[3]  | 0.21581    |
| AveragePolicyStd[4]  | 0.19113    |
| AveragePolicyStd[5]  | 0.24423    |
| AverageReturn        | 1473       |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 263.19     |
| AverageEpisodeLength | 952.21     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.94     |
| TotalNEpisodes       | 21144      |
| TotalNSamples        | 4.3337e+06 |
| ExplainedVariance    | 0.23547    |
-------------------------------------
[2018-01-21 14:47:35.923589 UTC] Saving snapshot
[2018-01-21 14:47:35.923814 UTC] Starting iteration 867
[2018-01-21 14:47:35.923995 UTC] Start collecting samples
[2018-01-21 14:47:40.446086 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:40.586343 UTC] Performing policy update
[2018-01-21 14:47:40.586985 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:40.704337 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:42.111729 UTC] Performing line search
[2018-01-21 14:47:42.315972 UTC] Updating baseline
[2018-01-21 14:47:44.100615 UTC] Computing logging information
-------------------------------------
| Iteration            | 867        |
| ExpectedImprovement  | 0.018103   |
| ActualImprovement    | 0.01629    |
| ImprovementRatio     | 0.89986    |
| MeanKL               | 0.0074253  |
| Entropy              | -0.86129   |
| Perplexity           | 0.42262    |
| AveragePolicyStd     | 0.21186    |
| AveragePolicyStd[0]  | 0.22504    |
| AveragePolicyStd[1]  | 0.23671    |
| AveragePolicyStd[2]  | 0.15746    |
| AveragePolicyStd[3]  | 0.2162     |
| AveragePolicyStd[4]  | 0.19155    |
| AveragePolicyStd[5]  | 0.24419    |
| AverageReturn        | 1463.5     |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 277.51     |
| AverageEpisodeLength | 947.69     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.54     |
| TotalNEpisodes       | 21151      |
| TotalNSamples        | 4.3401e+06 |
| ExplainedVariance    | 0.10379    |
-------------------------------------
[2018-01-21 14:47:44.818904 UTC] Saving snapshot
[2018-01-21 14:47:44.819187 UTC] Starting iteration 868
[2018-01-21 14:47:44.819390 UTC] Start collecting samples
[2018-01-21 14:47:49.390552 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:49.547504 UTC] Performing policy update
[2018-01-21 14:47:49.548581 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:49.656041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:47:51.056909 UTC] Performing line search
[2018-01-21 14:47:51.242549 UTC] Updating baseline
[2018-01-21 14:47:53.261187 UTC] Computing logging information
-------------------------------------
| Iteration            | 868        |
| ExpectedImprovement  | 0.017893   |
| ActualImprovement    | 0.016209   |
| ImprovementRatio     | 0.90584    |
| MeanKL               | 0.0076094  |
| Entropy              | -0.86325   |
| Perplexity           | 0.42179    |
| AveragePolicyStd     | 0.2118     |
| AveragePolicyStd[0]  | 0.22487    |
| AveragePolicyStd[1]  | 0.2362     |
| AveragePolicyStd[2]  | 0.15697    |
| AveragePolicyStd[3]  | 0.21635    |
| AveragePolicyStd[4]  | 0.19218    |
| AveragePolicyStd[5]  | 0.24421    |
| AverageReturn        | 1455.3     |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 285.59     |
| AverageEpisodeLength | 942.91     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.63     |
| TotalNEpisodes       | 21156      |
| TotalNSamples        | 4.3446e+06 |
| ExplainedVariance    | 0.15224    |
-------------------------------------
[2018-01-21 14:47:54.010156 UTC] Saving snapshot
[2018-01-21 14:47:54.010401 UTC] Starting iteration 869
[2018-01-21 14:47:54.010595 UTC] Start collecting samples
[2018-01-21 14:47:58.584856 UTC] Computing input variables for policy optimization
[2018-01-21 14:47:58.725278 UTC] Performing policy update
[2018-01-21 14:47:58.726463 UTC] Computing gradient in Euclidean space
[2018-01-21 14:47:58.853132 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:00.269708 UTC] Performing line search
[2018-01-21 14:48:00.467855 UTC] Updating baseline
[2018-01-21 14:48:03.250515 UTC] Computing logging information
-------------------------------------
| Iteration            | 869        |
| ExpectedImprovement  | 0.017738   |
| ActualImprovement    | 0.017426   |
| ImprovementRatio     | 0.98245    |
| MeanKL               | 0.0072731  |
| Entropy              | -0.86888   |
| Perplexity           | 0.41942    |
| AveragePolicyStd     | 0.21163    |
| AveragePolicyStd[0]  | 0.22464    |
| AveragePolicyStd[1]  | 0.23561    |
| AveragePolicyStd[2]  | 0.15681    |
| AveragePolicyStd[3]  | 0.2167     |
| AveragePolicyStd[4]  | 0.19121    |
| AveragePolicyStd[5]  | 0.2448     |
| AverageReturn        | 1454.8     |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 285.44     |
| AverageEpisodeLength | 942.91     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.63     |
| TotalNEpisodes       | 21160      |
| TotalNSamples        | 4.3486e+06 |
| ExplainedVariance    | -0.068999  |
-------------------------------------
[2018-01-21 14:48:03.970096 UTC] Saving snapshot
[2018-01-21 14:48:03.970395 UTC] Starting iteration 870
[2018-01-21 14:48:03.970618 UTC] Start collecting samples
[2018-01-21 14:48:08.481021 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:08.606892 UTC] Performing policy update
[2018-01-21 14:48:08.607570 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:08.732171 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:10.166390 UTC] Performing line search
[2018-01-21 14:48:10.355839 UTC] Updating baseline
[2018-01-21 14:48:12.878823 UTC] Computing logging information
-------------------------------------
| Iteration            | 870        |
| ExpectedImprovement  | 0.017902   |
| ActualImprovement    | 0.016202   |
| ImprovementRatio     | 0.90507    |
| MeanKL               | 0.0074146  |
| Entropy              | -0.8594    |
| Perplexity           | 0.42342    |
| AveragePolicyStd     | 0.21195    |
| AveragePolicyStd[0]  | 0.22478    |
| AveragePolicyStd[1]  | 0.236      |
| AveragePolicyStd[2]  | 0.15728    |
| AveragePolicyStd[3]  | 0.21722    |
| AveragePolicyStd[4]  | 0.19133    |
| AveragePolicyStd[5]  | 0.24509    |
| AverageReturn        | 1455       |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 285.61     |
| AverageEpisodeLength | 942.91     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.63     |
| TotalNEpisodes       | 21165      |
| TotalNSamples        | 4.3536e+06 |
| ExplainedVariance    | -0.0045914 |
-------------------------------------
[2018-01-21 14:48:13.625890 UTC] Saving snapshot
[2018-01-21 14:48:13.634957 UTC] Starting iteration 871
[2018-01-21 14:48:13.635191 UTC] Start collecting samples
[2018-01-21 14:48:18.238552 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:18.370266 UTC] Performing policy update
[2018-01-21 14:48:18.371039 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:18.492632 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:19.892205 UTC] Performing line search
[2018-01-21 14:48:20.079330 UTC] Updating baseline
[2018-01-21 14:48:22.118684 UTC] Computing logging information
-------------------------------------
| Iteration            | 871        |
| ExpectedImprovement  | 0.019543   |
| ActualImprovement    | 0.018808   |
| ImprovementRatio     | 0.96239    |
| MeanKL               | 0.0075334  |
| Entropy              | -0.86432   |
| Perplexity           | 0.42134    |
| AveragePolicyStd     | 0.21175    |
| AveragePolicyStd[0]  | 0.22534    |
| AveragePolicyStd[1]  | 0.23447    |
| AveragePolicyStd[2]  | 0.15756    |
| AveragePolicyStd[3]  | 0.21651    |
| AveragePolicyStd[4]  | 0.19114    |
| AveragePolicyStd[5]  | 0.24547    |
| AverageReturn        | 1441.9     |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 303.21     |
| AverageEpisodeLength | 935.98     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.21     |
| TotalNEpisodes       | 21174      |
| TotalNSamples        | 4.3614e+06 |
| ExplainedVariance    | 0.12024    |
-------------------------------------
[2018-01-21 14:48:22.914285 UTC] Saving snapshot
[2018-01-21 14:48:22.914627 UTC] Starting iteration 872
[2018-01-21 14:48:22.914829 UTC] Start collecting samples
[2018-01-21 14:48:27.380894 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:27.503557 UTC] Performing policy update
[2018-01-21 14:48:27.504620 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:27.632927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:29.058803 UTC] Performing line search
[2018-01-21 14:48:29.257794 UTC] Updating baseline
[2018-01-21 14:48:31.888100 UTC] Computing logging information
-------------------------------------
| Iteration            | 872        |
| ExpectedImprovement  | 0.022398   |
| ActualImprovement    | 0.018258   |
| ImprovementRatio     | 0.81519    |
| MeanKL               | 0.0072313  |
| Entropy              | -0.86563   |
| Perplexity           | 0.42079    |
| AveragePolicyStd     | 0.21171    |
| AveragePolicyStd[0]  | 0.22543    |
| AveragePolicyStd[1]  | 0.23441    |
| AveragePolicyStd[2]  | 0.15741    |
| AveragePolicyStd[3]  | 0.21618    |
| AveragePolicyStd[4]  | 0.19129    |
| AveragePolicyStd[5]  | 0.24554    |
| AverageReturn        | 1439.2     |
| MinReturn            | 307.89     |
| MaxReturn            | 1628.3     |
| StdReturn            | 302.85     |
| AverageEpisodeLength | 934.48     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.29     |
| TotalNEpisodes       | 21176      |
| TotalNSamples        | 4.3633e+06 |
| ExplainedVariance    | 0.28578    |
-------------------------------------
[2018-01-21 14:48:32.608330 UTC] Saving snapshot
[2018-01-21 14:48:32.608557 UTC] Starting iteration 873
[2018-01-21 14:48:32.608741 UTC] Start collecting samples
[2018-01-21 14:48:37.338292 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:37.478351 UTC] Performing policy update
[2018-01-21 14:48:37.479420 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:37.604733 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:39.008417 UTC] Performing line search
[2018-01-21 14:48:39.194751 UTC] Updating baseline
[2018-01-21 14:48:41.067601 UTC] Computing logging information
-------------------------------------
| Iteration            | 873        |
| ExpectedImprovement  | 0.018497   |
| ActualImprovement    | 0.017249   |
| ImprovementRatio     | 0.93256    |
| MeanKL               | 0.0074505  |
| Entropy              | -0.87794   |
| Perplexity           | 0.41564    |
| AveragePolicyStd     | 0.21126    |
| AveragePolicyStd[0]  | 0.22499    |
| AveragePolicyStd[1]  | 0.23245    |
| AveragePolicyStd[2]  | 0.15735    |
| AveragePolicyStd[3]  | 0.21609    |
| AveragePolicyStd[4]  | 0.19078    |
| AveragePolicyStd[5]  | 0.24591    |
| AverageReturn        | 1443.1     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 293.63     |
| AverageEpisodeLength | 937.61     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.05     |
| TotalNEpisodes       | 21182      |
| TotalNSamples        | 4.3684e+06 |
| ExplainedVariance    | 0.20504    |
-------------------------------------
[2018-01-21 14:48:41.795285 UTC] Saving snapshot
[2018-01-21 14:48:41.795691 UTC] Starting iteration 874
[2018-01-21 14:48:41.795936 UTC] Start collecting samples
[2018-01-21 14:48:46.410648 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:46.551727 UTC] Performing policy update
[2018-01-21 14:48:46.552456 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:46.681193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:48.130759 UTC] Performing line search
[2018-01-21 14:48:48.327770 UTC] Updating baseline
[2018-01-21 14:48:50.362263 UTC] Computing logging information
-------------------------------------
| Iteration            | 874        |
| ExpectedImprovement  | 0.017063   |
| ActualImprovement    | 0.016371   |
| ImprovementRatio     | 0.95944    |
| MeanKL               | 0.0078066  |
| Entropy              | -0.8818    |
| Perplexity           | 0.41404    |
| AveragePolicyStd     | 0.21111    |
| AveragePolicyStd[0]  | 0.22523    |
| AveragePolicyStd[1]  | 0.23138    |
| AveragePolicyStd[2]  | 0.15711    |
| AveragePolicyStd[3]  | 0.21606    |
| AveragePolicyStd[4]  | 0.19122    |
| AveragePolicyStd[5]  | 0.24567    |
| AverageReturn        | 1453.3     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 283.2      |
| AverageEpisodeLength | 942.96     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.26     |
| TotalNEpisodes       | 21190      |
| TotalNSamples        | 4.3763e+06 |
| ExplainedVariance    | 0.055267   |
-------------------------------------
[2018-01-21 14:48:51.158551 UTC] Saving snapshot
[2018-01-21 14:48:51.158790 UTC] Starting iteration 875
[2018-01-21 14:48:51.158943 UTC] Start collecting samples
[2018-01-21 14:48:55.850943 UTC] Computing input variables for policy optimization
[2018-01-21 14:48:56.005835 UTC] Performing policy update
[2018-01-21 14:48:56.008895 UTC] Computing gradient in Euclidean space
[2018-01-21 14:48:56.134767 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:48:57.576886 UTC] Performing line search
[2018-01-21 14:48:57.766301 UTC] Updating baseline
[2018-01-21 14:48:59.675619 UTC] Computing logging information
-------------------------------------
| Iteration            | 875        |
| ExpectedImprovement  | 0.020532   |
| ActualImprovement    | 0.019744   |
| ImprovementRatio     | 0.9616     |
| MeanKL               | 0.0074052  |
| Entropy              | -0.89205   |
| Perplexity           | 0.40982    |
| AveragePolicyStd     | 0.21065    |
| AveragePolicyStd[0]  | 0.22458    |
| AveragePolicyStd[1]  | 0.22997    |
| AveragePolicyStd[2]  | 0.15782    |
| AveragePolicyStd[3]  | 0.21539    |
| AveragePolicyStd[4]  | 0.1916     |
| AveragePolicyStd[5]  | 0.24453    |
| AverageReturn        | 1444       |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 292.8      |
| AverageEpisodeLength | 937.89     |
| MinEpisodeLength     | 233        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.69     |
| TotalNEpisodes       | 21192      |
| TotalNSamples        | 4.3778e+06 |
| ExplainedVariance    | 0.31141    |
-------------------------------------
[2018-01-21 14:49:00.395380 UTC] Saving snapshot
[2018-01-21 14:49:00.395615 UTC] Starting iteration 876
[2018-01-21 14:49:00.395765 UTC] Start collecting samples
[2018-01-21 14:49:04.988878 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:05.146948 UTC] Performing policy update
[2018-01-21 14:49:05.148035 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:05.268722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:06.715347 UTC] Performing line search
[2018-01-21 14:49:06.911729 UTC] Updating baseline
[2018-01-21 14:49:08.775668 UTC] Computing logging information
-------------------------------------
| Iteration            | 876        |
| ExpectedImprovement  | 0.019965   |
| ActualImprovement    | 0.018891   |
| ImprovementRatio     | 0.94622    |
| MeanKL               | 0.0076374  |
| Entropy              | -0.89354   |
| Perplexity           | 0.4092     |
| AveragePolicyStd     | 0.21063    |
| AveragePolicyStd[0]  | 0.22501    |
| AveragePolicyStd[1]  | 0.23009    |
| AveragePolicyStd[2]  | 0.15754    |
| AveragePolicyStd[3]  | 0.21494    |
| AveragePolicyStd[4]  | 0.19124    |
| AveragePolicyStd[5]  | 0.24498    |
| AverageReturn        | 1418.7     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 324.74     |
| AverageEpisodeLength | 920.77     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.68     |
| TotalNEpisodes       | 21201      |
| TotalNSamples        | 4.3851e+06 |
| ExplainedVariance    | 0.23124    |
-------------------------------------
[2018-01-21 14:49:09.487008 UTC] Saving snapshot
[2018-01-21 14:49:09.487253 UTC] Starting iteration 877
[2018-01-21 14:49:09.487435 UTC] Start collecting samples
[2018-01-21 14:49:13.985603 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:14.123836 UTC] Performing policy update
[2018-01-21 14:49:14.124921 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:14.243277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:15.650191 UTC] Performing line search
[2018-01-21 14:49:15.839585 UTC] Updating baseline
[2018-01-21 14:49:17.894510 UTC] Computing logging information
-------------------------------------
| Iteration            | 877        |
| ExpectedImprovement  | 0.017156   |
| ActualImprovement    | 0.016901   |
| ImprovementRatio     | 0.98516    |
| MeanKL               | 0.0072171  |
| Entropy              | -0.90008   |
| Perplexity           | 0.40654    |
| AveragePolicyStd     | 0.21038    |
| AveragePolicyStd[0]  | 0.22453    |
| AveragePolicyStd[1]  | 0.22977    |
| AveragePolicyStd[2]  | 0.15751    |
| AveragePolicyStd[3]  | 0.21548    |
| AveragePolicyStd[4]  | 0.19088    |
| AveragePolicyStd[5]  | 0.24413    |
| AverageReturn        | 1400.9     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 347.05     |
| AverageEpisodeLength | 908.06     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.9      |
| TotalNEpisodes       | 21208      |
| TotalNSamples        | 4.3908e+06 |
| ExplainedVariance    | 0.11977    |
-------------------------------------
[2018-01-21 14:49:18.618924 UTC] Saving snapshot
[2018-01-21 14:49:18.619635 UTC] Starting iteration 878
[2018-01-21 14:49:18.619820 UTC] Start collecting samples
[2018-01-21 14:49:23.003565 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:23.140844 UTC] Performing policy update
[2018-01-21 14:49:23.141646 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:23.272682 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:24.743323 UTC] Performing line search
[2018-01-21 14:49:24.945226 UTC] Updating baseline
[2018-01-21 14:49:26.815188 UTC] Computing logging information
-------------------------------------
| Iteration            | 878        |
| ExpectedImprovement  | 0.019618   |
| ActualImprovement    | 0.0182     |
| ImprovementRatio     | 0.92775    |
| MeanKL               | 0.0066917  |
| Entropy              | -0.90431   |
| Perplexity           | 0.40482    |
| AveragePolicyStd     | 0.21024    |
| AveragePolicyStd[0]  | 0.22412    |
| AveragePolicyStd[1]  | 0.22974    |
| AveragePolicyStd[2]  | 0.15742    |
| AveragePolicyStd[3]  | 0.21474    |
| AveragePolicyStd[4]  | 0.19087    |
| AveragePolicyStd[5]  | 0.24457    |
| AverageReturn        | 1402.4     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 347.74     |
| AverageEpisodeLength | 908.06     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.9      |
| TotalNEpisodes       | 21211      |
| TotalNSamples        | 4.3938e+06 |
| ExplainedVariance    | -0.029471  |
-------------------------------------
[2018-01-21 14:49:27.596450 UTC] Saving snapshot
[2018-01-21 14:49:27.596693 UTC] Starting iteration 879
[2018-01-21 14:49:27.596876 UTC] Start collecting samples
[2018-01-21 14:49:32.225546 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:32.378844 UTC] Performing policy update
[2018-01-21 14:49:32.379492 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:32.497159 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:33.939434 UTC] Performing line search
[2018-01-21 14:49:34.126131 UTC] Updating baseline
[2018-01-21 14:49:36.146036 UTC] Computing logging information
-------------------------------------
| Iteration            | 879        |
| ExpectedImprovement  | 0.019251   |
| ActualImprovement    | 0.018703   |
| ImprovementRatio     | 0.97156    |
| MeanKL               | 0.0070612  |
| Entropy              | -0.91025   |
| Perplexity           | 0.40242    |
| AveragePolicyStd     | 0.21004    |
| AveragePolicyStd[0]  | 0.22431    |
| AveragePolicyStd[1]  | 0.22995    |
| AveragePolicyStd[2]  | 0.15737    |
| AveragePolicyStd[3]  | 0.21409    |
| AveragePolicyStd[4]  | 0.19036    |
| AveragePolicyStd[5]  | 0.24417    |
| AverageReturn        | 1381.4     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 362.93     |
| AverageEpisodeLength | 894.55     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.6      |
| TotalNEpisodes       | 21217      |
| TotalNSamples        | 4.3985e+06 |
| ExplainedVariance    | 0.2297     |
-------------------------------------
[2018-01-21 14:49:36.901397 UTC] Saving snapshot
[2018-01-21 14:49:36.901613 UTC] Starting iteration 880
[2018-01-21 14:49:36.901776 UTC] Start collecting samples
[2018-01-21 14:49:41.435867 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:41.587870 UTC] Performing policy update
[2018-01-21 14:49:41.588536 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:41.710865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:43.172086 UTC] Performing line search
[2018-01-21 14:49:43.373764 UTC] Updating baseline
[2018-01-21 14:49:45.361488 UTC] Computing logging information
-------------------------------------
| Iteration            | 880        |
| ExpectedImprovement  | 0.017864   |
| ActualImprovement    | 0.017092   |
| ImprovementRatio     | 0.95679    |
| MeanKL               | 0.0074267  |
| Entropy              | -0.90691   |
| Perplexity           | 0.40377    |
| AveragePolicyStd     | 0.21016    |
| AveragePolicyStd[0]  | 0.22427    |
| AveragePolicyStd[1]  | 0.23034    |
| AveragePolicyStd[2]  | 0.15753    |
| AveragePolicyStd[3]  | 0.21425    |
| AveragePolicyStd[4]  | 0.19034    |
| AveragePolicyStd[5]  | 0.24421    |
| AverageReturn        | 1383.4     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 363.84     |
| AverageEpisodeLength | 894.55     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.6      |
| TotalNEpisodes       | 21224      |
| TotalNSamples        | 4.4055e+06 |
| ExplainedVariance    | 0.059129   |
-------------------------------------
[2018-01-21 14:49:46.117847 UTC] Saving snapshot
[2018-01-21 14:49:46.124146 UTC] Starting iteration 881
[2018-01-21 14:49:46.124336 UTC] Start collecting samples
[2018-01-21 14:49:50.538246 UTC] Computing input variables for policy optimization
[2018-01-21 14:49:50.657472 UTC] Performing policy update
[2018-01-21 14:49:50.658468 UTC] Computing gradient in Euclidean space
[2018-01-21 14:49:50.777156 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:49:52.173446 UTC] Performing line search
[2018-01-21 14:49:52.367179 UTC] Updating baseline
[2018-01-21 14:49:54.469055 UTC] Computing logging information
-------------------------------------
| Iteration            | 881        |
| ExpectedImprovement  | 0.017224   |
| ActualImprovement    | 0.016703   |
| ImprovementRatio     | 0.96977    |
| MeanKL               | 0.0071088  |
| Entropy              | -0.91192   |
| Perplexity           | 0.40175    |
| AveragePolicyStd     | 0.20998    |
| AveragePolicyStd[0]  | 0.22359    |
| AveragePolicyStd[1]  | 0.22944    |
| AveragePolicyStd[2]  | 0.15762    |
| AveragePolicyStd[3]  | 0.21467    |
| AveragePolicyStd[4]  | 0.18988    |
| AveragePolicyStd[5]  | 0.24466    |
| AverageReturn        | 1366.5     |
| MinReturn            | 307.89     |
| MaxReturn            | 1652       |
| StdReturn            | 379.73     |
| AverageEpisodeLength | 883.01     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.36     |
| TotalNEpisodes       | 21229      |
| TotalNSamples        | 4.4093e+06 |
| ExplainedVariance    | 0.19174    |
-------------------------------------
[2018-01-21 14:49:55.280283 UTC] Saving snapshot
[2018-01-21 14:49:55.280776 UTC] Starting iteration 882
[2018-01-21 14:49:55.281165 UTC] Start collecting samples
[2018-01-21 14:50:00.041860 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:00.187165 UTC] Performing policy update
[2018-01-21 14:50:00.188571 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:00.321798 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:01.768965 UTC] Performing line search
[2018-01-21 14:50:01.956164 UTC] Updating baseline
[2018-01-21 14:50:03.999608 UTC] Computing logging information
------------------------------------
| Iteration            | 882       |
| ExpectedImprovement  | 0.017479  |
| ActualImprovement    | 0.016557  |
| ImprovementRatio     | 0.94724   |
| MeanKL               | 0.0081961 |
| Entropy              | -0.9137   |
| Perplexity           | 0.40104   |
| AveragePolicyStd     | 0.20992   |
| AveragePolicyStd[0]  | 0.22388   |
| AveragePolicyStd[1]  | 0.22943   |
| AveragePolicyStd[2]  | 0.15773   |
| AveragePolicyStd[3]  | 0.21548   |
| AveragePolicyStd[4]  | 0.18894   |
| AveragePolicyStd[5]  | 0.24404   |
| AverageReturn        | 1378.6    |
| MinReturn            | 307.89    |
| MaxReturn            | 1652      |
| StdReturn            | 366.62    |
| AverageEpisodeLength | 890.66    |
| MinEpisodeLength     | 227       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 224.9     |
| TotalNEpisodes       | 21234     |
| TotalNSamples        | 4.414e+06 |
| ExplainedVariance    | 0.18262   |
------------------------------------
[2018-01-21 14:50:04.731386 UTC] Saving snapshot
[2018-01-21 14:50:04.731696 UTC] Starting iteration 883
[2018-01-21 14:50:04.731914 UTC] Start collecting samples
[2018-01-21 14:50:09.443586 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:09.577007 UTC] Performing policy update
[2018-01-21 14:50:09.578216 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:09.703404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:11.159979 UTC] Performing line search
[2018-01-21 14:50:11.355417 UTC] Updating baseline
[2018-01-21 14:50:14.870682 UTC] Computing logging information
------------------------------------
| Iteration            | 883       |
| ExpectedImprovement  | 0.019317  |
| ActualImprovement    | 0.019256  |
| ImprovementRatio     | 0.99685   |
| MeanKL               | 0.0073144 |
| Entropy              | -0.90958  |
| Perplexity           | 0.40269   |
| AveragePolicyStd     | 0.21008   |
| AveragePolicyStd[0]  | 0.22412   |
| AveragePolicyStd[1]  | 0.22899   |
| AveragePolicyStd[2]  | 0.1578    |
| AveragePolicyStd[3]  | 0.21554   |
| AveragePolicyStd[4]  | 0.18893   |
| AveragePolicyStd[5]  | 0.24508   |
| AverageReturn        | 1377.5    |
| MinReturn            | 307.89    |
| MaxReturn            | 1652      |
| StdReturn            | 366.14    |
| AverageEpisodeLength | 890.66    |
| MinEpisodeLength     | 227       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 224.9     |
| TotalNEpisodes       | 21240     |
| TotalNSamples        | 4.42e+06  |
| ExplainedVariance    | -0.12297  |
------------------------------------
[2018-01-21 14:50:15.588115 UTC] Saving snapshot
[2018-01-21 14:50:15.588357 UTC] Starting iteration 884
[2018-01-21 14:50:15.588534 UTC] Start collecting samples
[2018-01-21 14:50:19.864823 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:20.019271 UTC] Performing policy update
[2018-01-21 14:50:20.019965 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:20.149392 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:21.585708 UTC] Performing line search
[2018-01-21 14:50:21.781819 UTC] Updating baseline
[2018-01-21 14:50:23.731083 UTC] Computing logging information
-------------------------------------
| Iteration            | 884        |
| ExpectedImprovement  | 0.017639   |
| ActualImprovement    | 0.017187   |
| ImprovementRatio     | 0.97439    |
| MeanKL               | 0.0079349  |
| Entropy              | -0.91397   |
| Perplexity           | 0.40093    |
| AveragePolicyStd     | 0.20993    |
| AveragePolicyStd[0]  | 0.22391    |
| AveragePolicyStd[1]  | 0.22935    |
| AveragePolicyStd[2]  | 0.15772    |
| AveragePolicyStd[3]  | 0.2149     |
| AveragePolicyStd[4]  | 0.18864    |
| AveragePolicyStd[5]  | 0.24508    |
| AverageReturn        | 1379.9     |
| MinReturn            | 311.46     |
| MaxReturn            | 1652       |
| StdReturn            | 352.86     |
| AverageEpisodeLength | 892.09     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.38     |
| TotalNEpisodes       | 21247      |
| TotalNSamples        | 4.4259e+06 |
| ExplainedVariance    | 0.18355    |
-------------------------------------
[2018-01-21 14:50:24.518068 UTC] Saving snapshot
[2018-01-21 14:50:24.518306 UTC] Starting iteration 885
[2018-01-21 14:50:24.518475 UTC] Start collecting samples
[2018-01-21 14:50:29.101017 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:29.228032 UTC] Performing policy update
[2018-01-21 14:50:29.232003 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:29.350648 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:30.775161 UTC] Performing line search
[2018-01-21 14:50:30.970705 UTC] Updating baseline
[2018-01-21 14:50:32.826580 UTC] Computing logging information
-------------------------------------
| Iteration            | 885        |
| ExpectedImprovement  | 0.018814   |
| ActualImprovement    | 0.017829   |
| ImprovementRatio     | 0.94763    |
| MeanKL               | 0.0080816  |
| Entropy              | -0.9148    |
| Perplexity           | 0.4006     |
| AveragePolicyStd     | 0.20993    |
| AveragePolicyStd[0]  | 0.22337    |
| AveragePolicyStd[1]  | 0.22945    |
| AveragePolicyStd[2]  | 0.15719    |
| AveragePolicyStd[3]  | 0.215      |
| AveragePolicyStd[4]  | 0.18915    |
| AveragePolicyStd[5]  | 0.24541    |
| AverageReturn        | 1374.7     |
| MinReturn            | 311.46     |
| MaxReturn            | 1652       |
| StdReturn            | 355.56     |
| AverageEpisodeLength | 888.66     |
| MinEpisodeLength     | 227        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.97     |
| TotalNEpisodes       | 21254      |
| TotalNSamples        | 4.4315e+06 |
| ExplainedVariance    | 0.29354    |
-------------------------------------
[2018-01-21 14:50:33.642253 UTC] Saving snapshot
[2018-01-21 14:50:33.642515 UTC] Starting iteration 886
[2018-01-21 14:50:33.642676 UTC] Start collecting samples
[2018-01-21 14:50:38.245525 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:38.367830 UTC] Performing policy update
[2018-01-21 14:50:38.368630 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:38.506761 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:39.975596 UTC] Performing line search
[2018-01-21 14:50:40.180207 UTC] Updating baseline
[2018-01-21 14:50:42.137784 UTC] Computing logging information
-------------------------------------
| Iteration            | 886        |
| ExpectedImprovement  | 0.019839   |
| ActualImprovement    | 0.018711   |
| ImprovementRatio     | 0.94316    |
| MeanKL               | 0.0073181  |
| Entropy              | -0.91644   |
| Perplexity           | 0.39994    |
| AveragePolicyStd     | 0.2099     |
| AveragePolicyStd[0]  | 0.22344    |
| AveragePolicyStd[1]  | 0.22993    |
| AveragePolicyStd[2]  | 0.15684    |
| AveragePolicyStd[3]  | 0.21425    |
| AveragePolicyStd[4]  | 0.18934    |
| AveragePolicyStd[5]  | 0.24559    |
| AverageReturn        | 1362.8     |
| MinReturn            | 311.46     |
| MaxReturn            | 1652       |
| StdReturn            | 370.43     |
| AverageEpisodeLength | 880.87     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.52     |
| TotalNEpisodes       | 21256      |
| TotalNSamples        | 4.4327e+06 |
| ExplainedVariance    | 0.089908   |
-------------------------------------
[2018-01-21 14:50:42.864824 UTC] Saving snapshot
[2018-01-21 14:50:42.865122 UTC] Starting iteration 887
[2018-01-21 14:50:42.865321 UTC] Start collecting samples
[2018-01-21 14:50:47.199730 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:47.322133 UTC] Performing policy update
[2018-01-21 14:50:47.322781 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:47.448472 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:48.950557 UTC] Performing line search
[2018-01-21 14:50:49.218919 UTC] Updating baseline
[2018-01-21 14:50:51.532196 UTC] Computing logging information
-------------------------------------
| Iteration            | 887        |
| ExpectedImprovement  | 0.018211   |
| ActualImprovement    | 0.01734    |
| ImprovementRatio     | 0.95213    |
| MeanKL               | 0.0077712  |
| Entropy              | -0.93393   |
| Perplexity           | 0.39301    |
| AveragePolicyStd     | 0.20928    |
| AveragePolicyStd[0]  | 0.22276    |
| AveragePolicyStd[1]  | 0.2287     |
| AveragePolicyStd[2]  | 0.1564     |
| AveragePolicyStd[3]  | 0.21367    |
| AveragePolicyStd[4]  | 0.18897    |
| AveragePolicyStd[5]  | 0.2452     |
| AverageReturn        | 1362.8     |
| MinReturn            | 311.46     |
| MaxReturn            | 1652       |
| StdReturn            | 370.45     |
| AverageEpisodeLength | 880.2      |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.27     |
| TotalNEpisodes       | 21262      |
| TotalNSamples        | 4.4386e+06 |
| ExplainedVariance    | 0.11098    |
-------------------------------------
[2018-01-21 14:50:52.253473 UTC] Saving snapshot
[2018-01-21 14:50:52.253712 UTC] Starting iteration 888
[2018-01-21 14:50:52.253874 UTC] Start collecting samples
[2018-01-21 14:50:57.145889 UTC] Computing input variables for policy optimization
[2018-01-21 14:50:57.282060 UTC] Performing policy update
[2018-01-21 14:50:57.282820 UTC] Computing gradient in Euclidean space
[2018-01-21 14:50:57.410486 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:50:58.868836 UTC] Performing line search
[2018-01-21 14:50:59.068870 UTC] Updating baseline
[2018-01-21 14:51:00.960936 UTC] Computing logging information
------------------------------------
| Iteration            | 888       |
| ExpectedImprovement  | 0.017445  |
| ActualImprovement    | 0.01679   |
| ImprovementRatio     | 0.96245   |
| MeanKL               | 0.009032  |
| Entropy              | -0.93656  |
| Perplexity           | 0.39197   |
| AveragePolicyStd     | 0.20918   |
| AveragePolicyStd[0]  | 0.22301   |
| AveragePolicyStd[1]  | 0.22795   |
| AveragePolicyStd[2]  | 0.15681   |
| AveragePolicyStd[3]  | 0.21383   |
| AveragePolicyStd[4]  | 0.18804   |
| AveragePolicyStd[5]  | 0.24547   |
| AverageReturn        | 1372.1    |
| MinReturn            | 311.46    |
| MaxReturn            | 1652      |
| StdReturn            | 362.25    |
| AverageEpisodeLength | 885.81    |
| MinEpisodeLength     | 221       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 223.14    |
| TotalNEpisodes       | 21270     |
| TotalNSamples        | 4.446e+06 |
| ExplainedVariance    | 0.10327   |
------------------------------------
[2018-01-21 14:51:01.715840 UTC] Saving snapshot
[2018-01-21 14:51:01.716126 UTC] Starting iteration 889
[2018-01-21 14:51:01.716306 UTC] Start collecting samples
[2018-01-21 14:51:06.135078 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:06.259216 UTC] Performing policy update
[2018-01-21 14:51:06.259821 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:06.382746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:07.841254 UTC] Performing line search
[2018-01-21 14:51:08.053688 UTC] Updating baseline
[2018-01-21 14:51:10.089768 UTC] Computing logging information
------------------------------------
| Iteration            | 889       |
| ExpectedImprovement  | 0.017224  |
| ActualImprovement    | 0.01642   |
| ImprovementRatio     | 0.95332   |
| MeanKL               | 0.0074325 |
| Entropy              | -0.93085  |
| Perplexity           | 0.39422   |
| AveragePolicyStd     | 0.20943   |
| AveragePolicyStd[0]  | 0.22286   |
| AveragePolicyStd[1]  | 0.22859   |
| AveragePolicyStd[2]  | 0.15666   |
| AveragePolicyStd[3]  | 0.21374   |
| AveragePolicyStd[4]  | 0.18807   |
| AveragePolicyStd[5]  | 0.24664   |
| AverageReturn        | 1371      |
| MinReturn            | 311.46    |
| MaxReturn            | 1652      |
| StdReturn            | 361.77    |
| AverageEpisodeLength | 885.81    |
| MinEpisodeLength     | 221       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 223.14    |
| TotalNEpisodes       | 21273     |
| TotalNSamples        | 4.449e+06 |
| ExplainedVariance    | 0.019531  |
------------------------------------
[2018-01-21 14:51:10.876029 UTC] Saving snapshot
[2018-01-21 14:51:10.876264 UTC] Starting iteration 890
[2018-01-21 14:51:10.876414 UTC] Start collecting samples
[2018-01-21 14:51:15.405922 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:15.535863 UTC] Performing policy update
[2018-01-21 14:51:15.536518 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:15.655766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:17.074155 UTC] Performing line search
[2018-01-21 14:51:17.264877 UTC] Updating baseline
[2018-01-21 14:51:19.431233 UTC] Computing logging information
-------------------------------------
| Iteration            | 890        |
| ExpectedImprovement  | 0.016891   |
| ActualImprovement    | 0.016165   |
| ImprovementRatio     | 0.95703    |
| MeanKL               | 0.007738   |
| Entropy              | -0.93574   |
| Perplexity           | 0.39229    |
| AveragePolicyStd     | 0.20926    |
| AveragePolicyStd[0]  | 0.22248    |
| AveragePolicyStd[1]  | 0.2291     |
| AveragePolicyStd[2]  | 0.15646    |
| AveragePolicyStd[3]  | 0.21365    |
| AveragePolicyStd[4]  | 0.18776    |
| AveragePolicyStd[5]  | 0.24613    |
| AverageReturn        | 1369.4     |
| MinReturn            | 311.46     |
| MaxReturn            | 1652       |
| StdReturn            | 358.55     |
| AverageEpisodeLength | 885.05     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.46     |
| TotalNEpisodes       | 21279      |
| TotalNSamples        | 4.4541e+06 |
| ExplainedVariance    | 0.19548    |
-------------------------------------
[2018-01-21 14:51:20.140470 UTC] Saving snapshot
[2018-01-21 14:51:20.149813 UTC] Starting iteration 891
[2018-01-21 14:51:20.150040 UTC] Start collecting samples
[2018-01-21 14:51:24.688133 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:24.821912 UTC] Performing policy update
[2018-01-21 14:51:24.822549 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:24.951722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:26.366643 UTC] Performing line search
[2018-01-21 14:51:26.554656 UTC] Updating baseline
[2018-01-21 14:51:29.923490 UTC] Computing logging information
--------------------------------------
| Iteration            | 891         |
| ExpectedImprovement  | 0.015291    |
| ActualImprovement    | 0.014403    |
| ImprovementRatio     | 0.94194     |
| MeanKL               | 0.0074393   |
| Entropy              | -0.93577    |
| Perplexity           | 0.39229     |
| AveragePolicyStd     | 0.20927     |
| AveragePolicyStd[0]  | 0.22259     |
| AveragePolicyStd[1]  | 0.22892     |
| AveragePolicyStd[2]  | 0.15656     |
| AveragePolicyStd[3]  | 0.21423     |
| AveragePolicyStd[4]  | 0.18717     |
| AveragePolicyStd[5]  | 0.24614     |
| AverageReturn        | 1371.5      |
| MinReturn            | 311.46      |
| MaxReturn            | 1629.9      |
| StdReturn            | 357.05      |
| AverageEpisodeLength | 887.86      |
| MinEpisodeLength     | 221         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 221.71      |
| TotalNEpisodes       | 21285       |
| TotalNSamples        | 4.4601e+06  |
| ExplainedVariance    | -0.00074482 |
--------------------------------------
[2018-01-21 14:51:30.629037 UTC] Saving snapshot
[2018-01-21 14:51:30.629328 UTC] Starting iteration 892
[2018-01-21 14:51:30.629545 UTC] Start collecting samples
[2018-01-21 14:51:35.099716 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:35.244354 UTC] Performing policy update
[2018-01-21 14:51:35.245324 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:35.370522 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:36.811304 UTC] Performing line search
[2018-01-21 14:51:37.015729 UTC] Updating baseline
[2018-01-21 14:51:40.340568 UTC] Computing logging information
-------------------------------------
| Iteration            | 892        |
| ExpectedImprovement  | 0.016429   |
| ActualImprovement    | 0.015524   |
| ImprovementRatio     | 0.94491    |
| MeanKL               | 0.0080882  |
| Entropy              | -0.93828   |
| Perplexity           | 0.3913     |
| AveragePolicyStd     | 0.20918    |
| AveragePolicyStd[0]  | 0.22228    |
| AveragePolicyStd[1]  | 0.22852    |
| AveragePolicyStd[2]  | 0.15637    |
| AveragePolicyStd[3]  | 0.21424    |
| AveragePolicyStd[4]  | 0.18743    |
| AveragePolicyStd[5]  | 0.24624    |
| AverageReturn        | 1370.9     |
| MinReturn            | 311.46     |
| MaxReturn            | 1629.8     |
| StdReturn            | 356.62     |
| AverageEpisodeLength | 887.86     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.71     |
| TotalNEpisodes       | 21288      |
| TotalNSamples        | 4.4631e+06 |
| ExplainedVariance    | 1.8851e-05 |
-------------------------------------
[2018-01-21 14:51:41.096027 UTC] Saving snapshot
[2018-01-21 14:51:41.096271 UTC] Starting iteration 893
[2018-01-21 14:51:41.096439 UTC] Start collecting samples
[2018-01-21 14:51:45.587601 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:45.728833 UTC] Performing policy update
[2018-01-21 14:51:45.729861 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:45.872388 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:47.298509 UTC] Performing line search
[2018-01-21 14:51:47.491329 UTC] Updating baseline
[2018-01-21 14:51:49.920023 UTC] Computing logging information
--------------------------------------
| Iteration            | 893         |
| ExpectedImprovement  | 0.017929    |
| ActualImprovement    | 0.016667    |
| ImprovementRatio     | 0.92962     |
| MeanKL               | 0.0075149   |
| Entropy              | -0.93515    |
| Perplexity           | 0.39253     |
| AveragePolicyStd     | 0.20929     |
| AveragePolicyStd[0]  | 0.22217     |
| AveragePolicyStd[1]  | 0.22897     |
| AveragePolicyStd[2]  | 0.15636     |
| AveragePolicyStd[3]  | 0.21477     |
| AveragePolicyStd[4]  | 0.18755     |
| AveragePolicyStd[5]  | 0.24589     |
| AverageReturn        | 1380.3      |
| MinReturn            | 311.46      |
| MaxReturn            | 1629.8      |
| StdReturn            | 350.71      |
| AverageEpisodeLength | 893.23      |
| MinEpisodeLength     | 221         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 217.82      |
| TotalNEpisodes       | 21293       |
| TotalNSamples        | 4.4681e+06  |
| ExplainedVariance    | -4.9838e-05 |
--------------------------------------
[2018-01-21 14:51:50.707538 UTC] Saving snapshot
[2018-01-21 14:51:50.707913 UTC] Starting iteration 894
[2018-01-21 14:51:50.708136 UTC] Start collecting samples
[2018-01-21 14:51:55.096847 UTC] Computing input variables for policy optimization
[2018-01-21 14:51:55.226849 UTC] Performing policy update
[2018-01-21 14:51:55.227856 UTC] Computing gradient in Euclidean space
[2018-01-21 14:51:55.353279 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:51:56.813119 UTC] Performing line search
[2018-01-21 14:51:57.013518 UTC] Updating baseline
[2018-01-21 14:51:59.951800 UTC] Computing logging information
-------------------------------------
| Iteration            | 894        |
| ExpectedImprovement  | 0.018213   |
| ActualImprovement    | 0.017674   |
| ImprovementRatio     | 0.97041    |
| MeanKL               | 0.0079104  |
| Entropy              | -0.9366    |
| Perplexity           | 0.39196    |
| AveragePolicyStd     | 0.20927    |
| AveragePolicyStd[0]  | 0.22243    |
| AveragePolicyStd[1]  | 0.22941    |
| AveragePolicyStd[2]  | 0.15592    |
| AveragePolicyStd[3]  | 0.21449    |
| AveragePolicyStd[4]  | 0.1874     |
| AveragePolicyStd[5]  | 0.246      |
| AverageReturn        | 1403.8     |
| MinReturn            | 319.51     |
| MaxReturn            | 1629.8     |
| StdReturn            | 323.7      |
| AverageEpisodeLength | 910.35     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.01     |
| TotalNEpisodes       | 21300      |
| TotalNSamples        | 4.4751e+06 |
| ExplainedVariance    | -0.0042065 |
-------------------------------------
[2018-01-21 14:52:00.694496 UTC] Saving snapshot
[2018-01-21 14:52:00.694838 UTC] Starting iteration 895
[2018-01-21 14:52:00.695095 UTC] Start collecting samples
[2018-01-21 14:52:05.325618 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:05.468976 UTC] Performing policy update
[2018-01-21 14:52:05.469807 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:05.589778 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:06.996759 UTC] Performing line search
[2018-01-21 14:52:07.191919 UTC] Updating baseline
[2018-01-21 14:52:09.529585 UTC] Computing logging information
-------------------------------------
| Iteration            | 895        |
| ExpectedImprovement  | 0.018626   |
| ActualImprovement    | 0.017572   |
| ImprovementRatio     | 0.94339    |
| MeanKL               | 0.0073611  |
| Entropy              | -0.93777   |
| Perplexity           | 0.3915     |
| AveragePolicyStd     | 0.20923    |
| AveragePolicyStd[0]  | 0.22295    |
| AveragePolicyStd[1]  | 0.22962    |
| AveragePolicyStd[2]  | 0.15593    |
| AveragePolicyStd[3]  | 0.21398    |
| AveragePolicyStd[4]  | 0.18729    |
| AveragePolicyStd[5]  | 0.24564    |
| AverageReturn        | 1408.5     |
| MinReturn            | 319.51     |
| MaxReturn            | 1629.8     |
| StdReturn            | 311.98     |
| AverageEpisodeLength | 914.41     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.49     |
| TotalNEpisodes       | 21305      |
| TotalNSamples        | 4.4799e+06 |
| ExplainedVariance    | 0.12363    |
-------------------------------------
[2018-01-21 14:52:10.287341 UTC] Saving snapshot
[2018-01-21 14:52:10.287846 UTC] Starting iteration 896
[2018-01-21 14:52:10.288139 UTC] Start collecting samples
[2018-01-21 14:52:14.735425 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:14.859233 UTC] Performing policy update
[2018-01-21 14:52:14.860268 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:14.979484 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:16.350143 UTC] Performing line search
[2018-01-21 14:52:16.547647 UTC] Updating baseline
[2018-01-21 14:52:18.872785 UTC] Computing logging information
-------------------------------------
| Iteration            | 896        |
| ExpectedImprovement  | 0.018441   |
| ActualImprovement    | 0.018145   |
| ImprovementRatio     | 0.98391    |
| MeanKL               | 0.0073085  |
| Entropy              | -0.93952   |
| Perplexity           | 0.39082    |
| AveragePolicyStd     | 0.20915    |
| AveragePolicyStd[0]  | 0.22314    |
| AveragePolicyStd[1]  | 0.22934    |
| AveragePolicyStd[2]  | 0.15592    |
| AveragePolicyStd[3]  | 0.21388    |
| AveragePolicyStd[4]  | 0.18752    |
| AveragePolicyStd[5]  | 0.24512    |
| AverageReturn        | 1417.4     |
| MinReturn            | 319.51     |
| MaxReturn            | 1629.8     |
| StdReturn            | 300.51     |
| AverageEpisodeLength | 920.56     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.2      |
| TotalNEpisodes       | 21308      |
| TotalNSamples        | 4.4829e+06 |
| ExplainedVariance    | -0.10555   |
-------------------------------------
[2018-01-21 14:52:19.671594 UTC] Saving snapshot
[2018-01-21 14:52:19.671869 UTC] Starting iteration 897
[2018-01-21 14:52:19.672039 UTC] Start collecting samples
[2018-01-21 14:52:24.107894 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:24.235553 UTC] Performing policy update
[2018-01-21 14:52:24.236169 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:24.386263 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:25.806500 UTC] Performing line search
[2018-01-21 14:52:26.051283 UTC] Updating baseline
[2018-01-21 14:52:27.888727 UTC] Computing logging information
-------------------------------------
| Iteration            | 897        |
| ExpectedImprovement  | 0.018946   |
| ActualImprovement    | 0.017997   |
| ImprovementRatio     | 0.94994    |
| MeanKL               | 0.0076627  |
| Entropy              | -0.94155   |
| Perplexity           | 0.39002    |
| AveragePolicyStd     | 0.20907    |
| AveragePolicyStd[0]  | 0.22303    |
| AveragePolicyStd[1]  | 0.22876    |
| AveragePolicyStd[2]  | 0.15622    |
| AveragePolicyStd[3]  | 0.21308    |
| AveragePolicyStd[4]  | 0.18748    |
| AveragePolicyStd[5]  | 0.24586    |
| AverageReturn        | 1438.7     |
| MinReturn            | 319.51     |
| MaxReturn            | 1629.8     |
| StdReturn            | 279.47     |
| AverageEpisodeLength | 934.07     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.65     |
| TotalNEpisodes       | 21314      |
| TotalNSamples        | 4.4889e+06 |
| ExplainedVariance    | -0.0021871 |
-------------------------------------
[2018-01-21 14:52:28.620543 UTC] Saving snapshot
[2018-01-21 14:52:28.620807 UTC] Starting iteration 898
[2018-01-21 14:52:28.620979 UTC] Start collecting samples
[2018-01-21 14:52:33.006508 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:33.138358 UTC] Performing policy update
[2018-01-21 14:52:33.138989 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:33.261558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:34.708530 UTC] Performing line search
[2018-01-21 14:52:34.904874 UTC] Updating baseline
[2018-01-21 14:52:37.221736 UTC] Computing logging information
--------------------------------------
| Iteration            | 898         |
| ExpectedImprovement  | 0.017446    |
| ActualImprovement    | 0.016453    |
| ImprovementRatio     | 0.94305     |
| MeanKL               | 0.0074844   |
| Entropy              | -0.94166    |
| Perplexity           | 0.38998     |
| AveragePolicyStd     | 0.20906     |
| AveragePolicyStd[0]  | 0.22312     |
| AveragePolicyStd[1]  | 0.22895     |
| AveragePolicyStd[2]  | 0.15621     |
| AveragePolicyStd[3]  | 0.21357     |
| AveragePolicyStd[4]  | 0.18727     |
| AveragePolicyStd[5]  | 0.24525     |
| AverageReturn        | 1439.6      |
| MinReturn            | 319.51      |
| MaxReturn            | 1632        |
| StdReturn            | 279.85      |
| AverageEpisodeLength | 934.07      |
| MinEpisodeLength     | 221         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 173.65      |
| TotalNEpisodes       | 21320       |
| TotalNSamples        | 4.4949e+06  |
| ExplainedVariance    | -0.00063712 |
--------------------------------------
[2018-01-21 14:52:38.035919 UTC] Saving snapshot
[2018-01-21 14:52:38.036272 UTC] Starting iteration 899
[2018-01-21 14:52:38.036589 UTC] Start collecting samples
[2018-01-21 14:52:42.431050 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:42.569659 UTC] Performing policy update
[2018-01-21 14:52:42.570325 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:42.689614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:44.144962 UTC] Performing line search
[2018-01-21 14:52:44.335207 UTC] Updating baseline
[2018-01-21 14:52:47.405018 UTC] Computing logging information
-------------------------------------
| Iteration            | 899        |
| ExpectedImprovement  | 0.017615   |
| ActualImprovement    | 0.015834   |
| ImprovementRatio     | 0.89887    |
| MeanKL               | 0.0078358  |
| Entropy              | -0.94171   |
| Perplexity           | 0.38996    |
| AveragePolicyStd     | 0.20911    |
| AveragePolicyStd[0]  | 0.2234     |
| AveragePolicyStd[1]  | 0.22851    |
| AveragePolicyStd[2]  | 0.15562    |
| AveragePolicyStd[3]  | 0.21407    |
| AveragePolicyStd[4]  | 0.18723    |
| AveragePolicyStd[5]  | 0.2458     |
| AverageReturn        | 1438.4     |
| MinReturn            | 319.51     |
| MaxReturn            | 1632       |
| StdReturn            | 279.49     |
| AverageEpisodeLength | 934.07     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.65     |
| TotalNEpisodes       | 21324      |
| TotalNSamples        | 4.4989e+06 |
| ExplainedVariance    | 0.00031502 |
-------------------------------------
[2018-01-21 14:52:48.157297 UTC] Saving snapshot
[2018-01-21 14:52:48.157546 UTC] Starting iteration 900
[2018-01-21 14:52:48.157729 UTC] Start collecting samples
[2018-01-21 14:52:52.555682 UTC] Computing input variables for policy optimization
[2018-01-21 14:52:52.687850 UTC] Performing policy update
[2018-01-21 14:52:52.688510 UTC] Computing gradient in Euclidean space
[2018-01-21 14:52:52.810636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:52:54.222911 UTC] Performing line search
[2018-01-21 14:52:54.413201 UTC] Updating baseline
[2018-01-21 14:52:56.251820 UTC] Computing logging information
-------------------------------------
| Iteration            | 900        |
| ExpectedImprovement  | 0.016783   |
| ActualImprovement    | 0.015679   |
| ImprovementRatio     | 0.93423    |
| MeanKL               | 0.0071268  |
| Entropy              | -0.94362   |
| Perplexity           | 0.38922    |
| AveragePolicyStd     | 0.2091     |
| AveragePolicyStd[0]  | 0.22356    |
| AveragePolicyStd[1]  | 0.22843    |
| AveragePolicyStd[2]  | 0.15545    |
| AveragePolicyStd[3]  | 0.21368    |
| AveragePolicyStd[4]  | 0.18636    |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1460.7     |
| MinReturn            | 319.51     |
| MaxReturn            | 1632       |
| StdReturn            | 249.69     |
| AverageEpisodeLength | 948.52     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.84     |
| TotalNEpisodes       | 21330      |
| TotalNSamples        | 4.5049e+06 |
| ExplainedVariance    | 0.0035577  |
-------------------------------------
[2018-01-21 14:52:57.018866 UTC] Saving snapshot
[2018-01-21 14:52:57.029504 UTC] Starting iteration 901
[2018-01-21 14:52:57.030303 UTC] Start collecting samples
[2018-01-21 14:53:01.722930 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:01.850568 UTC] Performing policy update
[2018-01-21 14:53:01.851230 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:01.983728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:03.448537 UTC] Performing line search
[2018-01-21 14:53:03.634020 UTC] Updating baseline
[2018-01-21 14:53:05.611651 UTC] Computing logging information
-------------------------------------
| Iteration            | 901        |
| ExpectedImprovement  | 0.016832   |
| ActualImprovement    | 0.01616    |
| ImprovementRatio     | 0.96009    |
| MeanKL               | 0.0086643  |
| Entropy              | -0.9481    |
| Perplexity           | 0.38748    |
| AveragePolicyStd     | 0.20892    |
| AveragePolicyStd[0]  | 0.22326    |
| AveragePolicyStd[1]  | 0.22796    |
| AveragePolicyStd[2]  | 0.15574    |
| AveragePolicyStd[3]  | 0.21366    |
| AveragePolicyStd[4]  | 0.18598    |
| AveragePolicyStd[5]  | 0.24693    |
| AverageReturn        | 1460.7     |
| MinReturn            | 319.51     |
| MaxReturn            | 1642       |
| StdReturn            | 249.73     |
| AverageEpisodeLength | 948.52     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.84     |
| TotalNEpisodes       | 21336      |
| TotalNSamples        | 4.5109e+06 |
| ExplainedVariance    | 0.002633   |
-------------------------------------
[2018-01-21 14:53:06.367618 UTC] Saving snapshot
[2018-01-21 14:53:06.367877 UTC] Starting iteration 902
[2018-01-21 14:53:06.368069 UTC] Start collecting samples
[2018-01-21 14:53:10.707572 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:10.837741 UTC] Performing policy update
[2018-01-21 14:53:10.838469 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:10.958370 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:12.369248 UTC] Performing line search
[2018-01-21 14:53:12.555391 UTC] Updating baseline
[2018-01-21 14:53:15.503507 UTC] Computing logging information
-------------------------------------
| Iteration            | 902        |
| ExpectedImprovement  | 0.017475   |
| ActualImprovement    | 0.016529   |
| ImprovementRatio     | 0.94588    |
| MeanKL               | 0.0080998  |
| Entropy              | -0.94902   |
| Perplexity           | 0.38712    |
| AveragePolicyStd     | 0.20891    |
| AveragePolicyStd[0]  | 0.22339    |
| AveragePolicyStd[1]  | 0.22817    |
| AveragePolicyStd[2]  | 0.15555    |
| AveragePolicyStd[3]  | 0.21354    |
| AveragePolicyStd[4]  | 0.18586    |
| AveragePolicyStd[5]  | 0.24694    |
| AverageReturn        | 1460.8     |
| MinReturn            | 319.51     |
| MaxReturn            | 1642       |
| StdReturn            | 249.69     |
| AverageEpisodeLength | 948.52     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.84     |
| TotalNEpisodes       | 21338      |
| TotalNSamples        | 4.5129e+06 |
| ExplainedVariance    | 0.051883   |
-------------------------------------
[2018-01-21 14:53:16.248134 UTC] Saving snapshot
[2018-01-21 14:53:16.248369 UTC] Starting iteration 903
[2018-01-21 14:53:16.248534 UTC] Start collecting samples
[2018-01-21 14:53:20.741915 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:20.883531 UTC] Performing policy update
[2018-01-21 14:53:20.884658 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:21.001916 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:22.422963 UTC] Performing line search
[2018-01-21 14:53:22.614160 UTC] Updating baseline
[2018-01-21 14:53:24.576032 UTC] Computing logging information
-------------------------------------
| Iteration            | 903        |
| ExpectedImprovement  | 0.018606   |
| ActualImprovement    | 0.017152   |
| ImprovementRatio     | 0.92184    |
| MeanKL               | 0.0075793  |
| Entropy              | -0.95132   |
| Perplexity           | 0.38623    |
| AveragePolicyStd     | 0.20882    |
| AveragePolicyStd[0]  | 0.2237     |
| AveragePolicyStd[1]  | 0.22768    |
| AveragePolicyStd[2]  | 0.1555     |
| AveragePolicyStd[3]  | 0.21272    |
| AveragePolicyStd[4]  | 0.18624    |
| AveragePolicyStd[5]  | 0.24707    |
| AverageReturn        | 1469.7     |
| MinReturn            | 319.51     |
| MaxReturn            | 1642       |
| StdReturn            | 242.07     |
| AverageEpisodeLength | 953.43     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.11     |
| TotalNEpisodes       | 21345      |
| TotalNSamples        | 4.5197e+06 |
| ExplainedVariance    | 0.07439    |
-------------------------------------
[2018-01-21 14:53:25.319176 UTC] Saving snapshot
[2018-01-21 14:53:25.319410 UTC] Starting iteration 904
[2018-01-21 14:53:25.319593 UTC] Start collecting samples
[2018-01-21 14:53:30.124651 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:30.274863 UTC] Performing policy update
[2018-01-21 14:53:30.276012 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:30.398470 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:31.859131 UTC] Performing line search
[2018-01-21 14:53:32.055263 UTC] Updating baseline
[2018-01-21 14:53:34.115400 UTC] Computing logging information
-------------------------------------
| Iteration            | 904        |
| ExpectedImprovement  | 0.017919   |
| ActualImprovement    | 0.017185   |
| ImprovementRatio     | 0.95906    |
| MeanKL               | 0.0075371  |
| Entropy              | -0.95478   |
| Perplexity           | 0.3849     |
| AveragePolicyStd     | 0.20868    |
| AveragePolicyStd[0]  | 0.22386    |
| AveragePolicyStd[1]  | 0.22719    |
| AveragePolicyStd[2]  | 0.15561    |
| AveragePolicyStd[3]  | 0.21233    |
| AveragePolicyStd[4]  | 0.18621    |
| AveragePolicyStd[5]  | 0.24688    |
| AverageReturn        | 1502.1     |
| MinReturn            | 319.51     |
| MaxReturn            | 1642       |
| StdReturn            | 190.88     |
| AverageEpisodeLength | 972.78     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 117.05     |
| TotalNEpisodes       | 21351      |
| TotalNSamples        | 4.5257e+06 |
| ExplainedVariance    | -0.0037662 |
-------------------------------------
[2018-01-21 14:53:34.822813 UTC] Saving snapshot
[2018-01-21 14:53:34.823073 UTC] Starting iteration 905
[2018-01-21 14:53:34.823240 UTC] Start collecting samples
[2018-01-21 14:53:39.396231 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:39.538220 UTC] Performing policy update
[2018-01-21 14:53:39.538856 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:39.656636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:41.097989 UTC] Performing line search
[2018-01-21 14:53:41.286560 UTC] Updating baseline
[2018-01-21 14:53:43.718096 UTC] Computing logging information
-------------------------------------
| Iteration            | 905        |
| ExpectedImprovement  | 0.01949    |
| ActualImprovement    | 0.017845   |
| ImprovementRatio     | 0.91558    |
| MeanKL               | 0.0076616  |
| Entropy              | -0.95203   |
| Perplexity           | 0.38596    |
| AveragePolicyStd     | 0.20878    |
| AveragePolicyStd[0]  | 0.22432    |
| AveragePolicyStd[1]  | 0.22705    |
| AveragePolicyStd[2]  | 0.15559    |
| AveragePolicyStd[3]  | 0.21268    |
| AveragePolicyStd[4]  | 0.18633    |
| AveragePolicyStd[5]  | 0.24669    |
| AverageReturn        | 1502.7     |
| MinReturn            | 319.51     |
| MaxReturn            | 1642       |
| StdReturn            | 191.17     |
| AverageEpisodeLength | 972.78     |
| MinEpisodeLength     | 221        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 117.05     |
| TotalNEpisodes       | 21353      |
| TotalNSamples        | 4.5277e+06 |
| ExplainedVariance    | -0.047576  |
-------------------------------------
[2018-01-21 14:53:44.460882 UTC] Saving snapshot
[2018-01-21 14:53:44.461120 UTC] Starting iteration 906
[2018-01-21 14:53:44.461266 UTC] Start collecting samples
[2018-01-21 14:53:48.878799 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:49.018572 UTC] Performing policy update
[2018-01-21 14:53:49.019554 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:49.142549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:53:50.547130 UTC] Performing line search
[2018-01-21 14:53:50.748916 UTC] Updating baseline
[2018-01-21 14:53:53.192560 UTC] Computing logging information
-------------------------------------
| Iteration            | 906        |
| ExpectedImprovement  | 0.018533   |
| ActualImprovement    | 0.017649   |
| ImprovementRatio     | 0.95227    |
| MeanKL               | 0.0074991  |
| Entropy              | -0.95684   |
| Perplexity           | 0.3841     |
| AveragePolicyStd     | 0.20858    |
| AveragePolicyStd[0]  | 0.22355    |
| AveragePolicyStd[1]  | 0.22663    |
| AveragePolicyStd[2]  | 0.15584    |
| AveragePolicyStd[3]  | 0.21257    |
| AveragePolicyStd[4]  | 0.18626    |
| AveragePolicyStd[5]  | 0.24661    |
| AverageReturn        | 1509.5     |
| MinReturn            | 564.85     |
| MaxReturn            | 1642       |
| StdReturn            | 167.03     |
| AverageEpisodeLength | 976.29     |
| MinEpisodeLength     | 398        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 101.08     |
| TotalNEpisodes       | 21361      |
| TotalNSamples        | 4.5352e+06 |
| ExplainedVariance    | 0.061609   |
-------------------------------------
[2018-01-21 14:53:54.005808 UTC] Saving snapshot
[2018-01-21 14:53:54.006074 UTC] Starting iteration 907
[2018-01-21 14:53:54.006245 UTC] Start collecting samples
[2018-01-21 14:53:58.888320 UTC] Computing input variables for policy optimization
[2018-01-21 14:53:59.038894 UTC] Performing policy update
[2018-01-21 14:53:59.039712 UTC] Computing gradient in Euclidean space
[2018-01-21 14:53:59.165015 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:00.579079 UTC] Performing line search
[2018-01-21 14:54:00.782677 UTC] Updating baseline
[2018-01-21 14:54:02.992600 UTC] Computing logging information
------------------------------------
| Iteration            | 907       |
| ExpectedImprovement  | 0.017824  |
| ActualImprovement    | 0.017181  |
| ImprovementRatio     | 0.96393   |
| MeanKL               | 0.0077761 |
| Entropy              | -0.96564  |
| Perplexity           | 0.38074   |
| AveragePolicyStd     | 0.20825   |
| AveragePolicyStd[0]  | 0.22292   |
| AveragePolicyStd[1]  | 0.22584   |
| AveragePolicyStd[2]  | 0.15582   |
| AveragePolicyStd[3]  | 0.21223   |
| AveragePolicyStd[4]  | 0.18621   |
| AveragePolicyStd[5]  | 0.2465    |
| AverageReturn        | 1500.1    |
| MinReturn            | 497.99    |
| MaxReturn            | 1642      |
| StdReturn            | 190.78    |
| AverageEpisodeLength | 970.2     |
| MinEpisodeLength     | 349       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 116.79    |
| TotalNEpisodes       | 21368     |
| TotalNSamples        | 4.541e+06 |
| ExplainedVariance    | 0.17871   |
------------------------------------
[2018-01-21 14:54:03.730117 UTC] Saving snapshot
[2018-01-21 14:54:03.730439 UTC] Starting iteration 908
[2018-01-21 14:54:03.730641 UTC] Start collecting samples
[2018-01-21 14:54:08.033002 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:08.159379 UTC] Performing policy update
[2018-01-21 14:54:08.160109 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:08.278280 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:09.672195 UTC] Performing line search
[2018-01-21 14:54:09.866413 UTC] Updating baseline
[2018-01-21 14:54:11.722811 UTC] Computing logging information
------------------------------------
| Iteration            | 908       |
| ExpectedImprovement  | 0.018651  |
| ActualImprovement    | 0.018501  |
| ImprovementRatio     | 0.99191   |
| MeanKL               | 0.0069477 |
| Entropy              | -0.96256  |
| Perplexity           | 0.38191   |
| AveragePolicyStd     | 0.20836   |
| AveragePolicyStd[0]  | 0.22297   |
| AveragePolicyStd[1]  | 0.22578   |
| AveragePolicyStd[2]  | 0.15575   |
| AveragePolicyStd[3]  | 0.21202   |
| AveragePolicyStd[4]  | 0.18678   |
| AveragePolicyStd[5]  | 0.24688   |
| AverageReturn        | 1502      |
| MinReturn            | 497.99    |
| MaxReturn            | 1642      |
| StdReturn            | 191.24    |
| AverageEpisodeLength | 970.2     |
| MinEpisodeLength     | 349       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 116.79    |
| TotalNEpisodes       | 21371     |
| TotalNSamples        | 4.544e+06 |
| ExplainedVariance    | -0.025889 |
------------------------------------
[2018-01-21 14:54:12.534150 UTC] Saving snapshot
[2018-01-21 14:54:12.534424 UTC] Starting iteration 909
[2018-01-21 14:54:12.534616 UTC] Start collecting samples
[2018-01-21 14:54:16.912311 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:17.091958 UTC] Performing policy update
[2018-01-21 14:54:17.092622 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:17.232859 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:18.656764 UTC] Performing line search
[2018-01-21 14:54:18.849671 UTC] Updating baseline
[2018-01-21 14:54:21.135113 UTC] Computing logging information
------------------------------------
| Iteration            | 909       |
| ExpectedImprovement  | 0.017927  |
| ActualImprovement    | 0.016801  |
| ImprovementRatio     | 0.93716   |
| MeanKL               | 0.0077282 |
| Entropy              | -0.96394  |
| Perplexity           | 0.38139   |
| AveragePolicyStd     | 0.20833   |
| AveragePolicyStd[0]  | 0.22289   |
| AveragePolicyStd[1]  | 0.22547   |
| AveragePolicyStd[2]  | 0.15559   |
| AveragePolicyStd[3]  | 0.21176   |
| AveragePolicyStd[4]  | 0.18694   |
| AveragePolicyStd[5]  | 0.24731   |
| AverageReturn        | 1512.2    |
| MinReturn            | 497.99    |
| MaxReturn            | 1642      |
| StdReturn            | 176.32    |
| AverageEpisodeLength | 975.23    |
| MinEpisodeLength     | 349       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 106.7     |
| TotalNEpisodes       | 21375     |
| TotalNSamples        | 4.548e+06 |
| ExplainedVariance    | -0.017833 |
------------------------------------
[2018-01-21 14:54:21.933826 UTC] Saving snapshot
[2018-01-21 14:54:21.934036 UTC] Starting iteration 910
[2018-01-21 14:54:21.934195 UTC] Start collecting samples
[2018-01-21 14:54:26.360110 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:26.496820 UTC] Performing policy update
[2018-01-21 14:54:26.497415 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:26.615530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:28.059267 UTC] Performing line search
[2018-01-21 14:54:28.251225 UTC] Updating baseline
[2018-01-21 14:54:30.372994 UTC] Computing logging information
-------------------------------------
| Iteration            | 910        |
| ExpectedImprovement  | 0.016324   |
| ActualImprovement    | 0.015692   |
| ImprovementRatio     | 0.9613     |
| MeanKL               | 0.0081008  |
| Entropy              | -0.96608   |
| Perplexity           | 0.38057    |
| AveragePolicyStd     | 0.20826    |
| AveragePolicyStd[0]  | 0.22294    |
| AveragePolicyStd[1]  | 0.22605    |
| AveragePolicyStd[2]  | 0.1554     |
| AveragePolicyStd[3]  | 0.2115     |
| AveragePolicyStd[4]  | 0.18682    |
| AveragePolicyStd[5]  | 0.24686    |
| AverageReturn        | 1516.8     |
| MinReturn            | 497.99     |
| MaxReturn            | 1642       |
| StdReturn            | 169.76     |
| AverageEpisodeLength | 977        |
| MinEpisodeLength     | 349        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.15     |
| TotalNEpisodes       | 21383      |
| TotalNSamples        | 4.5558e+06 |
| ExplainedVariance    | 0.083754   |
-------------------------------------
[2018-01-21 14:54:31.098065 UTC] Saving snapshot
[2018-01-21 14:54:31.107729 UTC] Starting iteration 911
[2018-01-21 14:54:31.107956 UTC] Start collecting samples
[2018-01-21 14:54:35.709463 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:35.837808 UTC] Performing policy update
[2018-01-21 14:54:35.838482 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:35.964138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:37.405980 UTC] Performing line search
[2018-01-21 14:54:37.636515 UTC] Updating baseline
[2018-01-21 14:54:39.342048 UTC] Computing logging information
-------------------------------------
| Iteration            | 911        |
| ExpectedImprovement  | 0.018659   |
| ActualImprovement    | 0.018031   |
| ImprovementRatio     | 0.96633    |
| MeanKL               | 0.0075476  |
| Entropy              | -0.9635    |
| Perplexity           | 0.38156    |
| AveragePolicyStd     | 0.20832    |
| AveragePolicyStd[0]  | 0.22269    |
| AveragePolicyStd[1]  | 0.22573    |
| AveragePolicyStd[2]  | 0.15579    |
| AveragePolicyStd[3]  | 0.21204    |
| AveragePolicyStd[4]  | 0.18679    |
| AveragePolicyStd[5]  | 0.2469     |
| AverageReturn        | 1499       |
| MinReturn            | 269.26     |
| MaxReturn            | 1642       |
| StdReturn            | 213.67     |
| AverageEpisodeLength | 964.59     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.7      |
| TotalNEpisodes       | 21389      |
| TotalNSamples        | 4.5606e+06 |
| ExplainedVariance    | 0.34629    |
-------------------------------------
[2018-01-21 14:54:40.093267 UTC] Saving snapshot
[2018-01-21 14:54:40.093586 UTC] Starting iteration 912
[2018-01-21 14:54:40.093829 UTC] Start collecting samples
[2018-01-21 14:54:44.441628 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:44.576713 UTC] Performing policy update
[2018-01-21 14:54:44.577368 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:44.715488 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:46.193335 UTC] Performing line search
[2018-01-21 14:54:46.399058 UTC] Updating baseline
[2018-01-21 14:54:48.429628 UTC] Computing logging information
------------------------------------
| Iteration            | 912       |
| ExpectedImprovement  | 0.019269  |
| ActualImprovement    | 0.01787   |
| ImprovementRatio     | 0.92738   |
| MeanKL               | 0.007145  |
| Entropy              | -0.96057  |
| Perplexity           | 0.38267   |
| AveragePolicyStd     | 0.2084    |
| AveragePolicyStd[0]  | 0.22214   |
| AveragePolicyStd[1]  | 0.22465   |
| AveragePolicyStd[2]  | 0.15614   |
| AveragePolicyStd[3]  | 0.21266   |
| AveragePolicyStd[4]  | 0.18714   |
| AveragePolicyStd[5]  | 0.24769   |
| AverageReturn        | 1489.3    |
| MinReturn            | 269.26    |
| MaxReturn            | 1642      |
| StdReturn            | 219.7     |
| AverageEpisodeLength | 959.08    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 134.83    |
| TotalNEpisodes       | 21392     |
| TotalNSamples        | 4.563e+06 |
| ExplainedVariance    | 0.36626   |
------------------------------------
[2018-01-21 14:54:49.144609 UTC] Saving snapshot
[2018-01-21 14:54:49.144843 UTC] Starting iteration 913
[2018-01-21 14:54:49.144988 UTC] Start collecting samples
[2018-01-21 14:54:53.628040 UTC] Computing input variables for policy optimization
[2018-01-21 14:54:53.752379 UTC] Performing policy update
[2018-01-21 14:54:53.753382 UTC] Computing gradient in Euclidean space
[2018-01-21 14:54:53.873183 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:54:55.292328 UTC] Performing line search
[2018-01-21 14:54:55.483893 UTC] Updating baseline
[2018-01-21 14:54:57.877451 UTC] Computing logging information
-------------------------------------
| Iteration            | 913        |
| ExpectedImprovement  | 0.016636   |
| ActualImprovement    | 0.016159   |
| ImprovementRatio     | 0.97133    |
| MeanKL               | 0.0084195  |
| Entropy              | -0.96651   |
| Perplexity           | 0.38041    |
| AveragePolicyStd     | 0.2082     |
| AveragePolicyStd[0]  | 0.22181    |
| AveragePolicyStd[1]  | 0.22404    |
| AveragePolicyStd[2]  | 0.15573    |
| AveragePolicyStd[3]  | 0.21295    |
| AveragePolicyStd[4]  | 0.18723    |
| AveragePolicyStd[5]  | 0.24746    |
| AverageReturn        | 1480.2     |
| MinReturn            | 256.84     |
| MaxReturn            | 1647.4     |
| StdReturn            | 252.59     |
| AverageEpisodeLength | 950.9      |
| MinEpisodeLength     | 182        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.35     |
| TotalNEpisodes       | 21399      |
| TotalNSamples        | 4.5692e+06 |
| ExplainedVariance    | 0.088112   |
-------------------------------------
[2018-01-21 14:54:58.608342 UTC] Saving snapshot
[2018-01-21 14:54:58.608574 UTC] Starting iteration 914
[2018-01-21 14:54:58.608722 UTC] Start collecting samples
[2018-01-21 14:55:03.148991 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:03.300631 UTC] Performing policy update
[2018-01-21 14:55:03.301260 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:03.422876 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:04.848022 UTC] Performing line search
[2018-01-21 14:55:05.048616 UTC] Updating baseline
[2018-01-21 14:55:07.133151 UTC] Computing logging information
-------------------------------------
| Iteration            | 914        |
| ExpectedImprovement  | 0.017949   |
| ActualImprovement    | 0.017256   |
| ImprovementRatio     | 0.96139    |
| MeanKL               | 0.0085318  |
| Entropy              | -0.96713   |
| Perplexity           | 0.38017    |
| AveragePolicyStd     | 0.20816    |
| AveragePolicyStd[0]  | 0.22211    |
| AveragePolicyStd[1]  | 0.22473    |
| AveragePolicyStd[2]  | 0.15599    |
| AveragePolicyStd[3]  | 0.21209    |
| AveragePolicyStd[4]  | 0.18712    |
| AveragePolicyStd[5]  | 0.24694    |
| AverageReturn        | 1486.5     |
| MinReturn            | 256.84     |
| MaxReturn            | 1647.4     |
| StdReturn            | 250.7      |
| AverageEpisodeLength | 953.4      |
| MinEpisodeLength     | 182        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.11     |
| TotalNEpisodes       | 21405      |
| TotalNSamples        | 4.5752e+06 |
| ExplainedVariance    | -0.029034  |
-------------------------------------
[2018-01-21 14:55:07.932788 UTC] Saving snapshot
[2018-01-21 14:55:07.933019 UTC] Starting iteration 915
[2018-01-21 14:55:07.933163 UTC] Start collecting samples
[2018-01-21 14:55:12.437335 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:12.565271 UTC] Performing policy update
[2018-01-21 14:55:12.565915 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:12.684554 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:14.094426 UTC] Performing line search
[2018-01-21 14:55:14.285485 UTC] Updating baseline
[2018-01-21 14:55:16.239453 UTC] Computing logging information
-------------------------------------
| Iteration            | 915        |
| ExpectedImprovement  | 0.020148   |
| ActualImprovement    | 0.019126   |
| ImprovementRatio     | 0.94926    |
| MeanKL               | 0.0072944  |
| Entropy              | -0.96911   |
| Perplexity           | 0.37942    |
| AveragePolicyStd     | 0.20811    |
| AveragePolicyStd[0]  | 0.22214    |
| AveragePolicyStd[1]  | 0.22464    |
| AveragePolicyStd[2]  | 0.15571    |
| AveragePolicyStd[3]  | 0.21179    |
| AveragePolicyStd[4]  | 0.18732    |
| AveragePolicyStd[5]  | 0.24703    |
| AverageReturn        | 1477.9     |
| MinReturn            | 256.84     |
| MaxReturn            | 1647.4     |
| StdReturn            | 266.59     |
| AverageEpisodeLength | 947.08     |
| MinEpisodeLength     | 182        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.39     |
| TotalNEpisodes       | 21410      |
| TotalNSamples        | 4.5796e+06 |
| ExplainedVariance    | 0.20993    |
-------------------------------------
[2018-01-21 14:55:16.946964 UTC] Saving snapshot
[2018-01-21 14:55:16.947281 UTC] Starting iteration 916
[2018-01-21 14:55:16.947504 UTC] Start collecting samples
[2018-01-21 14:55:21.603645 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:21.741157 UTC] Performing policy update
[2018-01-21 14:55:21.741883 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:21.868439 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:23.324953 UTC] Performing line search
[2018-01-21 14:55:23.512972 UTC] Updating baseline
[2018-01-21 14:55:26.204873 UTC] Computing logging information
-------------------------------------
| Iteration            | 916        |
| ExpectedImprovement  | 0.018785   |
| ActualImprovement    | 0.01749    |
| ImprovementRatio     | 0.93109    |
| MeanKL               | 0.0080046  |
| Entropy              | -0.9763    |
| Perplexity           | 0.3767     |
| AveragePolicyStd     | 0.20786    |
| AveragePolicyStd[0]  | 0.22212    |
| AveragePolicyStd[1]  | 0.22434    |
| AveragePolicyStd[2]  | 0.15539    |
| AveragePolicyStd[3]  | 0.21171    |
| AveragePolicyStd[4]  | 0.18706    |
| AveragePolicyStd[5]  | 0.24657    |
| AverageReturn        | 1479.4     |
| MinReturn            | 256.84     |
| MaxReturn            | 1647.4     |
| StdReturn            | 267.11     |
| AverageEpisodeLength | 947.08     |
| MinEpisodeLength     | 182        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.39     |
| TotalNEpisodes       | 21414      |
| TotalNSamples        | 4.5836e+06 |
| ExplainedVariance    | -0.016738  |
-------------------------------------
[2018-01-21 14:55:26.980869 UTC] Saving snapshot
[2018-01-21 14:55:26.981067 UTC] Starting iteration 917
[2018-01-21 14:55:26.981282 UTC] Start collecting samples
[2018-01-21 14:55:31.416661 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:31.548521 UTC] Performing policy update
[2018-01-21 14:55:31.549138 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:31.667553 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:33.085270 UTC] Performing line search
[2018-01-21 14:55:33.269893 UTC] Updating baseline
[2018-01-21 14:55:35.438402 UTC] Computing logging information
-------------------------------------
| Iteration            | 917        |
| ExpectedImprovement  | 0.017712   |
| ActualImprovement    | 0.016252   |
| ImprovementRatio     | 0.91759    |
| MeanKL               | 0.0078029  |
| Entropy              | -0.97641   |
| Perplexity           | 0.37666    |
| AveragePolicyStd     | 0.20785    |
| AveragePolicyStd[0]  | 0.222      |
| AveragePolicyStd[1]  | 0.22397    |
| AveragePolicyStd[2]  | 0.15546    |
| AveragePolicyStd[3]  | 0.21162    |
| AveragePolicyStd[4]  | 0.18727    |
| AveragePolicyStd[5]  | 0.24681    |
| AverageReturn        | 1463.5     |
| MinReturn            | 256.84     |
| MaxReturn            | 1647.4     |
| StdReturn            | 286.85     |
| AverageEpisodeLength | 938.29     |
| MinEpisodeLength     | 182        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.81     |
| TotalNEpisodes       | 21423      |
| TotalNSamples        | 4.5917e+06 |
| ExplainedVariance    | 0.070674   |
-------------------------------------
[2018-01-21 14:55:36.240648 UTC] Saving snapshot
[2018-01-21 14:55:36.240882 UTC] Starting iteration 918
[2018-01-21 14:55:36.241030 UTC] Start collecting samples
[2018-01-21 14:55:40.905723 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:41.056084 UTC] Performing policy update
[2018-01-21 14:55:41.057221 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:41.180855 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:42.582976 UTC] Performing line search
[2018-01-21 14:55:42.773569 UTC] Updating baseline
[2018-01-21 14:55:45.743517 UTC] Computing logging information
-------------------------------------
| Iteration            | 918        |
| ExpectedImprovement  | 0.017017   |
| ActualImprovement    | 0.015976   |
| ImprovementRatio     | 0.93881    |
| MeanKL               | 0.0073721  |
| Entropy              | -0.98133   |
| Perplexity           | 0.37481    |
| AveragePolicyStd     | 0.20773    |
| AveragePolicyStd[0]  | 0.22184    |
| AveragePolicyStd[1]  | 0.22391    |
| AveragePolicyStd[2]  | 0.15503    |
| AveragePolicyStd[3]  | 0.2113     |
| AveragePolicyStd[4]  | 0.18692    |
| AveragePolicyStd[5]  | 0.24737    |
| AverageReturn        | 1438.9     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.4     |
| StdReturn            | 336.88     |
| AverageEpisodeLength | 923.65     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.76     |
| TotalNEpisodes       | 21429      |
| TotalNSamples        | 4.5962e+06 |
| ExplainedVariance    | 0.067941   |
-------------------------------------
[2018-01-21 14:55:46.452343 UTC] Saving snapshot
[2018-01-21 14:55:46.452581 UTC] Starting iteration 919
[2018-01-21 14:55:46.452734 UTC] Start collecting samples
[2018-01-21 14:55:50.886971 UTC] Computing input variables for policy optimization
[2018-01-21 14:55:51.011475 UTC] Performing policy update
[2018-01-21 14:55:51.012597 UTC] Computing gradient in Euclidean space
[2018-01-21 14:55:51.133618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:55:52.543467 UTC] Performing line search
[2018-01-21 14:55:52.737092 UTC] Updating baseline
[2018-01-21 14:55:55.855424 UTC] Computing logging information
-------------------------------------
| Iteration            | 919        |
| ExpectedImprovement  | 0.015719   |
| ActualImprovement    | 0.015041   |
| ImprovementRatio     | 0.95688    |
| MeanKL               | 0.0075024  |
| Entropy              | -0.97911   |
| Perplexity           | 0.37565    |
| AveragePolicyStd     | 0.20784    |
| AveragePolicyStd[0]  | 0.22214    |
| AveragePolicyStd[1]  | 0.22427    |
| AveragePolicyStd[2]  | 0.15466    |
| AveragePolicyStd[3]  | 0.2114     |
| AveragePolicyStd[4]  | 0.18694    |
| AveragePolicyStd[5]  | 0.24763    |
| AverageReturn        | 1438.2     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.4     |
| StdReturn            | 336.49     |
| AverageEpisodeLength | 923.65     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.76     |
| TotalNEpisodes       | 21432      |
| TotalNSamples        | 4.5992e+06 |
| ExplainedVariance    | 0.0050713  |
-------------------------------------
[2018-01-21 14:55:56.582941 UTC] Saving snapshot
[2018-01-21 14:55:56.583180 UTC] Starting iteration 920
[2018-01-21 14:55:56.583328 UTC] Start collecting samples
[2018-01-21 14:56:01.200146 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:01.327344 UTC] Performing policy update
[2018-01-21 14:56:01.328374 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:01.452209 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:02.845329 UTC] Performing line search
[2018-01-21 14:56:03.042989 UTC] Updating baseline
[2018-01-21 14:56:05.076009 UTC] Computing logging information
-------------------------------------
| Iteration            | 920        |
| ExpectedImprovement  | 0.016483   |
| ActualImprovement    | 0.015841   |
| ImprovementRatio     | 0.96101    |
| MeanKL               | 0.0076752  |
| Entropy              | -0.97298   |
| Perplexity           | 0.37796    |
| AveragePolicyStd     | 0.20804    |
| AveragePolicyStd[0]  | 0.22289    |
| AveragePolicyStd[1]  | 0.2241     |
| AveragePolicyStd[2]  | 0.15471    |
| AveragePolicyStd[3]  | 0.21223    |
| AveragePolicyStd[4]  | 0.1872     |
| AveragePolicyStd[5]  | 0.24709    |
| AverageReturn        | 1436.1     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.4     |
| StdReturn            | 336.74     |
| AverageEpisodeLength | 922.31     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.69     |
| TotalNEpisodes       | 21438      |
| TotalNSamples        | 4.6051e+06 |
| ExplainedVariance    | 0.096857   |
-------------------------------------
[2018-01-21 14:56:05.786286 UTC] Saving snapshot
[2018-01-21 14:56:05.795852 UTC] Starting iteration 921
[2018-01-21 14:56:05.796084 UTC] Start collecting samples
[2018-01-21 14:56:10.221989 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:10.382796 UTC] Performing policy update
[2018-01-21 14:56:10.383426 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:10.520509 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:11.946333 UTC] Performing line search
[2018-01-21 14:56:12.136867 UTC] Updating baseline
[2018-01-21 14:56:14.428677 UTC] Computing logging information
-------------------------------------
| Iteration            | 921        |
| ExpectedImprovement  | 0.019838   |
| ActualImprovement    | 0.01839    |
| ImprovementRatio     | 0.92701    |
| MeanKL               | 0.0072474  |
| Entropy              | -0.97479   |
| Perplexity           | 0.37727    |
| AveragePolicyStd     | 0.20797    |
| AveragePolicyStd[0]  | 0.22255    |
| AveragePolicyStd[1]  | 0.22389    |
| AveragePolicyStd[2]  | 0.15473    |
| AveragePolicyStd[3]  | 0.21173    |
| AveragePolicyStd[4]  | 0.18738    |
| AveragePolicyStd[5]  | 0.24755    |
| AverageReturn        | 1426.9     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.4     |
| StdReturn            | 354.86     |
| AverageEpisodeLength | 915.46     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.74     |
| TotalNEpisodes       | 21443      |
| TotalNSamples        | 4.6094e+06 |
| ExplainedVariance    | 0.13672    |
-------------------------------------
[2018-01-21 14:56:15.132218 UTC] Saving snapshot
[2018-01-21 14:56:15.132453 UTC] Starting iteration 922
[2018-01-21 14:56:15.132615 UTC] Start collecting samples
[2018-01-21 14:56:19.666683 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:19.811938 UTC] Performing policy update
[2018-01-21 14:56:19.812591 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:19.951221 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:21.462653 UTC] Performing line search
[2018-01-21 14:56:21.651879 UTC] Updating baseline
[2018-01-21 14:56:23.703386 UTC] Computing logging information
-------------------------------------
| Iteration            | 922        |
| ExpectedImprovement  | 0.015913   |
| ActualImprovement    | 0.015221   |
| ImprovementRatio     | 0.95653    |
| MeanKL               | 0.0079781  |
| Entropy              | -0.98197   |
| Perplexity           | 0.37457    |
| AveragePolicyStd     | 0.20771    |
| AveragePolicyStd[0]  | 0.22214    |
| AveragePolicyStd[1]  | 0.2233     |
| AveragePolicyStd[2]  | 0.15513    |
| AveragePolicyStd[3]  | 0.2112     |
| AveragePolicyStd[4]  | 0.18675    |
| AveragePolicyStd[5]  | 0.24772    |
| AverageReturn        | 1405.2     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 385.36     |
| AverageEpisodeLength | 901.32     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.92     |
| TotalNEpisodes       | 21450      |
| TotalNSamples        | 4.6149e+06 |
| ExplainedVariance    | 0.18463    |
-------------------------------------
[2018-01-21 14:56:24.526640 UTC] Saving snapshot
[2018-01-21 14:56:24.526949 UTC] Starting iteration 923
[2018-01-21 14:56:24.527152 UTC] Start collecting samples
[2018-01-21 14:56:28.995233 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:29.123512 UTC] Performing policy update
[2018-01-21 14:56:29.124166 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:29.242397 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:30.681751 UTC] Performing line search
[2018-01-21 14:56:30.859456 UTC] Updating baseline
[2018-01-21 14:56:32.675131 UTC] Computing logging information
-------------------------------------
| Iteration            | 923        |
| ExpectedImprovement  | 0.018226   |
| ActualImprovement    | 0.017332   |
| ImprovementRatio     | 0.95094    |
| MeanKL               | 0.0072692  |
| Entropy              | -0.98064   |
| Perplexity           | 0.37507    |
| AveragePolicyStd     | 0.20774    |
| AveragePolicyStd[0]  | 0.22206    |
| AveragePolicyStd[1]  | 0.22342    |
| AveragePolicyStd[2]  | 0.15492    |
| AveragePolicyStd[3]  | 0.21166    |
| AveragePolicyStd[4]  | 0.18732    |
| AveragePolicyStd[5]  | 0.24704    |
| AverageReturn        | 1400.8     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 382.14     |
| AverageEpisodeLength | 899.21     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.15     |
| TotalNEpisodes       | 21457      |
| TotalNSamples        | 4.6212e+06 |
| ExplainedVariance    | 0.35461    |
-------------------------------------
[2018-01-21 14:56:33.387042 UTC] Saving snapshot
[2018-01-21 14:56:33.387249 UTC] Starting iteration 924
[2018-01-21 14:56:33.387421 UTC] Start collecting samples
[2018-01-21 14:56:37.798093 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:37.922922 UTC] Performing policy update
[2018-01-21 14:56:37.923997 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:38.043894 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:39.419227 UTC] Performing line search
[2018-01-21 14:56:39.610144 UTC] Updating baseline
[2018-01-21 14:56:41.982800 UTC] Computing logging information
-------------------------------------
| Iteration            | 924        |
| ExpectedImprovement  | 0.018495   |
| ActualImprovement    | 0.017571   |
| ImprovementRatio     | 0.95006    |
| MeanKL               | 0.0073205  |
| Entropy              | -0.98055   |
| Perplexity           | 0.37511    |
| AveragePolicyStd     | 0.2078     |
| AveragePolicyStd[0]  | 0.22164    |
| AveragePolicyStd[1]  | 0.22463    |
| AveragePolicyStd[2]  | 0.15441    |
| AveragePolicyStd[3]  | 0.21159    |
| AveragePolicyStd[4]  | 0.18692    |
| AveragePolicyStd[5]  | 0.24763    |
| AverageReturn        | 1378.4     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 406.64     |
| AverageEpisodeLength | 884.77     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 241.78     |
| TotalNEpisodes       | 21462      |
| TotalNSamples        | 4.6247e+06 |
| ExplainedVariance    | 0.2567     |
-------------------------------------
[2018-01-21 14:56:42.681081 UTC] Saving snapshot
[2018-01-21 14:56:42.681327 UTC] Starting iteration 925
[2018-01-21 14:56:42.681486 UTC] Start collecting samples
[2018-01-21 14:56:47.138836 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:47.288535 UTC] Performing policy update
[2018-01-21 14:56:47.289731 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:47.416986 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:48.811516 UTC] Performing line search
[2018-01-21 14:56:49.004399 UTC] Updating baseline
[2018-01-21 14:56:50.962571 UTC] Computing logging information
-------------------------------------
| Iteration            | 925        |
| ExpectedImprovement  | 0.017178   |
| ActualImprovement    | 0.016158   |
| ImprovementRatio     | 0.94064    |
| MeanKL               | 0.0074758  |
| Entropy              | -0.98512   |
| Perplexity           | 0.37339    |
| AveragePolicyStd     | 0.20764    |
| AveragePolicyStd[0]  | 0.22151    |
| AveragePolicyStd[1]  | 0.22402    |
| AveragePolicyStd[2]  | 0.15454    |
| AveragePolicyStd[3]  | 0.21128    |
| AveragePolicyStd[4]  | 0.18669    |
| AveragePolicyStd[5]  | 0.24778    |
| AverageReturn        | 1362.9     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 429.16     |
| AverageEpisodeLength | 874.13     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.49     |
| TotalNEpisodes       | 21468      |
| TotalNSamples        | 4.6284e+06 |
| ExplainedVariance    | 0.3209     |
-------------------------------------
[2018-01-21 14:56:51.662472 UTC] Saving snapshot
[2018-01-21 14:56:51.662825 UTC] Starting iteration 926
[2018-01-21 14:56:51.663083 UTC] Start collecting samples
[2018-01-21 14:56:56.259723 UTC] Computing input variables for policy optimization
[2018-01-21 14:56:56.403865 UTC] Performing policy update
[2018-01-21 14:56:56.404984 UTC] Computing gradient in Euclidean space
[2018-01-21 14:56:56.528102 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:56:57.919149 UTC] Performing line search
[2018-01-21 14:56:58.168461 UTC] Updating baseline
[2018-01-21 14:57:00.110396 UTC] Computing logging information
-------------------------------------
| Iteration            | 926        |
| ExpectedImprovement  | 0.017643   |
| ActualImprovement    | 0.016818   |
| ImprovementRatio     | 0.95326    |
| MeanKL               | 0.0083938  |
| Entropy              | -0.99042   |
| Perplexity           | 0.37142    |
| AveragePolicyStd     | 0.20746    |
| AveragePolicyStd[0]  | 0.22154    |
| AveragePolicyStd[1]  | 0.22386    |
| AveragePolicyStd[2]  | 0.15432    |
| AveragePolicyStd[3]  | 0.21143    |
| AveragePolicyStd[4]  | 0.18623    |
| AveragePolicyStd[5]  | 0.24739    |
| AverageReturn        | 1358.3     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 429.63     |
| AverageEpisodeLength | 871.29     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 256.65     |
| TotalNEpisodes       | 21476      |
| TotalNSamples        | 4.6362e+06 |
| ExplainedVariance    | 0.14529    |
-------------------------------------
[2018-01-21 14:57:00.825695 UTC] Saving snapshot
[2018-01-21 14:57:00.825943 UTC] Starting iteration 927
[2018-01-21 14:57:00.826176 UTC] Start collecting samples
[2018-01-21 14:57:05.357466 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:05.499554 UTC] Performing policy update
[2018-01-21 14:57:05.500179 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:05.637638 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:07.080950 UTC] Performing line search
[2018-01-21 14:57:07.266571 UTC] Updating baseline
[2018-01-21 14:57:10.000696 UTC] Computing logging information
-------------------------------------
| Iteration            | 927        |
| ExpectedImprovement  | 0.017768   |
| ActualImprovement    | 0.01719    |
| ImprovementRatio     | 0.96747    |
| MeanKL               | 0.007615   |
| Entropy              | -0.9869    |
| Perplexity           | 0.37273    |
| AveragePolicyStd     | 0.2076     |
| AveragePolicyStd[0]  | 0.22164    |
| AveragePolicyStd[1]  | 0.22406    |
| AveragePolicyStd[2]  | 0.15436    |
| AveragePolicyStd[3]  | 0.21083    |
| AveragePolicyStd[4]  | 0.1864     |
| AveragePolicyStd[5]  | 0.24833    |
| AverageReturn        | 1347.7     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 435.64     |
| AverageEpisodeLength | 864.05     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 260.64     |
| TotalNEpisodes       | 21481      |
| TotalNSamples        | 4.6402e+06 |
| ExplainedVariance    | 0.28128    |
-------------------------------------
[2018-01-21 14:57:10.727662 UTC] Saving snapshot
[2018-01-21 14:57:10.727911 UTC] Starting iteration 928
[2018-01-21 14:57:10.728068 UTC] Start collecting samples
[2018-01-21 14:57:15.184206 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:15.327398 UTC] Performing policy update
[2018-01-21 14:57:15.328498 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:15.465039 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:16.894992 UTC] Performing line search
[2018-01-21 14:57:17.093563 UTC] Updating baseline
[2018-01-21 14:57:18.834548 UTC] Computing logging information
-------------------------------------
| Iteration            | 928        |
| ExpectedImprovement  | 0.019231   |
| ActualImprovement    | 0.017768   |
| ImprovementRatio     | 0.92389    |
| MeanKL               | 0.0080797  |
| Entropy              | -0.9911    |
| Perplexity           | 0.37117    |
| AveragePolicyStd     | 0.20748    |
| AveragePolicyStd[0]  | 0.22192    |
| AveragePolicyStd[1]  | 0.22349    |
| AveragePolicyStd[2]  | 0.15417    |
| AveragePolicyStd[3]  | 0.21127    |
| AveragePolicyStd[4]  | 0.18575    |
| AveragePolicyStd[5]  | 0.24828    |
| AverageReturn        | 1348.8     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 436.23     |
| AverageEpisodeLength | 864.05     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 260.64     |
| TotalNEpisodes       | 21483      |
| TotalNSamples        | 4.6422e+06 |
| ExplainedVariance    | -0.029736  |
-------------------------------------
[2018-01-21 14:57:19.632870 UTC] Saving snapshot
[2018-01-21 14:57:19.633099 UTC] Starting iteration 929
[2018-01-21 14:57:19.633277 UTC] Start collecting samples
[2018-01-21 14:57:24.209966 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:24.366682 UTC] Performing policy update
[2018-01-21 14:57:24.367314 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:24.494355 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:26.003292 UTC] Performing line search
[2018-01-21 14:57:26.218581 UTC] Updating baseline
[2018-01-21 14:57:29.100589 UTC] Computing logging information
-------------------------------------
| Iteration            | 929        |
| ExpectedImprovement  | 0.016918   |
| ActualImprovement    | 0.015498   |
| ImprovementRatio     | 0.91605    |
| MeanKL               | 0.0075114  |
| Entropy              | -0.99609   |
| Perplexity           | 0.36932    |
| AveragePolicyStd     | 0.20733    |
| AveragePolicyStd[0]  | 0.22133    |
| AveragePolicyStd[1]  | 0.22351    |
| AveragePolicyStd[2]  | 0.15364    |
| AveragePolicyStd[3]  | 0.21156    |
| AveragePolicyStd[4]  | 0.1857     |
| AveragePolicyStd[5]  | 0.24825    |
| AverageReturn        | 1375.4     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 424.62     |
| AverageEpisodeLength | 879.18     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 252.4      |
| TotalNEpisodes       | 21490      |
| TotalNSamples        | 4.6492e+06 |
| ExplainedVariance    | 0.0011039  |
-------------------------------------
[2018-01-21 14:57:29.811406 UTC] Saving snapshot
[2018-01-21 14:57:29.811665 UTC] Starting iteration 930
[2018-01-21 14:57:29.811848 UTC] Start collecting samples
[2018-01-21 14:57:34.544419 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:34.667742 UTC] Performing policy update
[2018-01-21 14:57:34.668874 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:34.786403 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:36.174093 UTC] Performing line search
[2018-01-21 14:57:36.368520 UTC] Updating baseline
[2018-01-21 14:57:38.485835 UTC] Computing logging information
-------------------------------------
| Iteration            | 930        |
| ExpectedImprovement  | 0.018412   |
| ActualImprovement    | 0.017118   |
| ImprovementRatio     | 0.92973    |
| MeanKL               | 0.0077665  |
| Entropy              | -0.99337   |
| Perplexity           | 0.37032    |
| AveragePolicyStd     | 0.20743    |
| AveragePolicyStd[0]  | 0.22191    |
| AveragePolicyStd[1]  | 0.22358    |
| AveragePolicyStd[2]  | 0.15369    |
| AveragePolicyStd[3]  | 0.21129    |
| AveragePolicyStd[4]  | 0.1857     |
| AveragePolicyStd[5]  | 0.24845    |
| AverageReturn        | 1360.5     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 440.12     |
| AverageEpisodeLength | 869.78     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 260.98     |
| TotalNEpisodes       | 21499      |
| TotalNSamples        | 4.6562e+06 |
| ExplainedVariance    | 0.26004    |
-------------------------------------
[2018-01-21 14:57:39.193069 UTC] Saving snapshot
[2018-01-21 14:57:39.203147 UTC] Starting iteration 931
[2018-01-21 14:57:39.203374 UTC] Start collecting samples
[2018-01-21 14:57:43.713426 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:43.848050 UTC] Performing policy update
[2018-01-21 14:57:43.849118 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:43.973846 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:45.354254 UTC] Performing line search
[2018-01-21 14:57:45.544495 UTC] Updating baseline
[2018-01-21 14:57:47.625206 UTC] Computing logging information
------------------------------------
| Iteration            | 931       |
| ExpectedImprovement  | 0.018682  |
| ActualImprovement    | 0.018261  |
| ImprovementRatio     | 0.97751   |
| MeanKL               | 0.0078467 |
| Entropy              | -0.99868  |
| Perplexity           | 0.36837   |
| AveragePolicyStd     | 0.20725   |
| AveragePolicyStd[0]  | 0.22171   |
| AveragePolicyStd[1]  | 0.22327   |
| AveragePolicyStd[2]  | 0.15381   |
| AveragePolicyStd[3]  | 0.21051   |
| AveragePolicyStd[4]  | 0.18547   |
| AveragePolicyStd[5]  | 0.24872   |
| AverageReturn        | 1341.7    |
| MinReturn            | 189.41    |
| MaxReturn            | 1647.7    |
| StdReturn            | 451.91    |
| AverageEpisodeLength | 858.13    |
| MinEpisodeLength     | 156       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 267.9     |
| TotalNEpisodes       | 21504     |
| TotalNSamples        | 4.66e+06  |
| ExplainedVariance    | 0.31557   |
------------------------------------
[2018-01-21 14:57:48.363049 UTC] Saving snapshot
[2018-01-21 14:57:48.363287 UTC] Starting iteration 932
[2018-01-21 14:57:48.363435 UTC] Start collecting samples
[2018-01-21 14:57:53.042562 UTC] Computing input variables for policy optimization
[2018-01-21 14:57:53.194977 UTC] Performing policy update
[2018-01-21 14:57:53.195587 UTC] Computing gradient in Euclidean space
[2018-01-21 14:57:53.315792 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:57:54.709444 UTC] Performing line search
[2018-01-21 14:57:54.896341 UTC] Updating baseline
[2018-01-21 14:57:56.906245 UTC] Computing logging information
-------------------------------------
| Iteration            | 932        |
| ExpectedImprovement  | 0.016704   |
| ActualImprovement    | 0.016165   |
| ImprovementRatio     | 0.96776    |
| MeanKL               | 0.008204   |
| Entropy              | -1.0037    |
| Perplexity           | 0.36653    |
| AveragePolicyStd     | 0.20709    |
| AveragePolicyStd[0]  | 0.22134    |
| AveragePolicyStd[1]  | 0.22306    |
| AveragePolicyStd[2]  | 0.15355    |
| AveragePolicyStd[3]  | 0.21038    |
| AveragePolicyStd[4]  | 0.18537    |
| AveragePolicyStd[5]  | 0.24884    |
| AverageReturn        | 1338.3     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 458.24     |
| AverageEpisodeLength | 856.39     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.38     |
| TotalNEpisodes       | 21509      |
| TotalNSamples        | 4.6642e+06 |
| ExplainedVariance    | 0.094789   |
-------------------------------------
[2018-01-21 14:57:57.730136 UTC] Saving snapshot
[2018-01-21 14:57:57.730413 UTC] Starting iteration 933
[2018-01-21 14:57:57.730624 UTC] Start collecting samples
[2018-01-21 14:58:02.505533 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:02.654220 UTC] Performing policy update
[2018-01-21 14:58:02.654988 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:02.787825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:04.243130 UTC] Performing line search
[2018-01-21 14:58:04.439513 UTC] Updating baseline
[2018-01-21 14:58:06.363261 UTC] Computing logging information
-------------------------------------
| Iteration            | 933        |
| ExpectedImprovement  | 0.015458   |
| ActualImprovement    | 0.015161   |
| ImprovementRatio     | 0.98076    |
| MeanKL               | 0.0078521  |
| Entropy              | -1.0005    |
| Perplexity           | 0.3677     |
| AveragePolicyStd     | 0.2072     |
| AveragePolicyStd[0]  | 0.22149    |
| AveragePolicyStd[1]  | 0.22366    |
| AveragePolicyStd[2]  | 0.15349    |
| AveragePolicyStd[3]  | 0.21102    |
| AveragePolicyStd[4]  | 0.18525    |
| AveragePolicyStd[5]  | 0.24831    |
| AverageReturn        | 1333.3     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 458.79     |
| AverageEpisodeLength | 853.01     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.68     |
| TotalNEpisodes       | 21516      |
| TotalNSamples        | 4.6709e+06 |
| ExplainedVariance    | 0.091715   |
-------------------------------------
[2018-01-21 14:58:07.170491 UTC] Saving snapshot
[2018-01-21 14:58:07.170798 UTC] Starting iteration 934
[2018-01-21 14:58:07.171011 UTC] Start collecting samples
[2018-01-21 14:58:11.742015 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:11.862127 UTC] Performing policy update
[2018-01-21 14:58:11.863209 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:11.982507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:13.416366 UTC] Performing line search
[2018-01-21 14:58:13.604633 UTC] Updating baseline
[2018-01-21 14:58:16.123272 UTC] Computing logging information
-------------------------------------
| Iteration            | 934        |
| ExpectedImprovement  | 0.015848   |
| ActualImprovement    | 0.014645   |
| ImprovementRatio     | 0.9241     |
| MeanKL               | 0.0075764  |
| Entropy              | -1         |
| Perplexity           | 0.36786    |
| AveragePolicyStd     | 0.20724    |
| AveragePolicyStd[0]  | 0.22183    |
| AveragePolicyStd[1]  | 0.22439    |
| AveragePolicyStd[2]  | 0.15317    |
| AveragePolicyStd[3]  | 0.21069    |
| AveragePolicyStd[4]  | 0.18533    |
| AveragePolicyStd[5]  | 0.24801    |
| AverageReturn        | 1332.6     |
| MinReturn            | 189.41     |
| MaxReturn            | 1647.7     |
| StdReturn            | 458.45     |
| AverageEpisodeLength | 853.01     |
| MinEpisodeLength     | 156        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 272.68     |
| TotalNEpisodes       | 21519      |
| TotalNSamples        | 4.6739e+06 |
| ExplainedVariance    | -0.023092  |
-------------------------------------
[2018-01-21 14:58:16.843857 UTC] Saving snapshot
[2018-01-21 14:58:16.844136 UTC] Starting iteration 935
[2018-01-21 14:58:16.844316 UTC] Start collecting samples
[2018-01-21 14:58:21.332528 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:21.456223 UTC] Performing policy update
[2018-01-21 14:58:21.456990 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:21.579037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:23.040682 UTC] Performing line search
[2018-01-21 14:58:23.226984 UTC] Updating baseline
[2018-01-21 14:58:27.182006 UTC] Computing logging information
--------------------------------------
| Iteration            | 935         |
| ExpectedImprovement  | 0.018456    |
| ActualImprovement    | 0.0168      |
| ImprovementRatio     | 0.91031     |
| MeanKL               | 0.0091433   |
| Entropy              | -1.0043     |
| Perplexity           | 0.3663      |
| AveragePolicyStd     | 0.20706     |
| AveragePolicyStd[0]  | 0.22243     |
| AveragePolicyStd[1]  | 0.22402     |
| AveragePolicyStd[2]  | 0.15341     |
| AveragePolicyStd[3]  | 0.21003     |
| AveragePolicyStd[4]  | 0.18523     |
| AveragePolicyStd[5]  | 0.24723     |
| AverageReturn        | 1348.7      |
| MinReturn            | 189.41      |
| MaxReturn            | 1647.7      |
| StdReturn            | 451.01      |
| AverageEpisodeLength | 861.8       |
| MinEpisodeLength     | 156         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 268.31      |
| TotalNEpisodes       | 21523       |
| TotalNSamples        | 4.6779e+06  |
| ExplainedVariance    | -0.00072052 |
--------------------------------------
[2018-01-21 14:58:27.889950 UTC] Saving snapshot
[2018-01-21 14:58:27.890233 UTC] Starting iteration 936
[2018-01-21 14:58:27.890455 UTC] Start collecting samples
[2018-01-21 14:58:32.406560 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:32.537997 UTC] Performing policy update
[2018-01-21 14:58:32.538617 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:32.655400 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:34.052359 UTC] Performing line search
[2018-01-21 14:58:34.250659 UTC] Updating baseline
[2018-01-21 14:58:36.129597 UTC] Computing logging information
--------------------------------------
| Iteration            | 936         |
| ExpectedImprovement  | 0.017164    |
| ActualImprovement    | 0.016482    |
| ImprovementRatio     | 0.96029     |
| MeanKL               | 0.0084373   |
| Entropy              | -1.0103     |
| Perplexity           | 0.36411     |
| AveragePolicyStd     | 0.2069      |
| AveragePolicyStd[0]  | 0.2221      |
| AveragePolicyStd[1]  | 0.22402     |
| AveragePolicyStd[2]  | 0.15279     |
| AveragePolicyStd[3]  | 0.20995     |
| AveragePolicyStd[4]  | 0.18492     |
| AveragePolicyStd[5]  | 0.24762     |
| AverageReturn        | 1377.4      |
| MinReturn            | 224.16      |
| MaxReturn            | 1647.7      |
| StdReturn            | 421.95      |
| AverageEpisodeLength | 876.44      |
| MinEpisodeLength     | 194         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 254.66      |
| TotalNEpisodes       | 21531       |
| TotalNSamples        | 4.6859e+06  |
| ExplainedVariance    | -1.3992e-05 |
--------------------------------------
[2018-01-21 14:58:36.931845 UTC] Saving snapshot
[2018-01-21 14:58:36.932093 UTC] Starting iteration 937
[2018-01-21 14:58:36.932273 UTC] Start collecting samples
[2018-01-21 14:58:41.370521 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:41.494005 UTC] Performing policy update
[2018-01-21 14:58:41.494654 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:41.614679 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:43.065577 UTC] Performing line search
[2018-01-21 14:58:43.260663 UTC] Updating baseline
[2018-01-21 14:58:44.398119 UTC] Computing logging information
--------------------------------------
| Iteration            | 937         |
| ExpectedImprovement  | 0.016769    |
| ActualImprovement    | 0.015658    |
| ImprovementRatio     | 0.93375     |
| MeanKL               | 0.007781    |
| Entropy              | -1.0095     |
| Perplexity           | 0.36438     |
| AveragePolicyStd     | 0.20692     |
| AveragePolicyStd[0]  | 0.22239     |
| AveragePolicyStd[1]  | 0.22347     |
| AveragePolicyStd[2]  | 0.15314     |
| AveragePolicyStd[3]  | 0.20998     |
| AveragePolicyStd[4]  | 0.18458     |
| AveragePolicyStd[5]  | 0.24793     |
| AverageReturn        | 1380.6      |
| MinReturn            | 224.16      |
| MaxReturn            | 1647.7      |
| StdReturn            | 422.58      |
| AverageEpisodeLength | 877.78      |
| MinEpisodeLength     | 194         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 254.96      |
| TotalNEpisodes       | 21535       |
| TotalNSamples        | 4.6899e+06  |
| ExplainedVariance    | -1.9356e-08 |
--------------------------------------
[2018-01-21 14:58:45.134743 UTC] Saving snapshot
[2018-01-21 14:58:45.134963 UTC] Starting iteration 938
[2018-01-21 14:58:45.135123 UTC] Start collecting samples
[2018-01-21 14:58:49.808802 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:49.938529 UTC] Performing policy update
[2018-01-21 14:58:49.939170 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:50.053744 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:58:51.494574 UTC] Performing line search
[2018-01-21 14:58:51.704287 UTC] Updating baseline
[2018-01-21 14:58:54.340225 UTC] Computing logging information
-------------------------------------
| Iteration            | 938        |
| ExpectedImprovement  | 0.016256   |
| ActualImprovement    | 0.015518   |
| ImprovementRatio     | 0.95458    |
| MeanKL               | 0.0078796  |
| Entropy              | -1.0031    |
| Perplexity           | 0.36676    |
| AveragePolicyStd     | 0.20711    |
| AveragePolicyStd[0]  | 0.22213    |
| AveragePolicyStd[1]  | 0.22404    |
| AveragePolicyStd[2]  | 0.15342    |
| AveragePolicyStd[3]  | 0.21013    |
| AveragePolicyStd[4]  | 0.18529    |
| AveragePolicyStd[5]  | 0.24765    |
| AverageReturn        | 1386.5     |
| MinReturn            | 224.16     |
| MaxReturn            | 1647.7     |
| StdReturn            | 415.17     |
| AverageEpisodeLength | 880.15     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 250.14     |
| TotalNEpisodes       | 21540      |
| TotalNSamples        | 4.6944e+06 |
| ExplainedVariance    | 0.11075    |
-------------------------------------
[2018-01-21 14:58:55.065586 UTC] Saving snapshot
[2018-01-21 14:58:55.065822 UTC] Starting iteration 939
[2018-01-21 14:58:55.065971 UTC] Start collecting samples
[2018-01-21 14:58:59.558815 UTC] Computing input variables for policy optimization
[2018-01-21 14:58:59.686984 UTC] Performing policy update
[2018-01-21 14:58:59.687614 UTC] Computing gradient in Euclidean space
[2018-01-21 14:58:59.810186 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:01.225585 UTC] Performing line search
[2018-01-21 14:59:01.411926 UTC] Updating baseline
[2018-01-21 14:59:03.188470 UTC] Computing logging information
-------------------------------------
| Iteration            | 939        |
| ExpectedImprovement  | 0.018376   |
| ActualImprovement    | 0.017597   |
| ImprovementRatio     | 0.95763    |
| MeanKL               | 0.0073412  |
| Entropy              | -1.0003    |
| Perplexity           | 0.36778    |
| AveragePolicyStd     | 0.2072     |
| AveragePolicyStd[0]  | 0.22181    |
| AveragePolicyStd[1]  | 0.22438    |
| AveragePolicyStd[2]  | 0.1535     |
| AveragePolicyStd[3]  | 0.21094    |
| AveragePolicyStd[4]  | 0.18506    |
| AveragePolicyStd[5]  | 0.24753    |
| AverageReturn        | 1390.4     |
| MinReturn            | 224.16     |
| MaxReturn            | 1647.7     |
| StdReturn            | 400.07     |
| AverageEpisodeLength | 882.6      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 240.48     |
| TotalNEpisodes       | 21549      |
| TotalNSamples        | 4.7021e+06 |
| ExplainedVariance    | 0.17893    |
-------------------------------------
[2018-01-21 14:59:03.889517 UTC] Saving snapshot
[2018-01-21 14:59:03.889761 UTC] Starting iteration 940
[2018-01-21 14:59:03.889941 UTC] Start collecting samples
[2018-01-21 14:59:08.266409 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:08.398732 UTC] Performing policy update
[2018-01-21 14:59:08.400932 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:08.532214 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:09.996253 UTC] Performing line search
[2018-01-21 14:59:10.191892 UTC] Updating baseline
[2018-01-21 14:59:12.304996 UTC] Computing logging information
-------------------------------------
| Iteration            | 940        |
| ExpectedImprovement  | 0.019562   |
| ActualImprovement    | 0.017903   |
| ImprovementRatio     | 0.91518    |
| MeanKL               | 0.0075228  |
| Entropy              | -1.0067    |
| Perplexity           | 0.36542    |
| AveragePolicyStd     | 0.207      |
| AveragePolicyStd[0]  | 0.22194    |
| AveragePolicyStd[1]  | 0.22333    |
| AveragePolicyStd[2]  | 0.15349    |
| AveragePolicyStd[3]  | 0.21097    |
| AveragePolicyStd[4]  | 0.18432    |
| AveragePolicyStd[5]  | 0.24792    |
| AverageReturn        | 1396       |
| MinReturn            | 224.16     |
| MaxReturn            | 1654.3     |
| StdReturn            | 398.94     |
| AverageEpisodeLength | 885.86     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.84     |
| TotalNEpisodes       | 21552      |
| TotalNSamples        | 4.7051e+06 |
| ExplainedVariance    | -0.0094412 |
-------------------------------------
[2018-01-21 14:59:13.018593 UTC] Saving snapshot
[2018-01-21 14:59:13.029042 UTC] Starting iteration 941
[2018-01-21 14:59:13.029284 UTC] Start collecting samples
[2018-01-21 14:59:17.596194 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:17.721489 UTC] Performing policy update
[2018-01-21 14:59:17.722632 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:17.850221 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:19.328962 UTC] Performing line search
[2018-01-21 14:59:19.538679 UTC] Updating baseline
[2018-01-21 14:59:21.335387 UTC] Computing logging information
-------------------------------------
| Iteration            | 941        |
| ExpectedImprovement  | 0.018231   |
| ActualImprovement    | 0.016913   |
| ImprovementRatio     | 0.92773    |
| MeanKL               | 0.0076019  |
| Entropy              | -1.0058    |
| Perplexity           | 0.36574    |
| AveragePolicyStd     | 0.20704    |
| AveragePolicyStd[0]  | 0.22217    |
| AveragePolicyStd[1]  | 0.2233     |
| AveragePolicyStd[2]  | 0.15362    |
| AveragePolicyStd[3]  | 0.21098    |
| AveragePolicyStd[4]  | 0.18393    |
| AveragePolicyStd[5]  | 0.24821    |
| AverageReturn        | 1392.8     |
| MinReturn            | 224.16     |
| MaxReturn            | 1654.3     |
| StdReturn            | 404.12     |
| AverageEpisodeLength | 882.99     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 242.88     |
| TotalNEpisodes       | 21556      |
| TotalNSamples        | 4.7085e+06 |
| ExplainedVariance    | 0.36095    |
-------------------------------------
[2018-01-21 14:59:22.034060 UTC] Saving snapshot
[2018-01-21 14:59:22.034295 UTC] Starting iteration 942
[2018-01-21 14:59:22.034463 UTC] Start collecting samples
[2018-01-21 14:59:26.588454 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:26.744670 UTC] Performing policy update
[2018-01-21 14:59:26.746122 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:26.872299 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:28.281551 UTC] Performing line search
[2018-01-21 14:59:28.475993 UTC] Updating baseline
[2018-01-21 14:59:30.421194 UTC] Computing logging information
-------------------------------------
| Iteration            | 942        |
| ExpectedImprovement  | 0.019105   |
| ActualImprovement    | 0.01787    |
| ImprovementRatio     | 0.93535    |
| MeanKL               | 0.0071743  |
| Entropy              | -1.0179    |
| Perplexity           | 0.36134    |
| AveragePolicyStd     | 0.20664    |
| AveragePolicyStd[0]  | 0.22258    |
| AveragePolicyStd[1]  | 0.22321    |
| AveragePolicyStd[2]  | 0.153      |
| AveragePolicyStd[3]  | 0.20976    |
| AveragePolicyStd[4]  | 0.1837     |
| AveragePolicyStd[5]  | 0.24763    |
| AverageReturn        | 1425       |
| MinReturn            | 224.16     |
| MaxReturn            | 1654.3     |
| StdReturn            | 359.52     |
| AverageEpisodeLength | 902.38     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.68     |
| TotalNEpisodes       | 21566      |
| TotalNSamples        | 4.7175e+06 |
| ExplainedVariance    | 0.19514    |
-------------------------------------
[2018-01-21 14:59:31.150510 UTC] Saving snapshot
[2018-01-21 14:59:31.150817 UTC] Starting iteration 943
[2018-01-21 14:59:31.151064 UTC] Start collecting samples
[2018-01-21 14:59:35.775662 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:35.910843 UTC] Performing policy update
[2018-01-21 14:59:35.911842 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:36.031931 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:37.453155 UTC] Performing line search
[2018-01-21 14:59:37.648870 UTC] Updating baseline
[2018-01-21 14:59:39.677794 UTC] Computing logging information
-------------------------------------
| Iteration            | 943        |
| ExpectedImprovement  | 0.018604   |
| ActualImprovement    | 0.017695   |
| ImprovementRatio     | 0.95112    |
| MeanKL               | 0.0071917  |
| Entropy              | -1.0203    |
| Perplexity           | 0.36047    |
| AveragePolicyStd     | 0.2066     |
| AveragePolicyStd[0]  | 0.22189    |
| AveragePolicyStd[1]  | 0.22401    |
| AveragePolicyStd[2]  | 0.15276    |
| AveragePolicyStd[3]  | 0.20964    |
| AveragePolicyStd[4]  | 0.18329    |
| AveragePolicyStd[5]  | 0.24797    |
| AverageReturn        | 1435.4     |
| MinReturn            | 224.16     |
| MaxReturn            | 1658.8     |
| StdReturn            | 343.45     |
| AverageEpisodeLength | 907.86     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.68     |
| TotalNEpisodes       | 21570      |
| TotalNSamples        | 4.7212e+06 |
| ExplainedVariance    | 0.14849    |
-------------------------------------
[2018-01-21 14:59:40.430240 UTC] Saving snapshot
[2018-01-21 14:59:40.430575 UTC] Starting iteration 944
[2018-01-21 14:59:40.430811 UTC] Start collecting samples
[2018-01-21 14:59:44.825823 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:44.958947 UTC] Performing policy update
[2018-01-21 14:59:44.959767 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:45.078801 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:46.506031 UTC] Performing line search
[2018-01-21 14:59:46.698753 UTC] Updating baseline
[2018-01-21 14:59:48.749839 UTC] Computing logging information
-------------------------------------
| Iteration            | 944        |
| ExpectedImprovement  | 0.017999   |
| ActualImprovement    | 0.016785   |
| ImprovementRatio     | 0.93253    |
| MeanKL               | 0.007722   |
| Entropy              | -1.0237    |
| Perplexity           | 0.35927    |
| AveragePolicyStd     | 0.20648    |
| AveragePolicyStd[0]  | 0.22161    |
| AveragePolicyStd[1]  | 0.22363    |
| AveragePolicyStd[2]  | 0.15267    |
| AveragePolicyStd[3]  | 0.20989    |
| AveragePolicyStd[4]  | 0.18317    |
| AveragePolicyStd[5]  | 0.24789    |
| AverageReturn        | 1418.5     |
| MinReturn            | 224.16     |
| MaxReturn            | 1658.8     |
| StdReturn            | 363.08     |
| AverageEpisodeLength | 896.96     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.83     |
| TotalNEpisodes       | 21573      |
| TotalNSamples        | 4.7231e+06 |
| ExplainedVariance    | 0.3835     |
-------------------------------------
[2018-01-21 14:59:49.499551 UTC] Saving snapshot
[2018-01-21 14:59:49.499782 UTC] Starting iteration 945
[2018-01-21 14:59:49.499933 UTC] Start collecting samples
[2018-01-21 14:59:53.988266 UTC] Computing input variables for policy optimization
[2018-01-21 14:59:54.232467 UTC] Performing policy update
[2018-01-21 14:59:54.233072 UTC] Computing gradient in Euclidean space
[2018-01-21 14:59:54.418663 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 14:59:55.796145 UTC] Performing line search
[2018-01-21 14:59:55.982746 UTC] Updating baseline
[2018-01-21 14:59:57.973599 UTC] Computing logging information
-------------------------------------
| Iteration            | 945        |
| ExpectedImprovement  | 0.017642   |
| ActualImprovement    | 0.016376   |
| ImprovementRatio     | 0.92827    |
| MeanKL               | 0.007796   |
| Entropy              | -1.028     |
| Perplexity           | 0.3577     |
| AveragePolicyStd     | 0.20631    |
| AveragePolicyStd[0]  | 0.22177    |
| AveragePolicyStd[1]  | 0.22289    |
| AveragePolicyStd[2]  | 0.15304    |
| AveragePolicyStd[3]  | 0.20989    |
| AveragePolicyStd[4]  | 0.18254    |
| AveragePolicyStd[5]  | 0.24771    |
| AverageReturn        | 1439       |
| MinReturn            | 224.16     |
| MaxReturn            | 1658.8     |
| StdReturn            | 352.64     |
| AverageEpisodeLength | 909.08     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.15     |
| TotalNEpisodes       | 21579      |
| TotalNSamples        | 4.7291e+06 |
| ExplainedVariance    | -0.0065166 |
-------------------------------------
[2018-01-21 14:59:58.751183 UTC] Saving snapshot
[2018-01-21 14:59:58.751484 UTC] Starting iteration 946
[2018-01-21 14:59:58.751712 UTC] Start collecting samples
[2018-01-21 15:00:03.260641 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:03.420248 UTC] Performing policy update
[2018-01-21 15:00:03.421012 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:03.546296 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:04.968560 UTC] Performing line search
[2018-01-21 15:00:05.165224 UTC] Updating baseline
[2018-01-21 15:00:08.685349 UTC] Computing logging information
--------------------------------------
| Iteration            | 946         |
| ExpectedImprovement  | 0.017795    |
| ActualImprovement    | 0.016501    |
| ImprovementRatio     | 0.92727     |
| MeanKL               | 0.0078772   |
| Entropy              | -1.0318     |
| Perplexity           | 0.35638     |
| AveragePolicyStd     | 0.20619     |
| AveragePolicyStd[0]  | 0.22159     |
| AveragePolicyStd[1]  | 0.22232     |
| AveragePolicyStd[2]  | 0.15289     |
| AveragePolicyStd[3]  | 0.20995     |
| AveragePolicyStd[4]  | 0.18247     |
| AveragePolicyStd[5]  | 0.24792     |
| AverageReturn        | 1439.2      |
| MinReturn            | 224.16      |
| MaxReturn            | 1658.8      |
| StdReturn            | 352.82      |
| AverageEpisodeLength | 909.08      |
| MinEpisodeLength     | 194         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 210.15      |
| TotalNEpisodes       | 21586       |
| TotalNSamples        | 4.7361e+06  |
| ExplainedVariance    | -5.5629e-06 |
--------------------------------------
[2018-01-21 15:00:09.432730 UTC] Saving snapshot
[2018-01-21 15:00:09.433058 UTC] Starting iteration 947
[2018-01-21 15:00:09.433295 UTC] Start collecting samples
[2018-01-21 15:00:14.068929 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:14.208046 UTC] Performing policy update
[2018-01-21 15:00:14.208570 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:14.324382 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:15.779918 UTC] Performing line search
[2018-01-21 15:00:15.995270 UTC] Updating baseline
[2018-01-21 15:00:18.471979 UTC] Computing logging information
-------------------------------------
| Iteration            | 947        |
| ExpectedImprovement  | 0.017364   |
| ActualImprovement    | 0.016713   |
| ImprovementRatio     | 0.96252    |
| MeanKL               | 0.0080322  |
| Entropy              | -1.0347    |
| Perplexity           | 0.35534    |
| AveragePolicyStd     | 0.20608    |
| AveragePolicyStd[0]  | 0.22161    |
| AveragePolicyStd[1]  | 0.22202    |
| AveragePolicyStd[2]  | 0.15292    |
| AveragePolicyStd[3]  | 0.20982    |
| AveragePolicyStd[4]  | 0.18233    |
| AveragePolicyStd[5]  | 0.24778    |
| AverageReturn        | 1416.1     |
| MinReturn            | 224.16     |
| MaxReturn            | 1658.8     |
| StdReturn            | 364.51     |
| AverageEpisodeLength | 895.51     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.38     |
| TotalNEpisodes       | 21592      |
| TotalNSamples        | 4.7408e+06 |
| ExplainedVariance    | 0.35688    |
-------------------------------------
[2018-01-21 15:00:19.274226 UTC] Saving snapshot
[2018-01-21 15:00:19.274568 UTC] Starting iteration 948
[2018-01-21 15:00:19.274760 UTC] Start collecting samples
[2018-01-21 15:00:23.831936 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:23.988469 UTC] Performing policy update
[2018-01-21 15:00:23.989096 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:24.104868 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:25.511972 UTC] Performing line search
[2018-01-21 15:00:25.707760 UTC] Updating baseline
[2018-01-21 15:00:27.727719 UTC] Computing logging information
-------------------------------------
| Iteration            | 948        |
| ExpectedImprovement  | 0.017511   |
| ActualImprovement    | 0.016458   |
| ImprovementRatio     | 0.93984    |
| MeanKL               | 0.0077316  |
| Entropy              | -1.0292    |
| Perplexity           | 0.35729    |
| AveragePolicyStd     | 0.20626    |
| AveragePolicyStd[0]  | 0.22152    |
| AveragePolicyStd[1]  | 0.2218     |
| AveragePolicyStd[2]  | 0.15354    |
| AveragePolicyStd[3]  | 0.21023    |
| AveragePolicyStd[4]  | 0.18195    |
| AveragePolicyStd[5]  | 0.24853    |
| AverageReturn        | 1438       |
| MinReturn            | 263.41     |
| MaxReturn            | 1658.8     |
| StdReturn            | 329.63     |
| AverageEpisodeLength | 908.22     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.1      |
| TotalNEpisodes       | 21596      |
| TotalNSamples        | 4.7445e+06 |
| ExplainedVariance    | 0.13771    |
-------------------------------------
[2018-01-21 15:00:28.459417 UTC] Saving snapshot
[2018-01-21 15:00:28.459719 UTC] Starting iteration 949
[2018-01-21 15:00:28.459936 UTC] Start collecting samples
[2018-01-21 15:00:33.137891 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:33.266782 UTC] Performing policy update
[2018-01-21 15:00:33.267451 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:33.384091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:34.769370 UTC] Performing line search
[2018-01-21 15:00:34.964211 UTC] Updating baseline
[2018-01-21 15:00:36.795906 UTC] Computing logging information
------------------------------------
| Iteration            | 949       |
| ExpectedImprovement  | 0.01827   |
| ActualImprovement    | 0.017417  |
| ImprovementRatio     | 0.9533    |
| MeanKL               | 0.0075975 |
| Entropy              | -1.0305   |
| Perplexity           | 0.35684   |
| AveragePolicyStd     | 0.20616   |
| AveragePolicyStd[0]  | 0.22145   |
| AveragePolicyStd[1]  | 0.22217   |
| AveragePolicyStd[2]  | 0.15386   |
| AveragePolicyStd[3]  | 0.21008   |
| AveragePolicyStd[4]  | 0.18216   |
| AveragePolicyStd[5]  | 0.24724   |
| AverageReturn        | 1448.9    |
| MinReturn            | 263.41    |
| MaxReturn            | 1658.8    |
| StdReturn            | 311.49    |
| AverageEpisodeLength | 914.59    |
| MinEpisodeLength     | 194       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.09    |
| TotalNEpisodes       | 21602     |
| TotalNSamples        | 4.75e+06  |
| ExplainedVariance    | 0.22212   |
------------------------------------
[2018-01-21 15:00:37.525007 UTC] Saving snapshot
[2018-01-21 15:00:37.525243 UTC] Starting iteration 950
[2018-01-21 15:00:37.525419 UTC] Start collecting samples
[2018-01-21 15:00:41.950407 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:42.085045 UTC] Performing policy update
[2018-01-21 15:00:42.085735 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:42.208554 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:43.686863 UTC] Performing line search
[2018-01-21 15:00:43.881502 UTC] Updating baseline
[2018-01-21 15:00:45.678302 UTC] Computing logging information
-------------------------------------
| Iteration            | 950        |
| ExpectedImprovement  | 0.018002   |
| ActualImprovement    | 0.017959   |
| ImprovementRatio     | 0.99763    |
| MeanKL               | 0.0083843  |
| Entropy              | -1.0344    |
| Perplexity           | 0.35543    |
| AveragePolicyStd     | 0.20605    |
| AveragePolicyStd[0]  | 0.22098    |
| AveragePolicyStd[1]  | 0.22198    |
| AveragePolicyStd[2]  | 0.1531     |
| AveragePolicyStd[3]  | 0.21085    |
| AveragePolicyStd[4]  | 0.1824     |
| AveragePolicyStd[5]  | 0.24699    |
| AverageReturn        | 1438.3     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 331.9      |
| AverageEpisodeLength | 906.63     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.85     |
| TotalNEpisodes       | 21608      |
| TotalNSamples        | 4.7547e+06 |
| ExplainedVariance    | 0.19692    |
-------------------------------------
[2018-01-21 15:00:46.456611 UTC] Saving snapshot
[2018-01-21 15:00:46.463455 UTC] Starting iteration 951
[2018-01-21 15:00:46.463660 UTC] Start collecting samples
[2018-01-21 15:00:50.976890 UTC] Computing input variables for policy optimization
[2018-01-21 15:00:51.123350 UTC] Performing policy update
[2018-01-21 15:00:51.124212 UTC] Computing gradient in Euclidean space
[2018-01-21 15:00:51.273643 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:00:52.776603 UTC] Performing line search
[2018-01-21 15:00:52.979942 UTC] Updating baseline
[2018-01-21 15:00:54.992217 UTC] Computing logging information
-------------------------------------
| Iteration            | 951        |
| ExpectedImprovement  | 0.01823    |
| ActualImprovement    | 0.017583   |
| ImprovementRatio     | 0.96446    |
| MeanKL               | 0.007104   |
| Entropy              | -1.0395    |
| Perplexity           | 0.35362    |
| AveragePolicyStd     | 0.20592    |
| AveragePolicyStd[0]  | 0.22067    |
| AveragePolicyStd[1]  | 0.22208    |
| AveragePolicyStd[2]  | 0.15264    |
| AveragePolicyStd[3]  | 0.21042    |
| AveragePolicyStd[4]  | 0.18214    |
| AveragePolicyStd[5]  | 0.24758    |
| AverageReturn        | 1455.1     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 307.31     |
| AverageEpisodeLength | 918.07     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.22     |
| TotalNEpisodes       | 21613      |
| TotalNSamples        | 4.7597e+06 |
| ExplainedVariance    | 0.001451   |
-------------------------------------
[2018-01-21 15:00:55.737219 UTC] Saving snapshot
[2018-01-21 15:00:55.737456 UTC] Starting iteration 952
[2018-01-21 15:00:55.737638 UTC] Start collecting samples
[2018-01-21 15:01:00.145501 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:00.277412 UTC] Performing policy update
[2018-01-21 15:01:00.278192 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:00.400045 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:01.853184 UTC] Performing line search
[2018-01-21 15:01:02.052582 UTC] Updating baseline
[2018-01-21 15:01:04.115665 UTC] Computing logging information
-------------------------------------
| Iteration            | 952        |
| ExpectedImprovement  | 0.016865   |
| ActualImprovement    | 0.01632    |
| ImprovementRatio     | 0.96771    |
| MeanKL               | 0.0078717  |
| Entropy              | -1.045     |
| Perplexity           | 0.3517     |
| AveragePolicyStd     | 0.20567    |
| AveragePolicyStd[0]  | 0.21966    |
| AveragePolicyStd[1]  | 0.22147    |
| AveragePolicyStd[2]  | 0.15281    |
| AveragePolicyStd[3]  | 0.2105     |
| AveragePolicyStd[4]  | 0.18276    |
| AveragePolicyStd[5]  | 0.24683    |
| AverageReturn        | 1449.5     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 315.49     |
| AverageEpisodeLength | 913.1      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.57     |
| TotalNEpisodes       | 21619      |
| TotalNSamples        | 4.7652e+06 |
| ExplainedVariance    | 0.094081   |
-------------------------------------
[2018-01-21 15:01:04.869230 UTC] Saving snapshot
[2018-01-21 15:01:04.869475 UTC] Starting iteration 953
[2018-01-21 15:01:04.869658 UTC] Start collecting samples
[2018-01-21 15:01:09.313678 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:09.440773 UTC] Performing policy update
[2018-01-21 15:01:09.441522 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:09.558657 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:10.943033 UTC] Performing line search
[2018-01-21 15:01:11.128337 UTC] Updating baseline
[2018-01-21 15:01:13.542003 UTC] Computing logging information
-------------------------------------
| Iteration            | 953        |
| ExpectedImprovement  | 0.017589   |
| ActualImprovement    | 0.016093   |
| ImprovementRatio     | 0.91496    |
| MeanKL               | 0.008394   |
| Entropy              | -1.0443    |
| Perplexity           | 0.35194    |
| AveragePolicyStd     | 0.20569    |
| AveragePolicyStd[0]  | 0.21991    |
| AveragePolicyStd[1]  | 0.22114    |
| AveragePolicyStd[2]  | 0.15284    |
| AveragePolicyStd[3]  | 0.2105     |
| AveragePolicyStd[4]  | 0.18293    |
| AveragePolicyStd[5]  | 0.2468     |
| AverageReturn        | 1449.1     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 315.36     |
| AverageEpisodeLength | 913.1      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.57     |
| TotalNEpisodes       | 21623      |
| TotalNSamples        | 4.7692e+06 |
| ExplainedVariance    | -0.0072269 |
-------------------------------------
[2018-01-21 15:01:14.251851 UTC] Saving snapshot
[2018-01-21 15:01:14.252151 UTC] Starting iteration 954
[2018-01-21 15:01:14.252374 UTC] Start collecting samples
[2018-01-21 15:01:18.932818 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:19.101150 UTC] Performing policy update
[2018-01-21 15:01:19.102009 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:19.228020 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:20.674038 UTC] Performing line search
[2018-01-21 15:01:20.875146 UTC] Updating baseline
[2018-01-21 15:01:22.663071 UTC] Computing logging information
-------------------------------------
| Iteration            | 954        |
| ExpectedImprovement  | 0.01867    |
| ActualImprovement    | 0.017674   |
| ImprovementRatio     | 0.94663    |
| MeanKL               | 0.0076738  |
| Entropy              | -1.0458    |
| Perplexity           | 0.3514     |
| AveragePolicyStd     | 0.20561    |
| AveragePolicyStd[0]  | 0.21928    |
| AveragePolicyStd[1]  | 0.22102    |
| AveragePolicyStd[2]  | 0.15311    |
| AveragePolicyStd[3]  | 0.21047    |
| AveragePolicyStd[4]  | 0.18302    |
| AveragePolicyStd[5]  | 0.24674    |
| AverageReturn        | 1444.9     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 314.33     |
| AverageEpisodeLength | 911.84     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.2      |
| TotalNEpisodes       | 21629      |
| TotalNSamples        | 4.7751e+06 |
| ExplainedVariance    | 0.20719    |
-------------------------------------
[2018-01-21 15:01:23.444036 UTC] Saving snapshot
[2018-01-21 15:01:23.444550 UTC] Starting iteration 955
[2018-01-21 15:01:23.444960 UTC] Start collecting samples
[2018-01-21 15:01:28.068704 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:28.205610 UTC] Performing policy update
[2018-01-21 15:01:28.206200 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:28.327715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:29.742085 UTC] Performing line search
[2018-01-21 15:01:29.951116 UTC] Updating baseline
[2018-01-21 15:01:31.863136 UTC] Computing logging information
-------------------------------------
| Iteration            | 955        |
| ExpectedImprovement  | 0.017469   |
| ActualImprovement    | 0.016593   |
| ImprovementRatio     | 0.94984    |
| MeanKL               | 0.0077523  |
| Entropy              | -1.0451    |
| Perplexity           | 0.35164    |
| AveragePolicyStd     | 0.20563    |
| AveragePolicyStd[0]  | 0.21922    |
| AveragePolicyStd[1]  | 0.22136    |
| AveragePolicyStd[2]  | 0.15292    |
| AveragePolicyStd[3]  | 0.21074    |
| AveragePolicyStd[4]  | 0.18326    |
| AveragePolicyStd[5]  | 0.24628    |
| AverageReturn        | 1444.3     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 314.19     |
| AverageEpisodeLength | 911.84     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.2      |
| TotalNEpisodes       | 21635      |
| TotalNSamples        | 4.7811e+06 |
| ExplainedVariance    | 0.0085241  |
-------------------------------------
[2018-01-21 15:01:32.611069 UTC] Saving snapshot
[2018-01-21 15:01:32.611308 UTC] Starting iteration 956
[2018-01-21 15:01:32.611459 UTC] Start collecting samples
[2018-01-21 15:01:37.252748 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:37.379393 UTC] Performing policy update
[2018-01-21 15:01:37.380024 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:37.499406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:38.936233 UTC] Performing line search
[2018-01-21 15:01:39.121672 UTC] Updating baseline
[2018-01-21 15:01:41.381688 UTC] Computing logging information
-------------------------------------
| Iteration            | 956        |
| ExpectedImprovement  | 0.019279   |
| ActualImprovement    | 0.019204   |
| ImprovementRatio     | 0.99615    |
| MeanKL               | 0.0075885  |
| Entropy              | -1.0425    |
| Perplexity           | 0.35258    |
| AveragePolicyStd     | 0.20574    |
| AveragePolicyStd[0]  | 0.21922    |
| AveragePolicyStd[1]  | 0.22165    |
| AveragePolicyStd[2]  | 0.15299    |
| AveragePolicyStd[3]  | 0.21085    |
| AveragePolicyStd[4]  | 0.18291    |
| AveragePolicyStd[5]  | 0.24683    |
| AverageReturn        | 1444.2     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 314.12     |
| AverageEpisodeLength | 911.82     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.25     |
| TotalNEpisodes       | 21640      |
| TotalNSamples        | 4.7856e+06 |
| ExplainedVariance    | 0.097704   |
-------------------------------------
[2018-01-21 15:01:42.095357 UTC] Saving snapshot
[2018-01-21 15:01:42.095586 UTC] Starting iteration 957
[2018-01-21 15:01:42.095767 UTC] Start collecting samples
[2018-01-21 15:01:46.432193 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:46.557868 UTC] Performing policy update
[2018-01-21 15:01:46.559093 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:46.681275 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:48.184628 UTC] Performing line search
[2018-01-21 15:01:48.386704 UTC] Updating baseline
[2018-01-21 15:01:50.298624 UTC] Computing logging information
-------------------------------------
| Iteration            | 957        |
| ExpectedImprovement  | 0.017127   |
| ActualImprovement    | 0.016047   |
| ImprovementRatio     | 0.93691    |
| MeanKL               | 0.0078346  |
| Entropy              | -1.0435    |
| Perplexity           | 0.35224    |
| AveragePolicyStd     | 0.20568    |
| AveragePolicyStd[0]  | 0.21959    |
| AveragePolicyStd[1]  | 0.22085    |
| AveragePolicyStd[2]  | 0.15322    |
| AveragePolicyStd[3]  | 0.21099    |
| AveragePolicyStd[4]  | 0.18291    |
| AveragePolicyStd[5]  | 0.24655    |
| AverageReturn        | 1452.2     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 306.75     |
| AverageEpisodeLength | 916.21     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.18     |
| TotalNEpisodes       | 21644      |
| TotalNSamples        | 4.7895e+06 |
| ExplainedVariance    | 0.25135    |
-------------------------------------
[2018-01-21 15:01:51.081311 UTC] Saving snapshot
[2018-01-21 15:01:51.081558 UTC] Starting iteration 958
[2018-01-21 15:01:51.081733 UTC] Start collecting samples
[2018-01-21 15:01:55.570375 UTC] Computing input variables for policy optimization
[2018-01-21 15:01:55.711815 UTC] Performing policy update
[2018-01-21 15:01:55.712546 UTC] Computing gradient in Euclidean space
[2018-01-21 15:01:55.829055 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:01:57.271042 UTC] Performing line search
[2018-01-21 15:01:57.463366 UTC] Updating baseline
[2018-01-21 15:01:59.545168 UTC] Computing logging information
-------------------------------------
| Iteration            | 958        |
| ExpectedImprovement  | 0.017137   |
| ActualImprovement    | 0.016064   |
| ImprovementRatio     | 0.93737    |
| MeanKL               | 0.0079369  |
| Entropy              | -1.0365    |
| Perplexity           | 0.35469    |
| AveragePolicyStd     | 0.20599    |
| AveragePolicyStd[0]  | 0.22088    |
| AveragePolicyStd[1]  | 0.22126    |
| AveragePolicyStd[2]  | 0.15289    |
| AveragePolicyStd[3]  | 0.21074    |
| AveragePolicyStd[4]  | 0.18267    |
| AveragePolicyStd[5]  | 0.2475     |
| AverageReturn        | 1459.8     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 299.56     |
| AverageEpisodeLength | 920.6      |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.58     |
| TotalNEpisodes       | 21650      |
| TotalNSamples        | 4.7952e+06 |
| ExplainedVariance    | 0.028411   |
-------------------------------------
[2018-01-21 15:02:00.342373 UTC] Saving snapshot
[2018-01-21 15:02:00.342628 UTC] Starting iteration 959
[2018-01-21 15:02:00.342843 UTC] Start collecting samples
[2018-01-21 15:02:04.763447 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:04.888619 UTC] Performing policy update
[2018-01-21 15:02:04.889302 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:05.015372 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:06.494268 UTC] Performing line search
[2018-01-21 15:02:06.696735 UTC] Updating baseline
[2018-01-21 15:02:08.901219 UTC] Computing logging information
-------------------------------------
| Iteration            | 959        |
| ExpectedImprovement  | 0.017623   |
| ActualImprovement    | 0.016392   |
| ImprovementRatio     | 0.93014    |
| MeanKL               | 0.0072806  |
| Entropy              | -1.0458    |
| Perplexity           | 0.35143    |
| AveragePolicyStd     | 0.20567    |
| AveragePolicyStd[0]  | 0.22057    |
| AveragePolicyStd[1]  | 0.22101    |
| AveragePolicyStd[2]  | 0.15278    |
| AveragePolicyStd[3]  | 0.20975    |
| AveragePolicyStd[4]  | 0.18258    |
| AveragePolicyStd[5]  | 0.2473     |
| AverageReturn        | 1459.2     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 299.03     |
| AverageEpisodeLength | 919.94     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.64     |
| TotalNEpisodes       | 21656      |
| TotalNSamples        | 4.8005e+06 |
| ExplainedVariance    | 0.26319    |
-------------------------------------
[2018-01-21 15:02:09.665125 UTC] Saving snapshot
[2018-01-21 15:02:09.665363 UTC] Starting iteration 960
[2018-01-21 15:02:09.665516 UTC] Start collecting samples
[2018-01-21 15:02:14.113966 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:14.249676 UTC] Performing policy update
[2018-01-21 15:02:14.250876 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:14.368834 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:15.814680 UTC] Performing line search
[2018-01-21 15:02:16.013609 UTC] Updating baseline
[2018-01-21 15:02:19.686721 UTC] Computing logging information
-------------------------------------
| Iteration            | 960        |
| ExpectedImprovement  | 0.017878   |
| ActualImprovement    | 0.017294   |
| ImprovementRatio     | 0.96729    |
| MeanKL               | 0.0077398  |
| Entropy              | -1.046     |
| Perplexity           | 0.35135    |
| AveragePolicyStd     | 0.20567    |
| AveragePolicyStd[0]  | 0.22054    |
| AveragePolicyStd[1]  | 0.22101    |
| AveragePolicyStd[2]  | 0.15267    |
| AveragePolicyStd[3]  | 0.20918    |
| AveragePolicyStd[4]  | 0.18277    |
| AveragePolicyStd[5]  | 0.24788    |
| AverageReturn        | 1465.7     |
| MinReturn            | 197.91     |
| MaxReturn            | 1658.8     |
| StdReturn            | 296.91     |
| AverageEpisodeLength | 923.38     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.33     |
| TotalNEpisodes       | 21661      |
| TotalNSamples        | 4.8049e+06 |
| ExplainedVariance    | 0.057434   |
-------------------------------------
[2018-01-21 15:02:20.392242 UTC] Saving snapshot
[2018-01-21 15:02:20.401837 UTC] Starting iteration 961
[2018-01-21 15:02:20.402071 UTC] Start collecting samples
[2018-01-21 15:02:24.845913 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:24.976088 UTC] Performing policy update
[2018-01-21 15:02:24.976985 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:25.098201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:26.521548 UTC] Performing line search
[2018-01-21 15:02:26.716064 UTC] Updating baseline
[2018-01-21 15:02:29.016570 UTC] Computing logging information
-------------------------------------
| Iteration            | 961        |
| ExpectedImprovement  | 0.017808   |
| ActualImprovement    | 0.016871   |
| ImprovementRatio     | 0.94739    |
| MeanKL               | 0.0073122  |
| Entropy              | -1.0496    |
| Perplexity           | 0.3501     |
| AveragePolicyStd     | 0.2055     |
| AveragePolicyStd[0]  | 0.22007    |
| AveragePolicyStd[1]  | 0.2205     |
| AveragePolicyStd[2]  | 0.15307    |
| AveragePolicyStd[3]  | 0.20883    |
| AveragePolicyStd[4]  | 0.18299    |
| AveragePolicyStd[5]  | 0.24757    |
| AverageReturn        | 1441.2     |
| MinReturn            | 62.123     |
| MaxReturn            | 1658.8     |
| StdReturn            | 332.18     |
| AverageEpisodeLength | 908.08     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.25     |
| TotalNEpisodes       | 21668      |
| TotalNSamples        | 4.8103e+06 |
| ExplainedVariance    | 0.28426    |
-------------------------------------
[2018-01-21 15:02:29.814790 UTC] Saving snapshot
[2018-01-21 15:02:29.815109 UTC] Starting iteration 962
[2018-01-21 15:02:29.815349 UTC] Start collecting samples
[2018-01-21 15:02:34.170637 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:34.309257 UTC] Performing policy update
[2018-01-21 15:02:34.309869 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:34.430108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:35.842034 UTC] Performing line search
[2018-01-21 15:02:36.033260 UTC] Updating baseline
[2018-01-21 15:02:38.590826 UTC] Computing logging information
-------------------------------------
| Iteration            | 962        |
| ExpectedImprovement  | 0.01818    |
| ActualImprovement    | 0.016443   |
| ImprovementRatio     | 0.90442    |
| MeanKL               | 0.0070639  |
| Entropy              | -1.0475    |
| Perplexity           | 0.3508     |
| AveragePolicyStd     | 0.20557    |
| AveragePolicyStd[0]  | 0.22052    |
| AveragePolicyStd[1]  | 0.22097    |
| AveragePolicyStd[2]  | 0.15317    |
| AveragePolicyStd[3]  | 0.20868    |
| AveragePolicyStd[4]  | 0.18281    |
| AveragePolicyStd[5]  | 0.24728    |
| AverageReturn        | 1458.7     |
| MinReturn            | 62.123     |
| MaxReturn            | 1657.3     |
| StdReturn            | 311.05     |
| AverageEpisodeLength | 918.52     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.83     |
| TotalNEpisodes       | 21671      |
| TotalNSamples        | 4.8133e+06 |
| ExplainedVariance    | -0.042714  |
-------------------------------------
[2018-01-21 15:02:39.327084 UTC] Saving snapshot
[2018-01-21 15:02:39.327331 UTC] Starting iteration 963
[2018-01-21 15:02:39.327514 UTC] Start collecting samples
[2018-01-21 15:02:44.019114 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:44.170703 UTC] Performing policy update
[2018-01-21 15:02:44.171245 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:44.277841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:45.637974 UTC] Performing line search
[2018-01-21 15:02:45.833030 UTC] Updating baseline
[2018-01-21 15:02:47.877898 UTC] Computing logging information
-------------------------------------
| Iteration            | 963        |
| ExpectedImprovement  | 0.017825   |
| ActualImprovement    | 0.016633   |
| ImprovementRatio     | 0.9331     |
| MeanKL               | 0.0071866  |
| Entropy              | -1.0535    |
| Perplexity           | 0.34871    |
| AveragePolicyStd     | 0.20535    |
| AveragePolicyStd[0]  | 0.22003    |
| AveragePolicyStd[1]  | 0.22093    |
| AveragePolicyStd[2]  | 0.15302    |
| AveragePolicyStd[3]  | 0.20889    |
| AveragePolicyStd[4]  | 0.18267    |
| AveragePolicyStd[5]  | 0.24657    |
| AverageReturn        | 1465.1     |
| MinReturn            | 62.123     |
| MaxReturn            | 1683.3     |
| StdReturn            | 309.48     |
| AverageEpisodeLength | 921.41     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.83     |
| TotalNEpisodes       | 21678      |
| TotalNSamples        | 4.8203e+06 |
| ExplainedVariance    | -0.027785  |
-------------------------------------
[2018-01-21 15:02:48.604322 UTC] Saving snapshot
[2018-01-21 15:02:48.604570 UTC] Starting iteration 964
[2018-01-21 15:02:48.604738 UTC] Start collecting samples
[2018-01-21 15:02:53.303976 UTC] Computing input variables for policy optimization
[2018-01-21 15:02:53.438752 UTC] Performing policy update
[2018-01-21 15:02:53.439343 UTC] Computing gradient in Euclidean space
[2018-01-21 15:02:53.554658 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:02:54.900265 UTC] Performing line search
[2018-01-21 15:02:55.089107 UTC] Updating baseline
[2018-01-21 15:02:56.871553 UTC] Computing logging information
-------------------------------------
| Iteration            | 964        |
| ExpectedImprovement  | 0.01727    |
| ActualImprovement    | 0.015957   |
| ImprovementRatio     | 0.92396    |
| MeanKL               | 0.0073596  |
| Entropy              | -1.0636    |
| Perplexity           | 0.3452     |
| AveragePolicyStd     | 0.20503    |
| AveragePolicyStd[0]  | 0.21996    |
| AveragePolicyStd[1]  | 0.22014    |
| AveragePolicyStd[2]  | 0.15252    |
| AveragePolicyStd[3]  | 0.20819    |
| AveragePolicyStd[4]  | 0.18269    |
| AveragePolicyStd[5]  | 0.24668    |
| AverageReturn        | 1466.5     |
| MinReturn            | 62.123     |
| MaxReturn            | 1683.9     |
| StdReturn            | 310.23     |
| AverageEpisodeLength | 921.36     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.81     |
| TotalNEpisodes       | 21682      |
| TotalNSamples        | 4.8243e+06 |
| ExplainedVariance    | 0.11287    |
-------------------------------------
[2018-01-21 15:02:57.576376 UTC] Saving snapshot
[2018-01-21 15:02:57.576609 UTC] Starting iteration 965
[2018-01-21 15:02:57.576787 UTC] Start collecting samples
[2018-01-21 15:03:02.025743 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:02.157635 UTC] Performing policy update
[2018-01-21 15:03:02.158283 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:02.275049 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:03.657253 UTC] Performing line search
[2018-01-21 15:03:03.844312 UTC] Updating baseline
[2018-01-21 15:03:06.402921 UTC] Computing logging information
-------------------------------------
| Iteration            | 965        |
| ExpectedImprovement  | 0.017117   |
| ActualImprovement    | 0.016104   |
| ImprovementRatio     | 0.94082    |
| MeanKL               | 0.0077005  |
| Entropy              | -1.0636    |
| Perplexity           | 0.34522    |
| AveragePolicyStd     | 0.20502    |
| AveragePolicyStd[0]  | 0.21997    |
| AveragePolicyStd[1]  | 0.22017    |
| AveragePolicyStd[2]  | 0.15242    |
| AveragePolicyStd[3]  | 0.20862    |
| AveragePolicyStd[4]  | 0.18275    |
| AveragePolicyStd[5]  | 0.24622    |
| AverageReturn        | 1470       |
| MinReturn            | 62.123     |
| MaxReturn            | 1683.9     |
| StdReturn            | 309.01     |
| AverageEpisodeLength | 923.75     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.26     |
| TotalNEpisodes       | 21687      |
| TotalNSamples        | 4.8293e+06 |
| ExplainedVariance    | -0.0028453 |
-------------------------------------
[2018-01-21 15:03:07.226893 UTC] Saving snapshot
[2018-01-21 15:03:07.227134 UTC] Starting iteration 966
[2018-01-21 15:03:07.227315 UTC] Start collecting samples
[2018-01-21 15:03:11.809714 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:11.947511 UTC] Performing policy update
[2018-01-21 15:03:11.948376 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:12.065378 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:13.492168 UTC] Performing line search
[2018-01-21 15:03:13.705895 UTC] Updating baseline
[2018-01-21 15:03:15.574696 UTC] Computing logging information
-------------------------------------
| Iteration            | 966        |
| ExpectedImprovement  | 0.016214   |
| ActualImprovement    | 0.015852   |
| ImprovementRatio     | 0.97768    |
| MeanKL               | 0.0080748  |
| Entropy              | -1.0639    |
| Perplexity           | 0.3451     |
| AveragePolicyStd     | 0.20504    |
| AveragePolicyStd[0]  | 0.21998    |
| AveragePolicyStd[1]  | 0.21958    |
| AveragePolicyStd[2]  | 0.15208    |
| AveragePolicyStd[3]  | 0.20928    |
| AveragePolicyStd[4]  | 0.18263    |
| AveragePolicyStd[5]  | 0.2467     |
| AverageReturn        | 1478.7     |
| MinReturn            | 55.896     |
| MaxReturn            | 1683.9     |
| StdReturn            | 324.48     |
| AverageEpisodeLength | 927.96     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.92     |
| TotalNEpisodes       | 21695      |
| TotalNSamples        | 4.8363e+06 |
| ExplainedVariance    | 0.061449   |
-------------------------------------
[2018-01-21 15:03:16.283510 UTC] Saving snapshot
[2018-01-21 15:03:16.283757 UTC] Starting iteration 967
[2018-01-21 15:03:16.283914 UTC] Start collecting samples
[2018-01-21 15:03:20.718620 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:20.850836 UTC] Performing policy update
[2018-01-21 15:03:20.851587 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:20.967602 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:22.381650 UTC] Performing line search
[2018-01-21 15:03:22.575306 UTC] Updating baseline
[2018-01-21 15:03:25.183719 UTC] Computing logging information
--------------------------------------
| Iteration            | 967         |
| ExpectedImprovement  | 0.017034    |
| ActualImprovement    | 0.015602    |
| ImprovementRatio     | 0.91588     |
| MeanKL               | 0.0078614   |
| Entropy              | -1.0631     |
| Perplexity           | 0.34537     |
| AveragePolicyStd     | 0.20505     |
| AveragePolicyStd[0]  | 0.21986     |
| AveragePolicyStd[1]  | 0.2202      |
| AveragePolicyStd[2]  | 0.15215     |
| AveragePolicyStd[3]  | 0.20886     |
| AveragePolicyStd[4]  | 0.18292     |
| AveragePolicyStd[5]  | 0.24633     |
| AverageReturn        | 1480        |
| MinReturn            | 55.896      |
| MaxReturn            | 1683.9      |
| StdReturn            | 325         |
| AverageEpisodeLength | 927.96      |
| MinEpisodeLength     | 62          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 194.92      |
| TotalNEpisodes       | 21698       |
| TotalNSamples        | 4.8393e+06  |
| ExplainedVariance    | -4.9497e-05 |
--------------------------------------
[2018-01-21 15:03:26.011862 UTC] Saving snapshot
[2018-01-21 15:03:26.012168 UTC] Starting iteration 968
[2018-01-21 15:03:26.012411 UTC] Start collecting samples
[2018-01-21 15:03:30.354963 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:30.492347 UTC] Performing policy update
[2018-01-21 15:03:30.492978 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:30.608307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:32.042967 UTC] Performing line search
[2018-01-21 15:03:32.248953 UTC] Updating baseline
[2018-01-21 15:03:35.226125 UTC] Computing logging information
-------------------------------------
| Iteration            | 968        |
| ExpectedImprovement  | 0.01594    |
| ActualImprovement    | 0.015057   |
| ImprovementRatio     | 0.94456    |
| MeanKL               | 0.0081471  |
| Entropy              | -1.0654    |
| Perplexity           | 0.34461    |
| AveragePolicyStd     | 0.20498    |
| AveragePolicyStd[0]  | 0.21966    |
| AveragePolicyStd[1]  | 0.22026    |
| AveragePolicyStd[2]  | 0.15189    |
| AveragePolicyStd[3]  | 0.20877    |
| AveragePolicyStd[4]  | 0.1831     |
| AveragePolicyStd[5]  | 0.24622    |
| AverageReturn        | 1488.6     |
| MinReturn            | 55.896     |
| MaxReturn            | 1683.9     |
| StdReturn            | 322.73     |
| AverageEpisodeLength | 933.12     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.29     |
| TotalNEpisodes       | 21703      |
| TotalNSamples        | 4.8443e+06 |
| ExplainedVariance    | -0.031311  |
-------------------------------------
[2018-01-21 15:03:36.028647 UTC] Saving snapshot
[2018-01-21 15:03:36.028986 UTC] Starting iteration 969
[2018-01-21 15:03:36.029239 UTC] Start collecting samples
[2018-01-21 15:03:40.620002 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:40.770444 UTC] Performing policy update
[2018-01-21 15:03:40.771107 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:40.884513 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:42.354203 UTC] Performing line search
[2018-01-21 15:03:42.533954 UTC] Updating baseline
[2018-01-21 15:03:44.748107 UTC] Computing logging information
-------------------------------------
| Iteration            | 969        |
| ExpectedImprovement  | 0.020858   |
| ActualImprovement    | 0.019321   |
| ImprovementRatio     | 0.92627    |
| MeanKL               | 0.0074308  |
| Entropy              | -1.0698    |
| Perplexity           | 0.34308    |
| AveragePolicyStd     | 0.20487    |
| AveragePolicyStd[0]  | 0.22047    |
| AveragePolicyStd[1]  | 0.22055    |
| AveragePolicyStd[2]  | 0.15194    |
| AveragePolicyStd[3]  | 0.20831    |
| AveragePolicyStd[4]  | 0.18175    |
| AveragePolicyStd[5]  | 0.24618    |
| AverageReturn        | 1494.5     |
| MinReturn            | 55.896     |
| MaxReturn            | 1683.9     |
| StdReturn            | 314.22     |
| AverageEpisodeLength | 936.07     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.04     |
| TotalNEpisodes       | 21710      |
| TotalNSamples        | 4.8503e+06 |
| ExplainedVariance    | 0.22018    |
-------------------------------------
[2018-01-21 15:03:45.566171 UTC] Saving snapshot
[2018-01-21 15:03:45.566402 UTC] Starting iteration 970
[2018-01-21 15:03:45.566566 UTC] Start collecting samples
[2018-01-21 15:03:50.050894 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:50.171059 UTC] Performing policy update
[2018-01-21 15:03:50.171747 UTC] Computing gradient in Euclidean space
[2018-01-21 15:03:50.288807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:03:51.680881 UTC] Performing line search
[2018-01-21 15:03:51.885634 UTC] Updating baseline
[2018-01-21 15:03:54.760303 UTC] Computing logging information
-------------------------------------
| Iteration            | 970        |
| ExpectedImprovement  | 0.017499   |
| ActualImprovement    | 0.016915   |
| ImprovementRatio     | 0.96662    |
| MeanKL               | 0.0079854  |
| Entropy              | -1.0724    |
| Perplexity           | 0.34217    |
| AveragePolicyStd     | 0.2048     |
| AveragePolicyStd[0]  | 0.22054    |
| AveragePolicyStd[1]  | 0.2204     |
| AveragePolicyStd[2]  | 0.15168    |
| AveragePolicyStd[3]  | 0.20835    |
| AveragePolicyStd[4]  | 0.18154    |
| AveragePolicyStd[5]  | 0.24627    |
| AverageReturn        | 1497.4     |
| MinReturn            | 55.896     |
| MaxReturn            | 1688.4     |
| StdReturn            | 315.18     |
| AverageEpisodeLength | 936.07     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.04     |
| TotalNEpisodes       | 21715      |
| TotalNSamples        | 4.8553e+06 |
| ExplainedVariance    | -0.050798  |
-------------------------------------
[2018-01-21 15:03:55.488049 UTC] Saving snapshot
[2018-01-21 15:03:55.494141 UTC] Starting iteration 971
[2018-01-21 15:03:55.494331 UTC] Start collecting samples
[2018-01-21 15:03:59.841365 UTC] Computing input variables for policy optimization
[2018-01-21 15:03:59.962743 UTC] Performing policy update
[2018-01-21 15:03:59.963440 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:00.078452 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:01.431872 UTC] Performing line search
[2018-01-21 15:04:01.617132 UTC] Updating baseline
[2018-01-21 15:04:03.286612 UTC] Computing logging information
-------------------------------------
| Iteration            | 971        |
| ExpectedImprovement  | 0.017866   |
| ActualImprovement    | 0.016944   |
| ImprovementRatio     | 0.94838    |
| MeanKL               | 0.0077688  |
| Entropy              | -1.0776    |
| Perplexity           | 0.34041    |
| AveragePolicyStd     | 0.20464    |
| AveragePolicyStd[0]  | 0.22087    |
| AveragePolicyStd[1]  | 0.22048    |
| AveragePolicyStd[2]  | 0.1516     |
| AveragePolicyStd[3]  | 0.20766    |
| AveragePolicyStd[4]  | 0.18099    |
| AveragePolicyStd[5]  | 0.24625    |
| AverageReturn        | 1507.3     |
| MinReturn            | 55.896     |
| MaxReturn            | 1688.4     |
| StdReturn            | 306.95     |
| AverageEpisodeLength | 941.04     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.98     |
| TotalNEpisodes       | 21719      |
| TotalNSamples        | 4.8593e+06 |
| ExplainedVariance    | -0.0070755 |
-------------------------------------
[2018-01-21 15:04:04.071524 UTC] Saving snapshot
[2018-01-21 15:04:04.071788 UTC] Starting iteration 972
[2018-01-21 15:04:04.071957 UTC] Start collecting samples
[2018-01-21 15:04:08.689649 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:08.814723 UTC] Performing policy update
[2018-01-21 15:04:08.815372 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:08.939869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:10.330628 UTC] Performing line search
[2018-01-21 15:04:10.524791 UTC] Updating baseline
[2018-01-21 15:04:14.178875 UTC] Computing logging information
-------------------------------------
| Iteration            | 972        |
| ExpectedImprovement  | 0.018078   |
| ActualImprovement    | 0.017112   |
| ImprovementRatio     | 0.94656    |
| MeanKL               | 0.0078159  |
| Entropy              | -1.0825    |
| Perplexity           | 0.33875    |
| AveragePolicyStd     | 0.2045     |
| AveragePolicyStd[0]  | 0.221      |
| AveragePolicyStd[1]  | 0.22036    |
| AveragePolicyStd[2]  | 0.15101    |
| AveragePolicyStd[3]  | 0.2069     |
| AveragePolicyStd[4]  | 0.18137    |
| AveragePolicyStd[5]  | 0.24636    |
| AverageReturn        | 1511.9     |
| MinReturn            | 55.896     |
| MaxReturn            | 1688.4     |
| StdReturn            | 307.66     |
| AverageEpisodeLength | 941.63     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.07     |
| TotalNEpisodes       | 21726      |
| TotalNSamples        | 4.8663e+06 |
| ExplainedVariance    | 5.0753e-07 |
-------------------------------------
[2018-01-21 15:04:15.006594 UTC] Saving snapshot
[2018-01-21 15:04:15.006826 UTC] Starting iteration 973
[2018-01-21 15:04:15.006983 UTC] Start collecting samples
[2018-01-21 15:04:19.510079 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:19.663909 UTC] Performing policy update
[2018-01-21 15:04:19.664697 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:19.784559 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:21.174935 UTC] Performing line search
[2018-01-21 15:04:21.366408 UTC] Updating baseline
[2018-01-21 15:04:23.789974 UTC] Computing logging information
-------------------------------------
| Iteration            | 973        |
| ExpectedImprovement  | 0.016075   |
| ActualImprovement    | 0.014729   |
| ImprovementRatio     | 0.91627    |
| MeanKL               | 0.0082852  |
| Entropy              | -1.0904    |
| Perplexity           | 0.33608    |
| AveragePolicyStd     | 0.20425    |
| AveragePolicyStd[0]  | 0.22021    |
| AveragePolicyStd[1]  | 0.21986    |
| AveragePolicyStd[2]  | 0.15074    |
| AveragePolicyStd[3]  | 0.20685    |
| AveragePolicyStd[4]  | 0.18106    |
| AveragePolicyStd[5]  | 0.24679    |
| AverageReturn        | 1516.2     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 308.62     |
| AverageEpisodeLength | 942.3      |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.16     |
| TotalNEpisodes       | 21730      |
| TotalNSamples        | 4.8703e+06 |
| ExplainedVariance    | 0.0012809  |
-------------------------------------
[2018-01-21 15:04:24.560027 UTC] Saving snapshot
[2018-01-21 15:04:24.560320 UTC] Starting iteration 974
[2018-01-21 15:04:24.560576 UTC] Start collecting samples
[2018-01-21 15:04:29.063324 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:29.222318 UTC] Performing policy update
[2018-01-21 15:04:29.222998 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:29.344153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:30.730715 UTC] Performing line search
[2018-01-21 15:04:30.943409 UTC] Updating baseline
[2018-01-21 15:04:33.269864 UTC] Computing logging information
------------------------------------
| Iteration            | 974       |
| ExpectedImprovement  | 0.019859  |
| ActualImprovement    | 0.018796  |
| ImprovementRatio     | 0.94645   |
| MeanKL               | 0.0077528 |
| Entropy              | -1.0888   |
| Perplexity           | 0.33662   |
| AveragePolicyStd     | 0.20434   |
| AveragePolicyStd[0]  | 0.22007   |
| AveragePolicyStd[1]  | 0.22041   |
| AveragePolicyStd[2]  | 0.15071   |
| AveragePolicyStd[3]  | 0.20694   |
| AveragePolicyStd[4]  | 0.18064   |
| AveragePolicyStd[5]  | 0.24726   |
| AverageReturn        | 1497.2    |
| MinReturn            | 55.896    |
| MaxReturn            | 1713.2    |
| StdReturn            | 330.07    |
| AverageEpisodeLength | 928.87    |
| MinEpisodeLength     | 62        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 194.3     |
| TotalNEpisodes       | 21737     |
| TotalNSamples        | 4.876e+06 |
| ExplainedVariance    | 0.21803   |
------------------------------------
[2018-01-21 15:04:34.009357 UTC] Saving snapshot
[2018-01-21 15:04:34.009585 UTC] Starting iteration 975
[2018-01-21 15:04:34.009766 UTC] Start collecting samples
[2018-01-21 15:04:38.554898 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:38.698246 UTC] Performing policy update
[2018-01-21 15:04:38.703245 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:38.813360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:40.182531 UTC] Performing line search
[2018-01-21 15:04:40.375797 UTC] Updating baseline
[2018-01-21 15:04:42.297823 UTC] Computing logging information
-------------------------------------
| Iteration            | 975        |
| ExpectedImprovement  | 0.017306   |
| ActualImprovement    | 0.016477   |
| ImprovementRatio     | 0.95207    |
| MeanKL               | 0.0081815  |
| Entropy              | -1.0928    |
| Perplexity           | 0.33529    |
| AveragePolicyStd     | 0.20422    |
| AveragePolicyStd[0]  | 0.21992    |
| AveragePolicyStd[1]  | 0.22075    |
| AveragePolicyStd[2]  | 0.15043    |
| AveragePolicyStd[3]  | 0.20672    |
| AveragePolicyStd[4]  | 0.18044    |
| AveragePolicyStd[5]  | 0.24705    |
| AverageReturn        | 1498.9     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 330.75     |
| AverageEpisodeLength | 929.06     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.9      |
| TotalNEpisodes       | 21742      |
| TotalNSamples        | 4.8804e+06 |
| ExplainedVariance    | 0.1343     |
-------------------------------------
[2018-01-21 15:04:43.114962 UTC] Saving snapshot
[2018-01-21 15:04:43.115172 UTC] Starting iteration 976
[2018-01-21 15:04:43.115350 UTC] Start collecting samples
[2018-01-21 15:04:47.666716 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:47.808552 UTC] Performing policy update
[2018-01-21 15:04:47.809316 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:47.932295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:49.338503 UTC] Performing line search
[2018-01-21 15:04:49.528146 UTC] Updating baseline
[2018-01-21 15:04:51.338366 UTC] Computing logging information
-------------------------------------
| Iteration            | 976        |
| ExpectedImprovement  | 0.018352   |
| ActualImprovement    | 0.017262   |
| ImprovementRatio     | 0.94062    |
| MeanKL               | 0.0075219  |
| Entropy              | -1.0968    |
| Perplexity           | 0.33395    |
| AveragePolicyStd     | 0.20408    |
| AveragePolicyStd[0]  | 0.21969    |
| AveragePolicyStd[1]  | 0.22102    |
| AveragePolicyStd[2]  | 0.15046    |
| AveragePolicyStd[3]  | 0.20651    |
| AveragePolicyStd[4]  | 0.18006    |
| AveragePolicyStd[5]  | 0.24672    |
| AverageReturn        | 1476.5     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 354.5      |
| AverageEpisodeLength | 914.43     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.47     |
| TotalNEpisodes       | 21749      |
| TotalNSamples        | 4.8856e+06 |
| ExplainedVariance    | 0.3549     |
-------------------------------------
[2018-01-21 15:04:52.079909 UTC] Saving snapshot
[2018-01-21 15:04:52.080219 UTC] Starting iteration 977
[2018-01-21 15:04:52.080462 UTC] Start collecting samples
[2018-01-21 15:04:56.482626 UTC] Computing input variables for policy optimization
[2018-01-21 15:04:56.608364 UTC] Performing policy update
[2018-01-21 15:04:56.608955 UTC] Computing gradient in Euclidean space
[2018-01-21 15:04:56.726080 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:04:58.083273 UTC] Performing line search
[2018-01-21 15:04:58.268329 UTC] Updating baseline
[2018-01-21 15:04:59.835678 UTC] Computing logging information
-------------------------------------
| Iteration            | 977        |
| ExpectedImprovement  | 0.018189   |
| ActualImprovement    | 0.017898   |
| ImprovementRatio     | 0.984      |
| MeanKL               | 0.0075754  |
| Entropy              | -1.1027    |
| Perplexity           | 0.33197    |
| AveragePolicyStd     | 0.20388    |
| AveragePolicyStd[0]  | 0.21958    |
| AveragePolicyStd[1]  | 0.22079    |
| AveragePolicyStd[2]  | 0.15042    |
| AveragePolicyStd[3]  | 0.20597    |
| AveragePolicyStd[4]  | 0.17973    |
| AveragePolicyStd[5]  | 0.24681    |
| AverageReturn        | 1482.7     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 353.6      |
| AverageEpisodeLength | 916.74     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.71     |
| TotalNEpisodes       | 21754      |
| TotalNSamples        | 4.8901e+06 |
| ExplainedVariance    | 0.0092584  |
-------------------------------------
[2018-01-21 15:05:00.636614 UTC] Saving snapshot
[2018-01-21 15:05:00.636842 UTC] Starting iteration 978
[2018-01-21 15:05:00.636991 UTC] Start collecting samples
[2018-01-21 15:05:05.152414 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:05.280663 UTC] Performing policy update
[2018-01-21 15:05:05.281270 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:05.399246 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:06.761448 UTC] Performing line search
[2018-01-21 15:05:06.959035 UTC] Updating baseline
[2018-01-21 15:05:08.776335 UTC] Computing logging information
-------------------------------------
| Iteration            | 978        |
| ExpectedImprovement  | 0.016827   |
| ActualImprovement    | 0.015875   |
| ImprovementRatio     | 0.94339    |
| MeanKL               | 0.0081463  |
| Entropy              | -1.1094    |
| Perplexity           | 0.32975    |
| AveragePolicyStd     | 0.20364    |
| AveragePolicyStd[0]  | 0.21977    |
| AveragePolicyStd[1]  | 0.21999    |
| AveragePolicyStd[2]  | 0.15056    |
| AveragePolicyStd[3]  | 0.20584    |
| AveragePolicyStd[4]  | 0.17931    |
| AveragePolicyStd[5]  | 0.24635    |
| AverageReturn        | 1478.9     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 366.67     |
| AverageEpisodeLength | 913.89     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.24     |
| TotalNEpisodes       | 21759      |
| TotalNSamples        | 4.8943e+06 |
| ExplainedVariance    | 0.11323    |
-------------------------------------
[2018-01-21 15:05:09.579082 UTC] Saving snapshot
[2018-01-21 15:05:09.579258 UTC] Starting iteration 979
[2018-01-21 15:05:09.579360 UTC] Start collecting samples
[2018-01-21 15:05:14.013734 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:14.143072 UTC] Performing policy update
[2018-01-21 15:05:14.143687 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:14.260306 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:15.631764 UTC] Performing line search
[2018-01-21 15:05:15.818931 UTC] Updating baseline
[2018-01-21 15:05:17.887200 UTC] Computing logging information
-------------------------------------
| Iteration            | 979        |
| ExpectedImprovement  | 0.016709   |
| ActualImprovement    | 0.015932   |
| ImprovementRatio     | 0.9535     |
| MeanKL               | 0.0074135  |
| Entropy              | -1.1004    |
| Perplexity           | 0.33273    |
| AveragePolicyStd     | 0.20393    |
| AveragePolicyStd[0]  | 0.21985    |
| AveragePolicyStd[1]  | 0.21969    |
| AveragePolicyStd[2]  | 0.15087    |
| AveragePolicyStd[3]  | 0.20651    |
| AveragePolicyStd[4]  | 0.17976    |
| AveragePolicyStd[5]  | 0.24689    |
| AverageReturn        | 1501.3     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 334.02     |
| AverageEpisodeLength | 927.64     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.4      |
| TotalNEpisodes       | 21767      |
| TotalNSamples        | 4.9021e+06 |
| ExplainedVariance    | 0.071143   |
-------------------------------------
[2018-01-21 15:05:18.643016 UTC] Saving snapshot
[2018-01-21 15:05:18.643346 UTC] Starting iteration 980
[2018-01-21 15:05:18.643602 UTC] Start collecting samples
[2018-01-21 15:05:23.130947 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:23.248243 UTC] Performing policy update
[2018-01-21 15:05:23.249304 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:23.372597 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:24.782157 UTC] Performing line search
[2018-01-21 15:05:24.984136 UTC] Updating baseline
[2018-01-21 15:05:27.109215 UTC] Computing logging information
-------------------------------------
| Iteration            | 980        |
| ExpectedImprovement  | 0.019282   |
| ActualImprovement    | 0.019184   |
| ImprovementRatio     | 0.9949     |
| MeanKL               | 0.0075415  |
| Entropy              | -1.099     |
| Perplexity           | 0.3332     |
| AveragePolicyStd     | 0.20398    |
| AveragePolicyStd[0]  | 0.21997    |
| AveragePolicyStd[1]  | 0.2195     |
| AveragePolicyStd[2]  | 0.1511     |
| AveragePolicyStd[3]  | 0.20631    |
| AveragePolicyStd[4]  | 0.17966    |
| AveragePolicyStd[5]  | 0.24731    |
| AverageReturn        | 1494       |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 339.31     |
| AverageEpisodeLength | 923.45     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.23     |
| TotalNEpisodes       | 21769      |
| TotalNSamples        | 4.9036e+06 |
| ExplainedVariance    | 0.16952    |
-------------------------------------
[2018-01-21 15:05:27.813077 UTC] Saving snapshot
[2018-01-21 15:05:27.823468 UTC] Starting iteration 981
[2018-01-21 15:05:27.824337 UTC] Start collecting samples
[2018-01-21 15:05:32.350536 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:32.484259 UTC] Performing policy update
[2018-01-21 15:05:32.484925 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:32.606034 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:33.986627 UTC] Performing line search
[2018-01-21 15:05:34.178019 UTC] Updating baseline
[2018-01-21 15:05:36.133753 UTC] Computing logging information
-------------------------------------
| Iteration            | 981        |
| ExpectedImprovement  | 0.017908   |
| ActualImprovement    | 0.016702   |
| ImprovementRatio     | 0.93265    |
| MeanKL               | 0.0076558  |
| Entropy              | -1.103     |
| Perplexity           | 0.33187    |
| AveragePolicyStd     | 0.20382    |
| AveragePolicyStd[0]  | 0.21936    |
| AveragePolicyStd[1]  | 0.21912    |
| AveragePolicyStd[2]  | 0.15155    |
| AveragePolicyStd[3]  | 0.20572    |
| AveragePolicyStd[4]  | 0.17945    |
| AveragePolicyStd[5]  | 0.2477     |
| AverageReturn        | 1486.6     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 347.51     |
| AverageEpisodeLength | 918.23     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.9      |
| TotalNEpisodes       | 21776      |
| TotalNSamples        | 4.9101e+06 |
| ExplainedVariance    | 0.098913   |
-------------------------------------
[2018-01-21 15:05:36.958221 UTC] Saving snapshot
[2018-01-21 15:05:36.958526 UTC] Starting iteration 982
[2018-01-21 15:05:36.958746 UTC] Start collecting samples
[2018-01-21 15:05:41.445023 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:41.580395 UTC] Performing policy update
[2018-01-21 15:05:41.581033 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:41.696457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:43.094741 UTC] Performing line search
[2018-01-21 15:05:43.286888 UTC] Updating baseline
[2018-01-21 15:05:44.933811 UTC] Computing logging information
--------------------------------------
| Iteration            | 982         |
| ExpectedImprovement  | 0.016915    |
| ActualImprovement    | 0.016155    |
| ImprovementRatio     | 0.95507     |
| MeanKL               | 0.0083588   |
| Entropy              | -1.1079     |
| Perplexity           | 0.33025     |
| AveragePolicyStd     | 0.20367     |
| AveragePolicyStd[0]  | 0.21928     |
| AveragePolicyStd[1]  | 0.21912     |
| AveragePolicyStd[2]  | 0.15136     |
| AveragePolicyStd[3]  | 0.20539     |
| AveragePolicyStd[4]  | 0.17915     |
| AveragePolicyStd[5]  | 0.24769     |
| AverageReturn        | 1486.2      |
| MinReturn            | 55.896      |
| MaxReturn            | 1713.2      |
| StdReturn            | 347.15      |
| AverageEpisodeLength | 918.28      |
| MinEpisodeLength     | 62          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 205.92      |
| TotalNEpisodes       | 21783       |
| TotalNSamples        | 4.9171e+06  |
| ExplainedVariance    | -0.00013121 |
--------------------------------------
[2018-01-21 15:05:45.747628 UTC] Saving snapshot
[2018-01-21 15:05:45.747858 UTC] Starting iteration 983
[2018-01-21 15:05:45.748002 UTC] Start collecting samples
[2018-01-21 15:05:50.009053 UTC] Computing input variables for policy optimization
[2018-01-21 15:05:50.139467 UTC] Performing policy update
[2018-01-21 15:05:50.140612 UTC] Computing gradient in Euclidean space
[2018-01-21 15:05:50.258545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:05:51.666151 UTC] Performing line search
[2018-01-21 15:05:51.855242 UTC] Updating baseline
[2018-01-21 15:05:54.749455 UTC] Computing logging information
-------------------------------------
| Iteration            | 983        |
| ExpectedImprovement  | 0.020968   |
| ActualImprovement    | 0.020307   |
| ImprovementRatio     | 0.96849    |
| MeanKL               | 0.0077497  |
| Entropy              | -1.1093    |
| Perplexity           | 0.3298     |
| AveragePolicyStd     | 0.20362    |
| AveragePolicyStd[0]  | 0.21853    |
| AveragePolicyStd[1]  | 0.21916    |
| AveragePolicyStd[2]  | 0.15162    |
| AveragePolicyStd[3]  | 0.20553    |
| AveragePolicyStd[4]  | 0.17881    |
| AveragePolicyStd[5]  | 0.24804    |
| AverageReturn        | 1487.4     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 347.56     |
| AverageEpisodeLength | 918.28     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.92     |
| TotalNEpisodes       | 21785      |
| TotalNSamples        | 4.9191e+06 |
| ExplainedVariance    | -0.051759  |
-------------------------------------
[2018-01-21 15:05:55.492798 UTC] Saving snapshot
[2018-01-21 15:05:55.492986 UTC] Starting iteration 984
[2018-01-21 15:05:55.493092 UTC] Start collecting samples
[2018-01-21 15:05:59.880422 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:00.011840 UTC] Performing policy update
[2018-01-21 15:06:00.012818 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:00.128578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:01.505362 UTC] Performing line search
[2018-01-21 15:06:01.701982 UTC] Updating baseline
[2018-01-21 15:06:03.612902 UTC] Computing logging information
-------------------------------------
| Iteration            | 984        |
| ExpectedImprovement  | 0.015603   |
| ActualImprovement    | 0.014867   |
| ImprovementRatio     | 0.95282    |
| MeanKL               | 0.008172   |
| Entropy              | -1.1125    |
| Perplexity           | 0.32873    |
| AveragePolicyStd     | 0.20348    |
| AveragePolicyStd[0]  | 0.2183     |
| AveragePolicyStd[1]  | 0.21898    |
| AveragePolicyStd[2]  | 0.1517     |
| AveragePolicyStd[3]  | 0.2051     |
| AveragePolicyStd[4]  | 0.17904    |
| AveragePolicyStd[5]  | 0.24778    |
| AverageReturn        | 1489.4     |
| MinReturn            | 55.896     |
| MaxReturn            | 1713.2     |
| StdReturn            | 348.43     |
| AverageEpisodeLength | 918.28     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.92     |
| TotalNEpisodes       | 21790      |
| TotalNSamples        | 4.9241e+06 |
| ExplainedVariance    | -0.020279  |
-------------------------------------
[2018-01-21 15:06:04.430188 UTC] Saving snapshot
[2018-01-21 15:06:04.430465 UTC] Starting iteration 985
[2018-01-21 15:06:04.430648 UTC] Start collecting samples
[2018-01-21 15:06:08.845209 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:08.977898 UTC] Performing policy update
[2018-01-21 15:06:08.978515 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:09.097742 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:10.465730 UTC] Performing line search
[2018-01-21 15:06:10.655513 UTC] Updating baseline
[2018-01-21 15:06:12.746956 UTC] Computing logging information
-------------------------------------
| Iteration            | 985        |
| ExpectedImprovement  | 0.018334   |
| ActualImprovement    | 0.017286   |
| ImprovementRatio     | 0.94284    |
| MeanKL               | 0.0078937  |
| Entropy              | -1.1095    |
| Perplexity           | 0.32972    |
| AveragePolicyStd     | 0.20363    |
| AveragePolicyStd[0]  | 0.21824    |
| AveragePolicyStd[1]  | 0.2195     |
| AveragePolicyStd[2]  | 0.15164    |
| AveragePolicyStd[3]  | 0.20538    |
| AveragePolicyStd[4]  | 0.17842    |
| AveragePolicyStd[5]  | 0.24862    |
| AverageReturn        | 1502.8     |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 318.98     |
| AverageEpisodeLength | 924.49     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.57     |
| TotalNEpisodes       | 21798      |
| TotalNSamples        | 4.9318e+06 |
| ExplainedVariance    | 0.15221    |
-------------------------------------
[2018-01-21 15:06:13.479562 UTC] Saving snapshot
[2018-01-21 15:06:13.479804 UTC] Starting iteration 986
[2018-01-21 15:06:13.479949 UTC] Start collecting samples
[2018-01-21 15:06:18.064439 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:18.188141 UTC] Performing policy update
[2018-01-21 15:06:18.188774 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:18.306103 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:19.720630 UTC] Performing line search
[2018-01-21 15:06:19.917990 UTC] Updating baseline
[2018-01-21 15:06:21.796662 UTC] Computing logging information
-------------------------------------
| Iteration            | 986        |
| ExpectedImprovement  | 0.017499   |
| ActualImprovement    | 0.016158   |
| ImprovementRatio     | 0.92339    |
| MeanKL               | 0.0085836  |
| Entropy              | -1.1093    |
| Perplexity           | 0.3298     |
| AveragePolicyStd     | 0.20367    |
| AveragePolicyStd[0]  | 0.21792    |
| AveragePolicyStd[1]  | 0.2201     |
| AveragePolicyStd[2]  | 0.15137    |
| AveragePolicyStd[3]  | 0.20579    |
| AveragePolicyStd[4]  | 0.17815    |
| AveragePolicyStd[5]  | 0.24869    |
| AverageReturn        | 1499       |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 321.19     |
| AverageEpisodeLength | 921.84     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.35     |
| TotalNEpisodes       | 21801      |
| TotalNSamples        | 4.9345e+06 |
| ExplainedVariance    | 0.21825    |
-------------------------------------
[2018-01-21 15:06:22.530060 UTC] Saving snapshot
[2018-01-21 15:06:22.530298 UTC] Starting iteration 987
[2018-01-21 15:06:22.530461 UTC] Start collecting samples
[2018-01-21 15:06:27.173448 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:27.297440 UTC] Performing policy update
[2018-01-21 15:06:27.298502 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:27.415588 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:28.768572 UTC] Performing line search
[2018-01-21 15:06:28.961834 UTC] Updating baseline
[2018-01-21 15:06:30.849076 UTC] Computing logging information
-------------------------------------
| Iteration            | 987        |
| ExpectedImprovement  | 0.016837   |
| ActualImprovement    | 0.016117   |
| ImprovementRatio     | 0.95724    |
| MeanKL               | 0.0085042  |
| Entropy              | -1.1135    |
| Perplexity           | 0.3284     |
| AveragePolicyStd     | 0.20354    |
| AveragePolicyStd[0]  | 0.21782    |
| AveragePolicyStd[1]  | 0.22014    |
| AveragePolicyStd[2]  | 0.15122    |
| AveragePolicyStd[3]  | 0.20564    |
| AveragePolicyStd[4]  | 0.17775    |
| AveragePolicyStd[5]  | 0.24868    |
| AverageReturn        | 1511       |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 300.65     |
| AverageEpisodeLength | 928.12     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.59     |
| TotalNEpisodes       | 21807      |
| TotalNSamples        | 4.9401e+06 |
| ExplainedVariance    | 0.15645    |
-------------------------------------
[2018-01-21 15:06:31.607326 UTC] Saving snapshot
[2018-01-21 15:06:31.607581 UTC] Starting iteration 988
[2018-01-21 15:06:31.607805 UTC] Start collecting samples
[2018-01-21 15:06:35.891925 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:36.022604 UTC] Performing policy update
[2018-01-21 15:06:36.030247 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:36.152854 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:37.581229 UTC] Performing line search
[2018-01-21 15:06:37.770804 UTC] Updating baseline
[2018-01-21 15:06:40.813670 UTC] Computing logging information
-------------------------------------
| Iteration            | 988        |
| ExpectedImprovement  | 0.017947   |
| ActualImprovement    | 0.016862   |
| ImprovementRatio     | 0.93954    |
| MeanKL               | 0.007011   |
| Entropy              | -1.1197    |
| Perplexity           | 0.32638    |
| AveragePolicyStd     | 0.20332    |
| AveragePolicyStd[0]  | 0.2182     |
| AveragePolicyStd[1]  | 0.21969    |
| AveragePolicyStd[2]  | 0.15111    |
| AveragePolicyStd[3]  | 0.20567    |
| AveragePolicyStd[4]  | 0.17742    |
| AveragePolicyStd[5]  | 0.24784    |
| AverageReturn        | 1512.5     |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 301.38     |
| AverageEpisodeLength | 928.12     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.59     |
| TotalNEpisodes       | 21811      |
| TotalNSamples        | 4.9441e+06 |
| ExplainedVariance    | -0.012005  |
-------------------------------------
[2018-01-21 15:06:41.570475 UTC] Saving snapshot
[2018-01-21 15:06:41.570753 UTC] Starting iteration 989
[2018-01-21 15:06:41.570954 UTC] Start collecting samples
[2018-01-21 15:06:46.063994 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:46.185625 UTC] Performing policy update
[2018-01-21 15:06:46.186258 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:46.303333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:47.693327 UTC] Performing line search
[2018-01-21 15:06:47.879313 UTC] Updating baseline
[2018-01-21 15:06:49.886989 UTC] Computing logging information
-------------------------------------
| Iteration            | 989        |
| ExpectedImprovement  | 0.019395   |
| ActualImprovement    | 0.017665   |
| ImprovementRatio     | 0.91076    |
| MeanKL               | 0.0073929  |
| Entropy              | -1.1148    |
| Perplexity           | 0.32798    |
| AveragePolicyStd     | 0.2035     |
| AveragePolicyStd[0]  | 0.2182     |
| AveragePolicyStd[1]  | 0.21917    |
| AveragePolicyStd[2]  | 0.15158    |
| AveragePolicyStd[3]  | 0.20551    |
| AveragePolicyStd[4]  | 0.1773     |
| AveragePolicyStd[5]  | 0.24922    |
| AverageReturn        | 1508.5     |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 305.29     |
| AverageEpisodeLength | 924.76     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.37     |
| TotalNEpisodes       | 21818      |
| TotalNSamples        | 4.9508e+06 |
| ExplainedVariance    | 0.069162   |
-------------------------------------
[2018-01-21 15:06:50.612553 UTC] Saving snapshot
[2018-01-21 15:06:50.612791 UTC] Starting iteration 990
[2018-01-21 15:06:50.612941 UTC] Start collecting samples
[2018-01-21 15:06:55.012809 UTC] Computing input variables for policy optimization
[2018-01-21 15:06:55.142833 UTC] Performing policy update
[2018-01-21 15:06:55.143588 UTC] Computing gradient in Euclidean space
[2018-01-21 15:06:55.260525 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:06:56.656607 UTC] Performing line search
[2018-01-21 15:06:56.847328 UTC] Updating baseline
[2018-01-21 15:06:58.758167 UTC] Computing logging information
-------------------------------------
| Iteration            | 990        |
| ExpectedImprovement  | 0.018494   |
| ActualImprovement    | 0.017883   |
| ImprovementRatio     | 0.96696    |
| MeanKL               | 0.0076816  |
| Entropy              | -1.1201    |
| Perplexity           | 0.32625    |
| AveragePolicyStd     | 0.20335    |
| AveragePolicyStd[0]  | 0.21777    |
| AveragePolicyStd[1]  | 0.21913    |
| AveragePolicyStd[2]  | 0.15133    |
| AveragePolicyStd[3]  | 0.20546    |
| AveragePolicyStd[4]  | 0.17687    |
| AveragePolicyStd[5]  | 0.24952    |
| AverageReturn        | 1498.8     |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 316.52     |
| AverageEpisodeLength | 918.1      |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.68     |
| TotalNEpisodes       | 21824      |
| TotalNSamples        | 4.9561e+06 |
| ExplainedVariance    | 0.14935    |
-------------------------------------
[2018-01-21 15:06:59.569923 UTC] Saving snapshot
[2018-01-21 15:06:59.580046 UTC] Starting iteration 991
[2018-01-21 15:06:59.580282 UTC] Start collecting samples
[2018-01-21 15:07:03.985605 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:04.117839 UTC] Performing policy update
[2018-01-21 15:07:04.118797 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:04.242549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:05.689168 UTC] Performing line search
[2018-01-21 15:07:05.886452 UTC] Updating baseline
[2018-01-21 15:07:07.832542 UTC] Computing logging information
-------------------------------------
| Iteration            | 991        |
| ExpectedImprovement  | 0.01761    |
| ActualImprovement    | 0.016636   |
| ImprovementRatio     | 0.94471    |
| MeanKL               | 0.0076063  |
| Entropy              | -1.1189    |
| Perplexity           | 0.32665    |
| AveragePolicyStd     | 0.2034     |
| AveragePolicyStd[0]  | 0.21769    |
| AveragePolicyStd[1]  | 0.21913    |
| AveragePolicyStd[2]  | 0.15133    |
| AveragePolicyStd[3]  | 0.20553    |
| AveragePolicyStd[4]  | 0.17686    |
| AveragePolicyStd[5]  | 0.24986    |
| AverageReturn        | 1486.4     |
| MinReturn            | 183.86     |
| MaxReturn            | 1718.2     |
| StdReturn            | 339.85     |
| AverageEpisodeLength | 909.93     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.36     |
| TotalNEpisodes       | 21828      |
| TotalNSamples        | 4.9593e+06 |
| ExplainedVariance    | 0.12897    |
-------------------------------------
[2018-01-21 15:07:08.590034 UTC] Saving snapshot
[2018-01-21 15:07:08.590278 UTC] Starting iteration 992
[2018-01-21 15:07:08.590439 UTC] Start collecting samples
[2018-01-21 15:07:13.254680 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:13.388491 UTC] Performing policy update
[2018-01-21 15:07:13.389679 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:13.511724 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:15.011248 UTC] Performing line search
[2018-01-21 15:07:15.198856 UTC] Updating baseline
[2018-01-21 15:07:17.110281 UTC] Computing logging information
------------------------------------
| Iteration            | 992       |
| ExpectedImprovement  | 0.01931   |
| ActualImprovement    | 0.018514  |
| ImprovementRatio     | 0.95875   |
| MeanKL               | 0.0080833 |
| Entropy              | -1.1144   |
| Perplexity           | 0.32812   |
| AveragePolicyStd     | 0.20357   |
| AveragePolicyStd[0]  | 0.21742   |
| AveragePolicyStd[1]  | 0.21955   |
| AveragePolicyStd[2]  | 0.1516    |
| AveragePolicyStd[3]  | 0.20523   |
| AveragePolicyStd[4]  | 0.17676   |
| AveragePolicyStd[5]  | 0.25087   |
| AverageReturn        | 1486.6    |
| MinReturn            | 183.86    |
| MaxReturn            | 1718.2    |
| StdReturn            | 336.79    |
| AverageEpisodeLength | 909.91    |
| MinEpisodeLength     | 132       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 197.86    |
| TotalNEpisodes       | 21837     |
| TotalNSamples        | 4.967e+06 |
| ExplainedVariance    | 0.19804   |
------------------------------------
[2018-01-21 15:07:17.872873 UTC] Saving snapshot
[2018-01-21 15:07:17.873160 UTC] Starting iteration 993
[2018-01-21 15:07:17.873362 UTC] Start collecting samples
[2018-01-21 15:07:22.285272 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:22.413249 UTC] Performing policy update
[2018-01-21 15:07:22.414255 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:22.542135 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:23.958820 UTC] Performing line search
[2018-01-21 15:07:24.154387 UTC] Updating baseline
[2018-01-21 15:07:26.319264 UTC] Computing logging information
------------------------------------
| Iteration            | 993       |
| ExpectedImprovement  | 0.018931  |
| ActualImprovement    | 0.017176  |
| ImprovementRatio     | 0.9073    |
| MeanKL               | 0.0083004 |
| Entropy              | -1.1094   |
| Perplexity           | 0.32977   |
| AveragePolicyStd     | 0.20374   |
| AveragePolicyStd[0]  | 0.21763   |
| AveragePolicyStd[1]  | 0.22      |
| AveragePolicyStd[2]  | 0.15176   |
| AveragePolicyStd[3]  | 0.20527   |
| AveragePolicyStd[4]  | 0.17685   |
| AveragePolicyStd[5]  | 0.25093   |
| AverageReturn        | 1497.5    |
| MinReturn            | 183.86    |
| MaxReturn            | 1722.3    |
| StdReturn            | 329.15    |
| AverageEpisodeLength | 915.3     |
| MinEpisodeLength     | 132       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 192.84    |
| TotalNEpisodes       | 21841     |
| TotalNSamples        | 4.971e+06 |
| ExplainedVariance    | 0.0042025 |
------------------------------------
[2018-01-21 15:07:27.055977 UTC] Saving snapshot
[2018-01-21 15:07:27.056281 UTC] Starting iteration 994
[2018-01-21 15:07:27.056478 UTC] Start collecting samples
[2018-01-21 15:07:31.690663 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:31.847093 UTC] Performing policy update
[2018-01-21 15:07:31.847939 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:31.964446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:33.416652 UTC] Performing line search
[2018-01-21 15:07:33.679925 UTC] Updating baseline
[2018-01-21 15:07:35.787399 UTC] Computing logging information
-------------------------------------
| Iteration            | 994        |
| ExpectedImprovement  | 0.020233   |
| ActualImprovement    | 0.018599   |
| ImprovementRatio     | 0.91924    |
| MeanKL               | 0.0078066  |
| Entropy              | -1.1103    |
| Perplexity           | 0.32945    |
| AveragePolicyStd     | 0.20372    |
| AveragePolicyStd[0]  | 0.2181     |
| AveragePolicyStd[1]  | 0.2199     |
| AveragePolicyStd[2]  | 0.15192    |
| AveragePolicyStd[3]  | 0.20528    |
| AveragePolicyStd[4]  | 0.17616    |
| AveragePolicyStd[5]  | 0.25095    |
| AverageReturn        | 1487.4     |
| MinReturn            | 183.86     |
| MaxReturn            | 1722.3     |
| StdReturn            | 345.6      |
| AverageEpisodeLength | 908.92     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.25     |
| TotalNEpisodes       | 21846      |
| TotalNSamples        | 4.9747e+06 |
| ExplainedVariance    | 0.42901    |
-------------------------------------
[2018-01-21 15:07:36.505976 UTC] Saving snapshot
[2018-01-21 15:07:36.506213 UTC] Starting iteration 995
[2018-01-21 15:07:36.506360 UTC] Start collecting samples
[2018-01-21 15:07:41.010977 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:41.134845 UTC] Performing policy update
[2018-01-21 15:07:41.135959 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:41.252853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:42.634667 UTC] Performing line search
[2018-01-21 15:07:42.819382 UTC] Updating baseline
[2018-01-21 15:07:44.643280 UTC] Computing logging information
-------------------------------------
| Iteration            | 995        |
| ExpectedImprovement  | 0.015986   |
| ActualImprovement    | 0.015434   |
| ImprovementRatio     | 0.96551    |
| MeanKL               | 0.007203   |
| Entropy              | -1.1145    |
| Perplexity           | 0.32809    |
| AveragePolicyStd     | 0.20361    |
| AveragePolicyStd[0]  | 0.21842    |
| AveragePolicyStd[1]  | 0.22003    |
| AveragePolicyStd[2]  | 0.15142    |
| AveragePolicyStd[3]  | 0.20533    |
| AveragePolicyStd[4]  | 0.17593    |
| AveragePolicyStd[5]  | 0.25051    |
| AverageReturn        | 1517       |
| MinReturn            | 183.86     |
| MaxReturn            | 1722.3     |
| StdReturn            | 316.78     |
| AverageEpisodeLength | 925.18     |
| MinEpisodeLength     | 132        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.75     |
| TotalNEpisodes       | 21852      |
| TotalNSamples        | 4.9807e+06 |
| ExplainedVariance    | 0.0017519  |
-------------------------------------
[2018-01-21 15:07:45.427158 UTC] Saving snapshot
[2018-01-21 15:07:45.427405 UTC] Starting iteration 996
[2018-01-21 15:07:45.427563 UTC] Start collecting samples
[2018-01-21 15:07:49.950178 UTC] Computing input variables for policy optimization
[2018-01-21 15:07:50.076263 UTC] Performing policy update
[2018-01-21 15:07:50.077297 UTC] Computing gradient in Euclidean space
[2018-01-21 15:07:50.192851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:07:51.602847 UTC] Performing line search
[2018-01-21 15:07:51.784658 UTC] Updating baseline
[2018-01-21 15:07:55.046323 UTC] Computing logging information
-------------------------------------
| Iteration            | 996        |
| ExpectedImprovement  | 0.019361   |
| ActualImprovement    | 0.018235   |
| ImprovementRatio     | 0.94184    |
| MeanKL               | 0.0080489  |
| Entropy              | -1.1156    |
| Perplexity           | 0.32773    |
| AveragePolicyStd     | 0.2036     |
| AveragePolicyStd[0]  | 0.21861    |
| AveragePolicyStd[1]  | 0.21991    |
| AveragePolicyStd[2]  | 0.15114    |
| AveragePolicyStd[3]  | 0.20543    |
| AveragePolicyStd[4]  | 0.17576    |
| AveragePolicyStd[5]  | 0.25073    |
| AverageReturn        | 1532.5     |
| MinReturn            | 262.83     |
| MaxReturn            | 1722.3     |
| StdReturn            | 287.57     |
| AverageEpisodeLength | 933.86     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.79     |
| TotalNEpisodes       | 21856      |
| TotalNSamples        | 4.9847e+06 |
| ExplainedVariance    | -0.025695  |
-------------------------------------
[2018-01-21 15:07:55.828137 UTC] Saving snapshot
[2018-01-21 15:07:55.828362 UTC] Starting iteration 997
[2018-01-21 15:07:55.828539 UTC] Start collecting samples
[2018-01-21 15:08:00.298305 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:00.422640 UTC] Performing policy update
[2018-01-21 15:08:00.423288 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:00.537892 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:01.971472 UTC] Performing line search
[2018-01-21 15:08:02.157704 UTC] Updating baseline
[2018-01-21 15:08:03.959943 UTC] Computing logging information
-------------------------------------
| Iteration            | 997        |
| ExpectedImprovement  | 0.016899   |
| ActualImprovement    | 0.015805   |
| ImprovementRatio     | 0.93524    |
| MeanKL               | 0.0080166  |
| Entropy              | -1.1133    |
| Perplexity           | 0.32847    |
| AveragePolicyStd     | 0.20368    |
| AveragePolicyStd[0]  | 0.21903    |
| AveragePolicyStd[1]  | 0.22       |
| AveragePolicyStd[2]  | 0.15121    |
| AveragePolicyStd[3]  | 0.20531    |
| AveragePolicyStd[4]  | 0.17579    |
| AveragePolicyStd[5]  | 0.25072    |
| AverageReturn        | 1535       |
| MinReturn            | 262.83     |
| MaxReturn            | 1722.3     |
| StdReturn            | 288.16     |
| AverageEpisodeLength | 933.86     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.79     |
| TotalNEpisodes       | 21860      |
| TotalNSamples        | 4.9887e+06 |
| ExplainedVariance    | 0.012171   |
-------------------------------------
[2018-01-21 15:08:04.742915 UTC] Saving snapshot
[2018-01-21 15:08:04.743161 UTC] Starting iteration 998
[2018-01-21 15:08:04.743484 UTC] Start collecting samples
[2018-01-21 15:08:09.397822 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:09.549278 UTC] Performing policy update
[2018-01-21 15:08:09.549990 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:09.667187 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:11.049843 UTC] Performing line search
[2018-01-21 15:08:11.237531 UTC] Updating baseline
[2018-01-21 15:08:13.617537 UTC] Computing logging information
-------------------------------------
| Iteration            | 998        |
| ExpectedImprovement  | 0.017331   |
| ActualImprovement    | 0.016665   |
| ImprovementRatio     | 0.9616     |
| MeanKL               | 0.0068956  |
| Entropy              | -1.1191    |
| Perplexity           | 0.32657    |
| AveragePolicyStd     | 0.20346    |
| AveragePolicyStd[0]  | 0.21854    |
| AveragePolicyStd[1]  | 0.22009    |
| AveragePolicyStd[2]  | 0.15115    |
| AveragePolicyStd[3]  | 0.20517    |
| AveragePolicyStd[4]  | 0.17568    |
| AveragePolicyStd[5]  | 0.25014    |
| AverageReturn        | 1535.9     |
| MinReturn            | 262.83     |
| MaxReturn            | 1722.3     |
| StdReturn            | 287.37     |
| AverageEpisodeLength | 934.13     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.57     |
| TotalNEpisodes       | 21867      |
| TotalNSamples        | 4.9955e+06 |
| ExplainedVariance    | 0.072096   |
-------------------------------------
[2018-01-21 15:08:14.438388 UTC] Saving snapshot
[2018-01-21 15:08:14.438634 UTC] Starting iteration 999
[2018-01-21 15:08:14.438776 UTC] Start collecting samples
[2018-01-21 15:08:19.186498 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:19.321504 UTC] Performing policy update
[2018-01-21 15:08:19.322216 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:19.450770 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:20.849672 UTC] Performing line search
[2018-01-21 15:08:21.039578 UTC] Updating baseline
[2018-01-21 15:08:22.843953 UTC] Computing logging information
------------------------------------
| Iteration            | 999       |
| ExpectedImprovement  | 0.018417  |
| ActualImprovement    | 0.017692  |
| ImprovementRatio     | 0.96065   |
| MeanKL               | 0.0073055 |
| Entropy              | -1.117    |
| Perplexity           | 0.32725   |
| AveragePolicyStd     | 0.20356   |
| AveragePolicyStd[0]  | 0.2192    |
| AveragePolicyStd[1]  | 0.22015   |
| AveragePolicyStd[2]  | 0.15105   |
| AveragePolicyStd[3]  | 0.20488   |
| AveragePolicyStd[4]  | 0.17558   |
| AveragePolicyStd[5]  | 0.25051   |
| AverageReturn        | 1530.5    |
| MinReturn            | 262.83    |
| MaxReturn            | 1722.3    |
| StdReturn            | 296.38    |
| AverageEpisodeLength | 929.29    |
| MinEpisodeLength     | 183       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 171.95    |
| TotalNEpisodes       | 21875     |
| TotalNSamples        | 5.002e+06 |
| ExplainedVariance    | 0.18157   |
------------------------------------
[2018-01-21 15:08:23.593068 UTC] Saving snapshot
[2018-01-21 15:08:23.593319 UTC] Starting iteration 1000
[2018-01-21 15:08:23.593537 UTC] Start collecting samples
[2018-01-21 15:08:27.821245 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:27.966664 UTC] Performing policy update
[2018-01-21 15:08:27.967285 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:28.085674 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:29.574457 UTC] Performing line search
[2018-01-21 15:08:29.775867 UTC] Updating baseline
[2018-01-21 15:08:32.988235 UTC] Computing logging information
------------------------------------
| Iteration            | 1000      |
| ExpectedImprovement  | 0.015776  |
| ActualImprovement    | 0.015059  |
| ImprovementRatio     | 0.95455   |
| MeanKL               | 0.0083288 |
| Entropy              | -1.1218   |
| Perplexity           | 0.32568   |
| AveragePolicyStd     | 0.20339   |
| AveragePolicyStd[0]  | 0.21924   |
| AveragePolicyStd[1]  | 0.21999   |
| AveragePolicyStd[2]  | 0.15123   |
| AveragePolicyStd[3]  | 0.20426   |
| AveragePolicyStd[4]  | 0.1753    |
| AveragePolicyStd[5]  | 0.2503    |
| AverageReturn        | 1531.3    |
| MinReturn            | 262.83    |
| MaxReturn            | 1726      |
| StdReturn            | 296.8     |
| AverageEpisodeLength | 929.29    |
| MinEpisodeLength     | 183       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 171.95    |
| TotalNEpisodes       | 21877     |
| TotalNSamples        | 5.004e+06 |
| ExplainedVariance    | 0.0091318 |
------------------------------------
[2018-01-21 15:08:33.875991 UTC] Saving snapshot
[2018-01-21 15:08:33.886383 UTC] Starting iteration 1001
[2018-01-21 15:08:33.886636 UTC] Start collecting samples
[2018-01-21 15:08:39.390781 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:39.549503 UTC] Performing policy update
[2018-01-21 15:08:39.550128 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:39.671226 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:41.084863 UTC] Performing line search
[2018-01-21 15:08:41.282301 UTC] Updating baseline
[2018-01-21 15:08:43.368278 UTC] Computing logging information
-------------------------------------
| Iteration            | 1001       |
| ExpectedImprovement  | 0.017621   |
| ActualImprovement    | 0.016549   |
| ImprovementRatio     | 0.93916    |
| MeanKL               | 0.0082434  |
| Entropy              | -1.1201    |
| Perplexity           | 0.32624    |
| AveragePolicyStd     | 0.20345    |
| AveragePolicyStd[0]  | 0.21933    |
| AveragePolicyStd[1]  | 0.22018    |
| AveragePolicyStd[2]  | 0.15079    |
| AveragePolicyStd[3]  | 0.20434    |
| AveragePolicyStd[4]  | 0.17595    |
| AveragePolicyStd[5]  | 0.2501     |
| AverageReturn        | 1528.2     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 300.88     |
| AverageEpisodeLength | 926.1      |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.56     |
| TotalNEpisodes       | 21883      |
| TotalNSamples        | 5.0097e+06 |
| ExplainedVariance    | 0.15677    |
-------------------------------------
[2018-01-21 15:08:44.088424 UTC] Saving snapshot
[2018-01-21 15:08:44.088692 UTC] Starting iteration 1002
[2018-01-21 15:08:44.088924 UTC] Start collecting samples
[2018-01-21 15:08:48.556290 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:48.679420 UTC] Performing policy update
[2018-01-21 15:08:48.680055 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:48.811269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:50.236094 UTC] Performing line search
[2018-01-21 15:08:50.424575 UTC] Updating baseline
[2018-01-21 15:08:52.521986 UTC] Computing logging information
-------------------------------------
| Iteration            | 1002       |
| ExpectedImprovement  | 0.017435   |
| ActualImprovement    | 0.015897   |
| ImprovementRatio     | 0.91178    |
| MeanKL               | 0.0081718  |
| Entropy              | -1.12      |
| Perplexity           | 0.32627    |
| AveragePolicyStd     | 0.20341    |
| AveragePolicyStd[0]  | 0.21896    |
| AveragePolicyStd[1]  | 0.21963    |
| AveragePolicyStd[2]  | 0.15114    |
| AveragePolicyStd[3]  | 0.20471    |
| AveragePolicyStd[4]  | 0.17613    |
| AveragePolicyStd[5]  | 0.24988    |
| AverageReturn        | 1529.2     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 301.24     |
| AverageEpisodeLength | 926.1      |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.56     |
| TotalNEpisodes       | 21890      |
| TotalNSamples        | 5.0167e+06 |
| ExplainedVariance    | 0.0017139  |
-------------------------------------
[2018-01-21 15:08:53.278118 UTC] Saving snapshot
[2018-01-21 15:08:53.278363 UTC] Starting iteration 1003
[2018-01-21 15:08:53.278536 UTC] Start collecting samples
[2018-01-21 15:08:57.664825 UTC] Computing input variables for policy optimization
[2018-01-21 15:08:57.786179 UTC] Performing policy update
[2018-01-21 15:08:57.786907 UTC] Computing gradient in Euclidean space
[2018-01-21 15:08:57.906727 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:08:59.292874 UTC] Performing line search
[2018-01-21 15:08:59.478311 UTC] Updating baseline
[2018-01-21 15:09:01.850804 UTC] Computing logging information
-------------------------------------
| Iteration            | 1003       |
| ExpectedImprovement  | 0.016783   |
| ActualImprovement    | 0.015668   |
| ImprovementRatio     | 0.93357    |
| MeanKL               | 0.0082933  |
| Entropy              | -1.1236    |
| Perplexity           | 0.32509    |
| AveragePolicyStd     | 0.20327    |
| AveragePolicyStd[0]  | 0.21868    |
| AveragePolicyStd[1]  | 0.21878    |
| AveragePolicyStd[2]  | 0.15144    |
| AveragePolicyStd[3]  | 0.20447    |
| AveragePolicyStd[4]  | 0.17602    |
| AveragePolicyStd[5]  | 0.25024    |
| AverageReturn        | 1530.1     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 301.33     |
| AverageEpisodeLength | 926.98     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.71     |
| TotalNEpisodes       | 21893      |
| TotalNSamples        | 5.0197e+06 |
| ExplainedVariance    | 0.0069074  |
-------------------------------------
[2018-01-21 15:09:02.608308 UTC] Saving snapshot
[2018-01-21 15:09:02.608527 UTC] Starting iteration 1004
[2018-01-21 15:09:02.608706 UTC] Start collecting samples
[2018-01-21 15:09:06.986037 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:07.141130 UTC] Performing policy update
[2018-01-21 15:09:07.142172 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:07.259720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:08.706746 UTC] Performing line search
[2018-01-21 15:09:08.894358 UTC] Updating baseline
[2018-01-21 15:09:11.080278 UTC] Computing logging information
-------------------------------------
| Iteration            | 1004       |
| ExpectedImprovement  | 0.017366   |
| ActualImprovement    | 0.016445   |
| ImprovementRatio     | 0.94698    |
| MeanKL               | 0.007802   |
| Entropy              | -1.1189    |
| Perplexity           | 0.32664    |
| AveragePolicyStd     | 0.20344    |
| AveragePolicyStd[0]  | 0.21908    |
| AveragePolicyStd[1]  | 0.21863    |
| AveragePolicyStd[2]  | 0.15155    |
| AveragePolicyStd[3]  | 0.20502    |
| AveragePolicyStd[4]  | 0.17588    |
| AveragePolicyStd[5]  | 0.25049    |
| AverageReturn        | 1534.1     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 300.29     |
| AverageEpisodeLength | 929.27     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.15     |
| TotalNEpisodes       | 21897      |
| TotalNSamples        | 5.0237e+06 |
| ExplainedVariance    | -0.040322  |
-------------------------------------
[2018-01-21 15:09:11.917804 UTC] Saving snapshot
[2018-01-21 15:09:11.918261 UTC] Starting iteration 1005
[2018-01-21 15:09:11.918623 UTC] Start collecting samples
[2018-01-21 15:09:16.568585 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:16.705746 UTC] Performing policy update
[2018-01-21 15:09:16.706368 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:16.834603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:18.218014 UTC] Performing line search
[2018-01-21 15:09:18.404038 UTC] Updating baseline
[2018-01-21 15:09:20.938587 UTC] Computing logging information
-------------------------------------
| Iteration            | 1005       |
| ExpectedImprovement  | 0.018461   |
| ActualImprovement    | 0.017581   |
| ImprovementRatio     | 0.95232    |
| MeanKL               | 0.0081742  |
| Entropy              | -1.12      |
| Perplexity           | 0.32629    |
| AveragePolicyStd     | 0.20334    |
| AveragePolicyStd[0]  | 0.21893    |
| AveragePolicyStd[1]  | 0.21822    |
| AveragePolicyStd[2]  | 0.15198    |
| AveragePolicyStd[3]  | 0.20485    |
| AveragePolicyStd[4]  | 0.17628    |
| AveragePolicyStd[5]  | 0.24978    |
| AverageReturn        | 1545.6     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 292.82     |
| AverageEpisodeLength | 936.02     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.08     |
| TotalNEpisodes       | 21906      |
| TotalNSamples        | 5.0327e+06 |
| ExplainedVariance    | 0.0069037  |
-------------------------------------
[2018-01-21 15:09:21.701031 UTC] Saving snapshot
[2018-01-21 15:09:21.701253 UTC] Starting iteration 1006
[2018-01-21 15:09:21.701433 UTC] Start collecting samples
[2018-01-21 15:09:25.969425 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:26.088727 UTC] Performing policy update
[2018-01-21 15:09:26.089329 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:26.204450 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:27.617634 UTC] Performing line search
[2018-01-21 15:09:27.810474 UTC] Updating baseline
[2018-01-21 15:09:30.058474 UTC] Computing logging information
-------------------------------------
| Iteration            | 1006       |
| ExpectedImprovement  | 0.019222   |
| ActualImprovement    | 0.017469   |
| ImprovementRatio     | 0.9088     |
| MeanKL               | 0.00721    |
| Entropy              | -1.123     |
| Perplexity           | 0.32531    |
| AveragePolicyStd     | 0.20324    |
| AveragePolicyStd[0]  | 0.21846    |
| AveragePolicyStd[1]  | 0.21816    |
| AveragePolicyStd[2]  | 0.15216    |
| AveragePolicyStd[3]  | 0.20484    |
| AveragePolicyStd[4]  | 0.17588    |
| AveragePolicyStd[5]  | 0.24992    |
| AverageReturn        | 1534.8     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 303.02     |
| AverageEpisodeLength | 930.85     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.85     |
| TotalNEpisodes       | 21909      |
| TotalNSamples        | 5.0352e+06 |
| ExplainedVariance    | 0.16393    |
-------------------------------------
[2018-01-21 15:09:30.756345 UTC] Saving snapshot
[2018-01-21 15:09:30.756603 UTC] Starting iteration 1007
[2018-01-21 15:09:30.756761 UTC] Start collecting samples
[2018-01-21 15:09:35.736712 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:35.864500 UTC] Performing policy update
[2018-01-21 15:09:35.865371 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:35.983412 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:37.548188 UTC] Performing line search
[2018-01-21 15:09:37.737746 UTC] Updating baseline
[2018-01-21 15:09:39.425419 UTC] Computing logging information
-------------------------------------
| Iteration            | 1007       |
| ExpectedImprovement  | 0.018513   |
| ActualImprovement    | 0.017553   |
| ImprovementRatio     | 0.94816    |
| MeanKL               | 0.0081018  |
| Entropy              | -1.1279    |
| Perplexity           | 0.32372    |
| AveragePolicyStd     | 0.20304    |
| AveragePolicyStd[0]  | 0.21878    |
| AveragePolicyStd[1]  | 0.21706    |
| AveragePolicyStd[2]  | 0.15213    |
| AveragePolicyStd[3]  | 0.20492    |
| AveragePolicyStd[4]  | 0.17601    |
| AveragePolicyStd[5]  | 0.24936    |
| AverageReturn        | 1533.7     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 302.62     |
| AverageEpisodeLength | 930.85     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.85     |
| TotalNEpisodes       | 21914      |
| TotalNSamples        | 5.0402e+06 |
| ExplainedVariance    | -0.0070796 |
-------------------------------------
[2018-01-21 15:09:40.144980 UTC] Saving snapshot
[2018-01-21 15:09:40.145157 UTC] Starting iteration 1008
[2018-01-21 15:09:40.145292 UTC] Start collecting samples
[2018-01-21 15:09:44.828984 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:44.986349 UTC] Performing policy update
[2018-01-21 15:09:44.987390 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:45.102699 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:46.505118 UTC] Performing line search
[2018-01-21 15:09:46.703362 UTC] Updating baseline
[2018-01-21 15:09:48.899543 UTC] Computing logging information
-------------------------------------
| Iteration            | 1008       |
| ExpectedImprovement  | 0.016736   |
| ActualImprovement    | 0.016189   |
| ImprovementRatio     | 0.96733    |
| MeanKL               | 0.0074835  |
| Entropy              | -1.1409    |
| Perplexity           | 0.31954    |
| AveragePolicyStd     | 0.20256    |
| AveragePolicyStd[0]  | 0.21797    |
| AveragePolicyStd[1]  | 0.21624    |
| AveragePolicyStd[2]  | 0.15226    |
| AveragePolicyStd[3]  | 0.20396    |
| AveragePolicyStd[4]  | 0.17606    |
| AveragePolicyStd[5]  | 0.24888    |
| AverageReturn        | 1524.6     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 313.69     |
| AverageEpisodeLength | 925.65     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.64     |
| TotalNEpisodes       | 21920      |
| TotalNSamples        | 5.0453e+06 |
| ExplainedVariance    | 0.16935    |
-------------------------------------
[2018-01-21 15:09:49.718328 UTC] Saving snapshot
[2018-01-21 15:09:49.718607 UTC] Starting iteration 1009
[2018-01-21 15:09:49.718787 UTC] Start collecting samples
[2018-01-21 15:09:54.151875 UTC] Computing input variables for policy optimization
[2018-01-21 15:09:54.274062 UTC] Performing policy update
[2018-01-21 15:09:54.275135 UTC] Computing gradient in Euclidean space
[2018-01-21 15:09:54.394258 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:09:55.761489 UTC] Performing line search
[2018-01-21 15:09:55.951277 UTC] Updating baseline
[2018-01-21 15:09:57.508156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1009       |
| ExpectedImprovement  | 0.018809   |
| ActualImprovement    | 0.01776    |
| ImprovementRatio     | 0.9442     |
| MeanKL               | 0.0084097  |
| Entropy              | -1.1416    |
| Perplexity           | 0.3193     |
| AveragePolicyStd     | 0.20253    |
| AveragePolicyStd[0]  | 0.21771    |
| AveragePolicyStd[1]  | 0.21659    |
| AveragePolicyStd[2]  | 0.15234    |
| AveragePolicyStd[3]  | 0.20368    |
| AveragePolicyStd[4]  | 0.17602    |
| AveragePolicyStd[5]  | 0.24885    |
| AverageReturn        | 1535.6     |
| MinReturn            | 262.83     |
| MaxReturn            | 1726       |
| StdReturn            | 301.94     |
| AverageEpisodeLength | 932.31     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.9      |
| TotalNEpisodes       | 21926      |
| TotalNSamples        | 5.0513e+06 |
| ExplainedVariance    | -0.031781  |
-------------------------------------
[2018-01-21 15:09:58.323619 UTC] Saving snapshot
[2018-01-21 15:09:58.323851 UTC] Starting iteration 1010
[2018-01-21 15:09:58.324044 UTC] Start collecting samples
[2018-01-21 15:10:02.820770 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:02.946423 UTC] Performing policy update
[2018-01-21 15:10:02.947514 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:03.067945 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:04.459390 UTC] Performing line search
[2018-01-21 15:10:04.647836 UTC] Updating baseline
[2018-01-21 15:10:06.831076 UTC] Computing logging information
-------------------------------------
| Iteration            | 1010       |
| ExpectedImprovement  | 0.018609   |
| ActualImprovement    | 0.016695   |
| ImprovementRatio     | 0.89712    |
| MeanKL               | 0.0077432  |
| Entropy              | -1.1435    |
| Perplexity           | 0.3187     |
| AveragePolicyStd     | 0.2025     |
| AveragePolicyStd[0]  | 0.21752    |
| AveragePolicyStd[1]  | 0.21653    |
| AveragePolicyStd[2]  | 0.15225    |
| AveragePolicyStd[3]  | 0.20364    |
| AveragePolicyStd[4]  | 0.17557    |
| AveragePolicyStd[5]  | 0.24948    |
| AverageReturn        | 1550.4     |
| MinReturn            | 350.18     |
| MaxReturn            | 1726       |
| StdReturn            | 274.01     |
| AverageEpisodeLength | 940.48     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.97     |
| TotalNEpisodes       | 21929      |
| TotalNSamples        | 5.0543e+06 |
| ExplainedVariance    | 0.012392   |
-------------------------------------
[2018-01-21 15:10:07.586919 UTC] Saving snapshot
[2018-01-21 15:10:07.596125 UTC] Starting iteration 1011
[2018-01-21 15:10:07.596362 UTC] Start collecting samples
[2018-01-21 15:10:12.054516 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:12.176569 UTC] Performing policy update
[2018-01-21 15:10:12.177404 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:12.308714 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:13.785355 UTC] Performing line search
[2018-01-21 15:10:13.985171 UTC] Updating baseline
[2018-01-21 15:10:15.934308 UTC] Computing logging information
-------------------------------------
| Iteration            | 1011       |
| ExpectedImprovement  | 0.017238   |
| ActualImprovement    | 0.016156   |
| ImprovementRatio     | 0.9372     |
| MeanKL               | 0.0081689  |
| Entropy              | -1.1517    |
| Perplexity           | 0.31611    |
| AveragePolicyStd     | 0.20221    |
| AveragePolicyStd[0]  | 0.21647    |
| AveragePolicyStd[1]  | 0.21639    |
| AveragePolicyStd[2]  | 0.1522     |
| AveragePolicyStd[3]  | 0.20346    |
| AveragePolicyStd[4]  | 0.17546    |
| AveragePolicyStd[5]  | 0.24929    |
| AverageReturn        | 1566.7     |
| MinReturn            | 350.18     |
| MaxReturn            | 1726       |
| StdReturn            | 258.53     |
| AverageEpisodeLength | 949.36     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.15     |
| TotalNEpisodes       | 21933      |
| TotalNSamples        | 5.0583e+06 |
| ExplainedVariance    | -0.012831  |
-------------------------------------
[2018-01-21 15:10:16.753639 UTC] Saving snapshot
[2018-01-21 15:10:16.753926 UTC] Starting iteration 1012
[2018-01-21 15:10:16.754125 UTC] Start collecting samples
[2018-01-21 15:10:21.229390 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:21.353681 UTC] Performing policy update
[2018-01-21 15:10:21.354320 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:21.472685 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:22.884103 UTC] Performing line search
[2018-01-21 15:10:23.075028 UTC] Updating baseline
[2018-01-21 15:10:24.733652 UTC] Computing logging information
-------------------------------------
| Iteration            | 1012       |
| ExpectedImprovement  | 0.016121   |
| ActualImprovement    | 0.015277   |
| ImprovementRatio     | 0.94761    |
| MeanKL               | 0.0083088  |
| Entropy              | -1.1534    |
| Perplexity           | 0.31555    |
| AveragePolicyStd     | 0.20216    |
| AveragePolicyStd[0]  | 0.21667    |
| AveragePolicyStd[1]  | 0.21615    |
| AveragePolicyStd[2]  | 0.15194    |
| AveragePolicyStd[3]  | 0.20334    |
| AveragePolicyStd[4]  | 0.17569    |
| AveragePolicyStd[5]  | 0.24915    |
| AverageReturn        | 1577.9     |
| MinReturn            | 350.18     |
| MaxReturn            | 1726       |
| StdReturn            | 244.93     |
| AverageEpisodeLength | 956.77     |
| MinEpisodeLength     | 243        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.53     |
| TotalNEpisodes       | 21942      |
| TotalNSamples        | 5.0673e+06 |
| ExplainedVariance    | -0.018008  |
-------------------------------------
[2018-01-21 15:10:25.500510 UTC] Saving snapshot
[2018-01-21 15:10:25.500814 UTC] Starting iteration 1013
[2018-01-21 15:10:25.501006 UTC] Start collecting samples
[2018-01-21 15:10:29.925957 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:30.045260 UTC] Performing policy update
[2018-01-21 15:10:30.045868 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:30.168685 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:31.531401 UTC] Performing line search
[2018-01-21 15:10:31.718254 UTC] Updating baseline
[2018-01-21 15:10:33.886933 UTC] Computing logging information
-------------------------------------
| Iteration            | 1013       |
| ExpectedImprovement  | 0.017437   |
| ActualImprovement    | 0.015456   |
| ImprovementRatio     | 0.88641    |
| MeanKL               | 0.0074597  |
| Entropy              | -1.1535    |
| Perplexity           | 0.31553    |
| AveragePolicyStd     | 0.20218    |
| AveragePolicyStd[0]  | 0.2172     |
| AveragePolicyStd[1]  | 0.21651    |
| AveragePolicyStd[2]  | 0.1516     |
| AveragePolicyStd[3]  | 0.20315    |
| AveragePolicyStd[4]  | 0.17566    |
| AveragePolicyStd[5]  | 0.24894    |
| AverageReturn        | 1591       |
| MinReturn            | 609.67     |
| MaxReturn            | 1726       |
| StdReturn            | 211.72     |
| AverageEpisodeLength | 964.34     |
| MinEpisodeLength     | 394        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.9      |
| TotalNEpisodes       | 21943      |
| TotalNSamples        | 5.0683e+06 |
| ExplainedVariance    | 0.24912    |
-------------------------------------
[2018-01-21 15:10:34.718134 UTC] Saving snapshot
[2018-01-21 15:10:34.718371 UTC] Starting iteration 1014
[2018-01-21 15:10:34.718633 UTC] Start collecting samples
[2018-01-21 15:10:39.560921 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:39.705823 UTC] Performing policy update
[2018-01-21 15:10:39.706912 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:39.834392 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:41.290339 UTC] Performing line search
[2018-01-21 15:10:41.491700 UTC] Updating baseline
[2018-01-21 15:10:44.245139 UTC] Computing logging information
-------------------------------------
| Iteration            | 1014       |
| ExpectedImprovement  | 0.01808    |
| ActualImprovement    | 0.018022   |
| ImprovementRatio     | 0.99678    |
| MeanKL               | 0.0078271  |
| Entropy              | -1.1463    |
| Perplexity           | 0.31781    |
| AveragePolicyStd     | 0.20243    |
| AveragePolicyStd[0]  | 0.21756    |
| AveragePolicyStd[1]  | 0.21701    |
| AveragePolicyStd[2]  | 0.15177    |
| AveragePolicyStd[3]  | 0.20378    |
| AveragePolicyStd[4]  | 0.17535    |
| AveragePolicyStd[5]  | 0.24913    |
| AverageReturn        | 1584.5     |
| MinReturn            | 609.67     |
| MaxReturn            | 1726       |
| StdReturn            | 223.54     |
| AverageEpisodeLength | 962.15     |
| MinEpisodeLength     | 394        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.14     |
| TotalNEpisodes       | 21949      |
| TotalNSamples        | 5.0739e+06 |
| ExplainedVariance    | -0.047433  |
-------------------------------------
[2018-01-21 15:10:44.978957 UTC] Saving snapshot
[2018-01-21 15:10:44.979192 UTC] Starting iteration 1015
[2018-01-21 15:10:44.979339 UTC] Start collecting samples
[2018-01-21 15:10:49.726998 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:49.873238 UTC] Performing policy update
[2018-01-21 15:10:49.874030 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:50.019669 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:10:51.505901 UTC] Performing line search
[2018-01-21 15:10:51.702616 UTC] Updating baseline
[2018-01-21 15:10:54.069976 UTC] Computing logging information
-------------------------------------
| Iteration            | 1015       |
| ExpectedImprovement  | 0.01575    |
| ActualImprovement    | 0.015458   |
| ImprovementRatio     | 0.98146    |
| MeanKL               | 0.0080164  |
| Entropy              | -1.1458    |
| Perplexity           | 0.31798    |
| AveragePolicyStd     | 0.20242    |
| AveragePolicyStd[0]  | 0.21768    |
| AveragePolicyStd[1]  | 0.21684    |
| AveragePolicyStd[2]  | 0.15211    |
| AveragePolicyStd[3]  | 0.20369    |
| AveragePolicyStd[4]  | 0.17536    |
| AveragePolicyStd[5]  | 0.24885    |
| AverageReturn        | 1571.1     |
| MinReturn            | 609.67     |
| MaxReturn            | 1726       |
| StdReturn            | 236.6      |
| AverageEpisodeLength | 955.63     |
| MinEpisodeLength     | 394        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.08     |
| TotalNEpisodes       | 21958      |
| TotalNSamples        | 5.0822e+06 |
| ExplainedVariance    | 0.11729    |
-------------------------------------
[2018-01-21 15:10:54.792686 UTC] Saving snapshot
[2018-01-21 15:10:54.792865 UTC] Starting iteration 1016
[2018-01-21 15:10:54.792968 UTC] Start collecting samples
[2018-01-21 15:10:59.237179 UTC] Computing input variables for policy optimization
[2018-01-21 15:10:59.378460 UTC] Performing policy update
[2018-01-21 15:10:59.379620 UTC] Computing gradient in Euclidean space
[2018-01-21 15:10:59.504701 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:00.916213 UTC] Performing line search
[2018-01-21 15:11:01.109859 UTC] Updating baseline
[2018-01-21 15:11:03.784014 UTC] Computing logging information
-------------------------------------
| Iteration            | 1016       |
| ExpectedImprovement  | 0.018741   |
| ActualImprovement    | 0.017856   |
| ImprovementRatio     | 0.95277    |
| MeanKL               | 0.0078098  |
| Entropy              | -1.1302    |
| Perplexity           | 0.32298    |
| AveragePolicyStd     | 0.20292    |
| AveragePolicyStd[0]  | 0.21799    |
| AveragePolicyStd[1]  | 0.21812    |
| AveragePolicyStd[2]  | 0.15266    |
| AveragePolicyStd[3]  | 0.20397    |
| AveragePolicyStd[4]  | 0.17602    |
| AveragePolicyStd[5]  | 0.24874    |
| AverageReturn        | 1560.7     |
| MinReturn            | 556.16     |
| MaxReturn            | 1726       |
| StdReturn            | 257.13     |
| AverageEpisodeLength | 949.34     |
| MinEpisodeLength     | 371        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.99     |
| TotalNEpisodes       | 21961      |
| TotalNSamples        | 5.0846e+06 |
| ExplainedVariance    | 0.19902    |
-------------------------------------
[2018-01-21 15:11:04.533762 UTC] Saving snapshot
[2018-01-21 15:11:04.534025 UTC] Starting iteration 1017
[2018-01-21 15:11:04.534216 UTC] Start collecting samples
[2018-01-21 15:11:09.113100 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:09.247857 UTC] Performing policy update
[2018-01-21 15:11:09.248478 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:09.374899 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:10.754464 UTC] Performing line search
[2018-01-21 15:11:10.947650 UTC] Updating baseline
[2018-01-21 15:11:12.822165 UTC] Computing logging information
-------------------------------------
| Iteration            | 1017       |
| ExpectedImprovement  | 0.016452   |
| ActualImprovement    | 0.015511   |
| ImprovementRatio     | 0.94277    |
| MeanKL               | 0.0079174  |
| Entropy              | -1.125     |
| Perplexity           | 0.32467    |
| AveragePolicyStd     | 0.2031     |
| AveragePolicyStd[0]  | 0.21799    |
| AveragePolicyStd[1]  | 0.21783    |
| AveragePolicyStd[2]  | 0.1531     |
| AveragePolicyStd[3]  | 0.20404    |
| AveragePolicyStd[4]  | 0.17586    |
| AveragePolicyStd[5]  | 0.24981    |
| AverageReturn        | 1563.8     |
| MinReturn            | 556.16     |
| MaxReturn            | 1726       |
| StdReturn            | 256.03     |
| AverageEpisodeLength | 951.24     |
| MinEpisodeLength     | 371        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.4      |
| TotalNEpisodes       | 21965      |
| TotalNSamples        | 5.0886e+06 |
| ExplainedVariance    | -0.0066214 |
-------------------------------------
[2018-01-21 15:11:13.563034 UTC] Saving snapshot
[2018-01-21 15:11:13.563327 UTC] Starting iteration 1018
[2018-01-21 15:11:13.563541 UTC] Start collecting samples
[2018-01-21 15:11:18.135445 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:18.257579 UTC] Performing policy update
[2018-01-21 15:11:18.258681 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:18.375689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:19.722942 UTC] Performing line search
[2018-01-21 15:11:19.914274 UTC] Updating baseline
[2018-01-21 15:11:21.918513 UTC] Computing logging information
-------------------------------------
| Iteration            | 1018       |
| ExpectedImprovement  | 0.018517   |
| ActualImprovement    | 0.017164   |
| ImprovementRatio     | 0.92691    |
| MeanKL               | 0.0076634  |
| Entropy              | -1.123     |
| Perplexity           | 0.32531    |
| AveragePolicyStd     | 0.20313    |
| AveragePolicyStd[0]  | 0.21835    |
| AveragePolicyStd[1]  | 0.21824    |
| AveragePolicyStd[2]  | 0.1534     |
| AveragePolicyStd[3]  | 0.204      |
| AveragePolicyStd[4]  | 0.17593    |
| AveragePolicyStd[5]  | 0.2489     |
| AverageReturn        | 1583.6     |
| MinReturn            | 556.16     |
| MaxReturn            | 1726       |
| StdReturn            | 219.84     |
| AverageEpisodeLength | 965.49     |
| MinEpisodeLength     | 371        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.13     |
| TotalNEpisodes       | 21974      |
| TotalNSamples        | 5.0976e+06 |
| ExplainedVariance    | 0.00044383 |
-------------------------------------
[2018-01-21 15:11:22.678872 UTC] Saving snapshot
[2018-01-21 15:11:22.679108 UTC] Starting iteration 1019
[2018-01-21 15:11:22.679292 UTC] Start collecting samples
[2018-01-21 15:11:27.191421 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:27.322798 UTC] Performing policy update
[2018-01-21 15:11:27.323908 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:27.442597 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:28.851061 UTC] Performing line search
[2018-01-21 15:11:29.045059 UTC] Updating baseline
[2018-01-21 15:11:31.035006 UTC] Computing logging information
-------------------------------------
| Iteration            | 1019       |
| ExpectedImprovement  | 0.019215   |
| ActualImprovement    | 0.017427   |
| ImprovementRatio     | 0.90697    |
| MeanKL               | 0.0076678  |
| Entropy              | -1.1304    |
| Perplexity           | 0.32291    |
| AveragePolicyStd     | 0.20288    |
| AveragePolicyStd[0]  | 0.21854    |
| AveragePolicyStd[1]  | 0.21791    |
| AveragePolicyStd[2]  | 0.15328    |
| AveragePolicyStd[3]  | 0.20388    |
| AveragePolicyStd[4]  | 0.17541    |
| AveragePolicyStd[5]  | 0.24827    |
| AverageReturn        | 1562.9     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 270.12     |
| AverageEpisodeLength | 954.38     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.75     |
| TotalNEpisodes       | 21979      |
| TotalNSamples        | 5.1012e+06 |
| ExplainedVariance    | 0.19743    |
-------------------------------------
[2018-01-21 15:11:31.837463 UTC] Saving snapshot
[2018-01-21 15:11:31.837694 UTC] Starting iteration 1020
[2018-01-21 15:11:31.837843 UTC] Start collecting samples
[2018-01-21 15:11:36.515130 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:36.659694 UTC] Performing policy update
[2018-01-21 15:11:36.660364 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:36.779737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:38.182360 UTC] Performing line search
[2018-01-21 15:11:38.381900 UTC] Updating baseline
[2018-01-21 15:11:40.531511 UTC] Computing logging information
-------------------------------------
| Iteration            | 1020       |
| ExpectedImprovement  | 0.016573   |
| ActualImprovement    | 0.015948   |
| ImprovementRatio     | 0.96229    |
| MeanKL               | 0.0083067  |
| Entropy              | -1.1314    |
| Perplexity           | 0.32258    |
| AveragePolicyStd     | 0.20286    |
| AveragePolicyStd[0]  | 0.21838    |
| AveragePolicyStd[1]  | 0.21794    |
| AveragePolicyStd[2]  | 0.15327    |
| AveragePolicyStd[3]  | 0.20377    |
| AveragePolicyStd[4]  | 0.17524    |
| AveragePolicyStd[5]  | 0.24856    |
| AverageReturn        | 1550.7     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 289.13     |
| AverageEpisodeLength | 947.93     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.69     |
| TotalNEpisodes       | 21983      |
| TotalNSamples        | 5.1045e+06 |
| ExplainedVariance    | 0.22825    |
-------------------------------------
[2018-01-21 15:11:41.360679 UTC] Saving snapshot
[2018-01-21 15:11:41.370017 UTC] Starting iteration 1021
[2018-01-21 15:11:41.370263 UTC] Start collecting samples
[2018-01-21 15:11:46.019515 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:46.153527 UTC] Performing policy update
[2018-01-21 15:11:46.154678 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:46.277801 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:47.720702 UTC] Performing line search
[2018-01-21 15:11:47.927359 UTC] Updating baseline
[2018-01-21 15:11:50.415465 UTC] Computing logging information
-------------------------------------
| Iteration            | 1021       |
| ExpectedImprovement  | 0.018195   |
| ActualImprovement    | 0.017429   |
| ImprovementRatio     | 0.95789    |
| MeanKL               | 0.0082512  |
| Entropy              | -1.1307    |
| Perplexity           | 0.32282    |
| AveragePolicyStd     | 0.20289    |
| AveragePolicyStd[0]  | 0.2182     |
| AveragePolicyStd[1]  | 0.21803    |
| AveragePolicyStd[2]  | 0.15332    |
| AveragePolicyStd[3]  | 0.20389    |
| AveragePolicyStd[4]  | 0.17521    |
| AveragePolicyStd[5]  | 0.24866    |
| AverageReturn        | 1539.8     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 304.59     |
| AverageEpisodeLength | 941.57     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.43     |
| TotalNEpisodes       | 21990      |
| TotalNSamples        | 5.1109e+06 |
| ExplainedVariance    | 0.066455   |
-------------------------------------
[2018-01-21 15:11:51.245346 UTC] Saving snapshot
[2018-01-21 15:11:51.245587 UTC] Starting iteration 1022
[2018-01-21 15:11:51.245759 UTC] Start collecting samples
[2018-01-21 15:11:55.873656 UTC] Computing input variables for policy optimization
[2018-01-21 15:11:55.996625 UTC] Performing policy update
[2018-01-21 15:11:55.997218 UTC] Computing gradient in Euclidean space
[2018-01-21 15:11:56.119603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:11:57.543473 UTC] Performing line search
[2018-01-21 15:11:57.737907 UTC] Updating baseline
[2018-01-21 15:12:00.127921 UTC] Computing logging information
-------------------------------------
| Iteration            | 1022       |
| ExpectedImprovement  | 0.01607    |
| ActualImprovement    | 0.015549   |
| ImprovementRatio     | 0.96759    |
| MeanKL               | 0.0079481  |
| Entropy              | -1.1367    |
| Perplexity           | 0.32088    |
| AveragePolicyStd     | 0.20269    |
| AveragePolicyStd[0]  | 0.21812    |
| AveragePolicyStd[1]  | 0.21725    |
| AveragePolicyStd[2]  | 0.15331    |
| AveragePolicyStd[3]  | 0.20354    |
| AveragePolicyStd[4]  | 0.17489    |
| AveragePolicyStd[5]  | 0.24906    |
| AverageReturn        | 1539.9     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 304.63     |
| AverageEpisodeLength | 941.57     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.43     |
| TotalNEpisodes       | 21995      |
| TotalNSamples        | 5.1159e+06 |
| ExplainedVariance    | -0.051642  |
-------------------------------------
[2018-01-21 15:12:00.966520 UTC] Saving snapshot
[2018-01-21 15:12:00.966806 UTC] Starting iteration 1023
[2018-01-21 15:12:00.967030 UTC] Start collecting samples
[2018-01-21 15:12:05.550383 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:05.679097 UTC] Performing policy update
[2018-01-21 15:12:05.680234 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:05.800636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:07.217354 UTC] Performing line search
[2018-01-21 15:12:07.408531 UTC] Updating baseline
[2018-01-21 15:12:09.574495 UTC] Computing logging information
-------------------------------------
| Iteration            | 1023       |
| ExpectedImprovement  | 0.018001   |
| ActualImprovement    | 0.01686    |
| ImprovementRatio     | 0.93664    |
| MeanKL               | 0.00785    |
| Entropy              | -1.1396    |
| Perplexity           | 0.31995    |
| AveragePolicyStd     | 0.20261    |
| AveragePolicyStd[0]  | 0.21802    |
| AveragePolicyStd[1]  | 0.2167     |
| AveragePolicyStd[2]  | 0.15313    |
| AveragePolicyStd[3]  | 0.20343    |
| AveragePolicyStd[4]  | 0.17492    |
| AveragePolicyStd[5]  | 0.24945    |
| AverageReturn        | 1534.3     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 309.85     |
| AverageEpisodeLength | 937.65     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.42     |
| TotalNEpisodes       | 22000      |
| TotalNSamples        | 5.1205e+06 |
| ExplainedVariance    | 0.13952    |
-------------------------------------
[2018-01-21 15:12:10.399088 UTC] Saving snapshot
[2018-01-21 15:12:10.399350 UTC] Starting iteration 1024
[2018-01-21 15:12:10.399534 UTC] Start collecting samples
[2018-01-21 15:12:15.121675 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:15.245211 UTC] Performing policy update
[2018-01-21 15:12:15.246277 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:15.364857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:16.779682 UTC] Performing line search
[2018-01-21 15:12:16.968619 UTC] Updating baseline
[2018-01-21 15:12:19.036054 UTC] Computing logging information
-------------------------------------
| Iteration            | 1024       |
| ExpectedImprovement  | 0.017937   |
| ActualImprovement    | 0.016788   |
| ImprovementRatio     | 0.93594    |
| MeanKL               | 0.0075659  |
| Entropy              | -1.1395    |
| Perplexity           | 0.31997    |
| AveragePolicyStd     | 0.20262    |
| AveragePolicyStd[0]  | 0.21781    |
| AveragePolicyStd[1]  | 0.21672    |
| AveragePolicyStd[2]  | 0.15321    |
| AveragePolicyStd[3]  | 0.20377    |
| AveragePolicyStd[4]  | 0.17463    |
| AveragePolicyStd[5]  | 0.24956    |
| AverageReturn        | 1518.9     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 326.23     |
| AverageEpisodeLength | 928.65     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.07     |
| TotalNEpisodes       | 22005      |
| TotalNSamples        | 5.1246e+06 |
| ExplainedVariance    | 0.23309    |
-------------------------------------
[2018-01-21 15:12:19.866079 UTC] Saving snapshot
[2018-01-21 15:12:19.866327 UTC] Starting iteration 1025
[2018-01-21 15:12:19.866524 UTC] Start collecting samples
[2018-01-21 15:12:24.297720 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:24.440085 UTC] Performing policy update
[2018-01-21 15:12:24.441206 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:24.562625 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:26.012713 UTC] Performing line search
[2018-01-21 15:12:26.214893 UTC] Updating baseline
[2018-01-21 15:12:28.280665 UTC] Computing logging information
-------------------------------------
| Iteration            | 1025       |
| ExpectedImprovement  | 0.018009   |
| ActualImprovement    | 0.017095   |
| ImprovementRatio     | 0.94928    |
| MeanKL               | 0.0080958  |
| Entropy              | -1.1468    |
| Perplexity           | 0.31764    |
| AveragePolicyStd     | 0.20235    |
| AveragePolicyStd[0]  | 0.21804    |
| AveragePolicyStd[1]  | 0.21641    |
| AveragePolicyStd[2]  | 0.15336    |
| AveragePolicyStd[3]  | 0.20287    |
| AveragePolicyStd[4]  | 0.17434    |
| AveragePolicyStd[5]  | 0.24911    |
| AverageReturn        | 1528.7     |
| MinReturn            | 226.89     |
| MaxReturn            | 1720.6     |
| StdReturn            | 316.84     |
| AverageEpisodeLength | 933.82     |
| MinEpisodeLength     | 166        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.8      |
| TotalNEpisodes       | 22010      |
| TotalNSamples        | 5.1296e+06 |
| ExplainedVariance    | 0.13375    |
-------------------------------------
[2018-01-21 15:12:29.105664 UTC] Saving snapshot
[2018-01-21 15:12:29.105931 UTC] Starting iteration 1026
[2018-01-21 15:12:29.106128 UTC] Start collecting samples
[2018-01-21 15:12:33.524417 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:33.645036 UTC] Performing policy update
[2018-01-21 15:12:33.646003 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:33.772875 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:35.214169 UTC] Performing line search
[2018-01-21 15:12:35.410753 UTC] Updating baseline
[2018-01-21 15:12:37.406893 UTC] Computing logging information
-------------------------------------
| Iteration            | 1026       |
| ExpectedImprovement  | 0.017208   |
| ActualImprovement    | 0.016525   |
| ImprovementRatio     | 0.96032    |
| MeanKL               | 0.0077645  |
| Entropy              | -1.1473    |
| Perplexity           | 0.3175     |
| AveragePolicyStd     | 0.20231    |
| AveragePolicyStd[0]  | 0.21774    |
| AveragePolicyStd[1]  | 0.21632    |
| AveragePolicyStd[2]  | 0.15372    |
| AveragePolicyStd[3]  | 0.20288    |
| AveragePolicyStd[4]  | 0.17424    |
| AveragePolicyStd[5]  | 0.24897    |
| AverageReturn        | 1490.6     |
| MinReturn            | 226.89     |
| MaxReturn            | 1706.5     |
| StdReturn            | 369.63     |
| AverageEpisodeLength | 912.54     |
| MinEpisodeLength     | 163        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.93     |
| TotalNEpisodes       | 22021      |
| TotalNSamples        | 5.1376e+06 |
| ExplainedVariance    | 0.23955    |
-------------------------------------
[2018-01-21 15:12:38.151567 UTC] Saving snapshot
[2018-01-21 15:12:38.151794 UTC] Starting iteration 1027
[2018-01-21 15:12:38.152008 UTC] Start collecting samples
[2018-01-21 15:12:42.482528 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:42.630145 UTC] Performing policy update
[2018-01-21 15:12:42.630807 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:42.746518 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:44.114298 UTC] Performing line search
[2018-01-21 15:12:44.301539 UTC] Updating baseline
[2018-01-21 15:12:46.402502 UTC] Computing logging information
-------------------------------------
| Iteration            | 1027       |
| ExpectedImprovement  | 0.017881   |
| ActualImprovement    | 0.017252   |
| ImprovementRatio     | 0.9648     |
| MeanKL               | 0.0079671  |
| Entropy              | -1.1442    |
| Perplexity           | 0.31847    |
| AveragePolicyStd     | 0.20237    |
| AveragePolicyStd[0]  | 0.21773    |
| AveragePolicyStd[1]  | 0.21617    |
| AveragePolicyStd[2]  | 0.15402    |
| AveragePolicyStd[3]  | 0.20343    |
| AveragePolicyStd[4]  | 0.17454    |
| AveragePolicyStd[5]  | 0.24833    |
| AverageReturn        | 1488       |
| MinReturn            | 226.89     |
| MaxReturn            | 1706.5     |
| StdReturn            | 370.69     |
| AverageEpisodeLength | 910.28     |
| MinEpisodeLength     | 163        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.18     |
| TotalNEpisodes       | 22025      |
| TotalNSamples        | 5.1414e+06 |
| ExplainedVariance    | 0.17416    |
-------------------------------------
[2018-01-21 15:12:47.159075 UTC] Saving snapshot
[2018-01-21 15:12:47.159291 UTC] Starting iteration 1028
[2018-01-21 15:12:47.159437 UTC] Start collecting samples
[2018-01-21 15:12:51.836970 UTC] Computing input variables for policy optimization
[2018-01-21 15:12:51.961552 UTC] Performing policy update
[2018-01-21 15:12:51.962185 UTC] Computing gradient in Euclidean space
[2018-01-21 15:12:52.078852 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:12:53.463650 UTC] Performing line search
[2018-01-21 15:12:53.658770 UTC] Updating baseline
[2018-01-21 15:12:55.847210 UTC] Computing logging information
-------------------------------------
| Iteration            | 1028       |
| ExpectedImprovement  | 0.016249   |
| ActualImprovement    | 0.015426   |
| ImprovementRatio     | 0.9493     |
| MeanKL               | 0.0080339  |
| Entropy              | -1.1457    |
| Perplexity           | 0.318      |
| AveragePolicyStd     | 0.20232    |
| AveragePolicyStd[0]  | 0.2177     |
| AveragePolicyStd[1]  | 0.216      |
| AveragePolicyStd[2]  | 0.15409    |
| AveragePolicyStd[3]  | 0.20353    |
| AveragePolicyStd[4]  | 0.17439    |
| AveragePolicyStd[5]  | 0.24819    |
| AverageReturn        | 1466.9     |
| MinReturn            | 226.89     |
| MaxReturn            | 1706.5     |
| StdReturn            | 389.71     |
| AverageEpisodeLength | 898.29     |
| MinEpisodeLength     | 163        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.22     |
| TotalNEpisodes       | 22029      |
| TotalNSamples        | 5.1442e+06 |
| ExplainedVariance    | 0.33624    |
-------------------------------------
[2018-01-21 15:12:56.583582 UTC] Saving snapshot
[2018-01-21 15:12:56.583882 UTC] Starting iteration 1029
[2018-01-21 15:12:56.584098 UTC] Start collecting samples
[2018-01-21 15:13:01.043362 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:01.172080 UTC] Performing policy update
[2018-01-21 15:13:01.173167 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:01.289673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:02.651869 UTC] Performing line search
[2018-01-21 15:13:02.842947 UTC] Updating baseline
[2018-01-21 15:13:04.756735 UTC] Computing logging information
-------------------------------------
| Iteration            | 1029       |
| ExpectedImprovement  | 0.018504   |
| ActualImprovement    | 0.017804   |
| ImprovementRatio     | 0.96217    |
| MeanKL               | 0.0084331  |
| Entropy              | -1.1521    |
| Perplexity           | 0.31597    |
| AveragePolicyStd     | 0.20206    |
| AveragePolicyStd[0]  | 0.2177     |
| AveragePolicyStd[1]  | 0.21519    |
| AveragePolicyStd[2]  | 0.15435    |
| AveragePolicyStd[3]  | 0.20298    |
| AveragePolicyStd[4]  | 0.17445    |
| AveragePolicyStd[5]  | 0.24769    |
| AverageReturn        | 1456.6     |
| MinReturn            | 226.89     |
| MaxReturn            | 1706.5     |
| StdReturn            | 388.97     |
| AverageEpisodeLength | 893.61     |
| MinEpisodeLength     | 163        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.8      |
| TotalNEpisodes       | 22036      |
| TotalNSamples        | 5.1507e+06 |
| ExplainedVariance    | 0.14762    |
-------------------------------------
[2018-01-21 15:13:05.551437 UTC] Saving snapshot
[2018-01-21 15:13:05.551650 UTC] Starting iteration 1030
[2018-01-21 15:13:05.551807 UTC] Start collecting samples
[2018-01-21 15:13:10.027479 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:10.156036 UTC] Performing policy update
[2018-01-21 15:13:10.156856 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:10.273471 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:11.646978 UTC] Performing line search
[2018-01-21 15:13:11.858782 UTC] Updating baseline
[2018-01-21 15:13:13.989071 UTC] Computing logging information
-------------------------------------
| Iteration            | 1030       |
| ExpectedImprovement  | 0.019507   |
| ActualImprovement    | 0.018499   |
| ImprovementRatio     | 0.9483     |
| MeanKL               | 0.0077629  |
| Entropy              | -1.1468    |
| Perplexity           | 0.31766    |
| AveragePolicyStd     | 0.20223    |
| AveragePolicyStd[0]  | 0.21837    |
| AveragePolicyStd[1]  | 0.2148     |
| AveragePolicyStd[2]  | 0.1543     |
| AveragePolicyStd[3]  | 0.20293    |
| AveragePolicyStd[4]  | 0.17513    |
| AveragePolicyStd[5]  | 0.24787    |
| AverageReturn        | 1443.6     |
| MinReturn            | 63.79      |
| MaxReturn            | 1706.5     |
| StdReturn            | 413.77     |
| AverageEpisodeLength | 884.32     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 241.78     |
| TotalNEpisodes       | 22042      |
| TotalNSamples        | 5.1558e+06 |
| ExplainedVariance    | 0.17115    |
-------------------------------------
[2018-01-21 15:13:14.722701 UTC] Saving snapshot
[2018-01-21 15:13:14.732514 UTC] Starting iteration 1031
[2018-01-21 15:13:14.732754 UTC] Start collecting samples
[2018-01-21 15:13:19.340116 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:19.477328 UTC] Performing policy update
[2018-01-21 15:13:19.478471 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:19.595770 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:20.955673 UTC] Performing line search
[2018-01-21 15:13:21.145284 UTC] Updating baseline
[2018-01-21 15:13:23.442602 UTC] Computing logging information
-------------------------------------
| Iteration            | 1031       |
| ExpectedImprovement  | 0.017617   |
| ActualImprovement    | 0.01671    |
| ImprovementRatio     | 0.94854    |
| MeanKL               | 0.006924   |
| Entropy              | -1.141     |
| Perplexity           | 0.31949    |
| AveragePolicyStd     | 0.20244    |
| AveragePolicyStd[0]  | 0.21892    |
| AveragePolicyStd[1]  | 0.21517    |
| AveragePolicyStd[2]  | 0.15436    |
| AveragePolicyStd[3]  | 0.20283    |
| AveragePolicyStd[4]  | 0.17518    |
| AveragePolicyStd[5]  | 0.24819    |
| AverageReturn        | 1451.8     |
| MinReturn            | 63.79      |
| MaxReturn            | 1706.5     |
| StdReturn            | 408.82     |
| AverageEpisodeLength | 889.1      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.28     |
| TotalNEpisodes       | 22046      |
| TotalNSamples        | 5.1598e+06 |
| ExplainedVariance    | -0.023441  |
-------------------------------------
[2018-01-21 15:13:24.181012 UTC] Saving snapshot
[2018-01-21 15:13:24.181284 UTC] Starting iteration 1032
[2018-01-21 15:13:24.181476 UTC] Start collecting samples
[2018-01-21 15:13:28.800299 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:28.946027 UTC] Performing policy update
[2018-01-21 15:13:28.946845 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:29.067149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:30.455564 UTC] Performing line search
[2018-01-21 15:13:30.646051 UTC] Updating baseline
[2018-01-21 15:13:32.536478 UTC] Computing logging information
-------------------------------------
| Iteration            | 1032       |
| ExpectedImprovement  | 0.016825   |
| ActualImprovement    | 0.016109   |
| ImprovementRatio     | 0.95743    |
| MeanKL               | 0.0085561  |
| Entropy              | -1.1392    |
| Perplexity           | 0.32008    |
| AveragePolicyStd     | 0.20248    |
| AveragePolicyStd[0]  | 0.21864    |
| AveragePolicyStd[1]  | 0.21453    |
| AveragePolicyStd[2]  | 0.15461    |
| AveragePolicyStd[3]  | 0.20293    |
| AveragePolicyStd[4]  | 0.17568    |
| AveragePolicyStd[5]  | 0.24848    |
| AverageReturn        | 1454       |
| MinReturn            | 63.79      |
| MaxReturn            | 1706.5     |
| StdReturn            | 409.81     |
| AverageEpisodeLength | 888.76     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.15     |
| TotalNEpisodes       | 22052      |
| TotalNSamples        | 5.1657e+06 |
| ExplainedVariance    | 0.072205   |
-------------------------------------
[2018-01-21 15:13:33.254917 UTC] Saving snapshot
[2018-01-21 15:13:33.255116 UTC] Starting iteration 1033
[2018-01-21 15:13:33.255292 UTC] Start collecting samples
[2018-01-21 15:13:37.887475 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:38.040836 UTC] Performing policy update
[2018-01-21 15:13:38.041462 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:38.189223 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:39.623292 UTC] Performing line search
[2018-01-21 15:13:39.815574 UTC] Updating baseline
[2018-01-21 15:13:41.621217 UTC] Computing logging information
-------------------------------------
| Iteration            | 1033       |
| ExpectedImprovement  | 0.017097   |
| ActualImprovement    | 0.016148   |
| ImprovementRatio     | 0.9445     |
| MeanKL               | 0.0080732  |
| Entropy              | -1.1233    |
| Perplexity           | 0.3252     |
| AveragePolicyStd     | 0.20302    |
| AveragePolicyStd[0]  | 0.21954    |
| AveragePolicyStd[1]  | 0.21539    |
| AveragePolicyStd[2]  | 0.15501    |
| AveragePolicyStd[3]  | 0.20287    |
| AveragePolicyStd[4]  | 0.17617    |
| AveragePolicyStd[5]  | 0.24917    |
| AverageReturn        | 1454.1     |
| MinReturn            | 63.79      |
| MaxReturn            | 1695.9     |
| StdReturn            | 409.84     |
| AverageEpisodeLength | 888.7      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 239.12     |
| TotalNEpisodes       | 22056      |
| TotalNSamples        | 5.1697e+06 |
| ExplainedVariance    | 0.14496    |
-------------------------------------
[2018-01-21 15:13:42.391777 UTC] Saving snapshot
[2018-01-21 15:13:42.392061 UTC] Starting iteration 1034
[2018-01-21 15:13:42.392250 UTC] Start collecting samples
[2018-01-21 15:13:46.999974 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:47.122196 UTC] Performing policy update
[2018-01-21 15:13:47.122842 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:47.239631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:48.625834 UTC] Performing line search
[2018-01-21 15:13:48.812144 UTC] Updating baseline
[2018-01-21 15:13:51.530192 UTC] Computing logging information
-------------------------------------
| Iteration            | 1034       |
| ExpectedImprovement  | 0.016619   |
| ActualImprovement    | 0.016424   |
| ImprovementRatio     | 0.98828    |
| MeanKL               | 0.0082773  |
| Entropy              | -1.1271    |
| Perplexity           | 0.32396    |
| AveragePolicyStd     | 0.20292    |
| AveragePolicyStd[0]  | 0.21959    |
| AveragePolicyStd[1]  | 0.21495    |
| AveragePolicyStd[2]  | 0.15455    |
| AveragePolicyStd[3]  | 0.203      |
| AveragePolicyStd[4]  | 0.17616    |
| AveragePolicyStd[5]  | 0.24926    |
| AverageReturn        | 1478.2     |
| MinReturn            | 63.79      |
| MaxReturn            | 1702.9     |
| StdReturn            | 396.26     |
| AverageEpisodeLength | 901.51     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 230.85     |
| TotalNEpisodes       | 22062      |
| TotalNSamples        | 5.1757e+06 |
| ExplainedVariance    | -0.057395  |
-------------------------------------
[2018-01-21 15:13:52.268296 UTC] Saving snapshot
[2018-01-21 15:13:52.268548 UTC] Starting iteration 1035
[2018-01-21 15:13:52.268725 UTC] Start collecting samples
[2018-01-21 15:13:57.033500 UTC] Computing input variables for policy optimization
[2018-01-21 15:13:57.170201 UTC] Performing policy update
[2018-01-21 15:13:57.170876 UTC] Computing gradient in Euclidean space
[2018-01-21 15:13:57.294420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:13:58.728604 UTC] Performing line search
[2018-01-21 15:13:58.922541 UTC] Updating baseline
[2018-01-21 15:14:01.091667 UTC] Computing logging information
-------------------------------------
| Iteration            | 1035       |
| ExpectedImprovement  | 0.017036   |
| ActualImprovement    | 0.016599   |
| ImprovementRatio     | 0.97436    |
| MeanKL               | 0.0078977  |
| Entropy              | -1.1333    |
| Perplexity           | 0.32196    |
| AveragePolicyStd     | 0.20268    |
| AveragePolicyStd[0]  | 0.2185     |
| AveragePolicyStd[1]  | 0.21515    |
| AveragePolicyStd[2]  | 0.15476    |
| AveragePolicyStd[3]  | 0.20263    |
| AveragePolicyStd[4]  | 0.17612    |
| AveragePolicyStd[5]  | 0.24891    |
| AverageReturn        | 1474.8     |
| MinReturn            | 63.79      |
| MaxReturn            | 1702.9     |
| StdReturn            | 395.87     |
| AverageEpisodeLength | 899.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 230.76     |
| TotalNEpisodes       | 22068      |
| TotalNSamples        | 5.1816e+06 |
| ExplainedVariance    | 0.080154   |
-------------------------------------
[2018-01-21 15:14:01.869963 UTC] Saving snapshot
[2018-01-21 15:14:01.870185 UTC] Starting iteration 1036
[2018-01-21 15:14:01.870357 UTC] Start collecting samples
[2018-01-21 15:14:06.187884 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:06.343229 UTC] Performing policy update
[2018-01-21 15:14:06.343890 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:06.464976 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:07.939560 UTC] Performing line search
[2018-01-21 15:14:08.131627 UTC] Updating baseline
[2018-01-21 15:14:10.061925 UTC] Computing logging information
------------------------------------
| Iteration            | 1036      |
| ExpectedImprovement  | 0.018102  |
| ActualImprovement    | 0.016603  |
| ImprovementRatio     | 0.9172    |
| MeanKL               | 0.0079017 |
| Entropy              | -1.1331   |
| Perplexity           | 0.32203   |
| AveragePolicyStd     | 0.2027    |
| AveragePolicyStd[0]  | 0.21835   |
| AveragePolicyStd[1]  | 0.21551   |
| AveragePolicyStd[2]  | 0.15449   |
| AveragePolicyStd[3]  | 0.20259   |
| AveragePolicyStd[4]  | 0.17625   |
| AveragePolicyStd[5]  | 0.24901   |
| AverageReturn        | 1449.6    |
| MinReturn            | 63.79     |
| MaxReturn            | 1702.9    |
| StdReturn            | 416.91    |
| AverageEpisodeLength | 884.29    |
| MinEpisodeLength     | 71        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 242.65    |
| TotalNEpisodes       | 22074     |
| TotalNSamples        | 5.186e+06 |
| ExplainedVariance    | 0.30807   |
------------------------------------
[2018-01-21 15:14:10.843786 UTC] Saving snapshot
[2018-01-21 15:14:10.844024 UTC] Starting iteration 1037
[2018-01-21 15:14:10.844179 UTC] Start collecting samples
[2018-01-21 15:14:15.356845 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:15.482098 UTC] Performing policy update
[2018-01-21 15:14:15.483214 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:15.600385 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:16.983905 UTC] Performing line search
[2018-01-21 15:14:17.190468 UTC] Updating baseline
[2018-01-21 15:14:18.105352 UTC] Computing logging information
------------------------------------
| Iteration            | 1037      |
| ExpectedImprovement  | 0.017007  |
| ActualImprovement    | 0.016093  |
| ImprovementRatio     | 0.94623   |
| MeanKL               | 0.0076262 |
| Entropy              | -1.1354   |
| Perplexity           | 0.32128   |
| AveragePolicyStd     | 0.20259   |
| AveragePolicyStd[0]  | 0.21887   |
| AveragePolicyStd[1]  | 0.21555   |
| AveragePolicyStd[2]  | 0.15448   |
| AveragePolicyStd[3]  | 0.2028    |
| AveragePolicyStd[4]  | 0.17615   |
| AveragePolicyStd[5]  | 0.2477    |
| AverageReturn        | 1473.9    |
| MinReturn            | 63.79     |
| MaxReturn            | 1702.9    |
| StdReturn            | 389.85    |
| AverageEpisodeLength | 898.59    |
| MinEpisodeLength     | 71        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 226.88    |
| TotalNEpisodes       | 22079     |
| TotalNSamples        | 5.191e+06 |
| ExplainedVariance    | -0.010946 |
------------------------------------
[2018-01-21 15:14:18.868491 UTC] Saving snapshot
[2018-01-21 15:14:18.868796 UTC] Starting iteration 1038
[2018-01-21 15:14:18.868996 UTC] Start collecting samples
[2018-01-21 15:14:23.254385 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:23.405131 UTC] Performing policy update
[2018-01-21 15:14:23.406113 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:23.536629 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:24.957238 UTC] Performing line search
[2018-01-21 15:14:25.147611 UTC] Updating baseline
[2018-01-21 15:14:27.173844 UTC] Computing logging information
------------------------------------
| Iteration            | 1038      |
| ExpectedImprovement  | 0.018211  |
| ActualImprovement    | 0.016914  |
| ImprovementRatio     | 0.92876   |
| MeanKL               | 0.0078514 |
| Entropy              | -1.134    |
| Perplexity           | 0.32175   |
| AveragePolicyStd     | 0.20262   |
| AveragePolicyStd[0]  | 0.2186    |
| AveragePolicyStd[1]  | 0.21591   |
| AveragePolicyStd[2]  | 0.15495   |
| AveragePolicyStd[3]  | 0.20254   |
| AveragePolicyStd[4]  | 0.176     |
| AveragePolicyStd[5]  | 0.24772   |
| AverageReturn        | 1483.3    |
| MinReturn            | 63.79     |
| MaxReturn            | 1702.9    |
| StdReturn            | 377.24    |
| AverageEpisodeLength | 905.04    |
| MinEpisodeLength     | 71        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 220.41    |
| TotalNEpisodes       | 22084     |
| TotalNSamples        | 5.196e+06 |
| ExplainedVariance    | 0.075295  |
------------------------------------
[2018-01-21 15:14:27.962687 UTC] Saving snapshot
[2018-01-21 15:14:27.962958 UTC] Starting iteration 1039
[2018-01-21 15:14:27.963178 UTC] Start collecting samples
[2018-01-21 15:14:32.656074 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:32.794697 UTC] Performing policy update
[2018-01-21 15:14:32.795376 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:32.928032 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:34.323252 UTC] Performing line search
[2018-01-21 15:14:34.515123 UTC] Updating baseline
[2018-01-21 15:14:36.689739 UTC] Computing logging information
------------------------------------
| Iteration            | 1039      |
| ExpectedImprovement  | 0.017513  |
| ActualImprovement    | 0.01697   |
| ImprovementRatio     | 0.96904   |
| MeanKL               | 0.0083051 |
| Entropy              | -1.1432   |
| Perplexity           | 0.3188    |
| AveragePolicyStd     | 0.2023    |
| AveragePolicyStd[0]  | 0.218     |
| AveragePolicyStd[1]  | 0.21586   |
| AveragePolicyStd[2]  | 0.15444   |
| AveragePolicyStd[3]  | 0.20236   |
| AveragePolicyStd[4]  | 0.17619   |
| AveragePolicyStd[5]  | 0.24696   |
| AverageReturn        | 1492.9    |
| MinReturn            | 63.79     |
| MaxReturn            | 1702.9    |
| StdReturn            | 366.14    |
| AverageEpisodeLength | 911.17    |
| MinEpisodeLength     | 71        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 213.7     |
| TotalNEpisodes       | 22090     |
| TotalNSamples        | 5.202e+06 |
| ExplainedVariance    | 0.10251   |
------------------------------------
[2018-01-21 15:14:37.495335 UTC] Saving snapshot
[2018-01-21 15:14:37.495580 UTC] Starting iteration 1040
[2018-01-21 15:14:37.495789 UTC] Start collecting samples
[2018-01-21 15:14:42.113579 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:42.243978 UTC] Performing policy update
[2018-01-21 15:14:42.245051 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:42.367715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:43.760852 UTC] Performing line search
[2018-01-21 15:14:43.959022 UTC] Updating baseline
[2018-01-21 15:14:46.066754 UTC] Computing logging information
-------------------------------------
| Iteration            | 1040       |
| ExpectedImprovement  | 0.017969   |
| ActualImprovement    | 0.017203   |
| ImprovementRatio     | 0.95736    |
| MeanKL               | 0.0078035  |
| Entropy              | -1.1424    |
| Perplexity           | 0.31906    |
| AveragePolicyStd     | 0.20235    |
| AveragePolicyStd[0]  | 0.21832    |
| AveragePolicyStd[1]  | 0.21559    |
| AveragePolicyStd[2]  | 0.15447    |
| AveragePolicyStd[3]  | 0.20281    |
| AveragePolicyStd[4]  | 0.17566    |
| AveragePolicyStd[5]  | 0.24724    |
| AverageReturn        | 1489.2     |
| MinReturn            | 63.79      |
| MaxReturn            | 1702.9     |
| StdReturn            | 367.18     |
| AverageEpisodeLength | 908.62     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.15     |
| TotalNEpisodes       | 22095      |
| TotalNSamples        | 5.2067e+06 |
| ExplainedVariance    | 0.097569   |
-------------------------------------
[2018-01-21 15:14:46.787493 UTC] Saving snapshot
[2018-01-21 15:14:46.797045 UTC] Starting iteration 1041
[2018-01-21 15:14:46.797259 UTC] Start collecting samples
[2018-01-21 15:14:51.396584 UTC] Computing input variables for policy optimization
[2018-01-21 15:14:51.524227 UTC] Performing policy update
[2018-01-21 15:14:51.525296 UTC] Computing gradient in Euclidean space
[2018-01-21 15:14:51.638786 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:14:53.037529 UTC] Performing line search
[2018-01-21 15:14:53.227187 UTC] Updating baseline
[2018-01-21 15:14:55.427538 UTC] Computing logging information
-------------------------------------
| Iteration            | 1041       |
| ExpectedImprovement  | 0.017745   |
| ActualImprovement    | 0.016856   |
| ImprovementRatio     | 0.94988    |
| MeanKL               | 0.0084848  |
| Entropy              | -1.1496    |
| Perplexity           | 0.31675    |
| AveragePolicyStd     | 0.20208    |
| AveragePolicyStd[0]  | 0.21799    |
| AveragePolicyStd[1]  | 0.21499    |
| AveragePolicyStd[2]  | 0.15433    |
| AveragePolicyStd[3]  | 0.20272    |
| AveragePolicyStd[4]  | 0.17577    |
| AveragePolicyStd[5]  | 0.24668    |
| AverageReturn        | 1489.4     |
| MinReturn            | 63.79      |
| MaxReturn            | 1702.9     |
| StdReturn            | 367.29     |
| AverageEpisodeLength | 908.62     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.15     |
| TotalNEpisodes       | 22097      |
| TotalNSamples        | 5.2087e+06 |
| ExplainedVariance    | -0.041288  |
-------------------------------------
[2018-01-21 15:14:56.246589 UTC] Saving snapshot
[2018-01-21 15:14:56.246847 UTC] Starting iteration 1042
[2018-01-21 15:14:56.247001 UTC] Start collecting samples
[2018-01-21 15:15:00.766877 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:00.914261 UTC] Performing policy update
[2018-01-21 15:15:00.915468 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:01.048622 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:02.428992 UTC] Performing line search
[2018-01-21 15:15:02.620185 UTC] Updating baseline
[2018-01-21 15:15:07.021409 UTC] Computing logging information
--------------------------------------
| Iteration            | 1042        |
| ExpectedImprovement  | 0.020044    |
| ActualImprovement    | 0.018176    |
| ImprovementRatio     | 0.90684     |
| MeanKL               | 0.0070562   |
| Entropy              | -1.1485     |
| Perplexity           | 0.31711     |
| AveragePolicyStd     | 0.2021      |
| AveragePolicyStd[0]  | 0.21729     |
| AveragePolicyStd[1]  | 0.21531     |
| AveragePolicyStd[2]  | 0.1543      |
| AveragePolicyStd[3]  | 0.20288     |
| AveragePolicyStd[4]  | 0.17623     |
| AveragePolicyStd[5]  | 0.24662     |
| AverageReturn        | 1511.8      |
| MinReturn            | 63.79       |
| MaxReturn            | 1711        |
| StdReturn            | 350.7       |
| AverageEpisodeLength | 921.54      |
| MinEpisodeLength     | 71          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 204.35      |
| TotalNEpisodes       | 22104       |
| TotalNSamples        | 5.2157e+06  |
| ExplainedVariance    | -6.8635e-05 |
--------------------------------------
[2018-01-21 15:15:07.759696 UTC] Saving snapshot
[2018-01-21 15:15:07.759904 UTC] Starting iteration 1043
[2018-01-21 15:15:07.760074 UTC] Start collecting samples
[2018-01-21 15:15:12.159402 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:12.296400 UTC] Performing policy update
[2018-01-21 15:15:12.297190 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:12.424315 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:13.868596 UTC] Performing line search
[2018-01-21 15:15:14.072670 UTC] Updating baseline
[2018-01-21 15:15:16.061078 UTC] Computing logging information
-------------------------------------
| Iteration            | 1043       |
| ExpectedImprovement  | 0.018259   |
| ActualImprovement    | 0.017191   |
| ImprovementRatio     | 0.94148    |
| MeanKL               | 0.0077558  |
| Entropy              | -1.1476    |
| Perplexity           | 0.31738    |
| AveragePolicyStd     | 0.20215    |
| AveragePolicyStd[0]  | 0.21763    |
| AveragePolicyStd[1]  | 0.21582    |
| AveragePolicyStd[2]  | 0.15454    |
| AveragePolicyStd[3]  | 0.20278    |
| AveragePolicyStd[4]  | 0.17543    |
| AveragePolicyStd[5]  | 0.2467     |
| AverageReturn        | 1512.6     |
| MinReturn            | 63.79      |
| MaxReturn            | 1711       |
| StdReturn            | 351        |
| AverageEpisodeLength | 921.54     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.35     |
| TotalNEpisodes       | 22108      |
| TotalNSamples        | 5.2197e+06 |
| ExplainedVariance    | 0.091501   |
-------------------------------------
[2018-01-21 15:15:16.891057 UTC] Saving snapshot
[2018-01-21 15:15:16.891346 UTC] Starting iteration 1044
[2018-01-21 15:15:16.891569 UTC] Start collecting samples
[2018-01-21 15:15:21.408417 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:21.527042 UTC] Performing policy update
[2018-01-21 15:15:21.527641 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:21.646959 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:23.024902 UTC] Performing line search
[2018-01-21 15:15:23.215472 UTC] Updating baseline
[2018-01-21 15:15:24.975269 UTC] Computing logging information
-------------------------------------
| Iteration            | 1044       |
| ExpectedImprovement  | 0.016292   |
| ActualImprovement    | 0.015846   |
| ImprovementRatio     | 0.97266    |
| MeanKL               | 0.0078599  |
| Entropy              | -1.1519    |
| Perplexity           | 0.31605    |
| AveragePolicyStd     | 0.20203    |
| AveragePolicyStd[0]  | 0.21775    |
| AveragePolicyStd[1]  | 0.21465    |
| AveragePolicyStd[2]  | 0.1541     |
| AveragePolicyStd[3]  | 0.20272    |
| AveragePolicyStd[4]  | 0.17574    |
| AveragePolicyStd[5]  | 0.24722    |
| AverageReturn        | 1517.1     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 351.87     |
| AverageEpisodeLength | 923.86     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.32     |
| TotalNEpisodes       | 22114      |
| TotalNSamples        | 5.2246e+06 |
| ExplainedVariance    | 0.31868    |
-------------------------------------
[2018-01-21 15:15:25.744761 UTC] Saving snapshot
[2018-01-21 15:15:25.745098 UTC] Starting iteration 1045
[2018-01-21 15:15:25.745328 UTC] Start collecting samples
[2018-01-21 15:15:30.474031 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:30.599399 UTC] Performing policy update
[2018-01-21 15:15:30.600529 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:30.728107 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:32.120554 UTC] Performing line search
[2018-01-21 15:15:32.311099 UTC] Updating baseline
[2018-01-21 15:15:34.300078 UTC] Computing logging information
-------------------------------------
| Iteration            | 1045       |
| ExpectedImprovement  | 0.018893   |
| ActualImprovement    | 0.018666   |
| ImprovementRatio     | 0.988      |
| MeanKL               | 0.0075334  |
| Entropy              | -1.1522    |
| Perplexity           | 0.31594    |
| AveragePolicyStd     | 0.20198    |
| AveragePolicyStd[0]  | 0.21723    |
| AveragePolicyStd[1]  | 0.21443    |
| AveragePolicyStd[2]  | 0.15442    |
| AveragePolicyStd[3]  | 0.20292    |
| AveragePolicyStd[4]  | 0.17593    |
| AveragePolicyStd[5]  | 0.24694    |
| AverageReturn        | 1531.6     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 338.31     |
| AverageEpisodeLength | 932.24     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.55     |
| TotalNEpisodes       | 22119      |
| TotalNSamples        | 5.2296e+06 |
| ExplainedVariance    | -0.028681  |
-------------------------------------
[2018-01-21 15:15:35.126301 UTC] Saving snapshot
[2018-01-21 15:15:35.126618 UTC] Starting iteration 1046
[2018-01-21 15:15:35.126805 UTC] Start collecting samples
[2018-01-21 15:15:39.767155 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:39.891382 UTC] Performing policy update
[2018-01-21 15:15:39.892007 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:40.027891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:41.483012 UTC] Performing line search
[2018-01-21 15:15:41.683947 UTC] Updating baseline
[2018-01-21 15:15:43.964379 UTC] Computing logging information
-------------------------------------
| Iteration            | 1046       |
| ExpectedImprovement  | 0.017996   |
| ActualImprovement    | 0.017164   |
| ImprovementRatio     | 0.95379    |
| MeanKL               | 0.0078366  |
| Entropy              | -1.1529    |
| Perplexity           | 0.31573    |
| AveragePolicyStd     | 0.20197    |
| AveragePolicyStd[0]  | 0.21782    |
| AveragePolicyStd[1]  | 0.21425    |
| AveragePolicyStd[2]  | 0.15445    |
| AveragePolicyStd[3]  | 0.20251    |
| AveragePolicyStd[4]  | 0.17568    |
| AveragePolicyStd[5]  | 0.24712    |
| AverageReturn        | 1540.4     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 323.28     |
| AverageEpisodeLength | 941.07     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.47     |
| TotalNEpisodes       | 22126      |
| TotalNSamples        | 5.2359e+06 |
| ExplainedVariance    | 0.30645    |
-------------------------------------
[2018-01-21 15:15:44.802053 UTC] Saving snapshot
[2018-01-21 15:15:44.802336 UTC] Starting iteration 1047
[2018-01-21 15:15:44.802552 UTC] Start collecting samples
[2018-01-21 15:15:49.562400 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:49.691296 UTC] Performing policy update
[2018-01-21 15:15:49.692422 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:49.816274 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:15:51.305251 UTC] Performing line search
[2018-01-21 15:15:51.511515 UTC] Updating baseline
[2018-01-21 15:15:53.443678 UTC] Computing logging information
------------------------------------
| Iteration            | 1047      |
| ExpectedImprovement  | 0.019472  |
| ActualImprovement    | 0.018495  |
| ImprovementRatio     | 0.94986   |
| MeanKL               | 0.0075867 |
| Entropy              | -1.1569   |
| Perplexity           | 0.31446   |
| AveragePolicyStd     | 0.20184   |
| AveragePolicyStd[0]  | 0.21795   |
| AveragePolicyStd[1]  | 0.21452   |
| AveragePolicyStd[2]  | 0.15405   |
| AveragePolicyStd[3]  | 0.20207   |
| AveragePolicyStd[4]  | 0.17594   |
| AveragePolicyStd[5]  | 0.24649   |
| AverageReturn        | 1530.8    |
| MinReturn            | 57.252    |
| MaxReturn            | 1711      |
| StdReturn            | 326.87    |
| AverageEpisodeLength | 938.21    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.99    |
| TotalNEpisodes       | 22133     |
| TotalNSamples        | 5.242e+06 |
| ExplainedVariance    | 0.29171   |
------------------------------------
[2018-01-21 15:15:54.219912 UTC] Saving snapshot
[2018-01-21 15:15:54.220125 UTC] Starting iteration 1048
[2018-01-21 15:15:54.220307 UTC] Start collecting samples
[2018-01-21 15:15:58.688777 UTC] Computing input variables for policy optimization
[2018-01-21 15:15:58.826758 UTC] Performing policy update
[2018-01-21 15:15:58.827305 UTC] Computing gradient in Euclidean space
[2018-01-21 15:15:58.947918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:00.487192 UTC] Performing line search
[2018-01-21 15:16:00.701416 UTC] Updating baseline
[2018-01-21 15:16:02.496780 UTC] Computing logging information
------------------------------------
| Iteration            | 1048      |
| ExpectedImprovement  | 0.017754  |
| ActualImprovement    | 0.017072  |
| ImprovementRatio     | 0.96162   |
| MeanKL               | 0.0082129 |
| Entropy              | -1.1681   |
| Perplexity           | 0.31097   |
| AveragePolicyStd     | 0.20146   |
| AveragePolicyStd[0]  | 0.21738   |
| AveragePolicyStd[1]  | 0.21401   |
| AveragePolicyStd[2]  | 0.15349   |
| AveragePolicyStd[3]  | 0.20204   |
| AveragePolicyStd[4]  | 0.17595   |
| AveragePolicyStd[5]  | 0.24588   |
| AverageReturn        | 1538.5    |
| MinReturn            | 57.252    |
| MaxReturn            | 1711      |
| StdReturn            | 324.4     |
| AverageEpisodeLength | 942.89    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.16    |
| TotalNEpisodes       | 22136     |
| TotalNSamples        | 5.245e+06 |
| ExplainedVariance    | -0.056827 |
------------------------------------
[2018-01-21 15:16:03.307513 UTC] Saving snapshot
[2018-01-21 15:16:03.307778 UTC] Starting iteration 1049
[2018-01-21 15:16:03.307960 UTC] Start collecting samples
[2018-01-21 15:16:07.745014 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:07.873396 UTC] Performing policy update
[2018-01-21 15:16:07.874016 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:08.005081 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:09.426739 UTC] Performing line search
[2018-01-21 15:16:09.617306 UTC] Updating baseline
[2018-01-21 15:16:11.738198 UTC] Computing logging information
-------------------------------------
| Iteration            | 1049       |
| ExpectedImprovement  | 0.015009   |
| ActualImprovement    | 0.014167   |
| ImprovementRatio     | 0.94392    |
| MeanKL               | 0.0081357  |
| Entropy              | -1.1677    |
| Perplexity           | 0.31108    |
| AveragePolicyStd     | 0.20148    |
| AveragePolicyStd[0]  | 0.21771    |
| AveragePolicyStd[1]  | 0.21452    |
| AveragePolicyStd[2]  | 0.1537     |
| AveragePolicyStd[3]  | 0.20153    |
| AveragePolicyStd[4]  | 0.17559    |
| AveragePolicyStd[5]  | 0.24581    |
| AverageReturn        | 1549.6     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 289.6      |
| AverageEpisodeLength | 952.18     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.18     |
| TotalNEpisodes       | 22139      |
| TotalNSamples        | 5.248e+06  |
| ExplainedVariance    | -0.0014591 |
-------------------------------------
[2018-01-21 15:16:12.626296 UTC] Saving snapshot
[2018-01-21 15:16:12.626815 UTC] Starting iteration 1050
[2018-01-21 15:16:12.627200 UTC] Start collecting samples
[2018-01-21 15:16:17.382995 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:17.525363 UTC] Performing policy update
[2018-01-21 15:16:17.525846 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:17.639082 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:19.078330 UTC] Performing line search
[2018-01-21 15:16:19.277582 UTC] Updating baseline
[2018-01-21 15:16:22.082300 UTC] Computing logging information
-------------------------------------
| Iteration            | 1050       |
| ExpectedImprovement  | 0.018099   |
| ActualImprovement    | 0.017566   |
| ImprovementRatio     | 0.97052    |
| MeanKL               | 0.0077092  |
| Entropy              | -1.1689    |
| Perplexity           | 0.31072    |
| AveragePolicyStd     | 0.20145    |
| AveragePolicyStd[0]  | 0.2185     |
| AveragePolicyStd[1]  | 0.21362    |
| AveragePolicyStd[2]  | 0.15306    |
| AveragePolicyStd[3]  | 0.20169    |
| AveragePolicyStd[4]  | 0.17618    |
| AveragePolicyStd[5]  | 0.24567    |
| AverageReturn        | 1548.2     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 288.99     |
| AverageEpisodeLength | 952.18     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.18     |
| TotalNEpisodes       | 22148      |
| TotalNSamples        | 5.257e+06  |
| ExplainedVariance    | 0.00087523 |
-------------------------------------
[2018-01-21 15:16:22.830586 UTC] Saving snapshot
[2018-01-21 15:16:22.840017 UTC] Starting iteration 1051
[2018-01-21 15:16:22.840227 UTC] Start collecting samples
[2018-01-21 15:16:27.251559 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:27.385738 UTC] Performing policy update
[2018-01-21 15:16:27.386463 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:27.504043 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:28.911706 UTC] Performing line search
[2018-01-21 15:16:29.103124 UTC] Updating baseline
[2018-01-21 15:16:31.197036 UTC] Computing logging information
-------------------------------------
| Iteration            | 1051       |
| ExpectedImprovement  | 0.020226   |
| ActualImprovement    | 0.019064   |
| ImprovementRatio     | 0.94256    |
| MeanKL               | 0.0068966  |
| Entropy              | -1.1739    |
| Perplexity           | 0.30916    |
| AveragePolicyStd     | 0.20128    |
| AveragePolicyStd[0]  | 0.21809    |
| AveragePolicyStd[1]  | 0.21409    |
| AveragePolicyStd[2]  | 0.15304    |
| AveragePolicyStd[3]  | 0.201      |
| AveragePolicyStd[4]  | 0.17601    |
| AveragePolicyStd[5]  | 0.24547    |
| AverageReturn        | 1544.5     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 288.66     |
| AverageEpisodeLength | 951.48     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.27     |
| TotalNEpisodes       | 22151      |
| TotalNSamples        | 5.2599e+06 |
| ExplainedVariance    | 0.11156    |
-------------------------------------
[2018-01-21 15:16:31.973129 UTC] Saving snapshot
[2018-01-21 15:16:31.973379 UTC] Starting iteration 1052
[2018-01-21 15:16:31.973563 UTC] Start collecting samples
[2018-01-21 15:16:36.591154 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:36.734469 UTC] Performing policy update
[2018-01-21 15:16:36.735420 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:36.850880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:38.291966 UTC] Performing line search
[2018-01-21 15:16:38.481287 UTC] Updating baseline
[2018-01-21 15:16:41.367784 UTC] Computing logging information
-------------------------------------
| Iteration            | 1052       |
| ExpectedImprovement  | 0.019158   |
| ActualImprovement    | 0.018238   |
| ImprovementRatio     | 0.952      |
| MeanKL               | 0.0072857  |
| Entropy              | -1.1747    |
| Perplexity           | 0.30892    |
| AveragePolicyStd     | 0.20124    |
| AveragePolicyStd[0]  | 0.21785    |
| AveragePolicyStd[1]  | 0.21434    |
| AveragePolicyStd[2]  | 0.15303    |
| AveragePolicyStd[3]  | 0.20135    |
| AveragePolicyStd[4]  | 0.17597    |
| AveragePolicyStd[5]  | 0.24491    |
| AverageReturn        | 1531.2     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 299.85     |
| AverageEpisodeLength | 946.07     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.57     |
| TotalNEpisodes       | 22156      |
| TotalNSamples        | 5.2643e+06 |
| ExplainedVariance    | 0.069383   |
-------------------------------------
[2018-01-21 15:16:42.138106 UTC] Saving snapshot
[2018-01-21 15:16:42.138342 UTC] Starting iteration 1053
[2018-01-21 15:16:42.138517 UTC] Start collecting samples
[2018-01-21 15:16:46.613184 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:46.746737 UTC] Performing policy update
[2018-01-21 15:16:46.747509 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:46.863345 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:48.236522 UTC] Performing line search
[2018-01-21 15:16:48.424928 UTC] Updating baseline
[2018-01-21 15:16:50.797026 UTC] Computing logging information
-------------------------------------
| Iteration            | 1053       |
| ExpectedImprovement  | 0.018201   |
| ActualImprovement    | 0.017677   |
| ImprovementRatio     | 0.97118    |
| MeanKL               | 0.007976   |
| Entropy              | -1.1716    |
| Perplexity           | 0.30986    |
| AveragePolicyStd     | 0.20132    |
| AveragePolicyStd[0]  | 0.21798    |
| AveragePolicyStd[1]  | 0.2147     |
| AveragePolicyStd[2]  | 0.15313    |
| AveragePolicyStd[3]  | 0.20115    |
| AveragePolicyStd[4]  | 0.17641    |
| AveragePolicyStd[5]  | 0.24457    |
| AverageReturn        | 1533.6     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 299.3      |
| AverageEpisodeLength | 947.8      |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.23     |
| TotalNEpisodes       | 22164      |
| TotalNSamples        | 5.2723e+06 |
| ExplainedVariance    | -0.076771  |
-------------------------------------
[2018-01-21 15:16:51.598097 UTC] Saving snapshot
[2018-01-21 15:16:51.598349 UTC] Starting iteration 1054
[2018-01-21 15:16:51.598547 UTC] Start collecting samples
[2018-01-21 15:16:56.097464 UTC] Computing input variables for policy optimization
[2018-01-21 15:16:56.229722 UTC] Performing policy update
[2018-01-21 15:16:56.230865 UTC] Computing gradient in Euclidean space
[2018-01-21 15:16:56.364963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:16:57.765634 UTC] Performing line search
[2018-01-21 15:16:57.957547 UTC] Updating baseline
[2018-01-21 15:17:00.424212 UTC] Computing logging information
-------------------------------------
| Iteration            | 1054       |
| ExpectedImprovement  | 0.019026   |
| ActualImprovement    | 0.017706   |
| ImprovementRatio     | 0.93058    |
| MeanKL               | 0.0077636  |
| Entropy              | -1.1711    |
| Perplexity           | 0.31004    |
| AveragePolicyStd     | 0.20132    |
| AveragePolicyStd[0]  | 0.21706    |
| AveragePolicyStd[1]  | 0.21562    |
| AveragePolicyStd[2]  | 0.1532     |
| AveragePolicyStd[3]  | 0.20124    |
| AveragePolicyStd[4]  | 0.17654    |
| AveragePolicyStd[5]  | 0.24429    |
| AverageReturn        | 1522       |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 316.42     |
| AverageEpisodeLength | 941.05     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.07     |
| TotalNEpisodes       | 22168      |
| TotalNSamples        | 5.2757e+06 |
| ExplainedVariance    | 0.098187   |
-------------------------------------
[2018-01-21 15:17:01.176984 UTC] Saving snapshot
[2018-01-21 15:17:01.177249 UTC] Starting iteration 1055
[2018-01-21 15:17:01.177432 UTC] Start collecting samples
[2018-01-21 15:17:05.865137 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:05.985136 UTC] Performing policy update
[2018-01-21 15:17:05.985749 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:06.110245 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:07.539695 UTC] Performing line search
[2018-01-21 15:17:07.745198 UTC] Updating baseline
[2018-01-21 15:17:10.078489 UTC] Computing logging information
-------------------------------------
| Iteration            | 1055       |
| ExpectedImprovement  | 0.018345   |
| ActualImprovement    | 0.018052   |
| ImprovementRatio     | 0.98398    |
| MeanKL               | 0.0073813  |
| Entropy              | -1.1792    |
| Perplexity           | 0.30751    |
| AveragePolicyStd     | 0.20106    |
| AveragePolicyStd[0]  | 0.21743    |
| AveragePolicyStd[1]  | 0.21568    |
| AveragePolicyStd[2]  | 0.15278    |
| AveragePolicyStd[3]  | 0.20132    |
| AveragePolicyStd[4]  | 0.17595    |
| AveragePolicyStd[5]  | 0.2432     |
| AverageReturn        | 1525.6     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 315.8      |
| AverageEpisodeLength | 943.94     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.75     |
| TotalNEpisodes       | 22175      |
| TotalNSamples        | 5.2814e+06 |
| ExplainedVariance    | 0.16365    |
-------------------------------------
[2018-01-21 15:17:10.842852 UTC] Saving snapshot
[2018-01-21 15:17:10.843104 UTC] Starting iteration 1056
[2018-01-21 15:17:10.843254 UTC] Start collecting samples
[2018-01-21 15:17:15.510712 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:15.635308 UTC] Performing policy update
[2018-01-21 15:17:15.636377 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:15.762487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:17.178825 UTC] Performing line search
[2018-01-21 15:17:17.367018 UTC] Updating baseline
[2018-01-21 15:17:19.720637 UTC] Computing logging information
------------------------------------
| Iteration            | 1056      |
| ExpectedImprovement  | 0.018694  |
| ActualImprovement    | 0.017204  |
| ImprovementRatio     | 0.92026   |
| MeanKL               | 0.0075802 |
| Entropy              | -1.1897   |
| Perplexity           | 0.30431   |
| AveragePolicyStd     | 0.20071   |
| AveragePolicyStd[0]  | 0.21686   |
| AveragePolicyStd[1]  | 0.21533   |
| AveragePolicyStd[2]  | 0.15262   |
| AveragePolicyStd[3]  | 0.201     |
| AveragePolicyStd[4]  | 0.17552   |
| AveragePolicyStd[5]  | 0.24292   |
| AverageReturn        | 1488.1    |
| MinReturn            | 57.252    |
| MaxReturn            | 1711      |
| StdReturn            | 378.46    |
| AverageEpisodeLength | 920.16    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 219.78    |
| TotalNEpisodes       | 22183     |
| TotalNSamples        | 5.287e+06 |
| ExplainedVariance    | 0.17018   |
------------------------------------
[2018-01-21 15:17:20.552378 UTC] Saving snapshot
[2018-01-21 15:17:20.552607 UTC] Starting iteration 1057
[2018-01-21 15:17:20.552754 UTC] Start collecting samples
[2018-01-21 15:17:25.184161 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:25.302620 UTC] Performing policy update
[2018-01-21 15:17:25.303254 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:25.424935 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:26.847258 UTC] Performing line search
[2018-01-21 15:17:27.044724 UTC] Updating baseline
[2018-01-21 15:17:29.187547 UTC] Computing logging information
------------------------------------
| Iteration            | 1057      |
| ExpectedImprovement  | 0.020051  |
| ActualImprovement    | 0.018322  |
| ImprovementRatio     | 0.9138    |
| MeanKL               | 0.0075114 |
| Entropy              | -1.1956   |
| Perplexity           | 0.30254   |
| AveragePolicyStd     | 0.20047   |
| AveragePolicyStd[0]  | 0.21654   |
| AveragePolicyStd[1]  | 0.2147    |
| AveragePolicyStd[2]  | 0.15281   |
| AveragePolicyStd[3]  | 0.20091   |
| AveragePolicyStd[4]  | 0.1757    |
| AveragePolicyStd[5]  | 0.24214   |
| AverageReturn        | 1487.1    |
| MinReturn            | 57.252    |
| MaxReturn            | 1711      |
| StdReturn            | 378.1     |
| AverageEpisodeLength | 920.16    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 219.78    |
| TotalNEpisodes       | 22187     |
| TotalNSamples        | 5.291e+06 |
| ExplainedVariance    | 0.055016  |
------------------------------------
[2018-01-21 15:17:30.024566 UTC] Saving snapshot
[2018-01-21 15:17:30.024862 UTC] Starting iteration 1058
[2018-01-21 15:17:30.025099 UTC] Start collecting samples
[2018-01-21 15:17:34.431435 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:34.573760 UTC] Performing policy update
[2018-01-21 15:17:34.574420 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:34.695844 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:36.118360 UTC] Performing line search
[2018-01-21 15:17:36.312854 UTC] Updating baseline
[2018-01-21 15:17:40.010773 UTC] Computing logging information
-------------------------------------
| Iteration            | 1058       |
| ExpectedImprovement  | 0.01729    |
| ActualImprovement    | 0.016186   |
| ImprovementRatio     | 0.93613    |
| MeanKL               | 0.0083108  |
| Entropy              | -1.2006    |
| Perplexity           | 0.30101    |
| AveragePolicyStd     | 0.20029    |
| AveragePolicyStd[0]  | 0.2161     |
| AveragePolicyStd[1]  | 0.21469    |
| AveragePolicyStd[2]  | 0.1528     |
| AveragePolicyStd[3]  | 0.20093    |
| AveragePolicyStd[4]  | 0.17542    |
| AveragePolicyStd[5]  | 0.24179    |
| AverageReturn        | 1476.7     |
| MinReturn            | 57.252     |
| MaxReturn            | 1711       |
| StdReturn            | 387.83     |
| AverageEpisodeLength | 914.19     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.17     |
| TotalNEpisodes       | 22191      |
| TotalNSamples        | 5.2944e+06 |
| ExplainedVariance    | 0.053733   |
-------------------------------------
[2018-01-21 15:17:40.732853 UTC] Saving snapshot
[2018-01-21 15:17:40.733099 UTC] Starting iteration 1059
[2018-01-21 15:17:40.733273 UTC] Start collecting samples
[2018-01-21 15:17:45.347842 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:45.468259 UTC] Performing policy update
[2018-01-21 15:17:45.469076 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:45.587234 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:47.030747 UTC] Performing line search
[2018-01-21 15:17:47.222874 UTC] Updating baseline
[2018-01-21 15:17:50.039335 UTC] Computing logging information
-------------------------------------
| Iteration            | 1059       |
| ExpectedImprovement  | 0.016607   |
| ActualImprovement    | 0.015953   |
| ImprovementRatio     | 0.96061    |
| MeanKL               | 0.0078785  |
| Entropy              | -1.2026    |
| Perplexity           | 0.30041    |
| AveragePolicyStd     | 0.20024    |
| AveragePolicyStd[0]  | 0.21606    |
| AveragePolicyStd[1]  | 0.21394    |
| AveragePolicyStd[2]  | 0.1528     |
| AveragePolicyStd[3]  | 0.20111    |
| AveragePolicyStd[4]  | 0.17511    |
| AveragePolicyStd[5]  | 0.24239    |
| AverageReturn        | 1471.6     |
| MinReturn            | 57.252     |
| MaxReturn            | 1718       |
| StdReturn            | 397.47     |
| AverageEpisodeLength | 910.39     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.09     |
| TotalNEpisodes       | 22198      |
| TotalNSamples        | 5.3008e+06 |
| ExplainedVariance    | 0.10424    |
-------------------------------------
[2018-01-21 15:17:50.810780 UTC] Saving snapshot
[2018-01-21 15:17:50.811115 UTC] Starting iteration 1060
[2018-01-21 15:17:50.811349 UTC] Start collecting samples
[2018-01-21 15:17:55.176228 UTC] Computing input variables for policy optimization
[2018-01-21 15:17:55.312243 UTC] Performing policy update
[2018-01-21 15:17:55.312859 UTC] Computing gradient in Euclidean space
[2018-01-21 15:17:55.436428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:17:56.856813 UTC] Performing line search
[2018-01-21 15:17:57.050662 UTC] Updating baseline
[2018-01-21 15:17:59.214193 UTC] Computing logging information
-------------------------------------
| Iteration            | 1060       |
| ExpectedImprovement  | 0.017101   |
| ActualImprovement    | 0.015827   |
| ImprovementRatio     | 0.92548    |
| MeanKL               | 0.0071016  |
| Entropy              | -1.1968    |
| Perplexity           | 0.30216    |
| AveragePolicyStd     | 0.20041    |
| AveragePolicyStd[0]  | 0.21635    |
| AveragePolicyStd[1]  | 0.21416    |
| AveragePolicyStd[2]  | 0.15317    |
| AveragePolicyStd[3]  | 0.20118    |
| AveragePolicyStd[4]  | 0.17537    |
| AveragePolicyStd[5]  | 0.24222    |
| AverageReturn        | 1466.3     |
| MinReturn            | 57.252     |
| MaxReturn            | 1718       |
| StdReturn            | 397.83     |
| AverageEpisodeLength | 907.44     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.81     |
| TotalNEpisodes       | 22204      |
| TotalNSamples        | 5.3065e+06 |
| ExplainedVariance    | 0.20495    |
-------------------------------------
[2018-01-21 15:17:59.939400 UTC] Saving snapshot
[2018-01-21 15:17:59.949007 UTC] Starting iteration 1061
[2018-01-21 15:17:59.949235 UTC] Start collecting samples
[2018-01-21 15:18:04.575124 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:04.696280 UTC] Performing policy update
[2018-01-21 15:18:04.696991 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:04.827576 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:06.359371 UTC] Performing line search
[2018-01-21 15:18:06.567004 UTC] Updating baseline
[2018-01-21 15:18:08.518047 UTC] Computing logging information
-------------------------------------
| Iteration            | 1061       |
| ExpectedImprovement  | 0.018402   |
| ActualImprovement    | 0.017895   |
| ImprovementRatio     | 0.97249    |
| MeanKL               | 0.0077824  |
| Entropy              | -1.2008    |
| Perplexity           | 0.30094    |
| AveragePolicyStd     | 0.20028    |
| AveragePolicyStd[0]  | 0.21612    |
| AveragePolicyStd[1]  | 0.21428    |
| AveragePolicyStd[2]  | 0.15304    |
| AveragePolicyStd[3]  | 0.20067    |
| AveragePolicyStd[4]  | 0.17538    |
| AveragePolicyStd[5]  | 0.24216    |
| AverageReturn        | 1456.5     |
| MinReturn            | 57.252     |
| MaxReturn            | 1718       |
| StdReturn            | 406.34     |
| AverageEpisodeLength | 901.65     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.59     |
| TotalNEpisodes       | 22207      |
| TotalNSamples        | 5.3089e+06 |
| ExplainedVariance    | 0.23342    |
-------------------------------------
[2018-01-21 15:18:09.349877 UTC] Saving snapshot
[2018-01-21 15:18:09.350096 UTC] Starting iteration 1062
[2018-01-21 15:18:09.350256 UTC] Start collecting samples
[2018-01-21 15:18:13.945290 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:14.088469 UTC] Performing policy update
[2018-01-21 15:18:14.089609 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:14.212414 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:15.633631 UTC] Performing line search
[2018-01-21 15:18:15.824299 UTC] Updating baseline
[2018-01-21 15:18:17.492958 UTC] Computing logging information
-------------------------------------
| Iteration            | 1062       |
| ExpectedImprovement  | 0.01891    |
| ActualImprovement    | 0.018194   |
| ImprovementRatio     | 0.96212    |
| MeanKL               | 0.0084198  |
| Entropy              | -1.1989    |
| Perplexity           | 0.30153    |
| AveragePolicyStd     | 0.20036    |
| AveragePolicyStd[0]  | 0.21591    |
| AveragePolicyStd[1]  | 0.21423    |
| AveragePolicyStd[2]  | 0.15312    |
| AveragePolicyStd[3]  | 0.20078    |
| AveragePolicyStd[4]  | 0.1753     |
| AveragePolicyStd[5]  | 0.2428     |
| AverageReturn        | 1477       |
| MinReturn            | 273.98     |
| MaxReturn            | 1718       |
| StdReturn            | 382.51     |
| AverageEpisodeLength | 912.05     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.14     |
| TotalNEpisodes       | 22215      |
| TotalNSamples        | 5.3168e+06 |
| ExplainedVariance    | 0.081776   |
-------------------------------------
[2018-01-21 15:18:18.289289 UTC] Saving snapshot
[2018-01-21 15:18:18.289688 UTC] Starting iteration 1063
[2018-01-21 15:18:18.289955 UTC] Start collecting samples
[2018-01-21 15:18:23.028649 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:23.156427 UTC] Performing policy update
[2018-01-21 15:18:23.157033 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:23.275571 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:24.667087 UTC] Performing line search
[2018-01-21 15:18:24.852758 UTC] Updating baseline
[2018-01-21 15:18:26.817581 UTC] Computing logging information
-------------------------------------
| Iteration            | 1063       |
| ExpectedImprovement  | 0.016998   |
| ActualImprovement    | 0.015889   |
| ImprovementRatio     | 0.93473    |
| MeanKL               | 0.0079339  |
| Entropy              | -1.1973    |
| Perplexity           | 0.30201    |
| AveragePolicyStd     | 0.20041    |
| AveragePolicyStd[0]  | 0.21605    |
| AveragePolicyStd[1]  | 0.21455    |
| AveragePolicyStd[2]  | 0.15324    |
| AveragePolicyStd[3]  | 0.20064    |
| AveragePolicyStd[4]  | 0.1752     |
| AveragePolicyStd[5]  | 0.24277    |
| AverageReturn        | 1455.6     |
| MinReturn            | 273.98     |
| MaxReturn            | 1718       |
| StdReturn            | 407.3      |
| AverageEpisodeLength | 897.66     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 237.02     |
| TotalNEpisodes       | 22222      |
| TotalNSamples        | 5.3224e+06 |
| ExplainedVariance    | 0.2837     |
-------------------------------------
[2018-01-21 15:18:27.563416 UTC] Saving snapshot
[2018-01-21 15:18:27.563651 UTC] Starting iteration 1064
[2018-01-21 15:18:27.563806 UTC] Start collecting samples
[2018-01-21 15:18:32.017877 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:32.150461 UTC] Performing policy update
[2018-01-21 15:18:32.151094 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:32.266343 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:33.688867 UTC] Performing line search
[2018-01-21 15:18:33.905907 UTC] Updating baseline
[2018-01-21 15:18:35.983712 UTC] Computing logging information
-------------------------------------
| Iteration            | 1064       |
| ExpectedImprovement  | 0.019412   |
| ActualImprovement    | 0.018131   |
| ImprovementRatio     | 0.93401    |
| MeanKL               | 0.00743    |
| Entropy              | -1.2039    |
| Perplexity           | 0.30004    |
| AveragePolicyStd     | 0.20018    |
| AveragePolicyStd[0]  | 0.21558    |
| AveragePolicyStd[1]  | 0.21438    |
| AveragePolicyStd[2]  | 0.1528     |
| AveragePolicyStd[3]  | 0.20088    |
| AveragePolicyStd[4]  | 0.17533    |
| AveragePolicyStd[5]  | 0.24213    |
| AverageReturn        | 1471.3     |
| MinReturn            | 273.98     |
| MaxReturn            | 1718       |
| StdReturn            | 390.38     |
| AverageEpisodeLength | 903.03     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.83     |
| TotalNEpisodes       | 22226      |
| TotalNSamples        | 5.3262e+06 |
| ExplainedVariance    | 0.15699    |
-------------------------------------
[2018-01-21 15:18:36.712640 UTC] Saving snapshot
[2018-01-21 15:18:36.712852 UTC] Starting iteration 1065
[2018-01-21 15:18:36.712989 UTC] Start collecting samples
[2018-01-21 15:18:41.139165 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:41.259333 UTC] Performing policy update
[2018-01-21 15:18:41.260066 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:41.378883 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:42.804310 UTC] Performing line search
[2018-01-21 15:18:42.997781 UTC] Updating baseline
[2018-01-21 15:18:44.857256 UTC] Computing logging information
-------------------------------------
| Iteration            | 1065       |
| ExpectedImprovement  | 0.018432   |
| ActualImprovement    | 0.018046   |
| ImprovementRatio     | 0.97906    |
| MeanKL               | 0.0076087  |
| Entropy              | -1.2054    |
| Perplexity           | 0.29957    |
| AveragePolicyStd     | 0.20015    |
| AveragePolicyStd[0]  | 0.21503    |
| AveragePolicyStd[1]  | 0.21469    |
| AveragePolicyStd[2]  | 0.15268    |
| AveragePolicyStd[3]  | 0.20031    |
| AveragePolicyStd[4]  | 0.17551    |
| AveragePolicyStd[5]  | 0.24265    |
| AverageReturn        | 1480       |
| MinReturn            | 273.98     |
| MaxReturn            | 1718       |
| StdReturn            | 387.41     |
| AverageEpisodeLength | 905.18     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.52     |
| TotalNEpisodes       | 22230      |
| TotalNSamples        | 5.3300e+06 |
| ExplainedVariance    | 0.1192     |
-------------------------------------
[2018-01-21 15:18:45.602515 UTC] Saving snapshot
[2018-01-21 15:18:45.602759 UTC] Starting iteration 1066
[2018-01-21 15:18:45.602904 UTC] Start collecting samples
[2018-01-21 15:18:50.141145 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:50.259660 UTC] Performing policy update
[2018-01-21 15:18:50.260630 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:50.375667 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:18:51.776826 UTC] Performing line search
[2018-01-21 15:18:51.964359 UTC] Updating baseline
[2018-01-21 15:18:53.714207 UTC] Computing logging information
-------------------------------------
| Iteration            | 1066       |
| ExpectedImprovement  | 0.018432   |
| ActualImprovement    | 0.018054   |
| ImprovementRatio     | 0.97948    |
| MeanKL               | 0.0077061  |
| Entropy              | -1.1989    |
| Perplexity           | 0.30152    |
| AveragePolicyStd     | 0.20039    |
| AveragePolicyStd[0]  | 0.21562    |
| AveragePolicyStd[1]  | 0.21491    |
| AveragePolicyStd[2]  | 0.15239    |
| AveragePolicyStd[3]  | 0.20063    |
| AveragePolicyStd[4]  | 0.17579    |
| AveragePolicyStd[5]  | 0.24301    |
| AverageReturn        | 1490.8     |
| MinReturn            | 273.98     |
| MaxReturn            | 1741.7     |
| StdReturn            | 380.84     |
| AverageEpisodeLength | 910.53     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.34     |
| TotalNEpisodes       | 22234      |
| TotalNSamples        | 5.3340e+06 |
| ExplainedVariance    | 0.060636   |
-------------------------------------
[2018-01-21 15:18:54.549440 UTC] Saving snapshot
[2018-01-21 15:18:54.549688 UTC] Starting iteration 1067
[2018-01-21 15:18:54.549844 UTC] Start collecting samples
[2018-01-21 15:18:59.063103 UTC] Computing input variables for policy optimization
[2018-01-21 15:18:59.185493 UTC] Performing policy update
[2018-01-21 15:18:59.186127 UTC] Computing gradient in Euclidean space
[2018-01-21 15:18:59.306912 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:00.784486 UTC] Performing line search
[2018-01-21 15:19:00.993193 UTC] Updating baseline
[2018-01-21 15:19:03.071480 UTC] Computing logging information
-------------------------------------
| Iteration            | 1067       |
| ExpectedImprovement  | 0.018263   |
| ActualImprovement    | 0.017423   |
| ImprovementRatio     | 0.95398    |
| MeanKL               | 0.0078275  |
| Entropy              | -1.1902    |
| Perplexity           | 0.30415    |
| AveragePolicyStd     | 0.20069    |
| AveragePolicyStd[0]  | 0.21583    |
| AveragePolicyStd[1]  | 0.21521    |
| AveragePolicyStd[2]  | 0.15274    |
| AveragePolicyStd[3]  | 0.20081    |
| AveragePolicyStd[4]  | 0.17571    |
| AveragePolicyStd[5]  | 0.24387    |
| AverageReturn        | 1488.8     |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 390.59     |
| AverageEpisodeLength | 904.72     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.46     |
| TotalNEpisodes       | 22242      |
| TotalNSamples        | 5.3415e+06 |
| ExplainedVariance    | 0.10706    |
-------------------------------------
[2018-01-21 15:19:03.908894 UTC] Saving snapshot
[2018-01-21 15:19:03.909242 UTC] Starting iteration 1068
[2018-01-21 15:19:03.909462 UTC] Start collecting samples
[2018-01-21 15:19:08.495557 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:08.626413 UTC] Performing policy update
[2018-01-21 15:19:08.627019 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:08.743498 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:10.232860 UTC] Performing line search
[2018-01-21 15:19:10.427402 UTC] Updating baseline
[2018-01-21 15:19:12.478195 UTC] Computing logging information
-------------------------------------
| Iteration            | 1068       |
| ExpectedImprovement  | 0.018583   |
| ActualImprovement    | 0.016823   |
| ImprovementRatio     | 0.90527    |
| MeanKL               | 0.0077353  |
| Entropy              | -1.1936    |
| Perplexity           | 0.30314    |
| AveragePolicyStd     | 0.20057    |
| AveragePolicyStd[0]  | 0.21535    |
| AveragePolicyStd[1]  | 0.21516    |
| AveragePolicyStd[2]  | 0.1532     |
| AveragePolicyStd[3]  | 0.2004     |
| AveragePolicyStd[4]  | 0.17521    |
| AveragePolicyStd[5]  | 0.24411    |
| AverageReturn        | 1490.5     |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 391.32     |
| AverageEpisodeLength | 904.72     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.46     |
| TotalNEpisodes       | 22246      |
| TotalNSamples        | 5.3455e+06 |
| ExplainedVariance    | 0.024749   |
-------------------------------------
[2018-01-21 15:19:13.226553 UTC] Saving snapshot
[2018-01-21 15:19:13.226936 UTC] Starting iteration 1069
[2018-01-21 15:19:13.227173 UTC] Start collecting samples
[2018-01-21 15:19:17.935449 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:18.068707 UTC] Performing policy update
[2018-01-21 15:19:18.069496 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:18.189558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:19.583227 UTC] Performing line search
[2018-01-21 15:19:19.770011 UTC] Updating baseline
[2018-01-21 15:19:21.646966 UTC] Computing logging information
-------------------------------------
| Iteration            | 1069       |
| ExpectedImprovement  | 0.018664   |
| ActualImprovement    | 0.017495   |
| ImprovementRatio     | 0.93738    |
| MeanKL               | 0.0073287  |
| Entropy              | -1.1982    |
| Perplexity           | 0.30173    |
| AveragePolicyStd     | 0.20036    |
| AveragePolicyStd[0]  | 0.21565    |
| AveragePolicyStd[1]  | 0.21532    |
| AveragePolicyStd[2]  | 0.15351    |
| AveragePolicyStd[3]  | 0.20031    |
| AveragePolicyStd[4]  | 0.175      |
| AveragePolicyStd[5]  | 0.24237    |
| AverageReturn        | 1496.1     |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 392.86     |
| AverageEpisodeLength | 905.76     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 227.65     |
| TotalNEpisodes       | 22250      |
| TotalNSamples        | 5.3495e+06 |
| ExplainedVariance    | -0.026018  |
-------------------------------------
[2018-01-21 15:19:22.390419 UTC] Saving snapshot
[2018-01-21 15:19:22.390653 UTC] Starting iteration 1070
[2018-01-21 15:19:22.390835 UTC] Start collecting samples
[2018-01-21 15:19:27.024922 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:27.150941 UTC] Performing policy update
[2018-01-21 15:19:27.151553 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:27.270425 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:28.672557 UTC] Performing line search
[2018-01-21 15:19:28.871601 UTC] Updating baseline
[2018-01-21 15:19:31.877240 UTC] Computing logging information
--------------------------------------
| Iteration            | 1070        |
| ExpectedImprovement  | 0.017087    |
| ActualImprovement    | 0.016154    |
| ImprovementRatio     | 0.94541     |
| MeanKL               | 0.0084927   |
| Entropy              | -1.205      |
| Perplexity           | 0.2997      |
| AveragePolicyStd     | 0.20017     |
| AveragePolicyStd[0]  | 0.21585     |
| AveragePolicyStd[1]  | 0.21549     |
| AveragePolicyStd[2]  | 0.15314     |
| AveragePolicyStd[3]  | 0.19938     |
| AveragePolicyStd[4]  | 0.17463     |
| AveragePolicyStd[5]  | 0.24255     |
| AverageReturn        | 1509.9      |
| MinReturn            | 273.98      |
| MaxReturn            | 1750        |
| StdReturn            | 385.73      |
| AverageEpisodeLength | 911.23      |
| MinEpisodeLength     | 190         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 223.24      |
| TotalNEpisodes       | 22257       |
| TotalNSamples        | 5.3565e+06  |
| ExplainedVariance    | -0.00052622 |
--------------------------------------
[2018-01-21 15:19:32.665769 UTC] Saving snapshot
[2018-01-21 15:19:32.677851 UTC] Starting iteration 1071
[2018-01-21 15:19:32.678173 UTC] Start collecting samples
[2018-01-21 15:19:37.240344 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:37.373826 UTC] Performing policy update
[2018-01-21 15:19:37.374459 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:37.499404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:38.953261 UTC] Performing line search
[2018-01-21 15:19:39.157893 UTC] Updating baseline
[2018-01-21 15:19:41.055060 UTC] Computing logging information
-------------------------------------
| Iteration            | 1071       |
| ExpectedImprovement  | 0.019567   |
| ActualImprovement    | 0.018968   |
| ImprovementRatio     | 0.9694     |
| MeanKL               | 0.0075474  |
| Entropy              | -1.208     |
| Perplexity           | 0.29879    |
| AveragePolicyStd     | 0.20007    |
| AveragePolicyStd[0]  | 0.2159     |
| AveragePolicyStd[1]  | 0.21546    |
| AveragePolicyStd[2]  | 0.15341    |
| AveragePolicyStd[3]  | 0.19905    |
| AveragePolicyStd[4]  | 0.17414    |
| AveragePolicyStd[5]  | 0.24245    |
| AverageReturn        | 1497.5     |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 402.59     |
| AverageEpisodeLength | 903.78     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.39     |
| TotalNEpisodes       | 22261      |
| TotalNSamples        | 5.3597e+06 |
| ExplainedVariance    | 0.15454    |
-------------------------------------
[2018-01-21 15:19:41.875056 UTC] Saving snapshot
[2018-01-21 15:19:41.875316 UTC] Starting iteration 1072
[2018-01-21 15:19:41.875492 UTC] Start collecting samples
[2018-01-21 15:19:46.490825 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:46.619615 UTC] Performing policy update
[2018-01-21 15:19:46.620292 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:46.750236 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:48.291668 UTC] Performing line search
[2018-01-21 15:19:48.485939 UTC] Updating baseline
[2018-01-21 15:19:50.350203 UTC] Computing logging information
-------------------------------------
| Iteration            | 1072       |
| ExpectedImprovement  | 0.018611   |
| ActualImprovement    | 0.017954   |
| ImprovementRatio     | 0.96472    |
| MeanKL               | 0.0079837  |
| Entropy              | -1.2121    |
| Perplexity           | 0.29758    |
| AveragePolicyStd     | 0.19996    |
| AveragePolicyStd[0]  | 0.21557    |
| AveragePolicyStd[1]  | 0.2155     |
| AveragePolicyStd[2]  | 0.15302    |
| AveragePolicyStd[3]  | 0.1989     |
| AveragePolicyStd[4]  | 0.174      |
| AveragePolicyStd[5]  | 0.2428     |
| AverageReturn        | 1510.1     |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 390.45     |
| AverageEpisodeLength | 910.53     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.18     |
| TotalNEpisodes       | 22267      |
| TotalNSamples        | 5.3657e+06 |
| ExplainedVariance    | 0.01789    |
-------------------------------------
[2018-01-21 15:19:51.120641 UTC] Saving snapshot
[2018-01-21 15:19:51.120915 UTC] Starting iteration 1073
[2018-01-21 15:19:51.121112 UTC] Start collecting samples
[2018-01-21 15:19:55.679975 UTC] Computing input variables for policy optimization
[2018-01-21 15:19:55.816172 UTC] Performing policy update
[2018-01-21 15:19:55.817264 UTC] Computing gradient in Euclidean space
[2018-01-21 15:19:55.940412 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:19:57.357008 UTC] Performing line search
[2018-01-21 15:19:57.551978 UTC] Updating baseline
[2018-01-21 15:19:59.676050 UTC] Computing logging information
--------------------------------------
| Iteration            | 1073        |
| ExpectedImprovement  | 0.019721    |
| ActualImprovement    | 0.018557    |
| ImprovementRatio     | 0.94095     |
| MeanKL               | 0.007587    |
| Entropy              | -1.2134     |
| Perplexity           | 0.2972      |
| AveragePolicyStd     | 0.19994     |
| AveragePolicyStd[0]  | 0.2157      |
| AveragePolicyStd[1]  | 0.21578     |
| AveragePolicyStd[2]  | 0.15293     |
| AveragePolicyStd[3]  | 0.19894     |
| AveragePolicyStd[4]  | 0.17345     |
| AveragePolicyStd[5]  | 0.24287     |
| AverageReturn        | 1534.5      |
| MinReturn            | 273.98      |
| MaxReturn            | 1750        |
| StdReturn            | 365.4       |
| AverageEpisodeLength | 923.13      |
| MinEpisodeLength     | 190         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 210.41      |
| TotalNEpisodes       | 22272       |
| TotalNSamples        | 5.3707e+06  |
| ExplainedVariance    | -3.8488e-05 |
--------------------------------------
[2018-01-21 15:20:00.457479 UTC] Saving snapshot
[2018-01-21 15:20:00.457974 UTC] Starting iteration 1074
[2018-01-21 15:20:00.458390 UTC] Start collecting samples
[2018-01-21 15:20:05.010343 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:05.129924 UTC] Performing policy update
[2018-01-21 15:20:05.130786 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:05.241614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:06.647227 UTC] Performing line search
[2018-01-21 15:20:06.840464 UTC] Updating baseline
[2018-01-21 15:20:09.446227 UTC] Computing logging information
-------------------------------------
| Iteration            | 1074       |
| ExpectedImprovement  | 0.016946   |
| ActualImprovement    | 0.016676   |
| ImprovementRatio     | 0.98405    |
| MeanKL               | 0.0081578  |
| Entropy              | -1.2157    |
| Perplexity           | 0.29651    |
| AveragePolicyStd     | 0.19985    |
| AveragePolicyStd[0]  | 0.21585    |
| AveragePolicyStd[1]  | 0.21549    |
| AveragePolicyStd[2]  | 0.15307    |
| AveragePolicyStd[3]  | 0.19851    |
| AveragePolicyStd[4]  | 0.17355    |
| AveragePolicyStd[5]  | 0.24263    |
| AverageReturn        | 1550       |
| MinReturn            | 273.98     |
| MaxReturn            | 1750       |
| StdReturn            | 346.38     |
| AverageEpisodeLength | 930.76     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.91     |
| TotalNEpisodes       | 22276      |
| TotalNSamples        | 5.3747e+06 |
| ExplainedVariance    | -0.008873  |
-------------------------------------
[2018-01-21 15:20:10.177848 UTC] Saving snapshot
[2018-01-21 15:20:10.178087 UTC] Starting iteration 1075
[2018-01-21 15:20:10.178242 UTC] Start collecting samples
[2018-01-21 15:20:14.977299 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:15.100208 UTC] Performing policy update
[2018-01-21 15:20:15.101412 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:15.222212 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:16.642999 UTC] Performing line search
[2018-01-21 15:20:16.838831 UTC] Updating baseline
[2018-01-21 15:20:19.125240 UTC] Computing logging information
-------------------------------------
| Iteration            | 1075       |
| ExpectedImprovement  | 0.018349   |
| ActualImprovement    | 0.017097   |
| ImprovementRatio     | 0.93178    |
| MeanKL               | 0.0078507  |
| Entropy              | -1.2085    |
| Perplexity           | 0.29866    |
| AveragePolicyStd     | 0.2001     |
| AveragePolicyStd[0]  | 0.21613    |
| AveragePolicyStd[1]  | 0.21575    |
| AveragePolicyStd[2]  | 0.15322    |
| AveragePolicyStd[3]  | 0.19866    |
| AveragePolicyStd[4]  | 0.17373    |
| AveragePolicyStd[5]  | 0.24311    |
| AverageReturn        | 1576.5     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 295.2      |
| AverageEpisodeLength | 946.91     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.81     |
| TotalNEpisodes       | 22281      |
| TotalNSamples        | 5.3797e+06 |
| ExplainedVariance    | 0.0062315  |
-------------------------------------
[2018-01-21 15:20:19.889630 UTC] Saving snapshot
[2018-01-21 15:20:19.889873 UTC] Starting iteration 1076
[2018-01-21 15:20:19.890027 UTC] Start collecting samples
[2018-01-21 15:20:24.503156 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:24.643010 UTC] Performing policy update
[2018-01-21 15:20:24.644001 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:24.764039 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:26.192716 UTC] Performing line search
[2018-01-21 15:20:26.388206 UTC] Updating baseline
[2018-01-21 15:20:28.298840 UTC] Computing logging information
-------------------------------------
| Iteration            | 1076       |
| ExpectedImprovement  | 0.018424   |
| ActualImprovement    | 0.017565   |
| ImprovementRatio     | 0.95337    |
| MeanKL               | 0.0080295  |
| Entropy              | -1.2123    |
| Perplexity           | 0.2975     |
| AveragePolicyStd     | 0.19993    |
| AveragePolicyStd[0]  | 0.21586    |
| AveragePolicyStd[1]  | 0.21543    |
| AveragePolicyStd[2]  | 0.1536     |
| AveragePolicyStd[3]  | 0.19839    |
| AveragePolicyStd[4]  | 0.17367    |
| AveragePolicyStd[5]  | 0.24263    |
| AverageReturn        | 1577.4     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 289.78     |
| AverageEpisodeLength | 946.37     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.38     |
| TotalNEpisodes       | 22289      |
| TotalNSamples        | 5.3870e+06 |
| ExplainedVariance    | 0.1655     |
-------------------------------------
[2018-01-21 15:20:29.031228 UTC] Saving snapshot
[2018-01-21 15:20:29.031464 UTC] Starting iteration 1077
[2018-01-21 15:20:29.031658 UTC] Start collecting samples
[2018-01-21 15:20:33.570258 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:33.691401 UTC] Performing policy update
[2018-01-21 15:20:33.692097 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:33.811783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:35.220966 UTC] Performing line search
[2018-01-21 15:20:35.406338 UTC] Updating baseline
[2018-01-21 15:20:37.676782 UTC] Computing logging information
-------------------------------------
| Iteration            | 1077       |
| ExpectedImprovement  | 0.017282   |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.95737    |
| MeanKL               | 0.0080967  |
| Entropy              | -1.2136    |
| Perplexity           | 0.29711    |
| AveragePolicyStd     | 0.19987    |
| AveragePolicyStd[0]  | 0.21527    |
| AveragePolicyStd[1]  | 0.21524    |
| AveragePolicyStd[2]  | 0.1536     |
| AveragePolicyStd[3]  | 0.19842    |
| AveragePolicyStd[4]  | 0.174      |
| AveragePolicyStd[5]  | 0.24269    |
| AverageReturn        | 1570.3     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 297.86     |
| AverageEpisodeLength | 941.82     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.06     |
| TotalNEpisodes       | 22293      |
| TotalNSamples        | 5.3906e+06 |
| ExplainedVariance    | 0.1444     |
-------------------------------------
[2018-01-21 15:20:38.428681 UTC] Saving snapshot
[2018-01-21 15:20:38.428920 UTC] Starting iteration 1078
[2018-01-21 15:20:38.429091 UTC] Start collecting samples
[2018-01-21 15:20:43.127807 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:43.257933 UTC] Performing policy update
[2018-01-21 15:20:43.258799 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:43.387163 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:44.824420 UTC] Performing line search
[2018-01-21 15:20:45.048010 UTC] Updating baseline
[2018-01-21 15:20:46.954199 UTC] Computing logging information
-------------------------------------
| Iteration            | 1078       |
| ExpectedImprovement  | 0.020295   |
| ActualImprovement    | 0.01967    |
| ImprovementRatio     | 0.96924    |
| MeanKL               | 0.0077316  |
| Entropy              | -1.2139    |
| Perplexity           | 0.29704    |
| AveragePolicyStd     | 0.19984    |
| AveragePolicyStd[0]  | 0.2147     |
| AveragePolicyStd[1]  | 0.2151     |
| AveragePolicyStd[2]  | 0.15389    |
| AveragePolicyStd[3]  | 0.19832    |
| AveragePolicyStd[4]  | 0.17415    |
| AveragePolicyStd[5]  | 0.2429     |
| AverageReturn        | 1581.5     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 281.36     |
| AverageEpisodeLength | 948.17     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.89     |
| TotalNEpisodes       | 22298      |
| TotalNSamples        | 5.3956e+06 |
| ExplainedVariance    | -0.02143   |
-------------------------------------
[2018-01-21 15:20:47.713842 UTC] Saving snapshot
[2018-01-21 15:20:47.714078 UTC] Starting iteration 1079
[2018-01-21 15:20:47.714224 UTC] Start collecting samples
[2018-01-21 15:20:52.268555 UTC] Computing input variables for policy optimization
[2018-01-21 15:20:52.388294 UTC] Performing policy update
[2018-01-21 15:20:52.388885 UTC] Computing gradient in Euclidean space
[2018-01-21 15:20:52.503910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:20:53.890758 UTC] Performing line search
[2018-01-21 15:20:54.079738 UTC] Updating baseline
[2018-01-21 15:20:56.198354 UTC] Computing logging information
--------------------------------------
| Iteration            | 1079        |
| ExpectedImprovement  | 0.017705    |
| ActualImprovement    | 0.016868    |
| ImprovementRatio     | 0.95274     |
| MeanKL               | 0.0078571   |
| Entropy              | -1.2183     |
| Perplexity           | 0.29573     |
| AveragePolicyStd     | 0.19969     |
| AveragePolicyStd[0]  | 0.21468     |
| AveragePolicyStd[1]  | 0.21482     |
| AveragePolicyStd[2]  | 0.15413     |
| AveragePolicyStd[3]  | 0.19761     |
| AveragePolicyStd[4]  | 0.17396     |
| AveragePolicyStd[5]  | 0.24291     |
| AverageReturn        | 1587.8      |
| MinReturn            | 349.37      |
| MaxReturn            | 1750        |
| StdReturn            | 278.74      |
| AverageEpisodeLength | 951.12      |
| MinEpisodeLength     | 255         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 157.08      |
| TotalNEpisodes       | 22303       |
| TotalNSamples        | 5.4006e+06  |
| ExplainedVariance    | -0.00034409 |
--------------------------------------
[2018-01-21 15:20:56.932572 UTC] Saving snapshot
[2018-01-21 15:20:56.932862 UTC] Starting iteration 1080
[2018-01-21 15:20:56.933039 UTC] Start collecting samples
[2018-01-21 15:21:01.543413 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:01.664439 UTC] Performing policy update
[2018-01-21 15:21:01.665026 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:01.781453 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:03.200856 UTC] Performing line search
[2018-01-21 15:21:03.390031 UTC] Updating baseline
[2018-01-21 15:21:05.730073 UTC] Computing logging information
-------------------------------------
| Iteration            | 1080       |
| ExpectedImprovement  | 0.016191   |
| ActualImprovement    | 0.015578   |
| ImprovementRatio     | 0.96213    |
| MeanKL               | 0.0083199  |
| Entropy              | -1.2174    |
| Perplexity           | 0.296      |
| AveragePolicyStd     | 0.1997     |
| AveragePolicyStd[0]  | 0.21486    |
| AveragePolicyStd[1]  | 0.21462    |
| AveragePolicyStd[2]  | 0.15412    |
| AveragePolicyStd[3]  | 0.1973     |
| AveragePolicyStd[4]  | 0.17453    |
| AveragePolicyStd[5]  | 0.24276    |
| AverageReturn        | 1597.8     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 261.44     |
| AverageEpisodeLength | 956.91     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.83     |
| TotalNEpisodes       | 22308      |
| TotalNSamples        | 5.4056e+06 |
| ExplainedVariance    | 0.0064708  |
-------------------------------------
[2018-01-21 15:21:06.567161 UTC] Saving snapshot
[2018-01-21 15:21:06.576508 UTC] Starting iteration 1081
[2018-01-21 15:21:06.576738 UTC] Start collecting samples
[2018-01-21 15:21:11.086821 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:11.206332 UTC] Performing policy update
[2018-01-21 15:21:11.206952 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:11.322446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:12.740761 UTC] Performing line search
[2018-01-21 15:21:12.951818 UTC] Updating baseline
[2018-01-21 15:21:15.971244 UTC] Computing logging information
-------------------------------------
| Iteration            | 1081       |
| ExpectedImprovement  | 0.018549   |
| ActualImprovement    | 0.017385   |
| ImprovementRatio     | 0.93723    |
| MeanKL               | 0.0071628  |
| Entropy              | -1.2183    |
| Perplexity           | 0.29575    |
| AveragePolicyStd     | 0.19967    |
| AveragePolicyStd[0]  | 0.21478    |
| AveragePolicyStd[1]  | 0.21436    |
| AveragePolicyStd[2]  | 0.15414    |
| AveragePolicyStd[3]  | 0.19767    |
| AveragePolicyStd[4]  | 0.17429    |
| AveragePolicyStd[5]  | 0.24278    |
| AverageReturn        | 1600.3     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 262.11     |
| AverageEpisodeLength | 957.5      |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.88     |
| TotalNEpisodes       | 22314      |
| TotalNSamples        | 5.4116e+06 |
| ExplainedVariance    | 0.032091   |
-------------------------------------
[2018-01-21 15:21:16.812629 UTC] Saving snapshot
[2018-01-21 15:21:16.812915 UTC] Starting iteration 1082
[2018-01-21 15:21:16.813097 UTC] Start collecting samples
[2018-01-21 15:21:21.328304 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:21.475995 UTC] Performing policy update
[2018-01-21 15:21:21.476629 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:21.611340 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:23.019638 UTC] Performing line search
[2018-01-21 15:21:23.209178 UTC] Updating baseline
[2018-01-21 15:21:24.939734 UTC] Computing logging information
-------------------------------------
| Iteration            | 1082       |
| ExpectedImprovement  | 0.01883    |
| ActualImprovement    | 0.018169   |
| ImprovementRatio     | 0.96488    |
| MeanKL               | 0.0071832  |
| Entropy              | -1.2227    |
| Perplexity           | 0.29443    |
| AveragePolicyStd     | 0.19949    |
| AveragePolicyStd[0]  | 0.21396    |
| AveragePolicyStd[1]  | 0.2141     |
| AveragePolicyStd[2]  | 0.15443    |
| AveragePolicyStd[3]  | 0.19774    |
| AveragePolicyStd[4]  | 0.17415    |
| AveragePolicyStd[5]  | 0.24259    |
| AverageReturn        | 1601       |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 266.94     |
| AverageEpisodeLength | 957.63     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.99     |
| TotalNEpisodes       | 22318      |
| TotalNSamples        | 5.4149e+06 |
| ExplainedVariance    | -0.06149   |
-------------------------------------
[2018-01-21 15:21:25.653405 UTC] Saving snapshot
[2018-01-21 15:21:25.653657 UTC] Starting iteration 1083
[2018-01-21 15:21:25.653845 UTC] Start collecting samples
[2018-01-21 15:21:30.429332 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:30.566658 UTC] Performing policy update
[2018-01-21 15:21:30.567277 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:30.676860 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:32.089964 UTC] Performing line search
[2018-01-21 15:21:32.279869 UTC] Updating baseline
[2018-01-21 15:21:34.444635 UTC] Computing logging information
-------------------------------------
| Iteration            | 1083       |
| ExpectedImprovement  | 0.016949   |
| ActualImprovement    | 0.016397   |
| ImprovementRatio     | 0.96743    |
| MeanKL               | 0.0080089  |
| Entropy              | -1.228     |
| Perplexity           | 0.29287    |
| AveragePolicyStd     | 0.1993     |
| AveragePolicyStd[0]  | 0.21328    |
| AveragePolicyStd[1]  | 0.21375    |
| AveragePolicyStd[2]  | 0.15433    |
| AveragePolicyStd[3]  | 0.1981     |
| AveragePolicyStd[4]  | 0.17413    |
| AveragePolicyStd[5]  | 0.24221    |
| AverageReturn        | 1608.3     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 248.25     |
| AverageEpisodeLength | 962.2      |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.51     |
| TotalNEpisodes       | 22326      |
| TotalNSamples        | 5.4224e+06 |
| ExplainedVariance    | 0.070908   |
-------------------------------------
[2018-01-21 15:21:35.187313 UTC] Saving snapshot
[2018-01-21 15:21:35.187813 UTC] Starting iteration 1084
[2018-01-21 15:21:35.188230 UTC] Start collecting samples
[2018-01-21 15:21:39.861330 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:39.991259 UTC] Performing policy update
[2018-01-21 15:21:39.992145 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:40.127320 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:41.582879 UTC] Performing line search
[2018-01-21 15:21:41.782066 UTC] Updating baseline
[2018-01-21 15:21:44.347150 UTC] Computing logging information
-------------------------------------
| Iteration            | 1084       |
| ExpectedImprovement  | 0.018434   |
| ActualImprovement    | 0.01785    |
| ImprovementRatio     | 0.96832    |
| MeanKL               | 0.0078528  |
| Entropy              | -1.2243    |
| Perplexity           | 0.29395    |
| AveragePolicyStd     | 0.19941    |
| AveragePolicyStd[0]  | 0.21272    |
| AveragePolicyStd[1]  | 0.214      |
| AveragePolicyStd[2]  | 0.15436    |
| AveragePolicyStd[3]  | 0.1985     |
| AveragePolicyStd[4]  | 0.1746     |
| AveragePolicyStd[5]  | 0.24226    |
| AverageReturn        | 1606.4     |
| MinReturn            | 349.37     |
| MaxReturn            | 1750       |
| StdReturn            | 252.13     |
| AverageEpisodeLength | 960.42     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.75     |
| TotalNEpisodes       | 22331      |
| TotalNSamples        | 5.4271e+06 |
| ExplainedVariance    | 0.10026    |
-------------------------------------
[2018-01-21 15:21:45.165024 UTC] Saving snapshot
[2018-01-21 15:21:45.165326 UTC] Starting iteration 1085
[2018-01-21 15:21:45.165513 UTC] Start collecting samples
[2018-01-21 15:21:49.656159 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:49.801092 UTC] Performing policy update
[2018-01-21 15:21:49.801715 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:49.941824 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:21:51.423726 UTC] Performing line search
[2018-01-21 15:21:51.611834 UTC] Updating baseline
[2018-01-21 15:21:54.004524 UTC] Computing logging information
-----------------------------------
| Iteration            | 1085     |
| ExpectedImprovement  | 0.018641 |
| ActualImprovement    | 0.017629 |
| ImprovementRatio     | 0.94575  |
| MeanKL               | 0.007642 |
| Entropy              | -1.2272  |
| Perplexity           | 0.2931   |
| AveragePolicyStd     | 0.19932  |
| AveragePolicyStd[0]  | 0.21231  |
| AveragePolicyStd[1]  | 0.21448  |
| AveragePolicyStd[2]  | 0.15429  |
| AveragePolicyStd[3]  | 0.19801  |
| AveragePolicyStd[4]  | 0.17444  |
| AveragePolicyStd[5]  | 0.24241  |
| AverageReturn        | 1586.4   |
| MinReturn            | 349.37   |
| MaxReturn            | 1750     |
| StdReturn            | 284.03   |
| AverageEpisodeLength | 949.43   |
| MinEpisodeLength     | 255      |
| MaxEpisodeLength     | 1000     |
| StdEpisodeLength     | 160.22   |
| TotalNEpisodes       | 22335    |
| TotalNSamples        | 5.43e+06 |
| ExplainedVariance    | 0.31313  |
-----------------------------------
[2018-01-21 15:21:54.736999 UTC] Saving snapshot
[2018-01-21 15:21:54.737239 UTC] Starting iteration 1086
[2018-01-21 15:21:54.737388 UTC] Start collecting samples
[2018-01-21 15:21:59.336266 UTC] Computing input variables for policy optimization
[2018-01-21 15:21:59.470000 UTC] Performing policy update
[2018-01-21 15:21:59.470671 UTC] Computing gradient in Euclidean space
[2018-01-21 15:21:59.606366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:01.018662 UTC] Performing line search
[2018-01-21 15:22:01.227126 UTC] Updating baseline
[2018-01-21 15:22:02.986935 UTC] Computing logging information
-------------------------------------
| Iteration            | 1086       |
| ExpectedImprovement  | 0.016554   |
| ActualImprovement    | 0.01581    |
| ImprovementRatio     | 0.95506    |
| MeanKL               | 0.0085335  |
| Entropy              | -1.2293    |
| Perplexity           | 0.2925     |
| AveragePolicyStd     | 0.19926    |
| AveragePolicyStd[0]  | 0.21211    |
| AveragePolicyStd[1]  | 0.21416    |
| AveragePolicyStd[2]  | 0.15415    |
| AveragePolicyStd[3]  | 0.1985     |
| AveragePolicyStd[4]  | 0.17423    |
| AveragePolicyStd[5]  | 0.24242    |
| AverageReturn        | 1580.9     |
| MinReturn            | 349.37     |
| MaxReturn            | 1752.6     |
| StdReturn            | 288.78     |
| AverageEpisodeLength | 945.86     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.48     |
| TotalNEpisodes       | 22344      |
| TotalNSamples        | 5.4381e+06 |
| ExplainedVariance    | 0.14434    |
-------------------------------------
[2018-01-21 15:22:03.825270 UTC] Saving snapshot
[2018-01-21 15:22:03.825512 UTC] Starting iteration 1087
[2018-01-21 15:22:03.825689 UTC] Start collecting samples
[2018-01-21 15:22:08.453282 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:08.581462 UTC] Performing policy update
[2018-01-21 15:22:08.582310 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:08.701393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:10.122089 UTC] Performing line search
[2018-01-21 15:22:10.310217 UTC] Updating baseline
[2018-01-21 15:22:12.630507 UTC] Computing logging information
-------------------------------------
| Iteration            | 1087       |
| ExpectedImprovement  | 0.019565   |
| ActualImprovement    | 0.018724   |
| ImprovementRatio     | 0.95702    |
| MeanKL               | 0.0072858  |
| Entropy              | -1.2313    |
| Perplexity           | 0.2919     |
| AveragePolicyStd     | 0.19919    |
| AveragePolicyStd[0]  | 0.21164    |
| AveragePolicyStd[1]  | 0.2145     |
| AveragePolicyStd[2]  | 0.154      |
| AveragePolicyStd[3]  | 0.19885    |
| AveragePolicyStd[4]  | 0.17417    |
| AveragePolicyStd[5]  | 0.24197    |
| AverageReturn        | 1566.6     |
| MinReturn            | 349.37     |
| MaxReturn            | 1752.6     |
| StdReturn            | 302.03     |
| AverageEpisodeLength | 937.66     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.4      |
| TotalNEpisodes       | 22347      |
| TotalNSamples        | 5.4402e+06 |
| ExplainedVariance    | 0.29197    |
-------------------------------------
[2018-01-21 15:22:13.377335 UTC] Saving snapshot
[2018-01-21 15:22:13.377578 UTC] Starting iteration 1088
[2018-01-21 15:22:13.377732 UTC] Start collecting samples
[2018-01-21 15:22:17.841283 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:17.975180 UTC] Performing policy update
[2018-01-21 15:22:17.975783 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:18.094206 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:19.487558 UTC] Performing line search
[2018-01-21 15:22:19.686527 UTC] Updating baseline
[2018-01-21 15:22:21.653162 UTC] Computing logging information
-------------------------------------
| Iteration            | 1088       |
| ExpectedImprovement  | 0.018237   |
| ActualImprovement    | 0.017495   |
| ImprovementRatio     | 0.95931    |
| MeanKL               | 0.0073819  |
| Entropy              | -1.2383    |
| Perplexity           | 0.28988    |
| AveragePolicyStd     | 0.19897    |
| AveragePolicyStd[0]  | 0.21157    |
| AveragePolicyStd[1]  | 0.21406    |
| AveragePolicyStd[2]  | 0.15382    |
| AveragePolicyStd[3]  | 0.1983     |
| AveragePolicyStd[4]  | 0.17391    |
| AveragePolicyStd[5]  | 0.2422     |
| AverageReturn        | 1555       |
| MinReturn            | 349.37     |
| MaxReturn            | 1752.6     |
| StdReturn            | 319.99     |
| AverageEpisodeLength | 930.99     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.58     |
| TotalNEpisodes       | 22352      |
| TotalNSamples        | 5.4446e+06 |
| ExplainedVariance    | 0.18652    |
-------------------------------------
[2018-01-21 15:22:22.392200 UTC] Saving snapshot
[2018-01-21 15:22:22.392500 UTC] Starting iteration 1089
[2018-01-21 15:22:22.392702 UTC] Start collecting samples
[2018-01-21 15:22:26.969676 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:27.093251 UTC] Performing policy update
[2018-01-21 15:22:27.093952 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:27.215982 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:28.622829 UTC] Performing line search
[2018-01-21 15:22:28.817279 UTC] Updating baseline
[2018-01-21 15:22:30.747717 UTC] Computing logging information
-------------------------------------
| Iteration            | 1089       |
| ExpectedImprovement  | 0.017539   |
| ActualImprovement    | 0.01702    |
| ImprovementRatio     | 0.97038    |
| MeanKL               | 0.0075156  |
| Entropy              | -1.2437    |
| Perplexity           | 0.28833    |
| AveragePolicyStd     | 0.19879    |
| AveragePolicyStd[0]  | 0.21142    |
| AveragePolicyStd[1]  | 0.21426    |
| AveragePolicyStd[2]  | 0.1539     |
| AveragePolicyStd[3]  | 0.19802    |
| AveragePolicyStd[4]  | 0.17351    |
| AveragePolicyStd[5]  | 0.24162    |
| AverageReturn        | 1567.4     |
| MinReturn            | 418.85     |
| MaxReturn            | 1752.6     |
| StdReturn            | 299.54     |
| AverageEpisodeLength | 938.44     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.43     |
| TotalNEpisodes       | 22361      |
| TotalNSamples        | 5.4536e+06 |
| ExplainedVariance    | -0.0038634 |
-------------------------------------
[2018-01-21 15:22:31.589556 UTC] Saving snapshot
[2018-01-21 15:22:31.589832 UTC] Starting iteration 1090
[2018-01-21 15:22:31.590024 UTC] Start collecting samples
[2018-01-21 15:22:35.958936 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:36.080380 UTC] Performing policy update
[2018-01-21 15:22:36.081418 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:36.199478 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:37.586308 UTC] Performing line search
[2018-01-21 15:22:37.773278 UTC] Updating baseline
[2018-01-21 15:22:39.527733 UTC] Computing logging information
-------------------------------------
| Iteration            | 1090       |
| ExpectedImprovement  | 0.017048   |
| ActualImprovement    | 0.015815   |
| ImprovementRatio     | 0.92769    |
| MeanKL               | 0.007431   |
| Entropy              | -1.2443    |
| Perplexity           | 0.28813    |
| AveragePolicyStd     | 0.19879    |
| AveragePolicyStd[0]  | 0.21093    |
| AveragePolicyStd[1]  | 0.21386    |
| AveragePolicyStd[2]  | 0.15365    |
| AveragePolicyStd[3]  | 0.19854    |
| AveragePolicyStd[4]  | 0.17342    |
| AveragePolicyStd[5]  | 0.24235    |
| AverageReturn        | 1548.6     |
| MinReturn            | 418.85     |
| MaxReturn            | 1758.6     |
| StdReturn            | 326.29     |
| AverageEpisodeLength | 926.48     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.74     |
| TotalNEpisodes       | 22365      |
| TotalNSamples        | 5.4564e+06 |
| ExplainedVariance    | 0.23445    |
-------------------------------------
[2018-01-21 15:22:40.286732 UTC] Saving snapshot
[2018-01-21 15:22:40.296473 UTC] Starting iteration 1091
[2018-01-21 15:22:40.296708 UTC] Start collecting samples
[2018-01-21 15:22:44.818884 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:44.962126 UTC] Performing policy update
[2018-01-21 15:22:44.963289 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:45.082537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:46.519141 UTC] Performing line search
[2018-01-21 15:22:46.715736 UTC] Updating baseline
[2018-01-21 15:22:48.705899 UTC] Computing logging information
-------------------------------------
| Iteration            | 1091       |
| ExpectedImprovement  | 0.018368   |
| ActualImprovement    | 0.018056   |
| ImprovementRatio     | 0.98299    |
| MeanKL               | 0.0076773  |
| Entropy              | -1.247     |
| Perplexity           | 0.28736    |
| AveragePolicyStd     | 0.19869    |
| AveragePolicyStd[0]  | 0.21119    |
| AveragePolicyStd[1]  | 0.21268    |
| AveragePolicyStd[2]  | 0.15388    |
| AveragePolicyStd[3]  | 0.19911    |
| AveragePolicyStd[4]  | 0.17297    |
| AveragePolicyStd[5]  | 0.2423     |
| AverageReturn        | 1550       |
| MinReturn            | 418.85     |
| MaxReturn            | 1758.6     |
| StdReturn            | 326.75     |
| AverageEpisodeLength | 926.48     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.74     |
| TotalNEpisodes       | 22370      |
| TotalNSamples        | 5.4614e+06 |
| ExplainedVariance    | -0.045461  |
-------------------------------------
[2018-01-21 15:22:49.516910 UTC] Saving snapshot
[2018-01-21 15:22:49.517149 UTC] Starting iteration 1092
[2018-01-21 15:22:49.517298 UTC] Start collecting samples
[2018-01-21 15:22:54.124487 UTC] Computing input variables for policy optimization
[2018-01-21 15:22:54.270002 UTC] Performing policy update
[2018-01-21 15:22:54.271127 UTC] Computing gradient in Euclidean space
[2018-01-21 15:22:54.403233 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:22:55.760183 UTC] Performing line search
[2018-01-21 15:22:55.963319 UTC] Updating baseline
[2018-01-21 15:22:58.243660 UTC] Computing logging information
--------------------------------------
| Iteration            | 1092        |
| ExpectedImprovement  | 0.01988     |
| ActualImprovement    | 0.018585    |
| ImprovementRatio     | 0.93486     |
| MeanKL               | 0.0078851   |
| Entropy              | -1.2515     |
| Perplexity           | 0.28606     |
| AveragePolicyStd     | 0.19853     |
| AveragePolicyStd[0]  | 0.21142     |
| AveragePolicyStd[1]  | 0.21226     |
| AveragePolicyStd[2]  | 0.15362     |
| AveragePolicyStd[3]  | 0.19893     |
| AveragePolicyStd[4]  | 0.17313     |
| AveragePolicyStd[5]  | 0.24184     |
| AverageReturn        | 1549.1      |
| MinReturn            | 418.85      |
| MaxReturn            | 1758.6      |
| StdReturn            | 326.44      |
| AverageEpisodeLength | 926.48      |
| MinEpisodeLength     | 288         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 183.74      |
| TotalNEpisodes       | 22376       |
| TotalNSamples        | 5.4674e+06  |
| ExplainedVariance    | -3.8025e-05 |
--------------------------------------
[2018-01-21 15:22:59.004437 UTC] Saving snapshot
[2018-01-21 15:22:59.004698 UTC] Starting iteration 1093
[2018-01-21 15:22:59.004877 UTC] Start collecting samples
[2018-01-21 15:23:03.558530 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:03.680029 UTC] Performing policy update
[2018-01-21 15:23:03.681116 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:03.803727 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:05.183171 UTC] Performing line search
[2018-01-21 15:23:05.370012 UTC] Updating baseline
[2018-01-21 15:23:07.689535 UTC] Computing logging information
------------------------------------
| Iteration            | 1093      |
| ExpectedImprovement  | 0.018859  |
| ActualImprovement    | 0.01733   |
| ImprovementRatio     | 0.91891   |
| MeanKL               | 0.0075626 |
| Entropy              | -1.2517   |
| Perplexity           | 0.28601   |
| AveragePolicyStd     | 0.19852   |
| AveragePolicyStd[0]  | 0.21133   |
| AveragePolicyStd[1]  | 0.21217   |
| AveragePolicyStd[2]  | 0.1536    |
| AveragePolicyStd[3]  | 0.19831   |
| AveragePolicyStd[4]  | 0.17356   |
| AveragePolicyStd[5]  | 0.24218   |
| AverageReturn        | 1544.5    |
| MinReturn            | 418.85    |
| MaxReturn            | 1758.6    |
| StdReturn            | 331.09    |
| AverageEpisodeLength | 923.01    |
| MinEpisodeLength     | 288       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 185.59    |
| TotalNEpisodes       | 22380     |
| TotalNSamples        | 5.471e+06 |
| ExplainedVariance    | 0.16508   |
------------------------------------
[2018-01-21 15:23:08.488302 UTC] Saving snapshot
[2018-01-21 15:23:08.488543 UTC] Starting iteration 1094
[2018-01-21 15:23:08.488724 UTC] Start collecting samples
[2018-01-21 15:23:13.113605 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:13.242551 UTC] Performing policy update
[2018-01-21 15:23:13.243970 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:13.366067 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:14.757106 UTC] Performing line search
[2018-01-21 15:23:14.959138 UTC] Updating baseline
[2018-01-21 15:23:16.995545 UTC] Computing logging information
-------------------------------------
| Iteration            | 1094       |
| ExpectedImprovement  | 0.018263   |
| ActualImprovement    | 0.01639    |
| ImprovementRatio     | 0.89746    |
| MeanKL               | 0.0078388  |
| Entropy              | -1.253     |
| Perplexity           | 0.28566    |
| AveragePolicyStd     | 0.19848    |
| AveragePolicyStd[0]  | 0.21137    |
| AveragePolicyStd[1]  | 0.21252    |
| AveragePolicyStd[2]  | 0.15336    |
| AveragePolicyStd[3]  | 0.19837    |
| AveragePolicyStd[4]  | 0.1736     |
| AveragePolicyStd[5]  | 0.24169    |
| AverageReturn        | 1544.4     |
| MinReturn            | 418.85     |
| MaxReturn            | 1761.3     |
| StdReturn            | 333.31     |
| AverageEpisodeLength | 923.04     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.53     |
| TotalNEpisodes       | 22385      |
| TotalNSamples        | 5.4756e+06 |
| ExplainedVariance    | 0.17156    |
-------------------------------------
[2018-01-21 15:23:17.730224 UTC] Saving snapshot
[2018-01-21 15:23:17.730457 UTC] Starting iteration 1095
[2018-01-21 15:23:17.730594 UTC] Start collecting samples
[2018-01-21 15:23:22.317177 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:22.481684 UTC] Performing policy update
[2018-01-21 15:23:22.482304 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:22.591550 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:23.967364 UTC] Performing line search
[2018-01-21 15:23:24.156684 UTC] Updating baseline
[2018-01-21 15:23:26.002235 UTC] Computing logging information
-------------------------------------
| Iteration            | 1095       |
| ExpectedImprovement  | 0.019479   |
| ActualImprovement    | 0.018444   |
| ImprovementRatio     | 0.94687    |
| MeanKL               | 0.007596   |
| Entropy              | -1.2575    |
| Perplexity           | 0.28438    |
| AveragePolicyStd     | 0.19832    |
| AveragePolicyStd[0]  | 0.21152    |
| AveragePolicyStd[1]  | 0.21227    |
| AveragePolicyStd[2]  | 0.15314    |
| AveragePolicyStd[3]  | 0.19796    |
| AveragePolicyStd[4]  | 0.17391    |
| AveragePolicyStd[5]  | 0.24111    |
| AverageReturn        | 1553.8     |
| MinReturn            | 418.85     |
| MaxReturn            | 1761.3     |
| StdReturn            | 333.98     |
| AverageEpisodeLength | 925.54     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.87     |
| TotalNEpisodes       | 22390      |
| TotalNSamples        | 5.4806e+06 |
| ExplainedVariance    | 0.079389   |
-------------------------------------
[2018-01-21 15:23:26.787487 UTC] Saving snapshot
[2018-01-21 15:23:26.787784 UTC] Starting iteration 1096
[2018-01-21 15:23:26.787949 UTC] Start collecting samples
[2018-01-21 15:23:31.427818 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:31.559924 UTC] Performing policy update
[2018-01-21 15:23:31.560792 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:31.677551 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:33.083510 UTC] Performing line search
[2018-01-21 15:23:33.296649 UTC] Updating baseline
[2018-01-21 15:23:35.598687 UTC] Computing logging information
-------------------------------------
| Iteration            | 1096       |
| ExpectedImprovement  | 0.017183   |
| ActualImprovement    | 0.016498   |
| ImprovementRatio     | 0.96016    |
| MeanKL               | 0.0083647  |
| Entropy              | -1.2583    |
| Perplexity           | 0.28413    |
| AveragePolicyStd     | 0.1983     |
| AveragePolicyStd[0]  | 0.21123    |
| AveragePolicyStd[1]  | 0.21276    |
| AveragePolicyStd[2]  | 0.15343    |
| AveragePolicyStd[3]  | 0.19774    |
| AveragePolicyStd[4]  | 0.17339    |
| AveragePolicyStd[5]  | 0.24122    |
| AverageReturn        | 1555.8     |
| MinReturn            | 418.85     |
| MaxReturn            | 1783.9     |
| StdReturn            | 337.58     |
| AverageEpisodeLength | 924.92     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.24     |
| TotalNEpisodes       | 22396      |
| TotalNSamples        | 5.4861e+06 |
| ExplainedVariance    | 0.038837   |
-------------------------------------
[2018-01-21 15:23:36.341043 UTC] Saving snapshot
[2018-01-21 15:23:36.341285 UTC] Starting iteration 1097
[2018-01-21 15:23:36.341510 UTC] Start collecting samples
[2018-01-21 15:23:41.011761 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:41.209518 UTC] Performing policy update
[2018-01-21 15:23:41.210245 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:41.342802 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:42.718654 UTC] Performing line search
[2018-01-21 15:23:42.917536 UTC] Updating baseline
[2018-01-21 15:23:44.865516 UTC] Computing logging information
-------------------------------------
| Iteration            | 1097       |
| ExpectedImprovement  | 0.019532   |
| ActualImprovement    | 0.018464   |
| ImprovementRatio     | 0.94532    |
| MeanKL               | 0.0075371  |
| Entropy              | -1.2546    |
| Perplexity           | 0.28519    |
| AveragePolicyStd     | 0.19844    |
| AveragePolicyStd[0]  | 0.21134    |
| AveragePolicyStd[1]  | 0.21297    |
| AveragePolicyStd[2]  | 0.15338    |
| AveragePolicyStd[3]  | 0.19735    |
| AveragePolicyStd[4]  | 0.17372    |
| AveragePolicyStd[5]  | 0.24186    |
| AverageReturn        | 1548.4     |
| MinReturn            | 418.85     |
| MaxReturn            | 1783.9     |
| StdReturn            | 349        |
| AverageEpisodeLength | 919.37     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.1      |
| TotalNEpisodes       | 22402      |
| TotalNSamples        | 5.4915e+06 |
| ExplainedVariance    | 0.11775    |
-------------------------------------
[2018-01-21 15:23:45.649569 UTC] Saving snapshot
[2018-01-21 15:23:45.649795 UTC] Starting iteration 1098
[2018-01-21 15:23:45.649965 UTC] Start collecting samples
[2018-01-21 15:23:50.199282 UTC] Computing input variables for policy optimization
[2018-01-21 15:23:50.337669 UTC] Performing policy update
[2018-01-21 15:23:50.338288 UTC] Computing gradient in Euclidean space
[2018-01-21 15:23:50.461557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:23:51.900954 UTC] Performing line search
[2018-01-21 15:23:52.104668 UTC] Updating baseline
[2018-01-21 15:23:54.962007 UTC] Computing logging information
-------------------------------------
| Iteration            | 1098       |
| ExpectedImprovement  | 0.018397   |
| ActualImprovement    | 0.016855   |
| ImprovementRatio     | 0.91616    |
| MeanKL               | 0.0075427  |
| Entropy              | -1.2589    |
| Perplexity           | 0.28398    |
| AveragePolicyStd     | 0.19828    |
| AveragePolicyStd[0]  | 0.21127    |
| AveragePolicyStd[1]  | 0.21291    |
| AveragePolicyStd[2]  | 0.1535     |
| AveragePolicyStd[3]  | 0.19652    |
| AveragePolicyStd[4]  | 0.17379    |
| AveragePolicyStd[5]  | 0.24171    |
| AverageReturn        | 1549.1     |
| MinReturn            | 418.85     |
| MaxReturn            | 1783.9     |
| StdReturn            | 349.25     |
| AverageEpisodeLength | 919.37     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.1      |
| TotalNEpisodes       | 22404      |
| TotalNSamples        | 5.4935e+06 |
| ExplainedVariance    | 0.0003879  |
-------------------------------------
[2018-01-21 15:23:55.729357 UTC] Saving snapshot
[2018-01-21 15:23:55.729599 UTC] Starting iteration 1099
[2018-01-21 15:23:55.729778 UTC] Start collecting samples
[2018-01-21 15:24:00.433020 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:00.565293 UTC] Performing policy update
[2018-01-21 15:24:00.566523 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:00.687546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:02.102367 UTC] Performing line search
[2018-01-21 15:24:02.284367 UTC] Updating baseline
[2018-01-21 15:24:04.330584 UTC] Computing logging information
------------------------------------
| Iteration            | 1099      |
| ExpectedImprovement  | 0.018302  |
| ActualImprovement    | 0.017709  |
| ImprovementRatio     | 0.96761   |
| MeanKL               | 0.0078173 |
| Entropy              | -1.2668   |
| Perplexity           | 0.28172   |
| AveragePolicyStd     | 0.19801   |
| AveragePolicyStd[0]  | 0.21078   |
| AveragePolicyStd[1]  | 0.21239   |
| AveragePolicyStd[2]  | 0.1533    |
| AveragePolicyStd[3]  | 0.19624   |
| AveragePolicyStd[4]  | 0.17385   |
| AveragePolicyStd[5]  | 0.2415    |
| AverageReturn        | 1542.8    |
| MinReturn            | 418.85    |
| MaxReturn            | 1783.9    |
| StdReturn            | 360.22    |
| AverageEpisodeLength | 913.63    |
| MinEpisodeLength     | 288       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 198.09    |
| TotalNEpisodes       | 22413     |
| TotalNSamples        | 5.502e+06 |
| ExplainedVariance    | 0.019533  |
------------------------------------
[2018-01-21 15:24:05.118766 UTC] Saving snapshot
[2018-01-21 15:24:05.119020 UTC] Starting iteration 1100
[2018-01-21 15:24:05.119188 UTC] Start collecting samples
[2018-01-21 15:24:09.468012 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:09.601641 UTC] Performing policy update
[2018-01-21 15:24:09.602272 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:09.724731 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:11.112528 UTC] Performing line search
[2018-01-21 15:24:11.301670 UTC] Updating baseline
[2018-01-21 15:24:13.315220 UTC] Computing logging information
-------------------------------------
| Iteration            | 1100       |
| ExpectedImprovement  | 0.018659   |
| ActualImprovement    | 0.017521   |
| ImprovementRatio     | 0.93899    |
| MeanKL               | 0.0073932  |
| Entropy              | -1.2754    |
| Perplexity           | 0.27931    |
| AveragePolicyStd     | 0.1977     |
| AveragePolicyStd[0]  | 0.21024    |
| AveragePolicyStd[1]  | 0.21199    |
| AveragePolicyStd[2]  | 0.15322    |
| AveragePolicyStd[3]  | 0.19652    |
| AveragePolicyStd[4]  | 0.17369    |
| AveragePolicyStd[5]  | 0.24052    |
| AverageReturn        | 1543.5     |
| MinReturn            | 418.85     |
| MaxReturn            | 1783.9     |
| StdReturn            | 354.7      |
| AverageEpisodeLength | 913.06     |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.63     |
| TotalNEpisodes       | 22419      |
| TotalNSamples        | 5.5072e+06 |
| ExplainedVariance    | 0.16014    |
-------------------------------------
[2018-01-21 15:24:14.130836 UTC] Saving snapshot
[2018-01-21 15:24:14.140346 UTC] Starting iteration 1101
[2018-01-21 15:24:14.140524 UTC] Start collecting samples
[2018-01-21 15:24:18.611008 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:18.734992 UTC] Performing policy update
[2018-01-21 15:24:18.735675 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:18.854901 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:20.278615 UTC] Performing line search
[2018-01-21 15:24:20.472492 UTC] Updating baseline
[2018-01-21 15:24:22.235798 UTC] Computing logging information
------------------------------------
| Iteration            | 1101      |
| ExpectedImprovement  | 0.018576  |
| ActualImprovement    | 0.017435  |
| ImprovementRatio     | 0.93859   |
| MeanKL               | 0.0072051 |
| Entropy              | -1.2795   |
| Perplexity           | 0.27817   |
| AveragePolicyStd     | 0.19758   |
| AveragePolicyStd[0]  | 0.2106    |
| AveragePolicyStd[1]  | 0.21202   |
| AveragePolicyStd[2]  | 0.15302   |
| AveragePolicyStd[3]  | 0.19649   |
| AveragePolicyStd[4]  | 0.17308   |
| AveragePolicyStd[5]  | 0.24026   |
| AverageReturn        | 1540.5    |
| MinReturn            | 418.85    |
| MaxReturn            | 1783.9    |
| StdReturn            | 355.33    |
| AverageEpisodeLength | 910.91    |
| MinEpisodeLength     | 288       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 194.84    |
| TotalNEpisodes       | 22422     |
| TotalNSamples        | 5.51e+06  |
| ExplainedVariance    | 0.18433   |
------------------------------------
[2018-01-21 15:24:23.011417 UTC] Saving snapshot
[2018-01-21 15:24:23.011718 UTC] Starting iteration 1102
[2018-01-21 15:24:23.011917 UTC] Start collecting samples
[2018-01-21 15:24:27.632394 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:27.760734 UTC] Performing policy update
[2018-01-21 15:24:27.761351 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:27.881441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:29.256240 UTC] Performing line search
[2018-01-21 15:24:29.442880 UTC] Updating baseline
[2018-01-21 15:24:31.198836 UTC] Computing logging information
-------------------------------------
| Iteration            | 1102       |
| ExpectedImprovement  | 0.017047   |
| ActualImprovement    | 0.016339   |
| ImprovementRatio     | 0.95845    |
| MeanKL               | 0.0083552  |
| Entropy              | -1.2868    |
| Perplexity           | 0.27616    |
| AveragePolicyStd     | 0.19733    |
| AveragePolicyStd[0]  | 0.21012    |
| AveragePolicyStd[1]  | 0.21178    |
| AveragePolicyStd[2]  | 0.15303    |
| AveragePolicyStd[3]  | 0.19604    |
| AveragePolicyStd[4]  | 0.17299    |
| AveragePolicyStd[5]  | 0.24003    |
| AverageReturn        | 1535.9     |
| MinReturn            | 418.85     |
| MaxReturn            | 1817.7     |
| StdReturn            | 363.47     |
| AverageEpisodeLength | 907.2      |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.69     |
| TotalNEpisodes       | 22430      |
| TotalNSamples        | 5.5168e+06 |
| ExplainedVariance    | 0.24373    |
-------------------------------------
[2018-01-21 15:24:31.961506 UTC] Saving snapshot
[2018-01-21 15:24:31.961774 UTC] Starting iteration 1103
[2018-01-21 15:24:31.961963 UTC] Start collecting samples
[2018-01-21 15:24:36.620191 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:36.738978 UTC] Performing policy update
[2018-01-21 15:24:36.739662 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:36.856330 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:38.257728 UTC] Performing line search
[2018-01-21 15:24:38.444032 UTC] Updating baseline
[2018-01-21 15:24:40.201492 UTC] Computing logging information
-------------------------------------
| Iteration            | 1103       |
| ExpectedImprovement  | 0.019727   |
| ActualImprovement    | 0.019316   |
| ImprovementRatio     | 0.97916    |
| MeanKL               | 0.0070064  |
| Entropy              | -1.2887    |
| Perplexity           | 0.27564    |
| AveragePolicyStd     | 0.19725    |
| AveragePolicyStd[0]  | 0.2105     |
| AveragePolicyStd[1]  | 0.21139    |
| AveragePolicyStd[2]  | 0.15347    |
| AveragePolicyStd[3]  | 0.19546    |
| AveragePolicyStd[4]  | 0.17268    |
| AveragePolicyStd[5]  | 0.24003    |
| AverageReturn        | 1542.6     |
| MinReturn            | 418.85     |
| MaxReturn            | 1817.7     |
| StdReturn            | 359.75     |
| AverageEpisodeLength | 909.9      |
| MinEpisodeLength     | 288        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.28     |
| TotalNEpisodes       | 22434      |
| TotalNSamples        | 5.5207e+06 |
| ExplainedVariance    | 0.50311    |
-------------------------------------
[2018-01-21 15:24:41.039446 UTC] Saving snapshot
[2018-01-21 15:24:41.039739 UTC] Starting iteration 1104
[2018-01-21 15:24:41.039946 UTC] Start collecting samples
[2018-01-21 15:24:45.600523 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:45.743098 UTC] Performing policy update
[2018-01-21 15:24:45.743814 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:45.877772 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:47.274191 UTC] Performing line search
[2018-01-21 15:24:47.469955 UTC] Updating baseline
[2018-01-21 15:24:49.346571 UTC] Computing logging information
-------------------------------------
| Iteration            | 1104       |
| ExpectedImprovement  | 0.017426   |
| ActualImprovement    | 0.016445   |
| ImprovementRatio     | 0.94373    |
| MeanKL               | 0.0076591  |
| Entropy              | -1.2943    |
| Perplexity           | 0.2741     |
| AveragePolicyStd     | 0.19708    |
| AveragePolicyStd[0]  | 0.20954    |
| AveragePolicyStd[1]  | 0.21194    |
| AveragePolicyStd[2]  | 0.15354    |
| AveragePolicyStd[3]  | 0.19469    |
| AveragePolicyStd[4]  | 0.17246    |
| AveragePolicyStd[5]  | 0.2403     |
| AverageReturn        | 1549.6     |
| MinReturn            | 498.39     |
| MaxReturn            | 1817.7     |
| StdReturn            | 346.52     |
| AverageEpisodeLength | 912.79     |
| MinEpisodeLength     | 304        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.89     |
| TotalNEpisodes       | 22440      |
| TotalNSamples        | 5.5257e+06 |
| ExplainedVariance    | 0.21833    |
-------------------------------------
[2018-01-21 15:24:50.086510 UTC] Saving snapshot
[2018-01-21 15:24:50.086765 UTC] Starting iteration 1105
[2018-01-21 15:24:50.086926 UTC] Start collecting samples
[2018-01-21 15:24:54.526931 UTC] Computing input variables for policy optimization
[2018-01-21 15:24:54.673786 UTC] Performing policy update
[2018-01-21 15:24:54.674506 UTC] Computing gradient in Euclidean space
[2018-01-21 15:24:54.791931 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:24:56.202850 UTC] Performing line search
[2018-01-21 15:24:56.391239 UTC] Updating baseline
[2018-01-21 15:24:58.285902 UTC] Computing logging information
-------------------------------------
| Iteration            | 1105       |
| ExpectedImprovement  | 0.01741    |
| ActualImprovement    | 0.016974   |
| ImprovementRatio     | 0.97495    |
| MeanKL               | 0.0081181  |
| Entropy              | -1.2963    |
| Perplexity           | 0.27355    |
| AveragePolicyStd     | 0.19704    |
| AveragePolicyStd[0]  | 0.20936    |
| AveragePolicyStd[1]  | 0.21175    |
| AveragePolicyStd[2]  | 0.15315    |
| AveragePolicyStd[3]  | 0.19533    |
| AveragePolicyStd[4]  | 0.17214    |
| AveragePolicyStd[5]  | 0.2405     |
| AverageReturn        | 1573.7     |
| MinReturn            | 498.39     |
| MaxReturn            | 1817.7     |
| StdReturn            | 332.78     |
| AverageEpisodeLength | 924.24     |
| MinEpisodeLength     | 304        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.99     |
| TotalNEpisodes       | 22447      |
| TotalNSamples        | 5.5327e+06 |
| ExplainedVariance    | 0.059586   |
-------------------------------------
[2018-01-21 15:24:59.127701 UTC] Saving snapshot
[2018-01-21 15:24:59.127995 UTC] Starting iteration 1106
[2018-01-21 15:24:59.128175 UTC] Start collecting samples
[2018-01-21 15:25:03.621681 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:03.768236 UTC] Performing policy update
[2018-01-21 15:25:03.769265 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:03.889298 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:05.288110 UTC] Performing line search
[2018-01-21 15:25:05.478883 UTC] Updating baseline
[2018-01-21 15:25:07.611737 UTC] Computing logging information
------------------------------------
| Iteration            | 1106      |
| ExpectedImprovement  | 0.017184  |
| ActualImprovement    | 0.016013  |
| ImprovementRatio     | 0.93184   |
| MeanKL               | 0.008221  |
| Entropy              | -1.2892   |
| Perplexity           | 0.27548   |
| AveragePolicyStd     | 0.19727   |
| AveragePolicyStd[0]  | 0.20926   |
| AveragePolicyStd[1]  | 0.21159   |
| AveragePolicyStd[2]  | 0.15326   |
| AveragePolicyStd[3]  | 0.19603   |
| AveragePolicyStd[4]  | 0.17244   |
| AveragePolicyStd[5]  | 0.24104   |
| AverageReturn        | 1563.4    |
| MinReturn            | 465.76    |
| MaxReturn            | 1817.7    |
| StdReturn            | 351.2     |
| AverageEpisodeLength | 917.25    |
| MinEpisodeLength     | 301       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 191.14    |
| TotalNEpisodes       | 22451     |
| TotalNSamples        | 5.536e+06 |
| ExplainedVariance    | 0.17255   |
------------------------------------
[2018-01-21 15:25:08.394253 UTC] Saving snapshot
[2018-01-21 15:25:08.394561 UTC] Starting iteration 1107
[2018-01-21 15:25:08.394743 UTC] Start collecting samples
[2018-01-21 15:25:12.998680 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:13.124534 UTC] Performing policy update
[2018-01-21 15:25:13.125188 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:13.246419 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:14.640036 UTC] Performing line search
[2018-01-21 15:25:14.835256 UTC] Updating baseline
[2018-01-21 15:25:17.021542 UTC] Computing logging information
-------------------------------------
| Iteration            | 1107       |
| ExpectedImprovement  | 0.017163   |
| ActualImprovement    | 0.016363   |
| ImprovementRatio     | 0.95343    |
| MeanKL               | 0.0080435  |
| Entropy              | -1.2903    |
| Perplexity           | 0.27518    |
| AveragePolicyStd     | 0.19724    |
| AveragePolicyStd[0]  | 0.20929    |
| AveragePolicyStd[1]  | 0.21146    |
| AveragePolicyStd[2]  | 0.15322    |
| AveragePolicyStd[3]  | 0.19569    |
| AveragePolicyStd[4]  | 0.17252    |
| AveragePolicyStd[5]  | 0.24126    |
| AverageReturn        | 1577.4     |
| MinReturn            | 465.76     |
| MaxReturn            | 1817.7     |
| StdReturn            | 335.78     |
| AverageEpisodeLength | 923.92     |
| MinEpisodeLength     | 301        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.06     |
| TotalNEpisodes       | 22457      |
| TotalNSamples        | 5.542e+06  |
| ExplainedVariance    | -0.0085312 |
-------------------------------------
[2018-01-21 15:25:17.788378 UTC] Saving snapshot
[2018-01-21 15:25:17.788630 UTC] Starting iteration 1108
[2018-01-21 15:25:17.788774 UTC] Start collecting samples
[2018-01-21 15:25:22.426953 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:22.559849 UTC] Performing policy update
[2018-01-21 15:25:22.560587 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:22.689119 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:24.121261 UTC] Performing line search
[2018-01-21 15:25:24.336301 UTC] Updating baseline
[2018-01-21 15:25:26.555046 UTC] Computing logging information
------------------------------------
| Iteration            | 1108      |
| ExpectedImprovement  | 0.018025  |
| ActualImprovement    | 0.016887  |
| ImprovementRatio     | 0.93687   |
| MeanKL               | 0.007969  |
| Entropy              | -1.2889   |
| Perplexity           | 0.27558   |
| AveragePolicyStd     | 0.1973    |
| AveragePolicyStd[0]  | 0.2093    |
| AveragePolicyStd[1]  | 0.21193   |
| AveragePolicyStd[2]  | 0.15314   |
| AveragePolicyStd[3]  | 0.19665   |
| AveragePolicyStd[4]  | 0.17191   |
| AveragePolicyStd[5]  | 0.24087   |
| AverageReturn        | 1594.1    |
| MinReturn            | 465.76    |
| MaxReturn            | 1817.7    |
| StdReturn            | 326.33    |
| AverageEpisodeLength | 928.92    |
| MinEpisodeLength     | 301       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 177.15    |
| TotalNEpisodes       | 22463     |
| TotalNSamples        | 5.548e+06 |
| ExplainedVariance    | 0.0028081 |
------------------------------------
[2018-01-21 15:25:27.385698 UTC] Saving snapshot
[2018-01-21 15:25:27.386009 UTC] Starting iteration 1109
[2018-01-21 15:25:27.386235 UTC] Start collecting samples
[2018-01-21 15:25:32.047153 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:32.163100 UTC] Performing policy update
[2018-01-21 15:25:32.163739 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:32.281621 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:33.672482 UTC] Performing line search
[2018-01-21 15:25:33.875549 UTC] Updating baseline
[2018-01-21 15:25:36.674421 UTC] Computing logging information
------------------------------------
| Iteration            | 1109      |
| ExpectedImprovement  | 0.018938  |
| ActualImprovement    | 0.017038  |
| ImprovementRatio     | 0.89971   |
| MeanKL               | 0.0074169 |
| Entropy              | -1.2891   |
| Perplexity           | 0.27552   |
| AveragePolicyStd     | 0.19732   |
| AveragePolicyStd[0]  | 0.20941   |
| AveragePolicyStd[1]  | 0.21152   |
| AveragePolicyStd[2]  | 0.15311   |
| AveragePolicyStd[3]  | 0.19651   |
| AveragePolicyStd[4]  | 0.17174   |
| AveragePolicyStd[5]  | 0.24161   |
| AverageReturn        | 1606      |
| MinReturn            | 465.76    |
| MaxReturn            | 1817.7    |
| StdReturn            | 307.55    |
| AverageEpisodeLength | 935.88    |
| MinEpisodeLength     | 301       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 165.76    |
| TotalNEpisodes       | 22466     |
| TotalNSamples        | 5.551e+06 |
| ExplainedVariance    | 0.0063737 |
------------------------------------
[2018-01-21 15:25:37.475086 UTC] Saving snapshot
[2018-01-21 15:25:37.475347 UTC] Starting iteration 1110
[2018-01-21 15:25:37.475513 UTC] Start collecting samples
[2018-01-21 15:25:42.021891 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:42.173504 UTC] Performing policy update
[2018-01-21 15:25:42.174256 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:42.300996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:43.741294 UTC] Performing line search
[2018-01-21 15:25:43.931057 UTC] Updating baseline
[2018-01-21 15:25:46.072913 UTC] Computing logging information
------------------------------------
| Iteration            | 1110      |
| ExpectedImprovement  | 0.016913  |
| ActualImprovement    | 0.01615   |
| ImprovementRatio     | 0.95491   |
| MeanKL               | 0.0081151 |
| Entropy              | -1.2927   |
| Perplexity           | 0.27453   |
| AveragePolicyStd     | 0.1972    |
| AveragePolicyStd[0]  | 0.20926   |
| AveragePolicyStd[1]  | 0.21146   |
| AveragePolicyStd[2]  | 0.15298   |
| AveragePolicyStd[3]  | 0.19665   |
| AveragePolicyStd[4]  | 0.17151   |
| AveragePolicyStd[5]  | 0.24134   |
| AverageReturn        | 1609.7    |
| MinReturn            | 465.76    |
| MaxReturn            | 1817.7    |
| StdReturn            | 309.07    |
| AverageEpisodeLength | 935.88    |
| MinEpisodeLength     | 301       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 165.76    |
| TotalNEpisodes       | 22471     |
| TotalNSamples        | 5.556e+06 |
| ExplainedVariance    | 0.025897  |
------------------------------------
[2018-01-21 15:25:46.827058 UTC] Saving snapshot
[2018-01-21 15:25:46.837890 UTC] Starting iteration 1111
[2018-01-21 15:25:46.838212 UTC] Start collecting samples
[2018-01-21 15:25:51.473923 UTC] Computing input variables for policy optimization
[2018-01-21 15:25:51.612492 UTC] Performing policy update
[2018-01-21 15:25:51.613489 UTC] Computing gradient in Euclidean space
[2018-01-21 15:25:51.734890 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:25:53.153556 UTC] Performing line search
[2018-01-21 15:25:53.343407 UTC] Updating baseline
[2018-01-21 15:25:55.091127 UTC] Computing logging information
-------------------------------------
| Iteration            | 1111       |
| ExpectedImprovement  | 0.018388   |
| ActualImprovement    | 0.017783   |
| ImprovementRatio     | 0.96707    |
| MeanKL               | 0.0085846  |
| Entropy              | -1.2882    |
| Perplexity           | 0.27577    |
| AveragePolicyStd     | 0.19739    |
| AveragePolicyStd[0]  | 0.20916    |
| AveragePolicyStd[1]  | 0.21189    |
| AveragePolicyStd[2]  | 0.15257    |
| AveragePolicyStd[3]  | 0.19708    |
| AveragePolicyStd[4]  | 0.17165    |
| AveragePolicyStd[5]  | 0.24199    |
| AverageReturn        | 1587.2     |
| MinReturn            | 465.76     |
| MaxReturn            | 1817.7     |
| StdReturn            | 341.42     |
| AverageEpisodeLength | 920.25     |
| MinEpisodeLength     | 301        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.66     |
| TotalNEpisodes       | 22480      |
| TotalNSamples        | 5.5630e+06 |
| ExplainedVariance    | 0.39371    |
-------------------------------------
[2018-01-21 15:25:55.918192 UTC] Saving snapshot
[2018-01-21 15:25:55.918469 UTC] Starting iteration 1112
[2018-01-21 15:25:55.918651 UTC] Start collecting samples
[2018-01-21 15:26:00.453130 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:00.584594 UTC] Performing policy update
[2018-01-21 15:26:00.586670 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:00.710242 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:02.171246 UTC] Performing line search
[2018-01-21 15:26:02.370107 UTC] Updating baseline
[2018-01-21 15:26:05.418994 UTC] Computing logging information
------------------------------------
| Iteration            | 1112      |
| ExpectedImprovement  | 0.0174    |
| ActualImprovement    | 0.016642  |
| ImprovementRatio     | 0.95642   |
| MeanKL               | 0.0085193 |
| Entropy              | -1.2882   |
| Perplexity           | 0.27577   |
| AveragePolicyStd     | 0.1974    |
| AveragePolicyStd[0]  | 0.20919   |
| AveragePolicyStd[1]  | 0.21231   |
| AveragePolicyStd[2]  | 0.15246   |
| AveragePolicyStd[3]  | 0.19665   |
| AveragePolicyStd[4]  | 0.17169   |
| AveragePolicyStd[5]  | 0.24211   |
| AverageReturn        | 1587.3    |
| MinReturn            | 465.76    |
| MaxReturn            | 1817.7    |
| StdReturn            | 341.51    |
| AverageEpisodeLength | 920.22    |
| MinEpisodeLength     | 301       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 183.65    |
| TotalNEpisodes       | 22482     |
| TotalNSamples        | 5.565e+06 |
| ExplainedVariance    | -0.0523   |
------------------------------------
[2018-01-21 15:26:06.174959 UTC] Saving snapshot
[2018-01-21 15:26:06.175137 UTC] Starting iteration 1113
[2018-01-21 15:26:06.175240 UTC] Start collecting samples
[2018-01-21 15:26:10.779798 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:10.922137 UTC] Performing policy update
[2018-01-21 15:26:10.922848 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:11.055409 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:12.448600 UTC] Performing line search
[2018-01-21 15:26:12.640104 UTC] Updating baseline
[2018-01-21 15:26:14.431970 UTC] Computing logging information
-------------------------------------
| Iteration            | 1113       |
| ExpectedImprovement  | 0.018522   |
| ActualImprovement    | 0.01759    |
| ImprovementRatio     | 0.94967    |
| MeanKL               | 0.0078428  |
| Entropy              | -1.2934    |
| Perplexity           | 0.27434    |
| AveragePolicyStd     | 0.19724    |
| AveragePolicyStd[0]  | 0.20891    |
| AveragePolicyStd[1]  | 0.21251    |
| AveragePolicyStd[2]  | 0.15235    |
| AveragePolicyStd[3]  | 0.19641    |
| AveragePolicyStd[4]  | 0.17128    |
| AveragePolicyStd[5]  | 0.242      |
| AverageReturn        | 1578.6     |
| MinReturn            | 463.29     |
| MaxReturn            | 1821.6     |
| StdReturn            | 355.55     |
| AverageEpisodeLength | 913.34     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.67     |
| TotalNEpisodes       | 22488      |
| TotalNSamples        | 5.5699e+06 |
| ExplainedVariance    | 0.21906    |
-------------------------------------
[2018-01-21 15:26:15.213740 UTC] Saving snapshot
[2018-01-21 15:26:15.213987 UTC] Starting iteration 1114
[2018-01-21 15:26:15.214164 UTC] Start collecting samples
[2018-01-21 15:26:19.796393 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:19.965297 UTC] Performing policy update
[2018-01-21 15:26:19.966462 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:20.087653 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:21.504376 UTC] Performing line search
[2018-01-21 15:26:21.706804 UTC] Updating baseline
[2018-01-21 15:26:23.748355 UTC] Computing logging information
-------------------------------------
| Iteration            | 1114       |
| ExpectedImprovement  | 0.017518   |
| ActualImprovement    | 0.016269   |
| ImprovementRatio     | 0.92872    |
| MeanKL               | 0.0081962  |
| Entropy              | -1.2914    |
| Perplexity           | 0.27489    |
| AveragePolicyStd     | 0.19729    |
| AveragePolicyStd[0]  | 0.20939    |
| AveragePolicyStd[1]  | 0.21274    |
| AveragePolicyStd[2]  | 0.15245    |
| AveragePolicyStd[3]  | 0.19605    |
| AveragePolicyStd[4]  | 0.1716     |
| AveragePolicyStd[5]  | 0.24148    |
| AverageReturn        | 1589       |
| MinReturn            | 463.29     |
| MaxReturn            | 1821.6     |
| StdReturn            | 347.04     |
| AverageEpisodeLength | 918.51     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.95     |
| TotalNEpisodes       | 22495      |
| TotalNSamples        | 5.5769e+06 |
| ExplainedVariance    | 0.1158     |
-------------------------------------
[2018-01-21 15:26:24.495633 UTC] Saving snapshot
[2018-01-21 15:26:24.495921 UTC] Starting iteration 1115
[2018-01-21 15:26:24.496098 UTC] Start collecting samples
[2018-01-21 15:26:28.930124 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:29.051509 UTC] Performing policy update
[2018-01-21 15:26:29.052382 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:29.168854 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:30.546079 UTC] Performing line search
[2018-01-21 15:26:30.734361 UTC] Updating baseline
[2018-01-21 15:26:32.343552 UTC] Computing logging information
-------------------------------------
| Iteration            | 1115       |
| ExpectedImprovement  | 0.019349   |
| ActualImprovement    | 0.019011   |
| ImprovementRatio     | 0.98253    |
| MeanKL               | 0.0079484  |
| Entropy              | -1.2979    |
| Perplexity           | 0.27311    |
| AveragePolicyStd     | 0.19709    |
| AveragePolicyStd[0]  | 0.20897    |
| AveragePolicyStd[1]  | 0.21344    |
| AveragePolicyStd[2]  | 0.15241    |
| AveragePolicyStd[3]  | 0.19481    |
| AveragePolicyStd[4]  | 0.17141    |
| AveragePolicyStd[5]  | 0.24149    |
| AverageReturn        | 1592.6     |
| MinReturn            | 463.29     |
| MaxReturn            | 1821.6     |
| StdReturn            | 341.82     |
| AverageEpisodeLength | 920.01     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.76     |
| TotalNEpisodes       | 22500      |
| TotalNSamples        | 5.5815e+06 |
| ExplainedVariance    | 0.025971   |
-------------------------------------
[2018-01-21 15:26:33.189870 UTC] Saving snapshot
[2018-01-21 15:26:33.190116 UTC] Starting iteration 1116
[2018-01-21 15:26:33.190270 UTC] Start collecting samples
[2018-01-21 15:26:37.697503 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:37.811684 UTC] Performing policy update
[2018-01-21 15:26:37.812297 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:37.936546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:39.303039 UTC] Performing line search
[2018-01-21 15:26:39.492753 UTC] Updating baseline
[2018-01-21 15:26:41.606881 UTC] Computing logging information
-------------------------------------
| Iteration            | 1116       |
| ExpectedImprovement  | 0.023491   |
| ActualImprovement    | 0.021575   |
| ImprovementRatio     | 0.9184     |
| MeanKL               | 0.0072744  |
| Entropy              | -1.2975    |
| Perplexity           | 0.27322    |
| AveragePolicyStd     | 0.19709    |
| AveragePolicyStd[0]  | 0.20907    |
| AveragePolicyStd[1]  | 0.21325    |
| AveragePolicyStd[2]  | 0.15246    |
| AveragePolicyStd[3]  | 0.19499    |
| AveragePolicyStd[4]  | 0.17145    |
| AveragePolicyStd[5]  | 0.24133    |
| AverageReturn        | 1570.2     |
| MinReturn            | 463.29     |
| MaxReturn            | 1821.6     |
| StdReturn            | 359.07     |
| AverageEpisodeLength | 906.91     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.05     |
| TotalNEpisodes       | 22504      |
| TotalNSamples        | 5.5842e+06 |
| ExplainedVariance    | 0.47306    |
-------------------------------------
[2018-01-21 15:26:42.402764 UTC] Saving snapshot
[2018-01-21 15:26:42.403021 UTC] Starting iteration 1117
[2018-01-21 15:26:42.403186 UTC] Start collecting samples
[2018-01-21 15:26:47.044887 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:47.171764 UTC] Performing policy update
[2018-01-21 15:26:47.172420 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:47.296960 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:48.780180 UTC] Performing line search
[2018-01-21 15:26:49.006567 UTC] Updating baseline
[2018-01-21 15:26:50.718290 UTC] Computing logging information
-------------------------------------
| Iteration            | 1117       |
| ExpectedImprovement  | 0.018413   |
| ActualImprovement    | 0.017842   |
| ImprovementRatio     | 0.96897    |
| MeanKL               | 0.0078278  |
| Entropy              | -1.3008    |
| Perplexity           | 0.27232    |
| AveragePolicyStd     | 0.19696    |
| AveragePolicyStd[0]  | 0.20862    |
| AveragePolicyStd[1]  | 0.21294    |
| AveragePolicyStd[2]  | 0.15259    |
| AveragePolicyStd[3]  | 0.19476    |
| AveragePolicyStd[4]  | 0.17165    |
| AveragePolicyStd[5]  | 0.24119    |
| AverageReturn        | 1569.7     |
| MinReturn            | 274.37     |
| MaxReturn            | 1821.6     |
| StdReturn            | 373.92     |
| AverageEpisodeLength | 904.57     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.05     |
| TotalNEpisodes       | 22513      |
| TotalNSamples        | 5.5924e+06 |
| ExplainedVariance    | 0.075459   |
-------------------------------------
[2018-01-21 15:26:51.485097 UTC] Saving snapshot
[2018-01-21 15:26:51.485329 UTC] Starting iteration 1118
[2018-01-21 15:26:51.485479 UTC] Start collecting samples
[2018-01-21 15:26:55.927640 UTC] Computing input variables for policy optimization
[2018-01-21 15:26:56.068520 UTC] Performing policy update
[2018-01-21 15:26:56.069627 UTC] Computing gradient in Euclidean space
[2018-01-21 15:26:56.188561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:26:57.552825 UTC] Performing line search
[2018-01-21 15:26:57.739125 UTC] Updating baseline
[2018-01-21 15:26:59.727025 UTC] Computing logging information
-------------------------------------
| Iteration            | 1118       |
| ExpectedImprovement  | 0.017574   |
| ActualImprovement    | 0.016633   |
| ImprovementRatio     | 0.94645    |
| MeanKL               | 0.0077086  |
| Entropy              | -1.3042    |
| Perplexity           | 0.27139    |
| AveragePolicyStd     | 0.19684    |
| AveragePolicyStd[0]  | 0.20828    |
| AveragePolicyStd[1]  | 0.21253    |
| AveragePolicyStd[2]  | 0.15254    |
| AveragePolicyStd[3]  | 0.19515    |
| AveragePolicyStd[4]  | 0.17158    |
| AveragePolicyStd[5]  | 0.24094    |
| AverageReturn        | 1582.2     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 365.72     |
| AverageEpisodeLength | 910.07     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.05     |
| TotalNEpisodes       | 22517      |
| TotalNSamples        | 5.5964e+06 |
| ExplainedVariance    | -0.0019207 |
-------------------------------------
[2018-01-21 15:27:00.481479 UTC] Saving snapshot
[2018-01-21 15:27:00.481781 UTC] Starting iteration 1119
[2018-01-21 15:27:00.482004 UTC] Start collecting samples
[2018-01-21 15:27:04.924045 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:05.058354 UTC] Performing policy update
[2018-01-21 15:27:05.059631 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:05.181788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:06.602173 UTC] Performing line search
[2018-01-21 15:27:06.804236 UTC] Updating baseline
[2018-01-21 15:27:09.023866 UTC] Computing logging information
-------------------------------------
| Iteration            | 1119       |
| ExpectedImprovement  | 0.018002   |
| ActualImprovement    | 0.016677   |
| ImprovementRatio     | 0.92638    |
| MeanKL               | 0.0072834  |
| Entropy              | -1.3023    |
| Perplexity           | 0.27191    |
| AveragePolicyStd     | 0.1969     |
| AveragePolicyStd[0]  | 0.20892    |
| AveragePolicyStd[1]  | 0.21272    |
| AveragePolicyStd[2]  | 0.15233    |
| AveragePolicyStd[3]  | 0.19538    |
| AveragePolicyStd[4]  | 0.1716     |
| AveragePolicyStd[5]  | 0.24044    |
| AverageReturn        | 1587       |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 365.34     |
| AverageEpisodeLength | 911.95     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198        |
| TotalNEpisodes       | 22520      |
| TotalNSamples        | 5.5994e+06 |
| ExplainedVariance    | -0.07536   |
-------------------------------------
[2018-01-21 15:27:09.789835 UTC] Saving snapshot
[2018-01-21 15:27:09.790077 UTC] Starting iteration 1120
[2018-01-21 15:27:09.790237 UTC] Start collecting samples
[2018-01-21 15:27:14.297775 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:14.446224 UTC] Performing policy update
[2018-01-21 15:27:14.446952 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:14.564881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:15.947669 UTC] Performing line search
[2018-01-21 15:27:16.145117 UTC] Updating baseline
[2018-01-21 15:27:18.273939 UTC] Computing logging information
-------------------------------------
| Iteration            | 1120       |
| ExpectedImprovement  | 0.018784   |
| ActualImprovement    | 0.017596   |
| ImprovementRatio     | 0.93677    |
| MeanKL               | 0.00776    |
| Entropy              | -1.3039    |
| Perplexity           | 0.27147    |
| AveragePolicyStd     | 0.19688    |
| AveragePolicyStd[0]  | 0.20922    |
| AveragePolicyStd[1]  | 0.21292    |
| AveragePolicyStd[2]  | 0.15217    |
| AveragePolicyStd[3]  | 0.19502    |
| AveragePolicyStd[4]  | 0.17119    |
| AveragePolicyStd[5]  | 0.24076    |
| AverageReturn        | 1615.3     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 345.67     |
| AverageEpisodeLength | 926.03     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.69     |
| TotalNEpisodes       | 22528      |
| TotalNSamples        | 5.6074e+06 |
| ExplainedVariance    | 0.00084768 |
-------------------------------------
[2018-01-21 15:27:19.044050 UTC] Saving snapshot
[2018-01-21 15:27:19.053451 UTC] Starting iteration 1121
[2018-01-21 15:27:19.053698 UTC] Start collecting samples
[2018-01-21 15:27:23.748495 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:23.888890 UTC] Performing policy update
[2018-01-21 15:27:23.889845 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:24.013777 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:25.428047 UTC] Performing line search
[2018-01-21 15:27:25.623403 UTC] Updating baseline
[2018-01-21 15:27:28.483735 UTC] Computing logging information
-------------------------------------
| Iteration            | 1121       |
| ExpectedImprovement  | 0.019748   |
| ActualImprovement    | 0.018367   |
| ImprovementRatio     | 0.93006    |
| MeanKL               | 0.0075027  |
| Entropy              | -1.3029    |
| Perplexity           | 0.27174    |
| AveragePolicyStd     | 0.19689    |
| AveragePolicyStd[0]  | 0.20905    |
| AveragePolicyStd[1]  | 0.21309    |
| AveragePolicyStd[2]  | 0.15237    |
| AveragePolicyStd[3]  | 0.19503    |
| AveragePolicyStd[4]  | 0.17137    |
| AveragePolicyStd[5]  | 0.24043    |
| AverageReturn        | 1611.6     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 354.87     |
| AverageEpisodeLength | 922.29     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.41     |
| TotalNEpisodes       | 22534      |
| TotalNSamples        | 5.6129e+06 |
| ExplainedVariance    | 0.045043   |
-------------------------------------
[2018-01-21 15:27:29.309130 UTC] Saving snapshot
[2018-01-21 15:27:29.309448 UTC] Starting iteration 1122
[2018-01-21 15:27:29.309644 UTC] Start collecting samples
[2018-01-21 15:27:33.843903 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:34.007977 UTC] Performing policy update
[2018-01-21 15:27:34.008662 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:34.130805 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:35.540767 UTC] Performing line search
[2018-01-21 15:27:35.751738 UTC] Updating baseline
[2018-01-21 15:27:37.806277 UTC] Computing logging information
-------------------------------------
| Iteration            | 1122       |
| ExpectedImprovement  | 0.021267   |
| ActualImprovement    | 0.018533   |
| ImprovementRatio     | 0.87146    |
| MeanKL               | 0.0071554  |
| Entropy              | -1.2991    |
| Perplexity           | 0.27278    |
| AveragePolicyStd     | 0.19703    |
| AveragePolicyStd[0]  | 0.21024    |
| AveragePolicyStd[1]  | 0.21317    |
| AveragePolicyStd[2]  | 0.15242    |
| AveragePolicyStd[3]  | 0.19461    |
| AveragePolicyStd[4]  | 0.17132    |
| AveragePolicyStd[5]  | 0.2404     |
| AverageReturn        | 1606.2     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 367.6      |
| AverageEpisodeLength | 919.14     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.2      |
| TotalNEpisodes       | 22536      |
| TotalNSamples        | 5.6143e+06 |
| ExplainedVariance    | 0.25635    |
-------------------------------------
[2018-01-21 15:27:38.610317 UTC] Saving snapshot
[2018-01-21 15:27:38.610643 UTC] Starting iteration 1123
[2018-01-21 15:27:38.610846 UTC] Start collecting samples
[2018-01-21 15:27:43.146987 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:43.302228 UTC] Performing policy update
[2018-01-21 15:27:43.303435 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:43.419694 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:44.888909 UTC] Performing line search
[2018-01-21 15:27:45.101336 UTC] Updating baseline
[2018-01-21 15:27:47.162355 UTC] Computing logging information
-------------------------------------
| Iteration            | 1123       |
| ExpectedImprovement  | 0.018889   |
| ActualImprovement    | 0.017567   |
| ImprovementRatio     | 0.93001    |
| MeanKL               | 0.0078006  |
| Entropy              | -1.3026    |
| Perplexity           | 0.27183    |
| AveragePolicyStd     | 0.19694    |
| AveragePolicyStd[0]  | 0.21073    |
| AveragePolicyStd[1]  | 0.2129     |
| AveragePolicyStd[2]  | 0.15227    |
| AveragePolicyStd[3]  | 0.19453    |
| AveragePolicyStd[4]  | 0.17073    |
| AveragePolicyStd[5]  | 0.24048    |
| AverageReturn        | 1620.4     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 355.89     |
| AverageEpisodeLength | 925.33     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.06     |
| TotalNEpisodes       | 22544      |
| TotalNSamples        | 5.6222e+06 |
| ExplainedVariance    | 0.044163   |
-------------------------------------
[2018-01-21 15:27:47.990051 UTC] Saving snapshot
[2018-01-21 15:27:47.990348 UTC] Starting iteration 1124
[2018-01-21 15:27:47.990519 UTC] Start collecting samples
[2018-01-21 15:27:52.734745 UTC] Computing input variables for policy optimization
[2018-01-21 15:27:52.894590 UTC] Performing policy update
[2018-01-21 15:27:52.895767 UTC] Computing gradient in Euclidean space
[2018-01-21 15:27:53.011728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:27:54.389837 UTC] Performing line search
[2018-01-21 15:27:54.582658 UTC] Updating baseline
[2018-01-21 15:27:56.604039 UTC] Computing logging information
-------------------------------------
| Iteration            | 1124       |
| ExpectedImprovement  | 0.017266   |
| ActualImprovement    | 0.01631    |
| ImprovementRatio     | 0.94466    |
| MeanKL               | 0.0073876  |
| Entropy              | -1.3056    |
| Perplexity           | 0.27101    |
| AveragePolicyStd     | 0.19686    |
| AveragePolicyStd[0]  | 0.21045    |
| AveragePolicyStd[1]  | 0.21228    |
| AveragePolicyStd[2]  | 0.15197    |
| AveragePolicyStd[3]  | 0.19472    |
| AveragePolicyStd[4]  | 0.17077    |
| AveragePolicyStd[5]  | 0.24095    |
| AverageReturn        | 1614.8     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 355.7      |
| AverageEpisodeLength | 921.81     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.56     |
| TotalNEpisodes       | 22552      |
| TotalNSamples        | 5.6291e+06 |
| ExplainedVariance    | 0.14711    |
-------------------------------------
[2018-01-21 15:27:57.381376 UTC] Saving snapshot
[2018-01-21 15:27:57.381559 UTC] Starting iteration 1125
[2018-01-21 15:27:57.381661 UTC] Start collecting samples
[2018-01-21 15:28:01.759996 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:01.891982 UTC] Performing policy update
[2018-01-21 15:28:01.893043 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:02.021272 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:03.432251 UTC] Performing line search
[2018-01-21 15:28:03.621984 UTC] Updating baseline
[2018-01-21 15:28:06.703427 UTC] Computing logging information
-------------------------------------
| Iteration            | 1125       |
| ExpectedImprovement  | 0.017527   |
| ActualImprovement    | 0.016189   |
| ImprovementRatio     | 0.92366    |
| MeanKL               | 0.0073162  |
| Entropy              | -1.3174    |
| Perplexity           | 0.26784    |
| AveragePolicyStd     | 0.1965     |
| AveragePolicyStd[0]  | 0.21069    |
| AveragePolicyStd[1]  | 0.21257    |
| AveragePolicyStd[2]  | 0.15165    |
| AveragePolicyStd[3]  | 0.1936     |
| AveragePolicyStd[4]  | 0.17007    |
| AveragePolicyStd[5]  | 0.24041    |
| AverageReturn        | 1614.9     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 355.73     |
| AverageEpisodeLength | 921.81     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.56     |
| TotalNEpisodes       | 22553      |
| TotalNSamples        | 5.6301e+06 |
| ExplainedVariance    | -0.003616  |
-------------------------------------
[2018-01-21 15:28:07.473664 UTC] Saving snapshot
[2018-01-21 15:28:07.474118 UTC] Starting iteration 1126
[2018-01-21 15:28:07.474500 UTC] Start collecting samples
[2018-01-21 15:28:12.154705 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:12.286133 UTC] Performing policy update
[2018-01-21 15:28:12.286783 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:12.402549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:13.806388 UTC] Performing line search
[2018-01-21 15:28:14.008256 UTC] Updating baseline
[2018-01-21 15:28:16.934713 UTC] Computing logging information
-------------------------------------
| Iteration            | 1126       |
| ExpectedImprovement  | 0.018629   |
| ActualImprovement    | 0.017608   |
| ImprovementRatio     | 0.94517    |
| MeanKL               | 0.0077302  |
| Entropy              | -1.3201    |
| Perplexity           | 0.26712    |
| AveragePolicyStd     | 0.19641    |
| AveragePolicyStd[0]  | 0.21022    |
| AveragePolicyStd[1]  | 0.21239    |
| AveragePolicyStd[2]  | 0.15186    |
| AveragePolicyStd[3]  | 0.19392    |
| AveragePolicyStd[4]  | 0.16946    |
| AveragePolicyStd[5]  | 0.24061    |
| AverageReturn        | 1615.5     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 355.92     |
| AverageEpisodeLength | 921.81     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.56     |
| TotalNEpisodes       | 22559      |
| TotalNSamples        | 5.6361e+06 |
| ExplainedVariance    | -0.009161  |
-------------------------------------
[2018-01-21 15:28:17.675608 UTC] Saving snapshot
[2018-01-21 15:28:17.675844 UTC] Starting iteration 1127
[2018-01-21 15:28:17.676003 UTC] Start collecting samples
[2018-01-21 15:28:22.464746 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:22.592563 UTC] Performing policy update
[2018-01-21 15:28:22.593273 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:22.710105 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:24.093829 UTC] Performing line search
[2018-01-21 15:28:24.286095 UTC] Updating baseline
[2018-01-21 15:28:27.421982 UTC] Computing logging information
-------------------------------------
| Iteration            | 1127       |
| ExpectedImprovement  | 0.017886   |
| ActualImprovement    | 0.0171     |
| ImprovementRatio     | 0.95608    |
| MeanKL               | 0.0073339  |
| Entropy              | -1.3152    |
| Perplexity           | 0.26841    |
| AveragePolicyStd     | 0.19658    |
| AveragePolicyStd[0]  | 0.21055    |
| AveragePolicyStd[1]  | 0.21256    |
| AveragePolicyStd[2]  | 0.15191    |
| AveragePolicyStd[3]  | 0.19422    |
| AveragePolicyStd[4]  | 0.16945    |
| AveragePolicyStd[5]  | 0.24078    |
| AverageReturn        | 1605.3     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 375.99     |
| AverageEpisodeLength | 914.53     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.94     |
| TotalNEpisodes       | 22568      |
| TotalNSamples        | 5.6444e+06 |
| ExplainedVariance    | 0.049644   |
-------------------------------------
[2018-01-21 15:28:28.164526 UTC] Saving snapshot
[2018-01-21 15:28:28.164867 UTC] Starting iteration 1128
[2018-01-21 15:28:28.165113 UTC] Start collecting samples
[2018-01-21 15:28:32.881827 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:33.025200 UTC] Performing policy update
[2018-01-21 15:28:33.025878 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:33.149618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:34.539706 UTC] Performing line search
[2018-01-21 15:28:34.738870 UTC] Updating baseline
[2018-01-21 15:28:36.508189 UTC] Computing logging information
-------------------------------------
| Iteration            | 1128       |
| ExpectedImprovement  | 0.021884   |
| ActualImprovement    | 0.022107   |
| ImprovementRatio     | 1.0102     |
| MeanKL               | 0.0075498  |
| Entropy              | -1.318     |
| Perplexity           | 0.26767    |
| AveragePolicyStd     | 0.19652    |
| AveragePolicyStd[0]  | 0.21049    |
| AveragePolicyStd[1]  | 0.21294    |
| AveragePolicyStd[2]  | 0.15147    |
| AveragePolicyStd[3]  | 0.19367    |
| AveragePolicyStd[4]  | 0.16947    |
| AveragePolicyStd[5]  | 0.24111    |
| AverageReturn        | 1591.7     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 394.05     |
| AverageEpisodeLength | 907.1      |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.03     |
| TotalNEpisodes       | 22571      |
| TotalNSamples        | 5.6467e+06 |
| ExplainedVariance    | 0.066529   |
-------------------------------------
[2018-01-21 15:28:37.312463 UTC] Saving snapshot
[2018-01-21 15:28:37.312761 UTC] Starting iteration 1129
[2018-01-21 15:28:37.312986 UTC] Start collecting samples
[2018-01-21 15:28:41.694301 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:41.848700 UTC] Performing policy update
[2018-01-21 15:28:41.849596 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:41.967471 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:43.399128 UTC] Performing line search
[2018-01-21 15:28:43.592105 UTC] Updating baseline
[2018-01-21 15:28:47.872286 UTC] Computing logging information
-------------------------------------
| Iteration            | 1129       |
| ExpectedImprovement  | 0.018259   |
| ActualImprovement    | 0.01668    |
| ImprovementRatio     | 0.91354    |
| MeanKL               | 0.0080056  |
| Entropy              | -1.3173    |
| Perplexity           | 0.26786    |
| AveragePolicyStd     | 0.19654    |
| AveragePolicyStd[0]  | 0.21039    |
| AveragePolicyStd[1]  | 0.2128     |
| AveragePolicyStd[2]  | 0.15161    |
| AveragePolicyStd[3]  | 0.19371    |
| AveragePolicyStd[4]  | 0.16947    |
| AveragePolicyStd[5]  | 0.24127    |
| AverageReturn        | 1601.8     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 385.63     |
| AverageEpisodeLength | 912.15     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 209.15     |
| TotalNEpisodes       | 22573      |
| TotalNSamples        | 5.6487e+06 |
| ExplainedVariance    | -0.0010952 |
-------------------------------------
[2018-01-21 15:28:48.667177 UTC] Saving snapshot
[2018-01-21 15:28:48.667382 UTC] Starting iteration 1130
[2018-01-21 15:28:48.667560 UTC] Start collecting samples
[2018-01-21 15:28:53.199383 UTC] Computing input variables for policy optimization
[2018-01-21 15:28:53.328104 UTC] Performing policy update
[2018-01-21 15:28:53.328982 UTC] Computing gradient in Euclidean space
[2018-01-21 15:28:53.445786 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:28:54.811612 UTC] Performing line search
[2018-01-21 15:28:55.011177 UTC] Updating baseline
[2018-01-21 15:28:57.100559 UTC] Computing logging information
-------------------------------------
| Iteration            | 1130       |
| ExpectedImprovement  | 0.017762   |
| ActualImprovement    | 0.0166     |
| ImprovementRatio     | 0.93454    |
| MeanKL               | 0.0078685  |
| Entropy              | -1.32      |
| Perplexity           | 0.26714    |
| AveragePolicyStd     | 0.19646    |
| AveragePolicyStd[0]  | 0.21044    |
| AveragePolicyStd[1]  | 0.21311    |
| AveragePolicyStd[2]  | 0.15171    |
| AveragePolicyStd[3]  | 0.19327    |
| AveragePolicyStd[4]  | 0.16912    |
| AveragePolicyStd[5]  | 0.2411     |
| AverageReturn        | 1627.9     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 363.25     |
| AverageEpisodeLength | 926.2      |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.13     |
| TotalNEpisodes       | 22580      |
| TotalNSamples        | 5.6557e+06 |
| ExplainedVariance    | -5.309e-06 |
-------------------------------------
[2018-01-21 15:28:57.820439 UTC] Saving snapshot
[2018-01-21 15:28:57.830811 UTC] Starting iteration 1131
[2018-01-21 15:28:57.831046 UTC] Start collecting samples
[2018-01-21 15:29:02.441539 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:02.567767 UTC] Performing policy update
[2018-01-21 15:29:02.568500 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:02.689107 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:04.128209 UTC] Performing line search
[2018-01-21 15:29:04.323018 UTC] Updating baseline
[2018-01-21 15:29:06.212651 UTC] Computing logging information
-------------------------------------
| Iteration            | 1131       |
| ExpectedImprovement  | 0.017856   |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.92655    |
| MeanKL               | 0.0081256  |
| Entropy              | -1.3186    |
| Perplexity           | 0.2675     |
| AveragePolicyStd     | 0.19653    |
| AveragePolicyStd[0]  | 0.21017    |
| AveragePolicyStd[1]  | 0.21305    |
| AveragePolicyStd[2]  | 0.15162    |
| AveragePolicyStd[3]  | 0.19305    |
| AveragePolicyStd[4]  | 0.16931    |
| AveragePolicyStd[5]  | 0.24194    |
| AverageReturn        | 1642.1     |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 344.38     |
| AverageEpisodeLength | 933.43     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.25     |
| TotalNEpisodes       | 22586      |
| TotalNSamples        | 5.6617e+06 |
| ExplainedVariance    | -0.0039039 |
-------------------------------------
[2018-01-21 15:29:06.952404 UTC] Saving snapshot
[2018-01-21 15:29:06.952659 UTC] Starting iteration 1132
[2018-01-21 15:29:06.952851 UTC] Start collecting samples
[2018-01-21 15:29:11.520332 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:11.648834 UTC] Performing policy update
[2018-01-21 15:29:11.649447 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:11.785066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:13.217344 UTC] Performing line search
[2018-01-21 15:29:13.404878 UTC] Updating baseline
[2018-01-21 15:29:15.429766 UTC] Computing logging information
-------------------------------------
| Iteration            | 1132       |
| ExpectedImprovement  | 0.019934   |
| ActualImprovement    | 0.018175   |
| ImprovementRatio     | 0.91174    |
| MeanKL               | 0.0074532  |
| Entropy              | -1.3195    |
| Perplexity           | 0.26726    |
| AveragePolicyStd     | 0.1965     |
| AveragePolicyStd[0]  | 0.21017    |
| AveragePolicyStd[1]  | 0.21296    |
| AveragePolicyStd[2]  | 0.15124    |
| AveragePolicyStd[3]  | 0.19325    |
| AveragePolicyStd[4]  | 0.16966    |
| AveragePolicyStd[5]  | 0.2417     |
| AverageReturn        | 1625       |
| MinReturn            | 274.37     |
| MaxReturn            | 1836.3     |
| StdReturn            | 371.47     |
| AverageEpisodeLength | 924.27     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.06     |
| TotalNEpisodes       | 22590      |
| TotalNSamples        | 5.6644e+06 |
| ExplainedVariance    | 0.27761    |
-------------------------------------
[2018-01-21 15:29:16.251011 UTC] Saving snapshot
[2018-01-21 15:29:16.251260 UTC] Starting iteration 1133
[2018-01-21 15:29:16.251429 UTC] Start collecting samples
[2018-01-21 15:29:20.753034 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:20.893613 UTC] Performing policy update
[2018-01-21 15:29:20.894625 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:21.015539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:22.459131 UTC] Performing line search
[2018-01-21 15:29:22.643279 UTC] Updating baseline
[2018-01-21 15:29:24.939462 UTC] Computing logging information
-------------------------------------
| Iteration            | 1133       |
| ExpectedImprovement  | 0.017189   |
| ActualImprovement    | 0.016734   |
| ImprovementRatio     | 0.97349    |
| MeanKL               | 0.0075393  |
| Entropy              | -1.3202    |
| Perplexity           | 0.26709    |
| AveragePolicyStd     | 0.19647    |
| AveragePolicyStd[0]  | 0.21022    |
| AveragePolicyStd[1]  | 0.21275    |
| AveragePolicyStd[2]  | 0.15138    |
| AveragePolicyStd[3]  | 0.19334    |
| AveragePolicyStd[4]  | 0.16941    |
| AveragePolicyStd[5]  | 0.24174    |
| AverageReturn        | 1634.8     |
| MinReturn            | 274.37     |
| MaxReturn            | 1842.8     |
| StdReturn            | 366.43     |
| AverageEpisodeLength | 928.32     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.45     |
| TotalNEpisodes       | 22596      |
| TotalNSamples        | 5.6704e+06 |
| ExplainedVariance    | -0.044659  |
-------------------------------------
[2018-01-21 15:29:25.684151 UTC] Saving snapshot
[2018-01-21 15:29:25.684436 UTC] Starting iteration 1134
[2018-01-21 15:29:25.684627 UTC] Start collecting samples
[2018-01-21 15:29:30.628679 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:30.796516 UTC] Performing policy update
[2018-01-21 15:29:30.797116 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:30.921593 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:32.401422 UTC] Performing line search
[2018-01-21 15:29:32.596751 UTC] Updating baseline
[2018-01-21 15:29:34.414936 UTC] Computing logging information
-------------------------------------
| Iteration            | 1134       |
| ExpectedImprovement  | 0.01958    |
| ActualImprovement    | 0.018194   |
| ImprovementRatio     | 0.92921    |
| MeanKL               | 0.0068401  |
| Entropy              | -1.3149    |
| Perplexity           | 0.26851    |
| AveragePolicyStd     | 0.19669    |
| AveragePolicyStd[0]  | 0.21036    |
| AveragePolicyStd[1]  | 0.21314    |
| AveragePolicyStd[2]  | 0.15107    |
| AveragePolicyStd[3]  | 0.19371    |
| AveragePolicyStd[4]  | 0.1694     |
| AveragePolicyStd[5]  | 0.24247    |
| AverageReturn        | 1625.3     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 403.88     |
| AverageEpisodeLength | 923.89     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.06     |
| TotalNEpisodes       | 22604      |
| TotalNSamples        | 5.6766e+06 |
| ExplainedVariance    | 0.2229     |
-------------------------------------
[2018-01-21 15:29:35.231553 UTC] Saving snapshot
[2018-01-21 15:29:35.231856 UTC] Starting iteration 1135
[2018-01-21 15:29:35.232081 UTC] Start collecting samples
[2018-01-21 15:29:40.146944 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:40.302822 UTC] Performing policy update
[2018-01-21 15:29:40.303754 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:40.428269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:41.828831 UTC] Performing line search
[2018-01-21 15:29:42.017168 UTC] Updating baseline
[2018-01-21 15:29:43.968947 UTC] Computing logging information
-------------------------------------
| Iteration            | 1135       |
| ExpectedImprovement  | 0.017975   |
| ActualImprovement    | 0.017042   |
| ImprovementRatio     | 0.94808    |
| MeanKL               | 0.0078917  |
| Entropy              | -1.3217    |
| Perplexity           | 0.26668    |
| AveragePolicyStd     | 0.19644    |
| AveragePolicyStd[0]  | 0.2102     |
| AveragePolicyStd[1]  | 0.21249    |
| AveragePolicyStd[2]  | 0.15108    |
| AveragePolicyStd[3]  | 0.1935     |
| AveragePolicyStd[4]  | 0.16945    |
| AveragePolicyStd[5]  | 0.24193    |
| AverageReturn        | 1626.2     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 400.3      |
| AverageEpisodeLength | 924.42     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.35     |
| TotalNEpisodes       | 22609      |
| TotalNSamples        | 5.6809e+06 |
| ExplainedVariance    | 0.051372   |
-------------------------------------
[2018-01-21 15:29:44.796744 UTC] Saving snapshot
[2018-01-21 15:29:44.796977 UTC] Starting iteration 1136
[2018-01-21 15:29:44.797124 UTC] Start collecting samples
[2018-01-21 15:29:49.297291 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:49.432651 UTC] Performing policy update
[2018-01-21 15:29:49.433701 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:49.550956 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:29:50.958570 UTC] Performing line search
[2018-01-21 15:29:51.157864 UTC] Updating baseline
[2018-01-21 15:29:53.667569 UTC] Computing logging information
--------------------------------------
| Iteration            | 1136        |
| ExpectedImprovement  | 0.017384    |
| ActualImprovement    | 0.016421    |
| ImprovementRatio     | 0.94462     |
| MeanKL               | 0.0080114   |
| Entropy              | -1.3229     |
| Perplexity           | 0.26636     |
| AveragePolicyStd     | 0.1964      |
| AveragePolicyStd[0]  | 0.21013     |
| AveragePolicyStd[1]  | 0.21266     |
| AveragePolicyStd[2]  | 0.15137     |
| AveragePolicyStd[3]  | 0.19399     |
| AveragePolicyStd[4]  | 0.16867     |
| AveragePolicyStd[5]  | 0.24156     |
| AverageReturn        | 1624.8      |
| MinReturn            | 44.958      |
| MaxReturn            | 1842.8      |
| StdReturn            | 399.77      |
| AverageEpisodeLength | 924.42      |
| MinEpisodeLength     | 55          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 217.35      |
| TotalNEpisodes       | 22614       |
| TotalNSamples        | 5.6859e+06  |
| ExplainedVariance    | -0.00077012 |
--------------------------------------
[2018-01-21 15:29:54.428568 UTC] Saving snapshot
[2018-01-21 15:29:54.428971 UTC] Starting iteration 1137
[2018-01-21 15:29:54.429186 UTC] Start collecting samples
[2018-01-21 15:29:59.036643 UTC] Computing input variables for policy optimization
[2018-01-21 15:29:59.178987 UTC] Performing policy update
[2018-01-21 15:29:59.179632 UTC] Computing gradient in Euclidean space
[2018-01-21 15:29:59.299527 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:00.680744 UTC] Performing line search
[2018-01-21 15:30:00.885713 UTC] Updating baseline
[2018-01-21 15:30:02.799234 UTC] Computing logging information
-------------------------------------
| Iteration            | 1137       |
| ExpectedImprovement  | 0.018158   |
| ActualImprovement    | 0.018071   |
| ImprovementRatio     | 0.99517    |
| MeanKL               | 0.0072341  |
| Entropy              | -1.3232    |
| Perplexity           | 0.26627    |
| AveragePolicyStd     | 0.19637    |
| AveragePolicyStd[0]  | 0.21       |
| AveragePolicyStd[1]  | 0.21247    |
| AveragePolicyStd[2]  | 0.15153    |
| AveragePolicyStd[3]  | 0.19436    |
| AveragePolicyStd[4]  | 0.16854    |
| AveragePolicyStd[5]  | 0.24131    |
| AverageReturn        | 1626.4     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 400.26     |
| AverageEpisodeLength | 924.42     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.35     |
| TotalNEpisodes       | 22620      |
| TotalNSamples        | 5.6919e+06 |
| ExplainedVariance    | 0.0079487  |
-------------------------------------
[2018-01-21 15:30:03.517058 UTC] Saving snapshot
[2018-01-21 15:30:03.517305 UTC] Starting iteration 1138
[2018-01-21 15:30:03.517481 UTC] Start collecting samples
[2018-01-21 15:30:08.404480 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:08.535932 UTC] Performing policy update
[2018-01-21 15:30:08.536519 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:08.653343 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:10.068211 UTC] Performing line search
[2018-01-21 15:30:10.258639 UTC] Updating baseline
[2018-01-21 15:30:12.640174 UTC] Computing logging information
-------------------------------------
| Iteration            | 1138       |
| ExpectedImprovement  | 0.017371   |
| ActualImprovement    | 0.016306   |
| ImprovementRatio     | 0.93871    |
| MeanKL               | 0.0078841  |
| Entropy              | -1.3219    |
| Perplexity           | 0.26664    |
| AveragePolicyStd     | 0.19642    |
| AveragePolicyStd[0]  | 0.21056    |
| AveragePolicyStd[1]  | 0.21258    |
| AveragePolicyStd[2]  | 0.15168    |
| AveragePolicyStd[3]  | 0.19436    |
| AveragePolicyStd[4]  | 0.16819    |
| AveragePolicyStd[5]  | 0.24114    |
| AverageReturn        | 1599.8     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 432.74     |
| AverageEpisodeLength | 910.13     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 234.7      |
| TotalNEpisodes       | 22626      |
| TotalNSamples        | 5.6964e+06 |
| ExplainedVariance    | 0.18674    |
-------------------------------------
[2018-01-21 15:30:13.428937 UTC] Saving snapshot
[2018-01-21 15:30:13.429140 UTC] Starting iteration 1139
[2018-01-21 15:30:13.429317 UTC] Start collecting samples
[2018-01-21 15:30:18.083219 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:18.232756 UTC] Performing policy update
[2018-01-21 15:30:18.233359 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:18.358423 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:19.772586 UTC] Performing line search
[2018-01-21 15:30:19.970566 UTC] Updating baseline
[2018-01-21 15:30:22.453441 UTC] Computing logging information
-------------------------------------
| Iteration            | 1139       |
| ExpectedImprovement  | 0.017246   |
| ActualImprovement    | 0.016121   |
| ImprovementRatio     | 0.93476    |
| MeanKL               | 0.0076083  |
| Entropy              | -1.3201    |
| Perplexity           | 0.26712    |
| AveragePolicyStd     | 0.1965     |
| AveragePolicyStd[0]  | 0.21034    |
| AveragePolicyStd[1]  | 0.21313    |
| AveragePolicyStd[2]  | 0.15155    |
| AveragePolicyStd[3]  | 0.19434    |
| AveragePolicyStd[4]  | 0.16814    |
| AveragePolicyStd[5]  | 0.24151    |
| AverageReturn        | 1607.9     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 426.28     |
| AverageEpisodeLength | 915.04     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.37     |
| TotalNEpisodes       | 22632      |
| TotalNSamples        | 5.7024e+06 |
| ExplainedVariance    | 0.0021508  |
-------------------------------------
[2018-01-21 15:30:23.188243 UTC] Saving snapshot
[2018-01-21 15:30:23.188529 UTC] Starting iteration 1140
[2018-01-21 15:30:23.188712 UTC] Start collecting samples
[2018-01-21 15:30:27.762042 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:27.918079 UTC] Performing policy update
[2018-01-21 15:30:27.918787 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:28.055334 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:29.505723 UTC] Performing line search
[2018-01-21 15:30:29.697419 UTC] Updating baseline
[2018-01-21 15:30:32.073730 UTC] Computing logging information
-------------------------------------
| Iteration            | 1140       |
| ExpectedImprovement  | 0.02203    |
| ActualImprovement    | 0.019618   |
| ImprovementRatio     | 0.8905     |
| MeanKL               | 0.0070659  |
| Entropy              | -1.3197    |
| Perplexity           | 0.26721    |
| AveragePolicyStd     | 0.19653    |
| AveragePolicyStd[0]  | 0.21039    |
| AveragePolicyStd[1]  | 0.21294    |
| AveragePolicyStd[2]  | 0.15132    |
| AveragePolicyStd[3]  | 0.19517    |
| AveragePolicyStd[4]  | 0.1678     |
| AveragePolicyStd[5]  | 0.24157    |
| AverageReturn        | 1606.7     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 426.23     |
| AverageEpisodeLength | 915.04     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 231.37     |
| TotalNEpisodes       | 22635      |
| TotalNSamples        | 5.7054e+06 |
| ExplainedVariance    | 0.0060668  |
-------------------------------------
[2018-01-21 15:30:32.832777 UTC] Saving snapshot
[2018-01-21 15:30:32.840116 UTC] Starting iteration 1141
[2018-01-21 15:30:32.840440 UTC] Start collecting samples
[2018-01-21 15:30:37.357128 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:37.489349 UTC] Performing policy update
[2018-01-21 15:30:37.490093 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:37.611783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:39.013746 UTC] Performing line search
[2018-01-21 15:30:39.204187 UTC] Updating baseline
[2018-01-21 15:30:42.366169 UTC] Computing logging information
-------------------------------------
| Iteration            | 1141       |
| ExpectedImprovement  | 0.017199   |
| ActualImprovement    | 0.01642    |
| ImprovementRatio     | 0.95467    |
| MeanKL               | 0.0073995  |
| Entropy              | -1.3207    |
| Perplexity           | 0.26694    |
| AveragePolicyStd     | 0.19649    |
| AveragePolicyStd[0]  | 0.21031    |
| AveragePolicyStd[1]  | 0.21291    |
| AveragePolicyStd[2]  | 0.15157    |
| AveragePolicyStd[3]  | 0.19522    |
| AveragePolicyStd[4]  | 0.16752    |
| AveragePolicyStd[5]  | 0.2414     |
| AverageReturn        | 1617.7     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 412.22     |
| AverageEpisodeLength | 922.36     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.34     |
| TotalNEpisodes       | 22641      |
| TotalNSamples        | 5.7114e+06 |
| ExplainedVariance    | -0.0017798 |
-------------------------------------
[2018-01-21 15:30:43.216103 UTC] Saving snapshot
[2018-01-21 15:30:43.216355 UTC] Starting iteration 1142
[2018-01-21 15:30:43.216520 UTC] Start collecting samples
[2018-01-21 15:30:47.766496 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:47.888451 UTC] Performing policy update
[2018-01-21 15:30:47.889122 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:48.004743 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:49.398475 UTC] Performing line search
[2018-01-21 15:30:49.599766 UTC] Updating baseline
[2018-01-21 15:30:51.498067 UTC] Computing logging information
-------------------------------------
| Iteration            | 1142       |
| ExpectedImprovement  | 0.019972   |
| ActualImprovement    | 0.01866    |
| ImprovementRatio     | 0.93428    |
| MeanKL               | 0.0079142  |
| Entropy              | -1.3255    |
| Perplexity           | 0.26567    |
| AveragePolicyStd     | 0.19637    |
| AveragePolicyStd[0]  | 0.2102     |
| AveragePolicyStd[1]  | 0.2127     |
| AveragePolicyStd[2]  | 0.151      |
| AveragePolicyStd[3]  | 0.19475    |
| AveragePolicyStd[4]  | 0.16766    |
| AveragePolicyStd[5]  | 0.24191    |
| AverageReturn        | 1620.1     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 407.99     |
| AverageEpisodeLength | 924.23     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.93     |
| TotalNEpisodes       | 22648      |
| TotalNSamples        | 5.7182e+06 |
| ExplainedVariance    | 0.070188   |
-------------------------------------
[2018-01-21 15:30:52.308774 UTC] Saving snapshot
[2018-01-21 15:30:52.309058 UTC] Starting iteration 1143
[2018-01-21 15:30:52.309246 UTC] Start collecting samples
[2018-01-21 15:30:56.801843 UTC] Computing input variables for policy optimization
[2018-01-21 15:30:56.934743 UTC] Performing policy update
[2018-01-21 15:30:56.935440 UTC] Computing gradient in Euclidean space
[2018-01-21 15:30:57.068559 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:30:58.464323 UTC] Performing line search
[2018-01-21 15:30:58.650733 UTC] Updating baseline
[2018-01-21 15:31:00.731742 UTC] Computing logging information
-------------------------------------
| Iteration            | 1143       |
| ExpectedImprovement  | 0.019551   |
| ActualImprovement    | 0.017579   |
| ImprovementRatio     | 0.89914    |
| MeanKL               | 0.0076779  |
| Entropy              | -1.331     |
| Perplexity           | 0.2642     |
| AveragePolicyStd     | 0.19619    |
| AveragePolicyStd[0]  | 0.21023    |
| AveragePolicyStd[1]  | 0.21277    |
| AveragePolicyStd[2]  | 0.15061    |
| AveragePolicyStd[3]  | 0.19427    |
| AveragePolicyStd[4]  | 0.16787    |
| AveragePolicyStd[5]  | 0.24136    |
| AverageReturn        | 1620.4     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 408.1      |
| AverageEpisodeLength | 924.23     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.93     |
| TotalNEpisodes       | 22651      |
| TotalNSamples        | 5.7212e+06 |
| ExplainedVariance    | 0.0031558  |
-------------------------------------
[2018-01-21 15:31:01.481392 UTC] Saving snapshot
[2018-01-21 15:31:01.481690 UTC] Starting iteration 1144
[2018-01-21 15:31:01.481869 UTC] Start collecting samples
[2018-01-21 15:31:05.971833 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:06.130406 UTC] Performing policy update
[2018-01-21 15:31:06.131084 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:06.259646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:07.671957 UTC] Performing line search
[2018-01-21 15:31:07.867888 UTC] Updating baseline
[2018-01-21 15:31:10.212396 UTC] Computing logging information
-------------------------------------
| Iteration            | 1144       |
| ExpectedImprovement  | 0.017418   |
| ActualImprovement    | 0.016661   |
| ImprovementRatio     | 0.95655    |
| MeanKL               | 0.0078766  |
| Entropy              | -1.3354    |
| Perplexity           | 0.26307    |
| AveragePolicyStd     | 0.19606    |
| AveragePolicyStd[0]  | 0.21011    |
| AveragePolicyStd[1]  | 0.21259    |
| AveragePolicyStd[2]  | 0.15052    |
| AveragePolicyStd[3]  | 0.19379    |
| AveragePolicyStd[4]  | 0.16774    |
| AveragePolicyStd[5]  | 0.2416     |
| AverageReturn        | 1630.5     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 397.37     |
| AverageEpisodeLength | 930.16     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.87     |
| TotalNEpisodes       | 22656      |
| TotalNSamples        | 5.7262e+06 |
| ExplainedVariance    | 0.023855   |
-------------------------------------
[2018-01-21 15:31:11.041258 UTC] Saving snapshot
[2018-01-21 15:31:11.041805 UTC] Starting iteration 1145
[2018-01-21 15:31:11.041994 UTC] Start collecting samples
[2018-01-21 15:31:15.658129 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:15.787036 UTC] Performing policy update
[2018-01-21 15:31:15.787700 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:15.909365 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:17.326406 UTC] Performing line search
[2018-01-21 15:31:17.528032 UTC] Updating baseline
[2018-01-21 15:31:19.807480 UTC] Computing logging information
-------------------------------------
| Iteration            | 1145       |
| ExpectedImprovement  | 0.018872   |
| ActualImprovement    | 0.017596   |
| ImprovementRatio     | 0.93241    |
| MeanKL               | 0.0073156  |
| Entropy              | -1.3389    |
| Perplexity           | 0.26213    |
| AveragePolicyStd     | 0.19597    |
| AveragePolicyStd[0]  | 0.20948    |
| AveragePolicyStd[1]  | 0.21229    |
| AveragePolicyStd[2]  | 0.15028    |
| AveragePolicyStd[3]  | 0.19408    |
| AveragePolicyStd[4]  | 0.16752    |
| AveragePolicyStd[5]  | 0.24214    |
| AverageReturn        | 1622.8     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 400.77     |
| AverageEpisodeLength | 926.44     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.83     |
| TotalNEpisodes       | 22662      |
| TotalNSamples        | 5.7318e+06 |
| ExplainedVariance    | 0.10695    |
-------------------------------------
[2018-01-21 15:31:20.529045 UTC] Saving snapshot
[2018-01-21 15:31:20.529378 UTC] Starting iteration 1146
[2018-01-21 15:31:20.529555 UTC] Start collecting samples
[2018-01-21 15:31:25.057166 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:25.208650 UTC] Performing policy update
[2018-01-21 15:31:25.209255 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:25.326709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:26.775533 UTC] Performing line search
[2018-01-21 15:31:26.972068 UTC] Updating baseline
[2018-01-21 15:31:28.872710 UTC] Computing logging information
-------------------------------------
| Iteration            | 1146       |
| ExpectedImprovement  | 0.019954   |
| ActualImprovement    | 0.018586   |
| ImprovementRatio     | 0.93146    |
| MeanKL               | 0.0076039  |
| Entropy              | -1.3408    |
| Perplexity           | 0.26163    |
| AveragePolicyStd     | 0.19587    |
| AveragePolicyStd[0]  | 0.20996    |
| AveragePolicyStd[1]  | 0.21242    |
| AveragePolicyStd[2]  | 0.15057    |
| AveragePolicyStd[3]  | 0.19411    |
| AveragePolicyStd[4]  | 0.16712    |
| AveragePolicyStd[5]  | 0.24106    |
| AverageReturn        | 1637.2     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 382.79     |
| AverageEpisodeLength | 933.72     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.77     |
| TotalNEpisodes       | 22666      |
| TotalNSamples        | 5.7358e+06 |
| ExplainedVariance    | -0.024381  |
-------------------------------------
[2018-01-21 15:31:29.673239 UTC] Saving snapshot
[2018-01-21 15:31:29.673502 UTC] Starting iteration 1147
[2018-01-21 15:31:29.673680 UTC] Start collecting samples
[2018-01-21 15:31:34.402349 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:34.535875 UTC] Performing policy update
[2018-01-21 15:31:34.536497 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:34.652352 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:36.033103 UTC] Performing line search
[2018-01-21 15:31:36.230075 UTC] Updating baseline
[2018-01-21 15:31:38.369876 UTC] Computing logging information
-------------------------------------
| Iteration            | 1147       |
| ExpectedImprovement  | 0.017647   |
| ActualImprovement    | 0.016524   |
| ImprovementRatio     | 0.93636    |
| MeanKL               | 0.0074453  |
| Entropy              | -1.3511    |
| Perplexity           | 0.25897    |
| AveragePolicyStd     | 0.19551    |
| AveragePolicyStd[0]  | 0.20985    |
| AveragePolicyStd[1]  | 0.21219    |
| AveragePolicyStd[2]  | 0.15018    |
| AveragePolicyStd[3]  | 0.19437    |
| AveragePolicyStd[4]  | 0.16697    |
| AveragePolicyStd[5]  | 0.23951    |
| AverageReturn        | 1641.6     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 370.8      |
| AverageEpisodeLength | 936.12     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.22     |
| TotalNEpisodes       | 22671      |
| TotalNSamples        | 5.7403e+06 |
| ExplainedVariance    | 0.11451    |
-------------------------------------
[2018-01-21 15:31:39.201945 UTC] Saving snapshot
[2018-01-21 15:31:39.202239 UTC] Starting iteration 1148
[2018-01-21 15:31:39.202439 UTC] Start collecting samples
[2018-01-21 15:31:43.789780 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:43.919862 UTC] Performing policy update
[2018-01-21 15:31:43.920588 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:44.039853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:45.444185 UTC] Performing line search
[2018-01-21 15:31:45.632286 UTC] Updating baseline
[2018-01-21 15:31:49.560721 UTC] Computing logging information
--------------------------------------
| Iteration            | 1148        |
| ExpectedImprovement  | 0.019159    |
| ActualImprovement    | 0.018267    |
| ImprovementRatio     | 0.95347     |
| MeanKL               | 0.0082266   |
| Entropy              | -1.3492     |
| Perplexity           | 0.25945     |
| AveragePolicyStd     | 0.19557     |
| AveragePolicyStd[0]  | 0.20967     |
| AveragePolicyStd[1]  | 0.21213     |
| AveragePolicyStd[2]  | 0.15033     |
| AveragePolicyStd[3]  | 0.1947      |
| AveragePolicyStd[4]  | 0.16691     |
| AveragePolicyStd[5]  | 0.23966     |
| AverageReturn        | 1643.6      |
| MinReturn            | 44.958      |
| MaxReturn            | 1842.8      |
| StdReturn            | 371.46      |
| AverageEpisodeLength | 936.12      |
| MinEpisodeLength     | 55          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 201.22      |
| TotalNEpisodes       | 22676       |
| TotalNSamples        | 5.7453e+06  |
| ExplainedVariance    | -3.9115e-05 |
--------------------------------------
[2018-01-21 15:31:50.384593 UTC] Saving snapshot
[2018-01-21 15:31:50.384829 UTC] Starting iteration 1149
[2018-01-21 15:31:50.385011 UTC] Start collecting samples
[2018-01-21 15:31:55.069387 UTC] Computing input variables for policy optimization
[2018-01-21 15:31:55.211314 UTC] Performing policy update
[2018-01-21 15:31:55.212014 UTC] Computing gradient in Euclidean space
[2018-01-21 15:31:55.334952 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:31:56.736078 UTC] Performing line search
[2018-01-21 15:31:56.927386 UTC] Updating baseline
[2018-01-21 15:32:00.664589 UTC] Computing logging information
-------------------------------------
| Iteration            | 1149       |
| ExpectedImprovement  | 0.019067   |
| ActualImprovement    | 0.0181     |
| ImprovementRatio     | 0.94927    |
| MeanKL               | 0.0078895  |
| Entropy              | -1.3514    |
| Perplexity           | 0.25889    |
| AveragePolicyStd     | 0.19546    |
| AveragePolicyStd[0]  | 0.20921    |
| AveragePolicyStd[1]  | 0.21167    |
| AveragePolicyStd[2]  | 0.15078    |
| AveragePolicyStd[3]  | 0.19476    |
| AveragePolicyStd[4]  | 0.16686    |
| AveragePolicyStd[5]  | 0.23947    |
| AverageReturn        | 1644.5     |
| MinReturn            | 44.958     |
| MaxReturn            | 1842.8     |
| StdReturn            | 371.73     |
| AverageEpisodeLength | 936.12     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.22     |
| TotalNEpisodes       | 22682      |
| TotalNSamples        | 5.7513e+06 |
| ExplainedVariance    | -0.0049219 |
-------------------------------------
[2018-01-21 15:32:01.492109 UTC] Saving snapshot
[2018-01-21 15:32:01.492349 UTC] Starting iteration 1150
[2018-01-21 15:32:01.492523 UTC] Start collecting samples
[2018-01-21 15:32:06.017831 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:06.146993 UTC] Performing policy update
[2018-01-21 15:32:06.147622 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:06.279359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:07.740810 UTC] Performing line search
[2018-01-21 15:32:07.952104 UTC] Updating baseline
[2018-01-21 15:32:10.365352 UTC] Computing logging information
-------------------------------------
| Iteration            | 1150       |
| ExpectedImprovement  | 0.018058   |
| ActualImprovement    | 0.01693    |
| ImprovementRatio     | 0.93753    |
| MeanKL               | 0.0078624  |
| Entropy              | -1.3544    |
| Perplexity           | 0.25809    |
| AveragePolicyStd     | 0.19535    |
| AveragePolicyStd[0]  | 0.20859    |
| AveragePolicyStd[1]  | 0.21138    |
| AveragePolicyStd[2]  | 0.15092    |
| AveragePolicyStd[3]  | 0.19519    |
| AveragePolicyStd[4]  | 0.16651    |
| AveragePolicyStd[5]  | 0.23955    |
| AverageReturn        | 1635.4     |
| MinReturn            | 44.958     |
| MaxReturn            | 1847.7     |
| StdReturn            | 378.76     |
| AverageEpisodeLength | 930.83     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.95     |
| TotalNEpisodes       | 22687      |
| TotalNSamples        | 5.7558e+06 |
| ExplainedVariance    | 0.18719    |
-------------------------------------
[2018-01-21 15:32:11.154940 UTC] Saving snapshot
[2018-01-21 15:32:11.163821 UTC] Starting iteration 1151
[2018-01-21 15:32:11.164023 UTC] Start collecting samples
[2018-01-21 15:32:15.738824 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:15.881199 UTC] Performing policy update
[2018-01-21 15:32:15.882223 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:16.004152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:17.381461 UTC] Performing line search
[2018-01-21 15:32:17.568966 UTC] Updating baseline
[2018-01-21 15:32:19.866523 UTC] Computing logging information
-------------------------------------
| Iteration            | 1151       |
| ExpectedImprovement  | 0.018084   |
| ActualImprovement    | 0.016872   |
| ImprovementRatio     | 0.93299    |
| MeanKL               | 0.0076948  |
| Entropy              | -1.356     |
| Perplexity           | 0.25768    |
| AveragePolicyStd     | 0.19534    |
| AveragePolicyStd[0]  | 0.20868    |
| AveragePolicyStd[1]  | 0.21118    |
| AveragePolicyStd[2]  | 0.15045    |
| AveragePolicyStd[3]  | 0.19514    |
| AveragePolicyStd[4]  | 0.16646    |
| AveragePolicyStd[5]  | 0.24013    |
| AverageReturn        | 1653.5     |
| MinReturn            | 44.958     |
| MaxReturn            | 1847.7     |
| StdReturn            | 349.7      |
| AverageEpisodeLength | 940.48     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.34     |
| TotalNEpisodes       | 22694      |
| TotalNSamples        | 5.7624e+06 |
| ExplainedVariance    | 0.11629    |
-------------------------------------
[2018-01-21 15:32:20.674829 UTC] Saving snapshot
[2018-01-21 15:32:20.675065 UTC] Starting iteration 1152
[2018-01-21 15:32:20.675213 UTC] Start collecting samples
[2018-01-21 15:32:25.389352 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:25.507730 UTC] Performing policy update
[2018-01-21 15:32:25.508882 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:25.624609 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:27.041964 UTC] Performing line search
[2018-01-21 15:32:27.229899 UTC] Updating baseline
[2018-01-21 15:32:29.256189 UTC] Computing logging information
-------------------------------------
| Iteration            | 1152       |
| ExpectedImprovement  | 0.017917   |
| ActualImprovement    | 0.016913   |
| ImprovementRatio     | 0.94399    |
| MeanKL               | 0.0079495  |
| Entropy              | -1.3633    |
| Perplexity           | 0.25582    |
| AveragePolicyStd     | 0.1951     |
| AveragePolicyStd[0]  | 0.20795    |
| AveragePolicyStd[1]  | 0.21089    |
| AveragePolicyStd[2]  | 0.15039    |
| AveragePolicyStd[3]  | 0.19495    |
| AveragePolicyStd[4]  | 0.16633    |
| AveragePolicyStd[5]  | 0.24008    |
| AverageReturn        | 1654.2     |
| MinReturn            | 44.958     |
| MaxReturn            | 1847.7     |
| StdReturn            | 349.97     |
| AverageEpisodeLength | 940.48     |
| MinEpisodeLength     | 55         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.34     |
| TotalNEpisodes       | 22698      |
| TotalNSamples        | 5.7664e+06 |
| ExplainedVariance    | -0.030492  |
-------------------------------------
[2018-01-21 15:32:29.995282 UTC] Saving snapshot
[2018-01-21 15:32:29.995533 UTC] Starting iteration 1153
[2018-01-21 15:32:29.995714 UTC] Start collecting samples
[2018-01-21 15:32:34.450997 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:34.587536 UTC] Performing policy update
[2018-01-21 15:32:34.588329 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:34.703543 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:36.110834 UTC] Performing line search
[2018-01-21 15:32:36.300521 UTC] Updating baseline
[2018-01-21 15:32:39.471024 UTC] Computing logging information
-------------------------------------
| Iteration            | 1153       |
| ExpectedImprovement  | 0.017397   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.96547    |
| MeanKL               | 0.0080145  |
| Entropy              | -1.364     |
| Perplexity           | 0.25565    |
| AveragePolicyStd     | 0.19508    |
| AveragePolicyStd[0]  | 0.2082     |
| AveragePolicyStd[1]  | 0.21079    |
| AveragePolicyStd[2]  | 0.15058    |
| AveragePolicyStd[3]  | 0.19431    |
| AveragePolicyStd[4]  | 0.16627    |
| AveragePolicyStd[5]  | 0.24033    |
| AverageReturn        | 1674.4     |
| MinReturn            | 271.58     |
| MaxReturn            | 1847.7     |
| StdReturn            | 311.34     |
| AverageEpisodeLength | 949.93     |
| MinEpisodeLength     | 192        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.2      |
| TotalNEpisodes       | 22703      |
| TotalNSamples        | 5.7714e+06 |
| ExplainedVariance    | 0.00033883 |
-------------------------------------
[2018-01-21 15:32:40.263484 UTC] Saving snapshot
[2018-01-21 15:32:40.263747 UTC] Starting iteration 1154
[2018-01-21 15:32:40.263915 UTC] Start collecting samples
[2018-01-21 15:32:44.887907 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:45.022843 UTC] Performing policy update
[2018-01-21 15:32:45.023442 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:45.141303 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:46.540479 UTC] Performing line search
[2018-01-21 15:32:46.729404 UTC] Updating baseline
[2018-01-21 15:32:49.245776 UTC] Computing logging information
-------------------------------------
| Iteration            | 1154       |
| ExpectedImprovement  | 0.016807   |
| ActualImprovement    | 0.015613   |
| ImprovementRatio     | 0.92895    |
| MeanKL               | 0.0079486  |
| Entropy              | -1.3631    |
| Perplexity           | 0.25586    |
| AveragePolicyStd     | 0.19513    |
| AveragePolicyStd[0]  | 0.20821    |
| AveragePolicyStd[1]  | 0.2109     |
| AveragePolicyStd[2]  | 0.15071    |
| AveragePolicyStd[3]  | 0.19386    |
| AveragePolicyStd[4]  | 0.16612    |
| AveragePolicyStd[5]  | 0.24096    |
| AverageReturn        | 1701.4     |
| MinReturn            | 419.05     |
| MaxReturn            | 1847.7     |
| StdReturn            | 244.79     |
| AverageEpisodeLength | 964.67     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.63     |
| TotalNEpisodes       | 22709      |
| TotalNSamples        | 5.7773e+06 |
| ExplainedVariance    | 0.08121    |
-------------------------------------
[2018-01-21 15:32:50.051577 UTC] Saving snapshot
[2018-01-21 15:32:50.051861 UTC] Starting iteration 1155
[2018-01-21 15:32:50.052042 UTC] Start collecting samples
[2018-01-21 15:32:54.527467 UTC] Computing input variables for policy optimization
[2018-01-21 15:32:54.658240 UTC] Performing policy update
[2018-01-21 15:32:54.659464 UTC] Computing gradient in Euclidean space
[2018-01-21 15:32:54.774730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:32:56.189353 UTC] Performing line search
[2018-01-21 15:32:56.380633 UTC] Updating baseline
[2018-01-21 15:32:58.494621 UTC] Computing logging information
--------------------------------------
| Iteration            | 1155        |
| ExpectedImprovement  | 0.017833    |
| ActualImprovement    | 0.017117    |
| ImprovementRatio     | 0.95982     |
| MeanKL               | 0.008361    |
| Entropy              | -1.3544     |
| Perplexity           | 0.25811     |
| AveragePolicyStd     | 0.1954      |
| AveragePolicyStd[0]  | 0.20874     |
| AveragePolicyStd[1]  | 0.21115     |
| AveragePolicyStd[2]  | 0.15087     |
| AveragePolicyStd[3]  | 0.19426     |
| AveragePolicyStd[4]  | 0.16645     |
| AveragePolicyStd[5]  | 0.24093     |
| AverageReturn        | 1703.1      |
| MinReturn            | 419.05      |
| MaxReturn            | 1847.7      |
| StdReturn            | 245.22      |
| AverageEpisodeLength | 964.67      |
| MinEpisodeLength     | 280         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 130.63      |
| TotalNEpisodes       | 22714       |
| TotalNSamples        | 5.7823e+06  |
| ExplainedVariance    | -0.00097939 |
--------------------------------------
[2018-01-21 15:32:59.257182 UTC] Saving snapshot
[2018-01-21 15:32:59.257397 UTC] Starting iteration 1156
[2018-01-21 15:32:59.257563 UTC] Start collecting samples
[2018-01-21 15:33:03.714609 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:03.831294 UTC] Performing policy update
[2018-01-21 15:33:03.832100 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:03.952327 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:05.355980 UTC] Performing line search
[2018-01-21 15:33:05.552691 UTC] Updating baseline
[2018-01-21 15:33:07.471897 UTC] Computing logging information
-------------------------------------
| Iteration            | 1156       |
| ExpectedImprovement  | 0.017634   |
| ActualImprovement    | 0.015415   |
| ImprovementRatio     | 0.87413    |
| MeanKL               | 0.0074214  |
| Entropy              | -1.3517    |
| Perplexity           | 0.25881    |
| AveragePolicyStd     | 0.19549    |
| AveragePolicyStd[0]  | 0.20809    |
| AveragePolicyStd[1]  | 0.21127    |
| AveragePolicyStd[2]  | 0.15114    |
| AveragePolicyStd[3]  | 0.19471    |
| AveragePolicyStd[4]  | 0.16632    |
| AveragePolicyStd[5]  | 0.24141    |
| AverageReturn        | 1702.3     |
| MinReturn            | 419.05     |
| MaxReturn            | 1847.7     |
| StdReturn            | 245.06     |
| AverageEpisodeLength | 964.67     |
| MinEpisodeLength     | 280        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.63     |
| TotalNEpisodes       | 22715      |
| TotalNSamples        | 5.7833e+06 |
| ExplainedVariance    | 1.912e-08  |
-------------------------------------
[2018-01-21 15:33:08.287283 UTC] Saving snapshot
[2018-01-21 15:33:08.287551 UTC] Starting iteration 1157
[2018-01-21 15:33:08.287739 UTC] Start collecting samples
[2018-01-21 15:33:12.888567 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:13.028128 UTC] Performing policy update
[2018-01-21 15:33:13.028716 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:13.146934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:14.537846 UTC] Performing line search
[2018-01-21 15:33:14.745175 UTC] Updating baseline
[2018-01-21 15:33:16.929881 UTC] Computing logging information
-------------------------------------
| Iteration            | 1157       |
| ExpectedImprovement  | 0.017144   |
| ActualImprovement    | 0.016452   |
| ImprovementRatio     | 0.95966    |
| MeanKL               | 0.0074338  |
| Entropy              | -1.3565    |
| Perplexity           | 0.25757    |
| AveragePolicyStd     | 0.19535    |
| AveragePolicyStd[0]  | 0.20801    |
| AveragePolicyStd[1]  | 0.21114    |
| AveragePolicyStd[2]  | 0.15108    |
| AveragePolicyStd[3]  | 0.19479    |
| AveragePolicyStd[4]  | 0.16568    |
| AveragePolicyStd[5]  | 0.24142    |
| AverageReturn        | 1715.6     |
| MinReturn            | 53.236     |
| MaxReturn            | 1847.7     |
| StdReturn            | 236.35     |
| AverageEpisodeLength | 969.05     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.31     |
| TotalNEpisodes       | 22726      |
| TotalNSamples        | 5.7933e+06 |
| ExplainedVariance    | 0.11686    |
-------------------------------------
[2018-01-21 15:33:17.690731 UTC] Saving snapshot
[2018-01-21 15:33:17.691060 UTC] Starting iteration 1158
[2018-01-21 15:33:17.691270 UTC] Start collecting samples
[2018-01-21 15:33:22.154041 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:22.284212 UTC] Performing policy update
[2018-01-21 15:33:22.285149 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:22.412947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:23.837530 UTC] Performing line search
[2018-01-21 15:33:24.034116 UTC] Updating baseline
[2018-01-21 15:33:26.301326 UTC] Computing logging information
-------------------------------------
| Iteration            | 1158       |
| ExpectedImprovement  | 0.01723    |
| ActualImprovement    | 0.016394   |
| ImprovementRatio     | 0.95149    |
| MeanKL               | 0.0076022  |
| Entropy              | -1.3562    |
| Perplexity           | 0.25764    |
| AveragePolicyStd     | 0.19535    |
| AveragePolicyStd[0]  | 0.20784    |
| AveragePolicyStd[1]  | 0.21125    |
| AveragePolicyStd[2]  | 0.15106    |
| AveragePolicyStd[3]  | 0.19508    |
| AveragePolicyStd[4]  | 0.16572    |
| AveragePolicyStd[5]  | 0.24117    |
| AverageReturn        | 1717.6     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 236.94     |
| AverageEpisodeLength | 969.05     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.31     |
| TotalNEpisodes       | 22730      |
| TotalNSamples        | 5.7973e+06 |
| ExplainedVariance    | -0.0079838 |
-------------------------------------
[2018-01-21 15:33:27.065383 UTC] Saving snapshot
[2018-01-21 15:33:27.065702 UTC] Starting iteration 1159
[2018-01-21 15:33:27.065946 UTC] Start collecting samples
[2018-01-21 15:33:31.516405 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:31.658364 UTC] Performing policy update
[2018-01-21 15:33:31.659109 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:31.775267 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:33.174447 UTC] Performing line search
[2018-01-21 15:33:33.364432 UTC] Updating baseline
[2018-01-21 15:33:35.368099 UTC] Computing logging information
-------------------------------------
| Iteration            | 1159       |
| ExpectedImprovement  | 0.018757   |
| ActualImprovement    | 0.017575   |
| ImprovementRatio     | 0.93698    |
| MeanKL               | 0.0078919  |
| Entropy              | -1.3529    |
| Perplexity           | 0.25848    |
| AveragePolicyStd     | 0.19543    |
| AveragePolicyStd[0]  | 0.20773    |
| AveragePolicyStd[1]  | 0.21073    |
| AveragePolicyStd[2]  | 0.15145    |
| AveragePolicyStd[3]  | 0.19471    |
| AveragePolicyStd[4]  | 0.16636    |
| AveragePolicyStd[5]  | 0.24159    |
| AverageReturn        | 1705.1     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 268.64     |
| AverageEpisodeLength | 961.9      |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.43     |
| TotalNEpisodes       | 22733      |
| TotalNSamples        | 5.7996e+06 |
| ExplainedVariance    | 0.22895    |
-------------------------------------
[2018-01-21 15:33:36.117889 UTC] Saving snapshot
[2018-01-21 15:33:36.118199 UTC] Starting iteration 1160
[2018-01-21 15:33:36.118453 UTC] Start collecting samples
[2018-01-21 15:33:40.816094 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:40.959907 UTC] Performing policy update
[2018-01-21 15:33:40.960537 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:41.078728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:42.470248 UTC] Performing line search
[2018-01-21 15:33:42.657389 UTC] Updating baseline
[2018-01-21 15:33:44.797786 UTC] Computing logging information
-------------------------------------
| Iteration            | 1160       |
| ExpectedImprovement  | 0.01793    |
| ActualImprovement    | 0.016665   |
| ImprovementRatio     | 0.92944    |
| MeanKL               | 0.0076009  |
| Entropy              | -1.3558    |
| Perplexity           | 0.25775    |
| AveragePolicyStd     | 0.19528    |
| AveragePolicyStd[0]  | 0.20742    |
| AveragePolicyStd[1]  | 0.21089    |
| AveragePolicyStd[2]  | 0.15182    |
| AveragePolicyStd[3]  | 0.19397    |
| AveragePolicyStd[4]  | 0.16679    |
| AveragePolicyStd[5]  | 0.24079    |
| AverageReturn        | 1711.5     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 269.28     |
| AverageEpisodeLength | 961.9      |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.43     |
| TotalNEpisodes       | 22741      |
| TotalNSamples        | 5.8076e+06 |
| ExplainedVariance    | -0.095709  |
-------------------------------------
[2018-01-21 15:33:45.550023 UTC] Saving snapshot
[2018-01-21 15:33:45.559636 UTC] Starting iteration 1161
[2018-01-21 15:33:45.559870 UTC] Start collecting samples
[2018-01-21 15:33:49.889362 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:50.018298 UTC] Performing policy update
[2018-01-21 15:33:50.019055 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:50.137966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:33:51.612397 UTC] Performing line search
[2018-01-21 15:33:51.817257 UTC] Updating baseline
[2018-01-21 15:33:53.928499 UTC] Computing logging information
-------------------------------------
| Iteration            | 1161       |
| ExpectedImprovement  | 0.018371   |
| ActualImprovement    | 0.017138   |
| ImprovementRatio     | 0.93288    |
| MeanKL               | 0.0073797  |
| Entropy              | -1.3637    |
| Perplexity           | 0.25571    |
| AveragePolicyStd     | 0.19499    |
| AveragePolicyStd[0]  | 0.20665    |
| AveragePolicyStd[1]  | 0.21032    |
| AveragePolicyStd[2]  | 0.15176    |
| AveragePolicyStd[3]  | 0.1939     |
| AveragePolicyStd[4]  | 0.16697    |
| AveragePolicyStd[5]  | 0.24035    |
| AverageReturn        | 1716.4     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 265        |
| AverageEpisodeLength | 964.61     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.55     |
| TotalNEpisodes       | 22744      |
| TotalNSamples        | 5.8106e+06 |
| ExplainedVariance    | 0.0013489  |
-------------------------------------
[2018-01-21 15:33:54.766592 UTC] Saving snapshot
[2018-01-21 15:33:54.766857 UTC] Starting iteration 1162
[2018-01-21 15:33:54.767028 UTC] Start collecting samples
[2018-01-21 15:33:59.363414 UTC] Computing input variables for policy optimization
[2018-01-21 15:33:59.485265 UTC] Performing policy update
[2018-01-21 15:33:59.486415 UTC] Computing gradient in Euclidean space
[2018-01-21 15:33:59.604540 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:01.014875 UTC] Performing line search
[2018-01-21 15:34:01.221856 UTC] Updating baseline
[2018-01-21 15:34:03.089188 UTC] Computing logging information
--------------------------------------
| Iteration            | 1162        |
| ExpectedImprovement  | 0.018348    |
| ActualImprovement    | 0.016865    |
| ImprovementRatio     | 0.91917     |
| MeanKL               | 0.008186    |
| Entropy              | -1.366      |
| Perplexity           | 0.25512     |
| AveragePolicyStd     | 0.1949      |
| AveragePolicyStd[0]  | 0.20636     |
| AveragePolicyStd[1]  | 0.21002     |
| AveragePolicyStd[2]  | 0.15189     |
| AveragePolicyStd[3]  | 0.19348     |
| AveragePolicyStd[4]  | 0.16721     |
| AveragePolicyStd[5]  | 0.24044     |
| AverageReturn        | 1718.1      |
| MinReturn            | 53.236      |
| MaxReturn            | 1848.9      |
| StdReturn            | 265.43      |
| AverageEpisodeLength | 964.61      |
| MinEpisodeLength     | 63          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 141.55      |
| TotalNEpisodes       | 22748       |
| TotalNSamples        | 5.8146e+06  |
| ExplainedVariance    | -3.2625e-05 |
--------------------------------------
[2018-01-21 15:34:03.845646 UTC] Saving snapshot
[2018-01-21 15:34:03.845826 UTC] Starting iteration 1163
[2018-01-21 15:34:03.845949 UTC] Start collecting samples
[2018-01-21 15:34:08.260985 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:08.388510 UTC] Performing policy update
[2018-01-21 15:34:08.389236 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:08.512497 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:09.899727 UTC] Performing line search
[2018-01-21 15:34:10.098996 UTC] Updating baseline
[2018-01-21 15:34:11.872794 UTC] Computing logging information
-------------------------------------
| Iteration            | 1163       |
| ExpectedImprovement  | 0.01731    |
| ActualImprovement    | 0.016777   |
| ImprovementRatio     | 0.96924    |
| MeanKL               | 0.0075961  |
| Entropy              | -1.3702    |
| Perplexity           | 0.25405    |
| AveragePolicyStd     | 0.19473    |
| AveragePolicyStd[0]  | 0.20605    |
| AveragePolicyStd[1]  | 0.20979    |
| AveragePolicyStd[2]  | 0.15189    |
| AveragePolicyStd[3]  | 0.19311    |
| AveragePolicyStd[4]  | 0.1676     |
| AveragePolicyStd[5]  | 0.23995    |
| AverageReturn        | 1707.7     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 296.65     |
| AverageEpisodeLength | 957.15     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.17     |
| TotalNEpisodes       | 22756      |
| TotalNSamples        | 5.8219e+06 |
| ExplainedVariance    | 0.053559   |
-------------------------------------
[2018-01-21 15:34:12.716267 UTC] Saving snapshot
[2018-01-21 15:34:12.716622 UTC] Starting iteration 1164
[2018-01-21 15:34:12.716865 UTC] Start collecting samples
[2018-01-21 15:34:17.323389 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:17.461530 UTC] Performing policy update
[2018-01-21 15:34:17.462147 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:17.574343 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:19.018311 UTC] Performing line search
[2018-01-21 15:34:19.213955 UTC] Updating baseline
[2018-01-21 15:34:20.992928 UTC] Computing logging information
-------------------------------------
| Iteration            | 1164       |
| ExpectedImprovement  | 0.01784    |
| ActualImprovement    | 0.016823   |
| ImprovementRatio     | 0.94303    |
| MeanKL               | 0.0075709  |
| Entropy              | -1.369     |
| Perplexity           | 0.25436    |
| AveragePolicyStd     | 0.19482    |
| AveragePolicyStd[0]  | 0.20593    |
| AveragePolicyStd[1]  | 0.21034    |
| AveragePolicyStd[2]  | 0.15165    |
| AveragePolicyStd[3]  | 0.19331    |
| AveragePolicyStd[4]  | 0.16717    |
| AveragePolicyStd[5]  | 0.2405     |
| AverageReturn        | 1695       |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 327.82     |
| AverageEpisodeLength | 949.73     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.23     |
| TotalNEpisodes       | 22763      |
| TotalNSamples        | 5.8278e+06 |
| ExplainedVariance    | 0.15909    |
-------------------------------------
[2018-01-21 15:34:21.848186 UTC] Saving snapshot
[2018-01-21 15:34:21.848447 UTC] Starting iteration 1165
[2018-01-21 15:34:21.848616 UTC] Start collecting samples
[2018-01-21 15:34:26.295777 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:26.433523 UTC] Performing policy update
[2018-01-21 15:34:26.434717 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:26.566311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:27.999597 UTC] Performing line search
[2018-01-21 15:34:28.198508 UTC] Updating baseline
[2018-01-21 15:34:31.173156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1165       |
| ExpectedImprovement  | 0.017967   |
| ActualImprovement    | 0.017343   |
| ImprovementRatio     | 0.96525    |
| MeanKL               | 0.0080708  |
| Entropy              | -1.3762    |
| Perplexity           | 0.25255    |
| AveragePolicyStd     | 0.19455    |
| AveragePolicyStd[0]  | 0.20573    |
| AveragePolicyStd[1]  | 0.21007    |
| AveragePolicyStd[2]  | 0.15179    |
| AveragePolicyStd[3]  | 0.19308    |
| AveragePolicyStd[4]  | 0.16702    |
| AveragePolicyStd[5]  | 0.2396     |
| AverageReturn        | 1695.7     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 328.02     |
| AverageEpisodeLength | 949.73     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.23     |
| TotalNEpisodes       | 22766      |
| TotalNSamples        | 5.8308e+06 |
| ExplainedVariance    | -0.019775  |
-------------------------------------
[2018-01-21 15:34:32.020563 UTC] Saving snapshot
[2018-01-21 15:34:32.020843 UTC] Starting iteration 1166
[2018-01-21 15:34:32.021030 UTC] Start collecting samples
[2018-01-21 15:34:36.556023 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:36.699849 UTC] Performing policy update
[2018-01-21 15:34:36.700484 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:36.823180 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:38.264197 UTC] Performing line search
[2018-01-21 15:34:38.557522 UTC] Updating baseline
[2018-01-21 15:34:41.965036 UTC] Computing logging information
-------------------------------------
| Iteration            | 1166       |
| ExpectedImprovement  | 0.020962   |
| ActualImprovement    | 0.019066   |
| ImprovementRatio     | 0.90956    |
| MeanKL               | 0.0073755  |
| Entropy              | -1.3879    |
| Perplexity           | 0.24961    |
| AveragePolicyStd     | 0.19414    |
| AveragePolicyStd[0]  | 0.20503    |
| AveragePolicyStd[1]  | 0.21017    |
| AveragePolicyStd[2]  | 0.15143    |
| AveragePolicyStd[3]  | 0.19332    |
| AveragePolicyStd[4]  | 0.16685    |
| AveragePolicyStd[5]  | 0.23803    |
| AverageReturn        | 1704.8     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 317.19     |
| AverageEpisodeLength | 954.76     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.28     |
| TotalNEpisodes       | 22770      |
| TotalNSamples        | 5.8348e+06 |
| ExplainedVariance    | -0.014867  |
-------------------------------------
[2018-01-21 15:34:42.793315 UTC] Saving snapshot
[2018-01-21 15:34:42.793580 UTC] Starting iteration 1167
[2018-01-21 15:34:42.793775 UTC] Start collecting samples
[2018-01-21 15:34:47.509054 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:47.640647 UTC] Performing policy update
[2018-01-21 15:34:47.641326 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:47.757782 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:49.152307 UTC] Performing line search
[2018-01-21 15:34:49.342923 UTC] Updating baseline
[2018-01-21 15:34:51.237601 UTC] Computing logging information
-------------------------------------
| Iteration            | 1167       |
| ExpectedImprovement  | 0.018929   |
| ActualImprovement    | 0.018161   |
| ImprovementRatio     | 0.95942    |
| MeanKL               | 0.0076052  |
| Entropy              | -1.3885    |
| Perplexity           | 0.24946    |
| AveragePolicyStd     | 0.19415    |
| AveragePolicyStd[0]  | 0.20523    |
| AveragePolicyStd[1]  | 0.21019    |
| AveragePolicyStd[2]  | 0.151      |
| AveragePolicyStd[3]  | 0.19307    |
| AveragePolicyStd[4]  | 0.16696    |
| AveragePolicyStd[5]  | 0.23845    |
| AverageReturn        | 1705.1     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 317.19     |
| AverageEpisodeLength | 954.76     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.28     |
| TotalNEpisodes       | 22779      |
| TotalNSamples        | 5.8438e+06 |
| ExplainedVariance    | 0.0068948  |
-------------------------------------
[2018-01-21 15:34:52.003208 UTC] Saving snapshot
[2018-01-21 15:34:52.003418 UTC] Starting iteration 1168
[2018-01-21 15:34:52.003587 UTC] Start collecting samples
[2018-01-21 15:34:56.662017 UTC] Computing input variables for policy optimization
[2018-01-21 15:34:56.784718 UTC] Performing policy update
[2018-01-21 15:34:56.785329 UTC] Computing gradient in Euclidean space
[2018-01-21 15:34:56.910519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:34:58.287838 UTC] Performing line search
[2018-01-21 15:34:58.476791 UTC] Updating baseline
[2018-01-21 15:35:01.606910 UTC] Computing logging information
------------------------------------
| Iteration            | 1168      |
| ExpectedImprovement  | 0.023426  |
| ActualImprovement    | 0.021247  |
| ImprovementRatio     | 0.907     |
| MeanKL               | 0.0070728 |
| Entropy              | -1.3874   |
| Perplexity           | 0.24971   |
| AveragePolicyStd     | 0.19417   |
| AveragePolicyStd[0]  | 0.20541   |
| AveragePolicyStd[1]  | 0.21014   |
| AveragePolicyStd[2]  | 0.15148   |
| AveragePolicyStd[3]  | 0.19276   |
| AveragePolicyStd[4]  | 0.16675   |
| AveragePolicyStd[5]  | 0.23848   |
| AverageReturn        | 1692.5    |
| MinReturn            | 53.236    |
| MaxReturn            | 1848.9    |
| StdReturn            | 340.72    |
| AverageEpisodeLength | 947.59    |
| MinEpisodeLength     | 63        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 181.92    |
| TotalNEpisodes       | 22782     |
| TotalNSamples        | 5.846e+06 |
| ExplainedVariance    | 0.16718   |
------------------------------------
[2018-01-21 15:35:02.456458 UTC] Saving snapshot
[2018-01-21 15:35:02.456805 UTC] Starting iteration 1169
[2018-01-21 15:35:02.457041 UTC] Start collecting samples
[2018-01-21 15:35:06.948065 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:07.076804 UTC] Performing policy update
[2018-01-21 15:35:07.077461 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:07.199841 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:08.664175 UTC] Performing line search
[2018-01-21 15:35:08.869852 UTC] Updating baseline
[2018-01-21 15:35:10.958586 UTC] Computing logging information
-------------------------------------
| Iteration            | 1169       |
| ExpectedImprovement  | 0.017212   |
| ActualImprovement    | 0.015883   |
| ImprovementRatio     | 0.92283    |
| MeanKL               | 0.0077972  |
| Entropy              | -1.3889    |
| Perplexity           | 0.24934    |
| AveragePolicyStd     | 0.19411    |
| AveragePolicyStd[0]  | 0.20528    |
| AveragePolicyStd[1]  | 0.21004    |
| AveragePolicyStd[2]  | 0.15154    |
| AveragePolicyStd[3]  | 0.19276    |
| AveragePolicyStd[4]  | 0.16678    |
| AveragePolicyStd[5]  | 0.23825    |
| AverageReturn        | 1702.4     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 331.56     |
| AverageEpisodeLength | 952.88     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.21     |
| TotalNEpisodes       | 22787      |
| TotalNSamples        | 5.851e+06  |
| ExplainedVariance    | -0.0002855 |
-------------------------------------
[2018-01-21 15:35:11.723706 UTC] Saving snapshot
[2018-01-21 15:35:11.723883 UTC] Starting iteration 1170
[2018-01-21 15:35:11.724012 UTC] Start collecting samples
[2018-01-21 15:35:16.161424 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:16.302385 UTC] Performing policy update
[2018-01-21 15:35:16.302993 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:16.422321 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:17.864025 UTC] Performing line search
[2018-01-21 15:35:18.078043 UTC] Updating baseline
[2018-01-21 15:35:19.743382 UTC] Computing logging information
------------------------------------
| Iteration            | 1170      |
| ExpectedImprovement  | 0.019105  |
| ActualImprovement    | 0.018474  |
| ImprovementRatio     | 0.967     |
| MeanKL               | 0.0073012 |
| Entropy              | -1.3919   |
| Perplexity           | 0.24861   |
| AveragePolicyStd     | 0.19402   |
| AveragePolicyStd[0]  | 0.20537   |
| AveragePolicyStd[1]  | 0.20995   |
| AveragePolicyStd[2]  | 0.15159   |
| AveragePolicyStd[3]  | 0.1926    |
| AveragePolicyStd[4]  | 0.16643   |
| AveragePolicyStd[5]  | 0.23817   |
| AverageReturn        | 1710.8    |
| MinReturn            | 53.236    |
| MaxReturn            | 1848.9    |
| StdReturn            | 327.37    |
| AverageEpisodeLength | 956.28    |
| MinEpisodeLength     | 63        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 174.8     |
| TotalNEpisodes       | 22794     |
| TotalNSamples        | 5.858e+06 |
| ExplainedVariance    | -0.004079 |
------------------------------------
[2018-01-21 15:35:20.523608 UTC] Saving snapshot
[2018-01-21 15:35:20.533101 UTC] Starting iteration 1171
[2018-01-21 15:35:20.533356 UTC] Start collecting samples
[2018-01-21 15:35:25.034892 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:25.165529 UTC] Performing policy update
[2018-01-21 15:35:25.166118 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:25.282757 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:26.687618 UTC] Performing line search
[2018-01-21 15:35:26.882732 UTC] Updating baseline
[2018-01-21 15:35:29.119160 UTC] Computing logging information
--------------------------------------
| Iteration            | 1171        |
| ExpectedImprovement  | 0.019014    |
| ActualImprovement    | 0.017914    |
| ImprovementRatio     | 0.94211     |
| MeanKL               | 0.0074221   |
| Entropy              | -1.3865     |
| Perplexity           | 0.24994     |
| AveragePolicyStd     | 0.19421     |
| AveragePolicyStd[0]  | 0.20591     |
| AveragePolicyStd[1]  | 0.21002     |
| AveragePolicyStd[2]  | 0.15165     |
| AveragePolicyStd[3]  | 0.19317     |
| AveragePolicyStd[4]  | 0.16616     |
| AveragePolicyStd[5]  | 0.23833     |
| AverageReturn        | 1711.8      |
| MinReturn            | 53.236      |
| MaxReturn            | 1848.9      |
| StdReturn            | 327.61      |
| AverageEpisodeLength | 956.28      |
| MinEpisodeLength     | 63          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 174.8       |
| TotalNEpisodes       | 22798       |
| TotalNSamples        | 5.862e+06   |
| ExplainedVariance    | -0.00013894 |
--------------------------------------
[2018-01-21 15:35:29.882549 UTC] Saving snapshot
[2018-01-21 15:35:29.882733 UTC] Starting iteration 1172
[2018-01-21 15:35:29.882838 UTC] Start collecting samples
[2018-01-21 15:35:34.301761 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:34.418756 UTC] Performing policy update
[2018-01-21 15:35:34.419453 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:34.535723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:35.986110 UTC] Performing line search
[2018-01-21 15:35:36.173709 UTC] Updating baseline
[2018-01-21 15:35:38.962886 UTC] Computing logging information
-------------------------------------
| Iteration            | 1172       |
| ExpectedImprovement  | 0.023484   |
| ActualImprovement    | 0.021619   |
| ImprovementRatio     | 0.92059    |
| MeanKL               | 0.0072474  |
| Entropy              | -1.383     |
| Perplexity           | 0.25082    |
| AveragePolicyStd     | 0.19432    |
| AveragePolicyStd[0]  | 0.20583    |
| AveragePolicyStd[1]  | 0.21034    |
| AveragePolicyStd[2]  | 0.15202    |
| AveragePolicyStd[3]  | 0.19289    |
| AveragePolicyStd[4]  | 0.16615    |
| AveragePolicyStd[5]  | 0.23866    |
| AverageReturn        | 1704.3     |
| MinReturn            | 53.236     |
| MaxReturn            | 1848.9     |
| StdReturn            | 335.75     |
| AverageEpisodeLength | 951.77     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.38     |
| TotalNEpisodes       | 22801      |
| TotalNSamples        | 5.8646e+06 |
| ExplainedVariance    | 0.15013    |
-------------------------------------
[2018-01-21 15:35:39.853568 UTC] Saving snapshot
[2018-01-21 15:35:39.853816 UTC] Starting iteration 1173
[2018-01-21 15:35:39.853990 UTC] Start collecting samples
[2018-01-21 15:35:44.400475 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:44.523425 UTC] Performing policy update
[2018-01-21 15:35:44.524036 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:44.639168 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:46.012155 UTC] Performing line search
[2018-01-21 15:35:46.205788 UTC] Updating baseline
[2018-01-21 15:35:47.916772 UTC] Computing logging information
-------------------------------------
| Iteration            | 1173       |
| ExpectedImprovement  | 0.017472   |
| ActualImprovement    | 0.016455   |
| ImprovementRatio     | 0.94179    |
| MeanKL               | 0.0077881  |
| Entropy              | -1.3888    |
| Perplexity           | 0.24937    |
| AveragePolicyStd     | 0.19417    |
| AveragePolicyStd[0]  | 0.20622    |
| AveragePolicyStd[1]  | 0.20986    |
| AveragePolicyStd[2]  | 0.15129    |
| AveragePolicyStd[3]  | 0.19289    |
| AveragePolicyStd[4]  | 0.16593    |
| AveragePolicyStd[5]  | 0.23886    |
| AverageReturn        | 1708.2     |
| MinReturn            | 53.236     |
| MaxReturn            | 1856.7     |
| StdReturn            | 336.26     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.39     |
| TotalNEpisodes       | 22810      |
| TotalNSamples        | 5.8736e+06 |
| ExplainedVariance    | 0.009663   |
-------------------------------------
[2018-01-21 15:35:48.704158 UTC] Saving snapshot
[2018-01-21 15:35:48.704395 UTC] Starting iteration 1174
[2018-01-21 15:35:48.704555 UTC] Start collecting samples
[2018-01-21 15:35:53.211955 UTC] Computing input variables for policy optimization
[2018-01-21 15:35:53.333992 UTC] Performing policy update
[2018-01-21 15:35:53.334669 UTC] Computing gradient in Euclidean space
[2018-01-21 15:35:53.452511 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:35:54.858210 UTC] Performing line search
[2018-01-21 15:35:55.052292 UTC] Updating baseline
[2018-01-21 15:35:58.059254 UTC] Computing logging information
-------------------------------------
| Iteration            | 1174       |
| ExpectedImprovement  | 0.0204     |
| ActualImprovement    | 0.020009   |
| ImprovementRatio     | 0.98085    |
| MeanKL               | 0.0078865  |
| Entropy              | -1.3904    |
| Perplexity           | 0.24898    |
| AveragePolicyStd     | 0.19409    |
| AveragePolicyStd[0]  | 0.20616    |
| AveragePolicyStd[1]  | 0.20998    |
| AveragePolicyStd[2]  | 0.15168    |
| AveragePolicyStd[3]  | 0.19265    |
| AveragePolicyStd[4]  | 0.16577    |
| AveragePolicyStd[5]  | 0.23832    |
| AverageReturn        | 1708.1     |
| MinReturn            | 53.236     |
| MaxReturn            | 1856.7     |
| StdReturn            | 336.28     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.39     |
| TotalNEpisodes       | 22813      |
| TotalNSamples        | 5.8766e+06 |
| ExplainedVariance    | -0.049454  |
-------------------------------------
[2018-01-21 15:35:58.909770 UTC] Saving snapshot
[2018-01-21 15:35:58.910123 UTC] Starting iteration 1175
[2018-01-21 15:35:58.910337 UTC] Start collecting samples
[2018-01-21 15:36:03.597199 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:03.729352 UTC] Performing policy update
[2018-01-21 15:36:03.730064 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:03.863958 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:05.382607 UTC] Performing line search
[2018-01-21 15:36:05.581980 UTC] Updating baseline
[2018-01-21 15:36:07.812989 UTC] Computing logging information
-------------------------------------
| Iteration            | 1175       |
| ExpectedImprovement  | 0.019228   |
| ActualImprovement    | 0.018258   |
| ImprovementRatio     | 0.94956    |
| MeanKL               | 0.0081642  |
| Entropy              | -1.3954    |
| Perplexity           | 0.24773    |
| AveragePolicyStd     | 0.19394    |
| AveragePolicyStd[0]  | 0.20591    |
| AveragePolicyStd[1]  | 0.20948    |
| AveragePolicyStd[2]  | 0.15166    |
| AveragePolicyStd[3]  | 0.19264    |
| AveragePolicyStd[4]  | 0.16542    |
| AveragePolicyStd[5]  | 0.23852    |
| AverageReturn        | 1709.3     |
| MinReturn            | 53.236     |
| MaxReturn            | 1865.2     |
| StdReturn            | 336.57     |
| AverageEpisodeLength | 952.66     |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.39     |
| TotalNEpisodes       | 22816      |
| TotalNSamples        | 5.8796e+06 |
| ExplainedVariance    | -0.049808  |
-------------------------------------
[2018-01-21 15:36:08.679998 UTC] Saving snapshot
[2018-01-21 15:36:08.680227 UTC] Starting iteration 1176
[2018-01-21 15:36:08.680405 UTC] Start collecting samples
[2018-01-21 15:36:13.261257 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:13.402516 UTC] Performing policy update
[2018-01-21 15:36:13.403262 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:13.531544 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:14.919627 UTC] Performing line search
[2018-01-21 15:36:15.112509 UTC] Updating baseline
[2018-01-21 15:36:16.852967 UTC] Computing logging information
-------------------------------------
| Iteration            | 1176       |
| ExpectedImprovement  | 0.019237   |
| ActualImprovement    | 0.018089   |
| ImprovementRatio     | 0.94032    |
| MeanKL               | 0.0076716  |
| Entropy              | -1.3998    |
| Perplexity           | 0.24666    |
| AveragePolicyStd     | 0.19381    |
| AveragePolicyStd[0]  | 0.20527    |
| AveragePolicyStd[1]  | 0.20951    |
| AveragePolicyStd[2]  | 0.15145    |
| AveragePolicyStd[3]  | 0.19252    |
| AveragePolicyStd[4]  | 0.16539    |
| AveragePolicyStd[5]  | 0.2387     |
| AverageReturn        | 1708.8     |
| MinReturn            | 53.236     |
| MaxReturn            | 1865.2     |
| StdReturn            | 336.27     |
| AverageEpisodeLength | 953.2      |
| MinEpisodeLength     | 63         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.45     |
| TotalNEpisodes       | 22824      |
| TotalNSamples        | 5.8876e+06 |
| ExplainedVariance    | -0.0023186 |
-------------------------------------
[2018-01-21 15:36:17.694463 UTC] Saving snapshot
[2018-01-21 15:36:17.694706 UTC] Starting iteration 1177
[2018-01-21 15:36:17.694869 UTC] Start collecting samples
[2018-01-21 15:36:22.200155 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:22.319820 UTC] Performing policy update
[2018-01-21 15:36:22.320498 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:22.434594 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:23.926671 UTC] Performing line search
[2018-01-21 15:36:24.125791 UTC] Updating baseline
[2018-01-21 15:36:27.040420 UTC] Computing logging information
-------------------------------------
| Iteration            | 1177       |
| ExpectedImprovement  | 0.01755    |
| ActualImprovement    | 0.016468   |
| ImprovementRatio     | 0.93833    |
| MeanKL               | 0.0077119  |
| Entropy              | -1.4074    |
| Perplexity           | 0.24477    |
| AveragePolicyStd     | 0.19358    |
| AveragePolicyStd[0]  | 0.20573    |
| AveragePolicyStd[1]  | 0.20884    |
| AveragePolicyStd[2]  | 0.15112    |
| AveragePolicyStd[3]  | 0.19286    |
| AveragePolicyStd[4]  | 0.16472    |
| AveragePolicyStd[5]  | 0.23818    |
| AverageReturn        | 1717.9     |
| MinReturn            | 229.63     |
| MaxReturn            | 1865.2     |
| StdReturn            | 301.3      |
| AverageEpisodeLength | 958.1      |
| MinEpisodeLength     | 164        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.8      |
| TotalNEpisodes       | 22829      |
| TotalNSamples        | 5.8921e+06 |
| ExplainedVariance    | 0.09834    |
-------------------------------------
[2018-01-21 15:36:27.770592 UTC] Saving snapshot
[2018-01-21 15:36:27.770817 UTC] Starting iteration 1178
[2018-01-21 15:36:27.771024 UTC] Start collecting samples
[2018-01-21 15:36:32.188864 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:32.317418 UTC] Performing policy update
[2018-01-21 15:36:32.318702 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:32.441164 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:33.905782 UTC] Performing line search
[2018-01-21 15:36:34.098650 UTC] Updating baseline
[2018-01-21 15:36:36.742908 UTC] Computing logging information
-------------------------------------
| Iteration            | 1178       |
| ExpectedImprovement  | 0.018306   |
| ActualImprovement    | 0.016466   |
| ImprovementRatio     | 0.89951    |
| MeanKL               | 0.0082123  |
| Entropy              | -1.4063    |
| Perplexity           | 0.24505    |
| AveragePolicyStd     | 0.19357    |
| AveragePolicyStd[0]  | 0.20505    |
| AveragePolicyStd[1]  | 0.20884    |
| AveragePolicyStd[2]  | 0.15128    |
| AveragePolicyStd[3]  | 0.19295    |
| AveragePolicyStd[4]  | 0.16539    |
| AveragePolicyStd[5]  | 0.23792    |
| AverageReturn        | 1732.1     |
| MinReturn            | 229.63     |
| MaxReturn            | 1865.2     |
| StdReturn            | 273.12     |
| AverageEpisodeLength | 965.25     |
| MinEpisodeLength     | 164        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.92     |
| TotalNEpisodes       | 22832      |
| TotalNSamples        | 5.8951e+06 |
| ExplainedVariance    | -0.0010418 |
-------------------------------------
[2018-01-21 15:36:37.580666 UTC] Saving snapshot
[2018-01-21 15:36:37.580907 UTC] Starting iteration 1179
[2018-01-21 15:36:37.581059 UTC] Start collecting samples
[2018-01-21 15:36:42.241410 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:42.377791 UTC] Performing policy update
[2018-01-21 15:36:42.378509 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:42.497788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:43.932482 UTC] Performing line search
[2018-01-21 15:36:44.142680 UTC] Updating baseline
[2018-01-21 15:36:46.572585 UTC] Computing logging information
--------------------------------------
| Iteration            | 1179        |
| ExpectedImprovement  | 0.017066    |
| ActualImprovement    | 0.015738    |
| ImprovementRatio     | 0.92216     |
| MeanKL               | 0.0086612   |
| Entropy              | -1.4116     |
| Perplexity           | 0.24375     |
| AveragePolicyStd     | 0.19342     |
| AveragePolicyStd[0]  | 0.20531     |
| AveragePolicyStd[1]  | 0.20878     |
| AveragePolicyStd[2]  | 0.15096     |
| AveragePolicyStd[3]  | 0.1927      |
| AveragePolicyStd[4]  | 0.16508     |
| AveragePolicyStd[5]  | 0.23766     |
| AverageReturn        | 1732.9      |
| MinReturn            | 229.63      |
| MaxReturn            | 1865.2      |
| StdReturn            | 273.29      |
| AverageEpisodeLength | 965.25      |
| MinEpisodeLength     | 164         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 145.92      |
| TotalNEpisodes       | 22838       |
| TotalNSamples        | 5.9011e+06  |
| ExplainedVariance    | -0.00045225 |
--------------------------------------
[2018-01-21 15:36:47.438534 UTC] Saving snapshot
[2018-01-21 15:36:47.438792 UTC] Starting iteration 1180
[2018-01-21 15:36:47.438961 UTC] Start collecting samples
[2018-01-21 15:36:51.988074 UTC] Computing input variables for policy optimization
[2018-01-21 15:36:52.131576 UTC] Performing policy update
[2018-01-21 15:36:52.132233 UTC] Computing gradient in Euclidean space
[2018-01-21 15:36:52.256886 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:36:53.748986 UTC] Performing line search
[2018-01-21 15:36:53.942937 UTC] Updating baseline
[2018-01-21 15:36:56.621253 UTC] Computing logging information
--------------------------------------
| Iteration            | 1180        |
| ExpectedImprovement  | 0.019578    |
| ActualImprovement    | 0.019011    |
| ImprovementRatio     | 0.97101     |
| MeanKL               | 0.0077642   |
| Entropy              | -1.4163     |
| Perplexity           | 0.24262     |
| AveragePolicyStd     | 0.19326     |
| AveragePolicyStd[0]  | 0.20543     |
| AveragePolicyStd[1]  | 0.20836     |
| AveragePolicyStd[2]  | 0.15096     |
| AveragePolicyStd[3]  | 0.19275     |
| AveragePolicyStd[4]  | 0.16481     |
| AveragePolicyStd[5]  | 0.23724     |
| AverageReturn        | 1735.2      |
| MinReturn            | 229.63      |
| MaxReturn            | 1865.2      |
| StdReturn            | 273.71      |
| AverageEpisodeLength | 965.25      |
| MinEpisodeLength     | 164         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 145.92      |
| TotalNEpisodes       | 22844       |
| TotalNSamples        | 5.9071e+06  |
| ExplainedVariance    | -0.00017994 |
--------------------------------------
[2018-01-21 15:36:57.385680 UTC] Saving snapshot
[2018-01-21 15:36:57.395378 UTC] Starting iteration 1181
[2018-01-21 15:36:57.395632 UTC] Start collecting samples
[2018-01-21 15:37:02.144692 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:02.275887 UTC] Performing policy update
[2018-01-21 15:37:02.276530 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:02.397867 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:03.813204 UTC] Performing line search
[2018-01-21 15:37:04.007213 UTC] Updating baseline
[2018-01-21 15:37:06.567308 UTC] Computing logging information
-------------------------------------
| Iteration            | 1181       |
| ExpectedImprovement  | 0.018958   |
| ActualImprovement    | 0.01759    |
| ImprovementRatio     | 0.92786    |
| MeanKL               | 0.0081004  |
| Entropy              | -1.4125    |
| Perplexity           | 0.24354    |
| AveragePolicyStd     | 0.19338    |
| AveragePolicyStd[0]  | 0.20539    |
| AveragePolicyStd[1]  | 0.20837    |
| AveragePolicyStd[2]  | 0.15105    |
| AveragePolicyStd[3]  | 0.19313    |
| AveragePolicyStd[4]  | 0.16482    |
| AveragePolicyStd[5]  | 0.23754    |
| AverageReturn        | 1723.4     |
| MinReturn            | 229.63     |
| MaxReturn            | 1865.2     |
| StdReturn            | 297.27     |
| AverageEpisodeLength | 958.79     |
| MinEpisodeLength     | 164        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.03     |
| TotalNEpisodes       | 22849      |
| TotalNSamples        | 5.9115e+06 |
| ExplainedVariance    | 0.11022    |
-------------------------------------
[2018-01-21 15:37:07.346290 UTC] Saving snapshot
[2018-01-21 15:37:07.346570 UTC] Starting iteration 1182
[2018-01-21 15:37:07.346756 UTC] Start collecting samples
[2018-01-21 15:37:11.780878 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:11.922664 UTC] Performing policy update
[2018-01-21 15:37:11.923470 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:12.043636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:13.474598 UTC] Performing line search
[2018-01-21 15:37:13.659334 UTC] Updating baseline
[2018-01-21 15:37:15.062090 UTC] Computing logging information
-------------------------------------
| Iteration            | 1182       |
| ExpectedImprovement  | 0.0178     |
| ActualImprovement    | 0.017271   |
| ImprovementRatio     | 0.97026    |
| MeanKL               | 0.0082835  |
| Entropy              | -1.4162    |
| Perplexity           | 0.24263    |
| AveragePolicyStd     | 0.19328    |
| AveragePolicyStd[0]  | 0.20573    |
| AveragePolicyStd[1]  | 0.20832    |
| AveragePolicyStd[2]  | 0.15112    |
| AveragePolicyStd[3]  | 0.19304    |
| AveragePolicyStd[4]  | 0.16414    |
| AveragePolicyStd[5]  | 0.23732    |
| AverageReturn        | 1721.6     |
| MinReturn            | 229.63     |
| MaxReturn            | 1865.2     |
| StdReturn            | 296.79     |
| AverageEpisodeLength | 958.79     |
| MinEpisodeLength     | 164        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.03     |
| TotalNEpisodes       | 22853      |
| TotalNSamples        | 5.9155e+06 |
| ExplainedVariance    | -0.011633  |
-------------------------------------
[2018-01-21 15:37:15.857385 UTC] Saving snapshot
[2018-01-21 15:37:15.857585 UTC] Starting iteration 1183
[2018-01-21 15:37:15.857707 UTC] Start collecting samples
[2018-01-21 15:37:20.492811 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:20.637173 UTC] Performing policy update
[2018-01-21 15:37:20.637809 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:20.762528 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:22.172317 UTC] Performing line search
[2018-01-21 15:37:22.371909 UTC] Updating baseline
[2018-01-21 15:37:24.981064 UTC] Computing logging information
-------------------------------------
| Iteration            | 1183       |
| ExpectedImprovement  | 0.017725   |
| ActualImprovement    | 0.017004   |
| ImprovementRatio     | 0.95931    |
| MeanKL               | 0.0073051  |
| Entropy              | -1.4151    |
| Perplexity           | 0.24291    |
| AveragePolicyStd     | 0.19331    |
| AveragePolicyStd[0]  | 0.20652    |
| AveragePolicyStd[1]  | 0.20849    |
| AveragePolicyStd[2]  | 0.15121    |
| AveragePolicyStd[3]  | 0.19299    |
| AveragePolicyStd[4]  | 0.16389    |
| AveragePolicyStd[5]  | 0.23677    |
| AverageReturn        | 1752.8     |
| MinReturn            | 456.09     |
| MaxReturn            | 1865.2     |
| StdReturn            | 218.71     |
| AverageEpisodeLength | 974.61     |
| MinEpisodeLength     | 283        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.08     |
| TotalNEpisodes       | 22860      |
| TotalNSamples        | 5.9225e+06 |
| ExplainedVariance    | -0.0060451 |
-------------------------------------
[2018-01-21 15:37:25.780584 UTC] Saving snapshot
[2018-01-21 15:37:25.780866 UTC] Starting iteration 1184
[2018-01-21 15:37:25.781048 UTC] Start collecting samples
[2018-01-21 15:37:30.402873 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:30.522867 UTC] Performing policy update
[2018-01-21 15:37:30.523738 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:30.642317 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:32.045685 UTC] Performing line search
[2018-01-21 15:37:32.238918 UTC] Updating baseline
[2018-01-21 15:37:33.838079 UTC] Computing logging information
-------------------------------------
| Iteration            | 1184       |
| ExpectedImprovement  | 0.018624   |
| ActualImprovement    | 0.017449   |
| ImprovementRatio     | 0.93689    |
| MeanKL               | 0.0085108  |
| Entropy              | -1.4141    |
| Perplexity           | 0.24315    |
| AveragePolicyStd     | 0.19331    |
| AveragePolicyStd[0]  | 0.20627    |
| AveragePolicyStd[1]  | 0.20824    |
| AveragePolicyStd[2]  | 0.15145    |
| AveragePolicyStd[3]  | 0.19281    |
| AveragePolicyStd[4]  | 0.16436    |
| AveragePolicyStd[5]  | 0.23673    |
| AverageReturn        | 1758.7     |
| MinReturn            | 456.09     |
| MaxReturn            | 1865.2     |
| StdReturn            | 213.61     |
| AverageEpisodeLength | 977.39     |
| MinEpisodeLength     | 283        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.29     |
| TotalNEpisodes       | 22863      |
| TotalNSamples        | 5.9255e+06 |
| ExplainedVariance    | 1.0083e-08 |
-------------------------------------
[2018-01-21 15:37:34.712719 UTC] Saving snapshot
[2018-01-21 15:37:34.712950 UTC] Starting iteration 1185
[2018-01-21 15:37:34.713093 UTC] Start collecting samples
[2018-01-21 15:37:39.241815 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:39.419043 UTC] Performing policy update
[2018-01-21 15:37:39.419644 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:39.537710 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:40.938268 UTC] Performing line search
[2018-01-21 15:37:41.132709 UTC] Updating baseline
[2018-01-21 15:37:43.479797 UTC] Computing logging information
-------------------------------------
| Iteration            | 1185       |
| ExpectedImprovement  | 0.019827   |
| ActualImprovement    | 0.018303   |
| ImprovementRatio     | 0.92311    |
| MeanKL               | 0.0076488  |
| Entropy              | -1.4094    |
| Perplexity           | 0.24428    |
| AveragePolicyStd     | 0.19347    |
| AveragePolicyStd[0]  | 0.20671    |
| AveragePolicyStd[1]  | 0.20834    |
| AveragePolicyStd[2]  | 0.1515     |
| AveragePolicyStd[3]  | 0.19304    |
| AveragePolicyStd[4]  | 0.16436    |
| AveragePolicyStd[5]  | 0.23685    |
| AverageReturn        | 1760.4     |
| MinReturn            | 456.09     |
| MaxReturn            | 1865.2     |
| StdReturn            | 213.87     |
| AverageEpisodeLength | 977.39     |
| MinEpisodeLength     | 283        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.29     |
| TotalNEpisodes       | 22869      |
| TotalNSamples        | 5.9315e+06 |
| ExplainedVariance    | 0.058517   |
-------------------------------------
[2018-01-21 15:37:44.242780 UTC] Saving snapshot
[2018-01-21 15:37:44.243035 UTC] Starting iteration 1186
[2018-01-21 15:37:44.243195 UTC] Start collecting samples
[2018-01-21 15:37:48.799800 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:48.943038 UTC] Performing policy update
[2018-01-21 15:37:48.943693 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:49.069356 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:50.480590 UTC] Performing line search
[2018-01-21 15:37:50.682858 UTC] Updating baseline
[2018-01-21 15:37:52.585205 UTC] Computing logging information
-------------------------------------
| Iteration            | 1186       |
| ExpectedImprovement  | 0.01721    |
| ActualImprovement    | 0.016229   |
| ImprovementRatio     | 0.94303    |
| MeanKL               | 0.008318   |
| Entropy              | -1.4058    |
| Perplexity           | 0.24517    |
| AveragePolicyStd     | 0.1936     |
| AveragePolicyStd[0]  | 0.2071     |
| AveragePolicyStd[1]  | 0.20856    |
| AveragePolicyStd[2]  | 0.15114    |
| AveragePolicyStd[3]  | 0.19333    |
| AveragePolicyStd[4]  | 0.16457    |
| AveragePolicyStd[5]  | 0.23691    |
| AverageReturn        | 1743.5     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 271.63     |
| AverageEpisodeLength | 968.29     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.6      |
| TotalNEpisodes       | 22876      |
| TotalNSamples        | 5.9376e+06 |
| ExplainedVariance    | 0.10206    |
-------------------------------------
[2018-01-21 15:37:53.341236 UTC] Saving snapshot
[2018-01-21 15:37:53.341551 UTC] Starting iteration 1187
[2018-01-21 15:37:53.341785 UTC] Start collecting samples
[2018-01-21 15:37:57.998061 UTC] Computing input variables for policy optimization
[2018-01-21 15:37:58.129412 UTC] Performing policy update
[2018-01-21 15:37:58.130626 UTC] Computing gradient in Euclidean space
[2018-01-21 15:37:58.251733 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:37:59.655028 UTC] Performing line search
[2018-01-21 15:37:59.842224 UTC] Updating baseline
[2018-01-21 15:38:01.928959 UTC] Computing logging information
-------------------------------------
| Iteration            | 1187       |
| ExpectedImprovement  | 0.01666    |
| ActualImprovement    | 0.015582   |
| ImprovementRatio     | 0.93529    |
| MeanKL               | 0.0084345  |
| Entropy              | -1.4046    |
| Perplexity           | 0.24546    |
| AveragePolicyStd     | 0.19363    |
| AveragePolicyStd[0]  | 0.20672    |
| AveragePolicyStd[1]  | 0.20832    |
| AveragePolicyStd[2]  | 0.15126    |
| AveragePolicyStd[3]  | 0.19378    |
| AveragePolicyStd[4]  | 0.1647     |
| AveragePolicyStd[5]  | 0.23699    |
| AverageReturn        | 1758.1     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 238.94     |
| AverageEpisodeLength | 975.46     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.03     |
| TotalNEpisodes       | 22880      |
| TotalNSamples        | 5.9416e+06 |
| ExplainedVariance    | 1.5195e-07 |
-------------------------------------
[2018-01-21 15:38:02.739482 UTC] Saving snapshot
[2018-01-21 15:38:02.739720 UTC] Starting iteration 1188
[2018-01-21 15:38:02.739895 UTC] Start collecting samples
[2018-01-21 15:38:07.405064 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:07.524644 UTC] Performing policy update
[2018-01-21 15:38:07.528762 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:07.656041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:09.039446 UTC] Performing line search
[2018-01-21 15:38:09.233309 UTC] Updating baseline
[2018-01-21 15:38:11.188363 UTC] Computing logging information
-------------------------------------
| Iteration            | 1188       |
| ExpectedImprovement  | 0.017651   |
| ActualImprovement    | 0.016119   |
| ImprovementRatio     | 0.91317    |
| MeanKL               | 0.0079361  |
| Entropy              | -1.4037    |
| Perplexity           | 0.24568    |
| AveragePolicyStd     | 0.19364    |
| AveragePolicyStd[0]  | 0.20737    |
| AveragePolicyStd[1]  | 0.20894    |
| AveragePolicyStd[2]  | 0.15128    |
| AveragePolicyStd[3]  | 0.19385    |
| AveragePolicyStd[4]  | 0.16449    |
| AveragePolicyStd[5]  | 0.23593    |
| AverageReturn        | 1758.8     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 238.9      |
| AverageEpisodeLength | 975.46     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.03     |
| TotalNEpisodes       | 22883      |
| TotalNSamples        | 5.9446e+06 |
| ExplainedVariance    | 0.0047554  |
-------------------------------------
[2018-01-21 15:38:11.975190 UTC] Saving snapshot
[2018-01-21 15:38:11.975456 UTC] Starting iteration 1189
[2018-01-21 15:38:11.975639 UTC] Start collecting samples
[2018-01-21 15:38:16.532211 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:16.685067 UTC] Performing policy update
[2018-01-21 15:38:16.686184 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:16.807546 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:18.192199 UTC] Performing line search
[2018-01-21 15:38:18.404044 UTC] Updating baseline
[2018-01-21 15:38:20.200793 UTC] Computing logging information
-------------------------------------
| Iteration            | 1189       |
| ExpectedImprovement  | 0.018474   |
| ActualImprovement    | 0.01769    |
| ImprovementRatio     | 0.95752    |
| MeanKL               | 0.0073676  |
| Entropy              | -1.41      |
| Perplexity           | 0.24413    |
| AveragePolicyStd     | 0.19347    |
| AveragePolicyStd[0]  | 0.20771    |
| AveragePolicyStd[1]  | 0.20876    |
| AveragePolicyStd[2]  | 0.15066    |
| AveragePolicyStd[3]  | 0.19312    |
| AveragePolicyStd[4]  | 0.16464    |
| AveragePolicyStd[5]  | 0.23591    |
| AverageReturn        | 1758.5     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 238.96     |
| AverageEpisodeLength | 975.46     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.03     |
| TotalNEpisodes       | 22892      |
| TotalNSamples        | 5.9536e+06 |
| ExplainedVariance    | -0.0028279 |
-------------------------------------
[2018-01-21 15:38:21.047200 UTC] Saving snapshot
[2018-01-21 15:38:21.047485 UTC] Starting iteration 1190
[2018-01-21 15:38:21.047702 UTC] Start collecting samples
[2018-01-21 15:38:25.489716 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:25.627562 UTC] Performing policy update
[2018-01-21 15:38:25.628678 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:25.748142 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:27.149742 UTC] Performing line search
[2018-01-21 15:38:27.335508 UTC] Updating baseline
[2018-01-21 15:38:29.232035 UTC] Computing logging information
-------------------------------------
| Iteration            | 1190       |
| ExpectedImprovement  | 0.017736   |
| ActualImprovement    | 0.017092   |
| ImprovementRatio     | 0.96368    |
| MeanKL               | 0.0075796  |
| Entropy              | -1.4122    |
| Perplexity           | 0.2436     |
| AveragePolicyStd     | 0.19337    |
| AveragePolicyStd[0]  | 0.20762    |
| AveragePolicyStd[1]  | 0.20894    |
| AveragePolicyStd[2]  | 0.15096    |
| AveragePolicyStd[3]  | 0.19246    |
| AveragePolicyStd[4]  | 0.16466    |
| AveragePolicyStd[5]  | 0.23558    |
| AverageReturn        | 1744.9     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 270.18     |
| AverageEpisodeLength | 968.39     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.13     |
| TotalNEpisodes       | 22896      |
| TotalNSamples        | 5.9569e+06 |
| ExplainedVariance    | 0.11225    |
-------------------------------------
[2018-01-21 15:38:30.007206 UTC] Saving snapshot
[2018-01-21 15:38:30.016541 UTC] Starting iteration 1191
[2018-01-21 15:38:30.016782 UTC] Start collecting samples
[2018-01-21 15:38:34.355218 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:34.484201 UTC] Performing policy update
[2018-01-21 15:38:34.485233 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:34.609419 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:36.071065 UTC] Performing line search
[2018-01-21 15:38:36.268479 UTC] Updating baseline
[2018-01-21 15:38:38.606662 UTC] Computing logging information
-------------------------------------
| Iteration            | 1191       |
| ExpectedImprovement  | 0.019208   |
| ActualImprovement    | 0.018134   |
| ImprovementRatio     | 0.94411    |
| MeanKL               | 0.0074594  |
| Entropy              | -1.4178    |
| Perplexity           | 0.24225    |
| AveragePolicyStd     | 0.19322    |
| AveragePolicyStd[0]  | 0.20742    |
| AveragePolicyStd[1]  | 0.20893    |
| AveragePolicyStd[2]  | 0.15064    |
| AveragePolicyStd[3]  | 0.19188    |
| AveragePolicyStd[4]  | 0.16452    |
| AveragePolicyStd[5]  | 0.23592    |
| AverageReturn        | 1749.7     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 260.03     |
| AverageEpisodeLength | 971.12     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.6      |
| TotalNEpisodes       | 22900      |
| TotalNSamples        | 5.9607e+06 |
| ExplainedVariance    | 0.17931    |
-------------------------------------
[2018-01-21 15:38:39.390925 UTC] Saving snapshot
[2018-01-21 15:38:39.391171 UTC] Starting iteration 1192
[2018-01-21 15:38:39.391328 UTC] Start collecting samples
[2018-01-21 15:38:43.933784 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:44.061816 UTC] Performing policy update
[2018-01-21 15:38:44.062771 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:44.195471 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:45.630637 UTC] Performing line search
[2018-01-21 15:38:45.816845 UTC] Updating baseline
[2018-01-21 15:38:47.760424 UTC] Computing logging information
-------------------------------------
| Iteration            | 1192       |
| ExpectedImprovement  | 0.017762   |
| ActualImprovement    | 0.017193   |
| ImprovementRatio     | 0.96797    |
| MeanKL               | 0.0072054  |
| Entropy              | -1.4215    |
| Perplexity           | 0.24135    |
| AveragePolicyStd     | 0.19312    |
| AveragePolicyStd[0]  | 0.20733    |
| AveragePolicyStd[1]  | 0.20893    |
| AveragePolicyStd[2]  | 0.15008    |
| AveragePolicyStd[3]  | 0.1919     |
| AveragePolicyStd[4]  | 0.1647     |
| AveragePolicyStd[5]  | 0.23576    |
| AverageReturn        | 1749.2     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 259.98     |
| AverageEpisodeLength | 971.12     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.6      |
| TotalNEpisodes       | 22907      |
| TotalNSamples        | 5.9677e+06 |
| ExplainedVariance    | 0.039956   |
-------------------------------------
[2018-01-21 15:38:48.604913 UTC] Saving snapshot
[2018-01-21 15:38:48.605182 UTC] Starting iteration 1193
[2018-01-21 15:38:48.605382 UTC] Start collecting samples
[2018-01-21 15:38:53.030813 UTC] Computing input variables for policy optimization
[2018-01-21 15:38:53.166007 UTC] Performing policy update
[2018-01-21 15:38:53.166732 UTC] Computing gradient in Euclidean space
[2018-01-21 15:38:53.285727 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:38:54.702730 UTC] Performing line search
[2018-01-21 15:38:54.890309 UTC] Updating baseline
[2018-01-21 15:38:56.850486 UTC] Computing logging information
-------------------------------------
| Iteration            | 1193       |
| ExpectedImprovement  | 0.017778   |
| ActualImprovement    | 0.016739   |
| ImprovementRatio     | 0.94157    |
| MeanKL               | 0.0078229  |
| Entropy              | -1.423     |
| Perplexity           | 0.24099    |
| AveragePolicyStd     | 0.19304    |
| AveragePolicyStd[0]  | 0.20725    |
| AveragePolicyStd[1]  | 0.20899    |
| AveragePolicyStd[2]  | 0.15024    |
| AveragePolicyStd[3]  | 0.19193    |
| AveragePolicyStd[4]  | 0.16468    |
| AveragePolicyStd[5]  | 0.23518    |
| AverageReturn        | 1739.1     |
| MinReturn            | 77.254     |
| MaxReturn            | 1865.2     |
| StdReturn            | 284.36     |
| AverageEpisodeLength | 964.78     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.16     |
| TotalNEpisodes       | 22911      |
| TotalNSamples        | 5.9711e+06 |
| ExplainedVariance    | 0.18394    |
-------------------------------------
[2018-01-21 15:38:57.593647 UTC] Saving snapshot
[2018-01-21 15:38:57.593903 UTC] Starting iteration 1194
[2018-01-21 15:38:57.594072 UTC] Start collecting samples
[2018-01-21 15:39:02.207333 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:02.326317 UTC] Performing policy update
[2018-01-21 15:39:02.326975 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:02.449633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:03.859214 UTC] Performing line search
[2018-01-21 15:39:04.048086 UTC] Updating baseline
[2018-01-21 15:39:06.600018 UTC] Computing logging information
-------------------------------------
| Iteration            | 1194       |
| ExpectedImprovement  | 0.018513   |
| ActualImprovement    | 0.017378   |
| ImprovementRatio     | 0.93871    |
| MeanKL               | 0.0073709  |
| Entropy              | -1.425     |
| Perplexity           | 0.24052    |
| AveragePolicyStd     | 0.19301    |
| AveragePolicyStd[0]  | 0.20789    |
| AveragePolicyStd[1]  | 0.2086     |
| AveragePolicyStd[2]  | 0.14988    |
| AveragePolicyStd[3]  | 0.19142    |
| AveragePolicyStd[4]  | 0.16475    |
| AveragePolicyStd[5]  | 0.23551    |
| AverageReturn        | 1740.3     |
| MinReturn            | 77.254     |
| MaxReturn            | 1862.6     |
| StdReturn            | 284.55     |
| AverageEpisodeLength | 964.78     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.16     |
| TotalNEpisodes       | 22917      |
| TotalNSamples        | 5.9771e+06 |
| ExplainedVariance    | -0.0022243 |
-------------------------------------
[2018-01-21 15:39:07.467449 UTC] Saving snapshot
[2018-01-21 15:39:07.467788 UTC] Starting iteration 1195
[2018-01-21 15:39:07.467977 UTC] Start collecting samples
[2018-01-21 15:39:11.980129 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:12.122535 UTC] Performing policy update
[2018-01-21 15:39:12.125272 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:12.256277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:13.666846 UTC] Performing line search
[2018-01-21 15:39:13.859105 UTC] Updating baseline
[2018-01-21 15:39:16.180938 UTC] Computing logging information
-------------------------------------
| Iteration            | 1195       |
| ExpectedImprovement  | 0.01827    |
| ActualImprovement    | 0.01743    |
| ImprovementRatio     | 0.95401    |
| MeanKL               | 0.0085612  |
| Entropy              | -1.4297    |
| Perplexity           | 0.23938    |
| AveragePolicyStd     | 0.19285    |
| AveragePolicyStd[0]  | 0.20801    |
| AveragePolicyStd[1]  | 0.20839    |
| AveragePolicyStd[2]  | 0.14979    |
| AveragePolicyStd[3]  | 0.19123    |
| AveragePolicyStd[4]  | 0.16457    |
| AveragePolicyStd[5]  | 0.23513    |
| AverageReturn        | 1741.1     |
| MinReturn            | 77.254     |
| MaxReturn            | 1862.6     |
| StdReturn            | 284.77     |
| AverageEpisodeLength | 964.78     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.16     |
| TotalNEpisodes       | 22921      |
| TotalNSamples        | 5.9811e+06 |
| ExplainedVariance    | -0.0045144 |
-------------------------------------
[2018-01-21 15:39:17.049132 UTC] Saving snapshot
[2018-01-21 15:39:17.049419 UTC] Starting iteration 1196
[2018-01-21 15:39:17.049584 UTC] Start collecting samples
[2018-01-21 15:39:21.441423 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:21.585419 UTC] Performing policy update
[2018-01-21 15:39:21.586016 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:21.702311 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:23.160372 UTC] Performing line search
[2018-01-21 15:39:23.356445 UTC] Updating baseline
[2018-01-21 15:39:25.013437 UTC] Computing logging information
--------------------------------------
| Iteration            | 1196        |
| ExpectedImprovement  | 0.018495    |
| ActualImprovement    | 0.017543    |
| ImprovementRatio     | 0.94849     |
| MeanKL               | 0.0077139   |
| Entropy              | -1.4279     |
| Perplexity           | 0.23982     |
| AveragePolicyStd     | 0.19287     |
| AveragePolicyStd[0]  | 0.20791     |
| AveragePolicyStd[1]  | 0.20843     |
| AveragePolicyStd[2]  | 0.15009     |
| AveragePolicyStd[3]  | 0.19146     |
| AveragePolicyStd[4]  | 0.16484     |
| AveragePolicyStd[5]  | 0.2345      |
| AverageReturn        | 1741.3      |
| MinReturn            | 77.254      |
| MaxReturn            | 1867.7      |
| StdReturn            | 284.94      |
| AverageEpisodeLength | 964.78      |
| MinEpisodeLength     | 90          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 150.16      |
| TotalNEpisodes       | 22926       |
| TotalNSamples        | 5.9861e+06  |
| ExplainedVariance    | -2.0892e-09 |
--------------------------------------
[2018-01-21 15:39:25.860216 UTC] Saving snapshot
[2018-01-21 15:39:25.860418 UTC] Starting iteration 1197
[2018-01-21 15:39:25.860597 UTC] Start collecting samples
[2018-01-21 15:39:30.427615 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:30.559856 UTC] Performing policy update
[2018-01-21 15:39:30.560471 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:30.679834 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:32.100777 UTC] Performing line search
[2018-01-21 15:39:32.287413 UTC] Updating baseline
[2018-01-21 15:39:34.963206 UTC] Computing logging information
-------------------------------------
| Iteration            | 1197       |
| ExpectedImprovement  | 0.017241   |
| ActualImprovement    | 0.016388   |
| ImprovementRatio     | 0.95055    |
| MeanKL               | 0.0085662  |
| Entropy              | -1.4304    |
| Perplexity           | 0.23922    |
| AveragePolicyStd     | 0.19278    |
| AveragePolicyStd[0]  | 0.20764    |
| AveragePolicyStd[1]  | 0.20853    |
| AveragePolicyStd[2]  | 0.15014    |
| AveragePolicyStd[3]  | 0.19115    |
| AveragePolicyStd[4]  | 0.16494    |
| AveragePolicyStd[5]  | 0.23427    |
| AverageReturn        | 1738.4     |
| MinReturn            | 77.254     |
| MaxReturn            | 1867.7     |
| StdReturn            | 288.8      |
| AverageEpisodeLength | 964.1      |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.16     |
| TotalNEpisodes       | 22933      |
| TotalNSamples        | 5.9926e+06 |
| ExplainedVariance    | 0.069648   |
-------------------------------------
[2018-01-21 15:39:35.761429 UTC] Saving snapshot
[2018-01-21 15:39:35.761680 UTC] Starting iteration 1198
[2018-01-21 15:39:35.761837 UTC] Start collecting samples
[2018-01-21 15:39:40.408734 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:40.566449 UTC] Performing policy update
[2018-01-21 15:39:40.567167 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:40.696538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:42.163153 UTC] Performing line search
[2018-01-21 15:39:42.378379 UTC] Updating baseline
[2018-01-21 15:39:44.823473 UTC] Computing logging information
-------------------------------------
| Iteration            | 1198       |
| ExpectedImprovement  | 0.020461   |
| ActualImprovement    | 0.018208   |
| ImprovementRatio     | 0.88988    |
| MeanKL               | 0.0074027  |
| Entropy              | -1.4256    |
| Perplexity           | 0.24037    |
| AveragePolicyStd     | 0.19293    |
| AveragePolicyStd[0]  | 0.20764    |
| AveragePolicyStd[1]  | 0.20884    |
| AveragePolicyStd[2]  | 0.15008    |
| AveragePolicyStd[3]  | 0.19136    |
| AveragePolicyStd[4]  | 0.16535    |
| AveragePolicyStd[5]  | 0.23431    |
| AverageReturn        | 1738.1     |
| MinReturn            | 77.254     |
| MaxReturn            | 1867.7     |
| StdReturn            | 288.76     |
| AverageEpisodeLength | 964.1      |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.16     |
| TotalNEpisodes       | 22936      |
| TotalNSamples        | 5.9956e+06 |
| ExplainedVariance    | -0.0062605 |
-------------------------------------
[2018-01-21 15:39:45.653935 UTC] Saving snapshot
[2018-01-21 15:39:45.654172 UTC] Starting iteration 1199
[2018-01-21 15:39:45.654351 UTC] Start collecting samples
[2018-01-21 15:39:50.280417 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:50.406002 UTC] Performing policy update
[2018-01-21 15:39:50.406670 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:50.545613 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:39:51.955273 UTC] Performing line search
[2018-01-21 15:39:52.149064 UTC] Updating baseline
[2018-01-21 15:39:53.795611 UTC] Computing logging information
-------------------------------------
| Iteration            | 1199       |
| ExpectedImprovement  | 0.017925   |
| ActualImprovement    | 0.016959   |
| ImprovementRatio     | 0.94611    |
| MeanKL               | 0.0080141  |
| Entropy              | -1.4281    |
| Perplexity           | 0.23976    |
| AveragePolicyStd     | 0.19284    |
| AveragePolicyStd[0]  | 0.20796    |
| AveragePolicyStd[1]  | 0.20845    |
| AveragePolicyStd[2]  | 0.15003    |
| AveragePolicyStd[3]  | 0.19109    |
| AveragePolicyStd[4]  | 0.16542    |
| AveragePolicyStd[5]  | 0.23408    |
| AverageReturn        | 1738       |
| MinReturn            | 77.254     |
| MaxReturn            | 1872.5     |
| StdReturn            | 288.8      |
| AverageEpisodeLength | 964.1      |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.16     |
| TotalNEpisodes       | 22942      |
| TotalNSamples        | 6.0016e+06 |
| ExplainedVariance    | -0.0030617 |
-------------------------------------
[2018-01-21 15:39:54.559045 UTC] Saving snapshot
[2018-01-21 15:39:54.559279 UTC] Starting iteration 1200
[2018-01-21 15:39:54.559429 UTC] Start collecting samples
[2018-01-21 15:39:59.031692 UTC] Computing input variables for policy optimization
[2018-01-21 15:39:59.170839 UTC] Performing policy update
[2018-01-21 15:39:59.171537 UTC] Computing gradient in Euclidean space
[2018-01-21 15:39:59.298310 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:00.737879 UTC] Performing line search
[2018-01-21 15:40:00.928797 UTC] Updating baseline
[2018-01-21 15:40:02.656099 UTC] Computing logging information
-------------------------------------
| Iteration            | 1200       |
| ExpectedImprovement  | 0.017839   |
| ActualImprovement    | 0.01654    |
| ImprovementRatio     | 0.92722    |
| MeanKL               | 0.0074087  |
| Entropy              | -1.4265    |
| Perplexity           | 0.24015    |
| AveragePolicyStd     | 0.19289    |
| AveragePolicyStd[0]  | 0.20774    |
| AveragePolicyStd[1]  | 0.20821    |
| AveragePolicyStd[2]  | 0.14993    |
| AveragePolicyStd[3]  | 0.19127    |
| AveragePolicyStd[4]  | 0.16572    |
| AveragePolicyStd[5]  | 0.2345     |
| AverageReturn        | 1736.4     |
| MinReturn            | 77.254     |
| MaxReturn            | 1872.5     |
| StdReturn            | 288.47     |
| AverageEpisodeLength | 964.1      |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.16     |
| TotalNEpisodes       | 22947      |
| TotalNSamples        | 6.0066e+06 |
| ExplainedVariance    | 2.005e-05  |
-------------------------------------
[2018-01-21 15:40:03.426158 UTC] Saving snapshot
[2018-01-21 15:40:03.438417 UTC] Starting iteration 1201
[2018-01-21 15:40:03.438676 UTC] Start collecting samples
[2018-01-21 15:40:07.732034 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:07.867373 UTC] Performing policy update
[2018-01-21 15:40:07.868438 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:07.993444 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:09.381268 UTC] Performing line search
[2018-01-21 15:40:09.580930 UTC] Updating baseline
[2018-01-21 15:40:11.818565 UTC] Computing logging information
-------------------------------------
| Iteration            | 1201       |
| ExpectedImprovement  | 0.020318   |
| ActualImprovement    | 0.01905    |
| ImprovementRatio     | 0.93762    |
| MeanKL               | 0.0078703  |
| Entropy              | -1.4262    |
| Perplexity           | 0.24023    |
| AveragePolicyStd     | 0.19287    |
| AveragePolicyStd[0]  | 0.20771    |
| AveragePolicyStd[1]  | 0.20814    |
| AveragePolicyStd[2]  | 0.15026    |
| AveragePolicyStd[3]  | 0.19078    |
| AveragePolicyStd[4]  | 0.16602    |
| AveragePolicyStd[5]  | 0.23433    |
| AverageReturn        | 1748       |
| MinReturn            | 77.254     |
| MaxReturn            | 1872.5     |
| StdReturn            | 263.5      |
| AverageEpisodeLength | 970.56     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.29     |
| TotalNEpisodes       | 22951      |
| TotalNSamples        | 6.0106e+06 |
| ExplainedVariance    | 4.5892e-08 |
-------------------------------------
[2018-01-21 15:40:12.561601 UTC] Saving snapshot
[2018-01-21 15:40:12.561875 UTC] Starting iteration 1202
[2018-01-21 15:40:12.562099 UTC] Start collecting samples
[2018-01-21 15:40:17.138046 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:17.268586 UTC] Performing policy update
[2018-01-21 15:40:17.269356 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:17.393714 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:18.864381 UTC] Performing line search
[2018-01-21 15:40:19.066870 UTC] Updating baseline
[2018-01-21 15:40:21.825064 UTC] Computing logging information
--------------------------------------
| Iteration            | 1202        |
| ExpectedImprovement  | 0.017218    |
| ActualImprovement    | 0.016363    |
| ImprovementRatio     | 0.95035     |
| MeanKL               | 0.0074391   |
| Entropy              | -1.4352     |
| Perplexity           | 0.23807     |
| AveragePolicyStd     | 0.19259     |
| AveragePolicyStd[0]  | 0.20736     |
| AveragePolicyStd[1]  | 0.20842     |
| AveragePolicyStd[2]  | 0.14987     |
| AveragePolicyStd[3]  | 0.19015     |
| AveragePolicyStd[4]  | 0.16597     |
| AveragePolicyStd[5]  | 0.23377     |
| AverageReturn        | 1749.7      |
| MinReturn            | 77.254      |
| MaxReturn            | 1872.5      |
| StdReturn            | 263.88      |
| AverageEpisodeLength | 970.56      |
| MinEpisodeLength     | 90          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 139.29      |
| TotalNEpisodes       | 22957       |
| TotalNSamples        | 6.0166e+06  |
| ExplainedVariance    | -2.6121e-06 |
--------------------------------------
[2018-01-21 15:40:22.596312 UTC] Saving snapshot
[2018-01-21 15:40:22.596556 UTC] Starting iteration 1203
[2018-01-21 15:40:22.596717 UTC] Start collecting samples
[2018-01-21 15:40:27.130879 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:27.268442 UTC] Performing policy update
[2018-01-21 15:40:27.269106 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:27.389313 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:28.792108 UTC] Performing line search
[2018-01-21 15:40:28.985916 UTC] Updating baseline
[2018-01-21 15:40:30.739890 UTC] Computing logging information
-------------------------------------
| Iteration            | 1203       |
| ExpectedImprovement  | 0.018674   |
| ActualImprovement    | 0.017396   |
| ImprovementRatio     | 0.93157    |
| MeanKL               | 0.0075838  |
| Entropy              | -1.436     |
| Perplexity           | 0.23787    |
| AveragePolicyStd     | 0.19259    |
| AveragePolicyStd[0]  | 0.20761    |
| AveragePolicyStd[1]  | 0.20879    |
| AveragePolicyStd[2]  | 0.14943    |
| AveragePolicyStd[3]  | 0.19021    |
| AveragePolicyStd[4]  | 0.16592    |
| AveragePolicyStd[5]  | 0.23356    |
| AverageReturn        | 1748.2     |
| MinReturn            | 77.254     |
| MaxReturn            | 1872.5     |
| StdReturn            | 263.63     |
| AverageEpisodeLength | 970.56     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.29     |
| TotalNEpisodes       | 22962      |
| TotalNSamples        | 6.0216e+06 |
| ExplainedVariance    | 1.8062e-08 |
-------------------------------------
[2018-01-21 15:40:31.505789 UTC] Saving snapshot
[2018-01-21 15:40:31.506032 UTC] Starting iteration 1204
[2018-01-21 15:40:31.506214 UTC] Start collecting samples
[2018-01-21 15:40:35.958798 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:36.082425 UTC] Performing policy update
[2018-01-21 15:40:36.083490 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:36.202341 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:37.613020 UTC] Performing line search
[2018-01-21 15:40:37.809905 UTC] Updating baseline
[2018-01-21 15:40:39.708005 UTC] Computing logging information
-------------------------------------
| Iteration            | 1204       |
| ExpectedImprovement  | 0.017255   |
| ActualImprovement    | 0.016168   |
| ImprovementRatio     | 0.93699    |
| MeanKL               | 0.0082706  |
| Entropy              | -1.4368    |
| Perplexity           | 0.23769    |
| AveragePolicyStd     | 0.19258    |
| AveragePolicyStd[0]  | 0.20815    |
| AveragePolicyStd[1]  | 0.20852    |
| AveragePolicyStd[2]  | 0.14927    |
| AveragePolicyStd[3]  | 0.19034    |
| AveragePolicyStd[4]  | 0.16561    |
| AveragePolicyStd[5]  | 0.2336     |
| AverageReturn        | 1747.1     |
| MinReturn            | 77.254     |
| MaxReturn            | 1872.5     |
| StdReturn            | 263.4      |
| AverageEpisodeLength | 970.56     |
| MinEpisodeLength     | 90         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.29     |
| TotalNEpisodes       | 22967      |
| TotalNSamples        | 6.0266e+06 |
| ExplainedVariance    | 5.7435e-07 |
-------------------------------------
[2018-01-21 15:40:40.516795 UTC] Saving snapshot
[2018-01-21 15:40:40.517015 UTC] Starting iteration 1205
[2018-01-21 15:40:40.517172 UTC] Start collecting samples
[2018-01-21 15:40:44.973154 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:45.103823 UTC] Performing policy update
[2018-01-21 15:40:45.104435 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:45.224636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:46.650720 UTC] Performing line search
[2018-01-21 15:40:46.847335 UTC] Updating baseline
[2018-01-21 15:40:48.494422 UTC] Computing logging information
-------------------------------------
| Iteration            | 1205       |
| ExpectedImprovement  | 0.017151   |
| ActualImprovement    | 0.016422   |
| ImprovementRatio     | 0.95748    |
| MeanKL               | 0.0079941  |
| Entropy              | -1.4398    |
| Perplexity           | 0.23697    |
| AveragePolicyStd     | 0.19247    |
| AveragePolicyStd[0]  | 0.20789    |
| AveragePolicyStd[1]  | 0.20822    |
| AveragePolicyStd[2]  | 0.14924    |
| AveragePolicyStd[3]  | 0.19053    |
| AveragePolicyStd[4]  | 0.16556    |
| AveragePolicyStd[5]  | 0.23341    |
| AverageReturn        | 1764       |
| MinReturn            | 488.27     |
| MaxReturn            | 1872.5     |
| StdReturn            | 202.99     |
| AverageEpisodeLength | 979.66     |
| MinEpisodeLength     | 293        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.58     |
| TotalNEpisodes       | 22972      |
| TotalNSamples        | 6.0316e+06 |
| ExplainedVariance    | 1.2491e-08 |
-------------------------------------
[2018-01-21 15:40:49.280445 UTC] Saving snapshot
[2018-01-21 15:40:49.280646 UTC] Starting iteration 1206
[2018-01-21 15:40:49.280823 UTC] Start collecting samples
[2018-01-21 15:40:54.071407 UTC] Computing input variables for policy optimization
[2018-01-21 15:40:54.207841 UTC] Performing policy update
[2018-01-21 15:40:54.208955 UTC] Computing gradient in Euclidean space
[2018-01-21 15:40:54.332940 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:40:55.726675 UTC] Performing line search
[2018-01-21 15:40:55.921769 UTC] Updating baseline
[2018-01-21 15:40:58.278614 UTC] Computing logging information
-------------------------------------
| Iteration            | 1206       |
| ExpectedImprovement  | 0.016459   |
| ActualImprovement    | 0.016009   |
| ImprovementRatio     | 0.97265    |
| MeanKL               | 0.0076542  |
| Entropy              | -1.4395    |
| Perplexity           | 0.23705    |
| AveragePolicyStd     | 0.19249    |
| AveragePolicyStd[0]  | 0.20829    |
| AveragePolicyStd[1]  | 0.20837    |
| AveragePolicyStd[2]  | 0.14931    |
| AveragePolicyStd[3]  | 0.19094    |
| AveragePolicyStd[4]  | 0.1651     |
| AveragePolicyStd[5]  | 0.23291    |
| AverageReturn        | 1764.7     |
| MinReturn            | 488.27     |
| MaxReturn            | 1872.5     |
| StdReturn            | 203.19     |
| AverageEpisodeLength | 979.66     |
| MinEpisodeLength     | 293        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.58     |
| TotalNEpisodes       | 22978      |
| TotalNSamples        | 6.0376e+06 |
| ExplainedVariance    | 0.042274   |
-------------------------------------
[2018-01-21 15:40:59.031946 UTC] Saving snapshot
[2018-01-21 15:40:59.032184 UTC] Starting iteration 1207
[2018-01-21 15:40:59.032333 UTC] Start collecting samples
[2018-01-21 15:41:03.550325 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:03.688042 UTC] Performing policy update
[2018-01-21 15:41:03.689112 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:03.806614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:05.196722 UTC] Performing line search
[2018-01-21 15:41:05.383832 UTC] Updating baseline
[2018-01-21 15:41:07.941765 UTC] Computing logging information
-------------------------------------
| Iteration            | 1207       |
| ExpectedImprovement  | 0.017757   |
| ActualImprovement    | 0.016761   |
| ImprovementRatio     | 0.9439     |
| MeanKL               | 0.0073516  |
| Entropy              | -1.4359    |
| Perplexity           | 0.23791    |
| AveragePolicyStd     | 0.19263    |
| AveragePolicyStd[0]  | 0.20824    |
| AveragePolicyStd[1]  | 0.20863    |
| AveragePolicyStd[2]  | 0.14913    |
| AveragePolicyStd[3]  | 0.19145    |
| AveragePolicyStd[4]  | 0.16502    |
| AveragePolicyStd[5]  | 0.23328    |
| AverageReturn        | 1761.8     |
| MinReturn            | 488.27     |
| MaxReturn            | 1872.5     |
| StdReturn            | 205.09     |
| AverageEpisodeLength | 977.86     |
| MinEpisodeLength     | 293        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.72     |
| TotalNEpisodes       | 22982      |
| TotalNSamples        | 6.0414e+06 |
| ExplainedVariance    | 0.066547   |
-------------------------------------
[2018-01-21 15:41:08.729726 UTC] Saving snapshot
[2018-01-21 15:41:08.729960 UTC] Starting iteration 1208
[2018-01-21 15:41:08.730108 UTC] Start collecting samples
[2018-01-21 15:41:13.302988 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:13.423906 UTC] Performing policy update
[2018-01-21 15:41:13.424974 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:13.547217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:14.933556 UTC] Performing line search
[2018-01-21 15:41:15.121729 UTC] Updating baseline
[2018-01-21 15:41:16.988064 UTC] Computing logging information
------------------------------------
| Iteration            | 1208      |
| ExpectedImprovement  | 0.018418  |
| ActualImprovement    | 0.017839  |
| ImprovementRatio     | 0.96858   |
| MeanKL               | 0.0079074 |
| Entropy              | -1.4396   |
| Perplexity           | 0.23703   |
| AveragePolicyStd     | 0.19257   |
| AveragePolicyStd[0]  | 0.20821   |
| AveragePolicyStd[1]  | 0.20869   |
| AveragePolicyStd[2]  | 0.14878   |
| AveragePolicyStd[3]  | 0.19111   |
| AveragePolicyStd[4]  | 0.16445   |
| AveragePolicyStd[5]  | 0.23417   |
| AverageReturn        | 1755.6    |
| MinReturn            | 488.27    |
| MaxReturn            | 1872.5    |
| StdReturn            | 213.88    |
| AverageEpisodeLength | 974.4     |
| MinEpisodeLength     | 293       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 113.37    |
| TotalNEpisodes       | 22989     |
| TotalNSamples        | 6.048e+06 |
| ExplainedVariance    | 0.127     |
------------------------------------
[2018-01-21 15:41:17.826777 UTC] Saving snapshot
[2018-01-21 15:41:17.827023 UTC] Starting iteration 1209
[2018-01-21 15:41:17.827203 UTC] Start collecting samples
[2018-01-21 15:41:22.193602 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:22.352770 UTC] Performing policy update
[2018-01-21 15:41:22.353382 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:22.476041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:23.847777 UTC] Performing line search
[2018-01-21 15:41:24.045157 UTC] Updating baseline
[2018-01-21 15:41:25.923388 UTC] Computing logging information
-------------------------------------
| Iteration            | 1209       |
| ExpectedImprovement  | 0.019688   |
| ActualImprovement    | 0.01826    |
| ImprovementRatio     | 0.92746    |
| MeanKL               | 0.0082905  |
| Entropy              | -1.4378    |
| Perplexity           | 0.23745    |
| AveragePolicyStd     | 0.19262    |
| AveragePolicyStd[0]  | 0.20818    |
| AveragePolicyStd[1]  | 0.2088     |
| AveragePolicyStd[2]  | 0.14899    |
| AveragePolicyStd[3]  | 0.19091    |
| AveragePolicyStd[4]  | 0.16453    |
| AveragePolicyStd[5]  | 0.23429    |
| AverageReturn        | 1741.9     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 251.73     |
| AverageEpisodeLength | 967.08     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.35     |
| TotalNEpisodes       | 22993      |
| TotalNSamples        | 6.0513e+06 |
| ExplainedVariance    | 0.18902    |
-------------------------------------
[2018-01-21 15:41:26.660053 UTC] Saving snapshot
[2018-01-21 15:41:26.660287 UTC] Starting iteration 1210
[2018-01-21 15:41:26.660436 UTC] Start collecting samples
[2018-01-21 15:41:31.015439 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:31.136786 UTC] Performing policy update
[2018-01-21 15:41:31.137441 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:31.262217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:32.653005 UTC] Performing line search
[2018-01-21 15:41:32.860812 UTC] Updating baseline
[2018-01-21 15:41:36.627474 UTC] Computing logging information
-------------------------------------
| Iteration            | 1210       |
| ExpectedImprovement  | 0.017922   |
| ActualImprovement    | 0.017121   |
| ImprovementRatio     | 0.9553     |
| MeanKL               | 0.0080121  |
| Entropy              | -1.4407    |
| Perplexity           | 0.23676    |
| AveragePolicyStd     | 0.19251    |
| AveragePolicyStd[0]  | 0.20718    |
| AveragePolicyStd[1]  | 0.20873    |
| AveragePolicyStd[2]  | 0.1492     |
| AveragePolicyStd[3]  | 0.19083    |
| AveragePolicyStd[4]  | 0.1645     |
| AveragePolicyStd[5]  | 0.23461    |
| AverageReturn        | 1754.3     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 217.97     |
| AverageEpisodeLength | 974.15     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.89     |
| TotalNEpisodes       | 22999      |
| TotalNSamples        | 6.0573e+06 |
| ExplainedVariance    | -0.028393  |
-------------------------------------
[2018-01-21 15:41:37.490120 UTC] Saving snapshot
[2018-01-21 15:41:37.501365 UTC] Starting iteration 1211
[2018-01-21 15:41:37.501598 UTC] Start collecting samples
[2018-01-21 15:41:41.852479 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:42.004181 UTC] Performing policy update
[2018-01-21 15:41:42.004978 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:42.131823 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:43.540513 UTC] Performing line search
[2018-01-21 15:41:43.739207 UTC] Updating baseline
[2018-01-21 15:41:46.976616 UTC] Computing logging information
-------------------------------------
| Iteration            | 1211       |
| ExpectedImprovement  | 0.016787   |
| ActualImprovement    | 0.015677   |
| ImprovementRatio     | 0.93385    |
| MeanKL               | 0.0082238  |
| Entropy              | -1.442     |
| Perplexity           | 0.23646    |
| AveragePolicyStd     | 0.19249    |
| AveragePolicyStd[0]  | 0.20758    |
| AveragePolicyStd[1]  | 0.20864    |
| AveragePolicyStd[2]  | 0.14879    |
| AveragePolicyStd[3]  | 0.19147    |
| AveragePolicyStd[4]  | 0.16418    |
| AveragePolicyStd[5]  | 0.23429    |
| AverageReturn        | 1755.9     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 216.28     |
| AverageEpisodeLength | 975.93     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.89     |
| TotalNEpisodes       | 23003      |
| TotalNSamples        | 6.0613e+06 |
| ExplainedVariance    | -0.0072418 |
-------------------------------------
[2018-01-21 15:41:47.782806 UTC] Saving snapshot
[2018-01-21 15:41:47.782983 UTC] Starting iteration 1212
[2018-01-21 15:41:47.783114 UTC] Start collecting samples
[2018-01-21 15:41:52.296551 UTC] Computing input variables for policy optimization
[2018-01-21 15:41:52.417758 UTC] Performing policy update
[2018-01-21 15:41:52.418355 UTC] Computing gradient in Euclidean space
[2018-01-21 15:41:52.536626 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:41:54.081120 UTC] Performing line search
[2018-01-21 15:41:54.282863 UTC] Updating baseline
[2018-01-21 15:41:56.860112 UTC] Computing logging information
-------------------------------------
| Iteration            | 1212       |
| ExpectedImprovement  | 0.015688   |
| ActualImprovement    | 0.014847   |
| ImprovementRatio     | 0.94639    |
| MeanKL               | 0.0079276  |
| Entropy              | -1.4447    |
| Perplexity           | 0.23582    |
| AveragePolicyStd     | 0.19238    |
| AveragePolicyStd[0]  | 0.20703    |
| AveragePolicyStd[1]  | 0.20882    |
| AveragePolicyStd[2]  | 0.14898    |
| AveragePolicyStd[3]  | 0.19149    |
| AveragePolicyStd[4]  | 0.16408    |
| AveragePolicyStd[5]  | 0.23389    |
| AverageReturn        | 1754.5     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 215.77     |
| AverageEpisodeLength | 975.93     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.89     |
| TotalNEpisodes       | 23008      |
| TotalNSamples        | 6.0663e+06 |
| ExplainedVariance    | 5.3811e-06 |
-------------------------------------
[2018-01-21 15:41:57.699035 UTC] Saving snapshot
[2018-01-21 15:41:57.699286 UTC] Starting iteration 1213
[2018-01-21 15:41:57.699446 UTC] Start collecting samples
[2018-01-21 15:42:02.129760 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:02.262877 UTC] Performing policy update
[2018-01-21 15:42:02.263473 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:02.381075 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:03.746111 UTC] Performing line search
[2018-01-21 15:42:03.940588 UTC] Updating baseline
[2018-01-21 15:42:06.194563 UTC] Computing logging information
--------------------------------------
| Iteration            | 1213        |
| ExpectedImprovement  | 0.017565    |
| ActualImprovement    | 0.016678    |
| ImprovementRatio     | 0.94947     |
| MeanKL               | 0.0087097   |
| Entropy              | -1.4465     |
| Perplexity           | 0.2354      |
| AveragePolicyStd     | 0.1923      |
| AveragePolicyStd[0]  | 0.20697     |
| AveragePolicyStd[1]  | 0.20835     |
| AveragePolicyStd[2]  | 0.14923     |
| AveragePolicyStd[3]  | 0.19126     |
| AveragePolicyStd[4]  | 0.16423     |
| AveragePolicyStd[5]  | 0.23375     |
| AverageReturn        | 1762.7      |
| MinReturn            | 420.38      |
| MaxReturn            | 1872.5      |
| StdReturn            | 181.58      |
| AverageEpisodeLength | 982.27      |
| MinEpisodeLength     | 268         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 96.007      |
| TotalNEpisodes       | 23015       |
| TotalNSamples        | 6.0733e+06  |
| ExplainedVariance    | -0.00083445 |
--------------------------------------
[2018-01-21 15:42:06.992466 UTC] Saving snapshot
[2018-01-21 15:42:06.992684 UTC] Starting iteration 1214
[2018-01-21 15:42:06.992850 UTC] Start collecting samples
[2018-01-21 15:42:11.389822 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:11.534683 UTC] Performing policy update
[2018-01-21 15:42:11.535464 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:11.659678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:13.125850 UTC] Performing line search
[2018-01-21 15:42:13.330516 UTC] Updating baseline
[2018-01-21 15:42:15.351541 UTC] Computing logging information
-------------------------------------
| Iteration            | 1214       |
| ExpectedImprovement  | 0.020467   |
| ActualImprovement    | 0.018433   |
| ImprovementRatio     | 0.90061    |
| MeanKL               | 0.0073636  |
| Entropy              | -1.4476    |
| Perplexity           | 0.23514    |
| AveragePolicyStd     | 0.19226    |
| AveragePolicyStd[0]  | 0.20698    |
| AveragePolicyStd[1]  | 0.20842    |
| AveragePolicyStd[2]  | 0.14918    |
| AveragePolicyStd[3]  | 0.19114    |
| AveragePolicyStd[4]  | 0.16427    |
| AveragePolicyStd[5]  | 0.23356    |
| AverageReturn        | 1749.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 215.47     |
| AverageEpisodeLength | 976.03     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.36     |
| TotalNEpisodes       | 23018      |
| TotalNSamples        | 6.0757e+06 |
| ExplainedVariance    | 0.1917     |
-------------------------------------
[2018-01-21 15:42:16.104511 UTC] Saving snapshot
[2018-01-21 15:42:16.104885 UTC] Starting iteration 1215
[2018-01-21 15:42:16.105162 UTC] Start collecting samples
[2018-01-21 15:42:20.559047 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:20.697004 UTC] Performing policy update
[2018-01-21 15:42:20.697750 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:20.842486 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:22.206162 UTC] Performing line search
[2018-01-21 15:42:22.406187 UTC] Updating baseline
[2018-01-21 15:42:26.579804 UTC] Computing logging information
-------------------------------------
| Iteration            | 1215       |
| ExpectedImprovement  | 0.018622   |
| ActualImprovement    | 0.017538   |
| ImprovementRatio     | 0.94178    |
| MeanKL               | 0.0078735  |
| Entropy              | -1.453     |
| Perplexity           | 0.23387    |
| AveragePolicyStd     | 0.19207    |
| AveragePolicyStd[0]  | 0.20639    |
| AveragePolicyStd[1]  | 0.20832    |
| AveragePolicyStd[2]  | 0.14916    |
| AveragePolicyStd[3]  | 0.19072    |
| AveragePolicyStd[4]  | 0.16438    |
| AveragePolicyStd[5]  | 0.23347    |
| AverageReturn        | 1747.6     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 215.19     |
| AverageEpisodeLength | 976.03     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.36     |
| TotalNEpisodes       | 23024      |
| TotalNSamples        | 6.0817e+06 |
| ExplainedVariance    | -0.013543  |
-------------------------------------
[2018-01-21 15:42:27.334683 UTC] Saving snapshot
[2018-01-21 15:42:27.334933 UTC] Starting iteration 1216
[2018-01-21 15:42:27.335107 UTC] Start collecting samples
[2018-01-21 15:42:31.830175 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:31.958446 UTC] Performing policy update
[2018-01-21 15:42:31.959110 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:32.073510 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:33.459249 UTC] Performing line search
[2018-01-21 15:42:33.673096 UTC] Updating baseline
[2018-01-21 15:42:36.408333 UTC] Computing logging information
--------------------------------------
| Iteration            | 1216        |
| ExpectedImprovement  | 0.015205    |
| ActualImprovement    | 0.01432     |
| ImprovementRatio     | 0.94179     |
| MeanKL               | 0.0079501   |
| Entropy              | -1.457      |
| Perplexity           | 0.23293     |
| AveragePolicyStd     | 0.19198     |
| AveragePolicyStd[0]  | 0.20625     |
| AveragePolicyStd[1]  | 0.20798     |
| AveragePolicyStd[2]  | 0.14865     |
| AveragePolicyStd[3]  | 0.19095     |
| AveragePolicyStd[4]  | 0.16421     |
| AveragePolicyStd[5]  | 0.23382     |
| AverageReturn        | 1745.2      |
| MinReturn            | 420.38      |
| MaxReturn            | 1872.5      |
| StdReturn            | 214.64      |
| AverageEpisodeLength | 976.03      |
| MinEpisodeLength     | 268         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 113.36      |
| TotalNEpisodes       | 23030       |
| TotalNSamples        | 6.0877e+06  |
| ExplainedVariance    | -0.00032219 |
--------------------------------------
[2018-01-21 15:42:37.197903 UTC] Saving snapshot
[2018-01-21 15:42:37.198201 UTC] Starting iteration 1217
[2018-01-21 15:42:37.198396 UTC] Start collecting samples
[2018-01-21 15:42:41.601560 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:41.717583 UTC] Performing policy update
[2018-01-21 15:42:41.718196 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:41.839512 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:43.212532 UTC] Performing line search
[2018-01-21 15:42:43.405108 UTC] Updating baseline
[2018-01-21 15:42:46.601343 UTC] Computing logging information
-------------------------------------
| Iteration            | 1217       |
| ExpectedImprovement  | 0.016884   |
| ActualImprovement    | 0.016046   |
| ImprovementRatio     | 0.95041    |
| MeanKL               | 0.0076184  |
| Entropy              | -1.45      |
| Perplexity           | 0.23458    |
| AveragePolicyStd     | 0.19221    |
| AveragePolicyStd[0]  | 0.2065     |
| AveragePolicyStd[1]  | 0.20783    |
| AveragePolicyStd[2]  | 0.1487     |
| AveragePolicyStd[3]  | 0.19149    |
| AveragePolicyStd[4]  | 0.16441    |
| AveragePolicyStd[5]  | 0.23433    |
| AverageReturn        | 1753.9     |
| MinReturn            | 420.38     |
| MaxReturn            | 1872.5     |
| StdReturn            | 194.44     |
| AverageEpisodeLength | 981.18     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.07     |
| TotalNEpisodes       | 23033      |
| TotalNSamples        | 6.0907e+06 |
| ExplainedVariance    | 3.4821e-05 |
-------------------------------------
[2018-01-21 15:42:47.418211 UTC] Saving snapshot
[2018-01-21 15:42:47.418501 UTC] Starting iteration 1218
[2018-01-21 15:42:47.418682 UTC] Start collecting samples
[2018-01-21 15:42:51.821882 UTC] Computing input variables for policy optimization
[2018-01-21 15:42:51.967843 UTC] Performing policy update
[2018-01-21 15:42:51.968543 UTC] Computing gradient in Euclidean space
[2018-01-21 15:42:52.089718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:42:53.453573 UTC] Performing line search
[2018-01-21 15:42:53.644681 UTC] Updating baseline
[2018-01-21 15:42:56.845715 UTC] Computing logging information
--------------------------------------
| Iteration            | 1218        |
| ExpectedImprovement  | 0.016318    |
| ActualImprovement    | 0.015644    |
| ImprovementRatio     | 0.95866     |
| MeanKL               | 0.0076533   |
| Entropy              | -1.4419     |
| Perplexity           | 0.23649     |
| AveragePolicyStd     | 0.19246     |
| AveragePolicyStd[0]  | 0.20685     |
| AveragePolicyStd[1]  | 0.20769     |
| AveragePolicyStd[2]  | 0.14927     |
| AveragePolicyStd[3]  | 0.19147     |
| AveragePolicyStd[4]  | 0.1646      |
| AveragePolicyStd[5]  | 0.23488     |
| AverageReturn        | 1751.1      |
| MinReturn            | 420.38      |
| MaxReturn            | 1863        |
| StdReturn            | 193.96      |
| AverageEpisodeLength | 981.18      |
| MinEpisodeLength     | 268         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 102.07      |
| TotalNEpisodes       | 23039       |
| TotalNSamples        | 6.0967e+06  |
| ExplainedVariance    | -5.8765e-05 |
--------------------------------------
[2018-01-21 15:42:57.625307 UTC] Saving snapshot
[2018-01-21 15:42:57.625668 UTC] Starting iteration 1219
[2018-01-21 15:42:57.625860 UTC] Start collecting samples
[2018-01-21 15:43:02.162366 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:02.301129 UTC] Performing policy update
[2018-01-21 15:43:02.301745 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:02.423927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:03.874765 UTC] Performing line search
[2018-01-21 15:43:04.072851 UTC] Updating baseline
[2018-01-21 15:43:07.915352 UTC] Computing logging information
-------------------------------------
| Iteration            | 1219       |
| ExpectedImprovement  | 0.01878    |
| ActualImprovement    | 0.017734   |
| ImprovementRatio     | 0.94431    |
| MeanKL               | 0.0079597  |
| Entropy              | -1.4396    |
| Perplexity           | 0.23702    |
| AveragePolicyStd     | 0.19255    |
| AveragePolicyStd[0]  | 0.20658    |
| AveragePolicyStd[1]  | 0.20763    |
| AveragePolicyStd[2]  | 0.14929    |
| AveragePolicyStd[3]  | 0.1917     |
| AveragePolicyStd[4]  | 0.16455    |
| AveragePolicyStd[5]  | 0.23553    |
| AverageReturn        | 1749.6     |
| MinReturn            | 420.38     |
| MaxReturn            | 1863       |
| StdReturn            | 193.75     |
| AverageEpisodeLength | 981.18     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.07     |
| TotalNEpisodes       | 23044      |
| TotalNSamples        | 6.1017e+06 |
| ExplainedVariance    | 0.00029312 |
-------------------------------------
[2018-01-21 15:43:08.739795 UTC] Saving snapshot
[2018-01-21 15:43:08.740084 UTC] Starting iteration 1220
[2018-01-21 15:43:08.740270 UTC] Start collecting samples
[2018-01-21 15:43:13.175818 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:13.305027 UTC] Performing policy update
[2018-01-21 15:43:13.306471 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:13.420807 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:14.796465 UTC] Performing line search
[2018-01-21 15:43:14.994666 UTC] Updating baseline
[2018-01-21 15:43:17.075751 UTC] Computing logging information
-------------------------------------
| Iteration            | 1220       |
| ExpectedImprovement  | 0.019582   |
| ActualImprovement    | 0.018195   |
| ImprovementRatio     | 0.92917    |
| MeanKL               | 0.0076665  |
| Entropy              | -1.4455    |
| Perplexity           | 0.23563    |
| AveragePolicyStd     | 0.19235    |
| AveragePolicyStd[0]  | 0.20642    |
| AveragePolicyStd[1]  | 0.20727    |
| AveragePolicyStd[2]  | 0.14938    |
| AveragePolicyStd[3]  | 0.19124    |
| AveragePolicyStd[4]  | 0.1643     |
| AveragePolicyStd[5]  | 0.2355     |
| AverageReturn        | 1749.1     |
| MinReturn            | 420.38     |
| MaxReturn            | 1863       |
| StdReturn            | 193.8      |
| AverageEpisodeLength | 981.18     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.07     |
| TotalNEpisodes       | 23049      |
| TotalNSamples        | 6.1067e+06 |
| ExplainedVariance    | 0.0021701  |
-------------------------------------
[2018-01-21 15:43:17.829930 UTC] Saving snapshot
[2018-01-21 15:43:17.840278 UTC] Starting iteration 1221
[2018-01-21 15:43:17.840514 UTC] Start collecting samples
[2018-01-21 15:43:22.759854 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:22.903659 UTC] Performing policy update
[2018-01-21 15:43:22.904369 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:23.032004 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:24.462789 UTC] Performing line search
[2018-01-21 15:43:24.652749 UTC] Updating baseline
[2018-01-21 15:43:28.467886 UTC] Computing logging information
-------------------------------------
| Iteration            | 1221       |
| ExpectedImprovement  | 0.019267   |
| ActualImprovement    | 0.017784   |
| ImprovementRatio     | 0.92301    |
| MeanKL               | 0.0073115  |
| Entropy              | -1.4467    |
| Perplexity           | 0.23536    |
| AveragePolicyStd     | 0.19232    |
| AveragePolicyStd[0]  | 0.20667    |
| AveragePolicyStd[1]  | 0.20714    |
| AveragePolicyStd[2]  | 0.14922    |
| AveragePolicyStd[3]  | 0.19109    |
| AveragePolicyStd[4]  | 0.16433    |
| AveragePolicyStd[5]  | 0.23549    |
| AverageReturn        | 1746.5     |
| MinReturn            | 420.38     |
| MaxReturn            | 1863       |
| StdReturn            | 193.45     |
| AverageEpisodeLength | 981.18     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.07     |
| TotalNEpisodes       | 23054      |
| TotalNSamples        | 6.1117e+06 |
| ExplainedVariance    | -0.0015051 |
-------------------------------------
[2018-01-21 15:43:29.329995 UTC] Saving snapshot
[2018-01-21 15:43:29.330204 UTC] Starting iteration 1222
[2018-01-21 15:43:29.330345 UTC] Start collecting samples
[2018-01-21 15:43:33.899400 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:34.033650 UTC] Performing policy update
[2018-01-21 15:43:34.034764 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:34.152975 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:35.546085 UTC] Performing line search
[2018-01-21 15:43:35.756251 UTC] Updating baseline
[2018-01-21 15:43:38.427332 UTC] Computing logging information
-------------------------------------
| Iteration            | 1222       |
| ExpectedImprovement  | 0.018709   |
| ActualImprovement    | 0.017805   |
| ImprovementRatio     | 0.95172    |
| MeanKL               | 0.0078308  |
| Entropy              | -1.4527    |
| Perplexity           | 0.23393    |
| AveragePolicyStd     | 0.19212    |
| AveragePolicyStd[0]  | 0.20701    |
| AveragePolicyStd[1]  | 0.20662    |
| AveragePolicyStd[2]  | 0.14923    |
| AveragePolicyStd[3]  | 0.19105    |
| AveragePolicyStd[4]  | 0.16392    |
| AveragePolicyStd[5]  | 0.23488    |
| AverageReturn        | 1733.4     |
| MinReturn            | 420.38     |
| MaxReturn            | 1849.3     |
| StdReturn            | 218.28     |
| AverageEpisodeLength | 975.46     |
| MinEpisodeLength     | 268        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.94     |
| TotalNEpisodes       | 23060      |
| TotalNSamples        | 6.1171e+06 |
| ExplainedVariance    | 0.10063    |
-------------------------------------
[2018-01-21 15:43:39.231461 UTC] Saving snapshot
[2018-01-21 15:43:39.231789 UTC] Starting iteration 1223
[2018-01-21 15:43:39.232015 UTC] Start collecting samples
[2018-01-21 15:43:43.686236 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:43.851774 UTC] Performing policy update
[2018-01-21 15:43:43.852890 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:43.970457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:45.407052 UTC] Performing line search
[2018-01-21 15:43:45.603218 UTC] Updating baseline
[2018-01-21 15:43:47.544909 UTC] Computing logging information
-------------------------------------
| Iteration            | 1223       |
| ExpectedImprovement  | 0.018446   |
| ActualImprovement    | 0.017389   |
| ImprovementRatio     | 0.94271    |
| MeanKL               | 0.0086279  |
| Entropy              | -1.4485    |
| Perplexity           | 0.23493    |
| AveragePolicyStd     | 0.19226    |
| AveragePolicyStd[0]  | 0.20727    |
| AveragePolicyStd[1]  | 0.20668    |
| AveragePolicyStd[2]  | 0.14909    |
| AveragePolicyStd[3]  | 0.19144    |
| AveragePolicyStd[4]  | 0.16419    |
| AveragePolicyStd[5]  | 0.23489    |
| AverageReturn        | 1717.5     |
| MinReturn            | 277.46     |
| MaxReturn            | 1849.3     |
| StdReturn            | 261.75     |
| AverageEpisodeLength | 967.37     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.73     |
| TotalNEpisodes       | 23065      |
| TotalNSamples        | 6.1213e+06 |
| ExplainedVariance    | 0.10665    |
-------------------------------------
[2018-01-21 15:43:48.399680 UTC] Saving snapshot
[2018-01-21 15:43:48.399986 UTC] Starting iteration 1224
[2018-01-21 15:43:48.400203 UTC] Start collecting samples
[2018-01-21 15:43:52.915722 UTC] Computing input variables for policy optimization
[2018-01-21 15:43:53.055385 UTC] Performing policy update
[2018-01-21 15:43:53.056325 UTC] Computing gradient in Euclidean space
[2018-01-21 15:43:53.172101 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:43:54.611192 UTC] Performing line search
[2018-01-21 15:43:54.813926 UTC] Updating baseline
[2018-01-21 15:43:56.966204 UTC] Computing logging information
-------------------------------------
| Iteration            | 1224       |
| ExpectedImprovement  | 0.017911   |
| ActualImprovement    | 0.016676   |
| ImprovementRatio     | 0.93105    |
| MeanKL               | 0.0072166  |
| Entropy              | -1.4572    |
| Perplexity           | 0.23289    |
| AveragePolicyStd     | 0.19198    |
| AveragePolicyStd[0]  | 0.20679    |
| AveragePolicyStd[1]  | 0.20673    |
| AveragePolicyStd[2]  | 0.14902    |
| AveragePolicyStd[3]  | 0.19055    |
| AveragePolicyStd[4]  | 0.164      |
| AveragePolicyStd[5]  | 0.23481    |
| AverageReturn        | 1716       |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 261.47     |
| AverageEpisodeLength | 967.37     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.73     |
| TotalNEpisodes       | 23071      |
| TotalNSamples        | 6.1273e+06 |
| ExplainedVariance    | -0.013846  |
-------------------------------------
[2018-01-21 15:43:57.748212 UTC] Saving snapshot
[2018-01-21 15:43:57.748496 UTC] Starting iteration 1225
[2018-01-21 15:43:57.748728 UTC] Start collecting samples
[2018-01-21 15:44:02.201639 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:02.338113 UTC] Performing policy update
[2018-01-21 15:44:02.338757 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:02.458189 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:03.894816 UTC] Performing line search
[2018-01-21 15:44:04.083630 UTC] Updating baseline
[2018-01-21 15:44:06.663300 UTC] Computing logging information
--------------------------------------
| Iteration            | 1225        |
| ExpectedImprovement  | 0.018115    |
| ActualImprovement    | 0.017252    |
| ImprovementRatio     | 0.95236     |
| MeanKL               | 0.0078714   |
| Entropy              | -1.4556     |
| Perplexity           | 0.23326     |
| AveragePolicyStd     | 0.19206     |
| AveragePolicyStd[0]  | 0.20712     |
| AveragePolicyStd[1]  | 0.20698     |
| AveragePolicyStd[2]  | 0.1488      |
| AveragePolicyStd[3]  | 0.1904      |
| AveragePolicyStd[4]  | 0.16397     |
| AveragePolicyStd[5]  | 0.23508     |
| AverageReturn        | 1714.7      |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 261.16      |
| AverageEpisodeLength | 967.37      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 139.73      |
| TotalNEpisodes       | 23076       |
| TotalNSamples        | 6.1323e+06  |
| ExplainedVariance    | -2.4432e-05 |
--------------------------------------
[2018-01-21 15:44:07.517183 UTC] Saving snapshot
[2018-01-21 15:44:07.517411 UTC] Starting iteration 1226
[2018-01-21 15:44:07.517561 UTC] Start collecting samples
[2018-01-21 15:44:12.067210 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:12.205275 UTC] Performing policy update
[2018-01-21 15:44:12.205885 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:12.323524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:13.754092 UTC] Performing line search
[2018-01-21 15:44:13.943828 UTC] Updating baseline
[2018-01-21 15:44:17.195130 UTC] Computing logging information
-------------------------------------
| Iteration            | 1226       |
| ExpectedImprovement  | 0.019551   |
| ActualImprovement    | 0.018017   |
| ImprovementRatio     | 0.92156    |
| MeanKL               | 0.0072227  |
| Entropy              | -1.4564    |
| Perplexity           | 0.23307    |
| AveragePolicyStd     | 0.19206    |
| AveragePolicyStd[0]  | 0.20745    |
| AveragePolicyStd[1]  | 0.20693    |
| AveragePolicyStd[2]  | 0.14863    |
| AveragePolicyStd[3]  | 0.19045    |
| AveragePolicyStd[4]  | 0.16367    |
| AveragePolicyStd[5]  | 0.2352     |
| AverageReturn        | 1716.3     |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 259.8      |
| AverageEpisodeLength | 969.17     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.98     |
| TotalNEpisodes       | 23081      |
| TotalNSamples        | 6.1373e+06 |
| ExplainedVariance    | 4.5086e-05 |
-------------------------------------
[2018-01-21 15:44:17.957367 UTC] Saving snapshot
[2018-01-21 15:44:17.957612 UTC] Starting iteration 1227
[2018-01-21 15:44:17.957779 UTC] Start collecting samples
[2018-01-21 15:44:22.326257 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:22.446668 UTC] Performing policy update
[2018-01-21 15:44:22.447306 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:22.565844 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:24.007016 UTC] Performing line search
[2018-01-21 15:44:24.196660 UTC] Updating baseline
[2018-01-21 15:44:27.111727 UTC] Computing logging information
--------------------------------------
| Iteration            | 1227        |
| ExpectedImprovement  | 0.017954    |
| ActualImprovement    | 0.016561    |
| ImprovementRatio     | 0.92244     |
| MeanKL               | 0.0081522   |
| Entropy              | -1.4602     |
| Perplexity           | 0.23219     |
| AveragePolicyStd     | 0.19191     |
| AveragePolicyStd[0]  | 0.20759     |
| AveragePolicyStd[1]  | 0.20672     |
| AveragePolicyStd[2]  | 0.14911     |
| AveragePolicyStd[3]  | 0.1901      |
| AveragePolicyStd[4]  | 0.16322     |
| AveragePolicyStd[5]  | 0.23472     |
| AverageReturn        | 1715        |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 259.35      |
| AverageEpisodeLength | 969.17      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 138.98      |
| TotalNEpisodes       | 23086       |
| TotalNSamples        | 6.1423e+06  |
| ExplainedVariance    | -0.00059044 |
--------------------------------------
[2018-01-21 15:44:27.868762 UTC] Saving snapshot
[2018-01-21 15:44:27.869071 UTC] Starting iteration 1228
[2018-01-21 15:44:27.869274 UTC] Start collecting samples
[2018-01-21 15:44:32.432056 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:32.565289 UTC] Performing policy update
[2018-01-21 15:44:32.566543 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:32.685900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:34.098939 UTC] Performing line search
[2018-01-21 15:44:34.286618 UTC] Updating baseline
[2018-01-21 15:44:36.452645 UTC] Computing logging information
--------------------------------------
| Iteration            | 1228        |
| ExpectedImprovement  | 0.019256    |
| ActualImprovement    | 0.017826    |
| ImprovementRatio     | 0.9257      |
| MeanKL               | 0.008212    |
| Entropy              | -1.4679     |
| Perplexity           | 0.23041     |
| AveragePolicyStd     | 0.19167     |
| AveragePolicyStd[0]  | 0.2079      |
| AveragePolicyStd[1]  | 0.20633     |
| AveragePolicyStd[2]  | 0.14896     |
| AveragePolicyStd[3]  | 0.18952     |
| AveragePolicyStd[4]  | 0.16298     |
| AveragePolicyStd[5]  | 0.23431     |
| AverageReturn        | 1723.2      |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 253.73      |
| AverageEpisodeLength | 972.63      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 135.35      |
| TotalNEpisodes       | 23091       |
| TotalNSamples        | 6.1473e+06  |
| ExplainedVariance    | -0.00019577 |
--------------------------------------
[2018-01-21 15:44:37.332329 UTC] Saving snapshot
[2018-01-21 15:44:37.332608 UTC] Starting iteration 1229
[2018-01-21 15:44:37.332783 UTC] Start collecting samples
[2018-01-21 15:44:41.727484 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:41.870047 UTC] Performing policy update
[2018-01-21 15:44:41.870733 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:41.988898 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:43.432786 UTC] Performing line search
[2018-01-21 15:44:43.625070 UTC] Updating baseline
[2018-01-21 15:44:46.209446 UTC] Computing logging information
--------------------------------------
| Iteration            | 1229        |
| ExpectedImprovement  | 0.019617    |
| ActualImprovement    | 0.018742    |
| ImprovementRatio     | 0.95541     |
| MeanKL               | 0.0073163   |
| Entropy              | -1.468      |
| Perplexity           | 0.23039     |
| AveragePolicyStd     | 0.19167     |
| AveragePolicyStd[0]  | 0.20787     |
| AveragePolicyStd[1]  | 0.20642     |
| AveragePolicyStd[2]  | 0.14901     |
| AveragePolicyStd[3]  | 0.18952     |
| AveragePolicyStd[4]  | 0.16277     |
| AveragePolicyStd[5]  | 0.23444     |
| AverageReturn        | 1736.5      |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 217.35      |
| AverageEpisodeLength | 979.95      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 115.36      |
| TotalNEpisodes       | 23096       |
| TotalNSamples        | 6.1523e+06  |
| ExplainedVariance    | -0.00029612 |
--------------------------------------
[2018-01-21 15:44:47.066382 UTC] Saving snapshot
[2018-01-21 15:44:47.066643 UTC] Starting iteration 1230
[2018-01-21 15:44:47.067119 UTC] Start collecting samples
[2018-01-21 15:44:51.426903 UTC] Computing input variables for policy optimization
[2018-01-21 15:44:51.562517 UTC] Performing policy update
[2018-01-21 15:44:51.563216 UTC] Computing gradient in Euclidean space
[2018-01-21 15:44:51.697307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:44:53.104086 UTC] Performing line search
[2018-01-21 15:44:53.293674 UTC] Updating baseline
[2018-01-21 15:44:54.911394 UTC] Computing logging information
--------------------------------------
| Iteration            | 1230        |
| ExpectedImprovement  | 0.019126    |
| ActualImprovement    | 0.016971    |
| ImprovementRatio     | 0.8873      |
| MeanKL               | 0.0078557   |
| Entropy              | -1.4661     |
| Perplexity           | 0.23081     |
| AveragePolicyStd     | 0.19174     |
| AveragePolicyStd[0]  | 0.2082      |
| AveragePolicyStd[1]  | 0.20676     |
| AveragePolicyStd[2]  | 0.1488      |
| AveragePolicyStd[3]  | 0.18963     |
| AveragePolicyStd[4]  | 0.16284     |
| AveragePolicyStd[5]  | 0.2342      |
| AverageReturn        | 1736.3      |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 217.23      |
| AverageEpisodeLength | 979.95      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 115.36      |
| TotalNEpisodes       | 23099       |
| TotalNSamples        | 6.1553e+06  |
| ExplainedVariance    | -3.6994e-09 |
--------------------------------------
[2018-01-21 15:44:55.688441 UTC] Saving snapshot
[2018-01-21 15:44:55.694805 UTC] Starting iteration 1231
[2018-01-21 15:44:55.694990 UTC] Start collecting samples
[2018-01-21 15:45:00.547103 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:00.668458 UTC] Performing policy update
[2018-01-21 15:45:00.669091 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:00.785812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:02.170621 UTC] Performing line search
[2018-01-21 15:45:02.363956 UTC] Updating baseline
[2018-01-21 15:45:04.326983 UTC] Computing logging information
--------------------------------------
| Iteration            | 1231        |
| ExpectedImprovement  | 0.015932    |
| ActualImprovement    | 0.015258    |
| ImprovementRatio     | 0.95766     |
| MeanKL               | 0.0082704   |
| Entropy              | -1.4692     |
| Perplexity           | 0.23011     |
| AveragePolicyStd     | 0.19166     |
| AveragePolicyStd[0]  | 0.20846     |
| AveragePolicyStd[1]  | 0.20692     |
| AveragePolicyStd[2]  | 0.14852     |
| AveragePolicyStd[3]  | 0.18963     |
| AveragePolicyStd[4]  | 0.16257     |
| AveragePolicyStd[5]  | 0.23386     |
| AverageReturn        | 1735.5      |
| MinReturn            | 277.46      |
| MaxReturn            | 1853.6      |
| StdReturn            | 217.25      |
| AverageEpisodeLength | 979.95      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 115.36      |
| TotalNEpisodes       | 23107       |
| TotalNSamples        | 6.1633e+06  |
| ExplainedVariance    | -4.4416e-09 |
--------------------------------------
[2018-01-21 15:45:05.089722 UTC] Saving snapshot
[2018-01-21 15:45:05.089999 UTC] Starting iteration 1232
[2018-01-21 15:45:05.090194 UTC] Start collecting samples
[2018-01-21 15:45:09.358039 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:09.513024 UTC] Performing policy update
[2018-01-21 15:45:09.513656 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:09.630361 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:11.012449 UTC] Performing line search
[2018-01-21 15:45:11.207449 UTC] Updating baseline
[2018-01-21 15:45:13.608181 UTC] Computing logging information
-------------------------------------
| Iteration            | 1232       |
| ExpectedImprovement  | 0.017039   |
| ActualImprovement    | 0.016241   |
| ImprovementRatio     | 0.95313    |
| MeanKL               | 0.0077041  |
| Entropy              | -1.4684    |
| Perplexity           | 0.23028    |
| AveragePolicyStd     | 0.19169    |
| AveragePolicyStd[0]  | 0.20858    |
| AveragePolicyStd[1]  | 0.20676    |
| AveragePolicyStd[2]  | 0.14844    |
| AveragePolicyStd[3]  | 0.18989    |
| AveragePolicyStd[4]  | 0.16253    |
| AveragePolicyStd[5]  | 0.23395    |
| AverageReturn        | 1736       |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 217.42     |
| AverageEpisodeLength | 979.95     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.36     |
| TotalNEpisodes       | 23111      |
| TotalNSamples        | 6.1673e+06 |
| ExplainedVariance    | 6.421e-08  |
-------------------------------------
[2018-01-21 15:45:14.348285 UTC] Saving snapshot
[2018-01-21 15:45:14.348522 UTC] Starting iteration 1233
[2018-01-21 15:45:14.348669 UTC] Start collecting samples
[2018-01-21 15:45:18.865919 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:18.990191 UTC] Performing policy update
[2018-01-21 15:45:18.991218 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:19.115768 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:20.527824 UTC] Performing line search
[2018-01-21 15:45:20.726391 UTC] Updating baseline
[2018-01-21 15:45:23.251286 UTC] Computing logging information
-------------------------------------
| Iteration            | 1233       |
| ExpectedImprovement  | 0.017615   |
| ActualImprovement    | 0.016283   |
| ImprovementRatio     | 0.92442    |
| MeanKL               | 0.0084389  |
| Entropy              | -1.4671    |
| Perplexity           | 0.2306     |
| AveragePolicyStd     | 0.19173    |
| AveragePolicyStd[0]  | 0.20875    |
| AveragePolicyStd[1]  | 0.20603    |
| AveragePolicyStd[2]  | 0.1485     |
| AveragePolicyStd[3]  | 0.19028    |
| AveragePolicyStd[4]  | 0.1626     |
| AveragePolicyStd[5]  | 0.23422    |
| AverageReturn        | 1737.2     |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 217.69     |
| AverageEpisodeLength | 979.95     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 115.36     |
| TotalNEpisodes       | 23114      |
| TotalNSamples        | 6.1703e+06 |
| ExplainedVariance    | 7.6164e-09 |
-------------------------------------
[2018-01-21 15:45:24.024033 UTC] Saving snapshot
[2018-01-21 15:45:24.024292 UTC] Starting iteration 1234
[2018-01-21 15:45:24.024477 UTC] Start collecting samples
[2018-01-21 15:45:28.693039 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:28.825602 UTC] Performing policy update
[2018-01-21 15:45:28.827014 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:28.952433 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:30.361118 UTC] Performing line search
[2018-01-21 15:45:30.561235 UTC] Updating baseline
[2018-01-21 15:45:31.691456 UTC] Computing logging information
-------------------------------------
| Iteration            | 1234       |
| ExpectedImprovement  | 0.018472   |
| ActualImprovement    | 0.017851   |
| ImprovementRatio     | 0.96639    |
| MeanKL               | 0.0075534  |
| Entropy              | -1.4744    |
| Perplexity           | 0.22893    |
| AveragePolicyStd     | 0.1915     |
| AveragePolicyStd[0]  | 0.20888    |
| AveragePolicyStd[1]  | 0.20605    |
| AveragePolicyStd[2]  | 0.14827    |
| AveragePolicyStd[3]  | 0.19015    |
| AveragePolicyStd[4]  | 0.16219    |
| AveragePolicyStd[5]  | 0.23349    |
| AverageReturn        | 1750.7     |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 185.72     |
| AverageEpisodeLength | 986.19     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.112     |
| TotalNEpisodes       | 23121      |
| TotalNSamples        | 6.1773e+06 |
| ExplainedVariance    | -1.041e-08 |
-------------------------------------
[2018-01-21 15:45:32.566138 UTC] Saving snapshot
[2018-01-21 15:45:32.566470 UTC] Starting iteration 1235
[2018-01-21 15:45:32.566691 UTC] Start collecting samples
[2018-01-21 15:45:37.039830 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:37.164691 UTC] Performing policy update
[2018-01-21 15:45:37.165300 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:37.281959 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:38.693169 UTC] Performing line search
[2018-01-21 15:45:38.882927 UTC] Updating baseline
[2018-01-21 15:45:44.163641 UTC] Computing logging information
-------------------------------------
| Iteration            | 1235       |
| ExpectedImprovement  | 0.018977   |
| ActualImprovement    | 0.017476   |
| ImprovementRatio     | 0.92092    |
| MeanKL               | 0.0081931  |
| Entropy              | -1.4791    |
| Perplexity           | 0.22785    |
| AveragePolicyStd     | 0.19133    |
| AveragePolicyStd[0]  | 0.20865    |
| AveragePolicyStd[1]  | 0.20599    |
| AveragePolicyStd[2]  | 0.14832    |
| AveragePolicyStd[3]  | 0.1903     |
| AveragePolicyStd[4]  | 0.16196    |
| AveragePolicyStd[5]  | 0.23277    |
| AverageReturn        | 1752.8     |
| MinReturn            | 277.46     |
| MaxReturn            | 1853.6     |
| StdReturn            | 186.02     |
| AverageEpisodeLength | 986.19     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.112     |
| TotalNEpisodes       | 23125      |
| TotalNSamples        | 6.1813e+06 |
| ExplainedVariance    | 2.4646e-10 |
-------------------------------------
[2018-01-21 15:45:44.941556 UTC] Saving snapshot
[2018-01-21 15:45:44.941768 UTC] Starting iteration 1236
[2018-01-21 15:45:44.941921 UTC] Start collecting samples
[2018-01-21 15:45:49.555561 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:49.686394 UTC] Performing policy update
[2018-01-21 15:45:49.687557 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:49.806726 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:45:51.220215 UTC] Performing line search
[2018-01-21 15:45:51.426229 UTC] Updating baseline
[2018-01-21 15:45:54.185750 UTC] Computing logging information
--------------------------------------
| Iteration            | 1236        |
| ExpectedImprovement  | 0.016175    |
| ActualImprovement    | 0.015393    |
| ImprovementRatio     | 0.95162     |
| MeanKL               | 0.0079791   |
| Entropy              | -1.4837     |
| Perplexity           | 0.2268      |
| AveragePolicyStd     | 0.19119     |
| AveragePolicyStd[0]  | 0.20919     |
| AveragePolicyStd[1]  | 0.20597     |
| AveragePolicyStd[2]  | 0.14819     |
| AveragePolicyStd[3]  | 0.18949     |
| AveragePolicyStd[4]  | 0.16183     |
| AveragePolicyStd[5]  | 0.23251     |
| AverageReturn        | 1756.5      |
| MinReturn            | 277.46      |
| MaxReturn            | 1874.3      |
| StdReturn            | 187.2       |
| AverageEpisodeLength | 986.19      |
| MinEpisodeLength     | 191         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 98.112      |
| TotalNEpisodes       | 23130       |
| TotalNSamples        | 6.1863e+06  |
| ExplainedVariance    | -0.00070222 |
--------------------------------------
[2018-01-21 15:45:54.962766 UTC] Saving snapshot
[2018-01-21 15:45:54.963086 UTC] Starting iteration 1237
[2018-01-21 15:45:54.963309 UTC] Start collecting samples
[2018-01-21 15:45:59.475707 UTC] Computing input variables for policy optimization
[2018-01-21 15:45:59.635359 UTC] Performing policy update
[2018-01-21 15:45:59.636596 UTC] Computing gradient in Euclidean space
[2018-01-21 15:45:59.759385 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:01.172050 UTC] Performing line search
[2018-01-21 15:46:01.366687 UTC] Updating baseline
[2018-01-21 15:46:02.886825 UTC] Computing logging information
-------------------------------------
| Iteration            | 1237       |
| ExpectedImprovement  | 0.016511   |
| ActualImprovement    | 0.015598   |
| ImprovementRatio     | 0.94471    |
| MeanKL               | 0.0075977  |
| Entropy              | -1.4834    |
| Perplexity           | 0.22687    |
| AveragePolicyStd     | 0.19122    |
| AveragePolicyStd[0]  | 0.20959    |
| AveragePolicyStd[1]  | 0.20585    |
| AveragePolicyStd[2]  | 0.148      |
| AveragePolicyStd[3]  | 0.1893     |
| AveragePolicyStd[4]  | 0.16192    |
| AveragePolicyStd[5]  | 0.23266    |
| AverageReturn        | 1759.5     |
| MinReturn            | 277.46     |
| MaxReturn            | 1874.3     |
| StdReturn            | 187.51     |
| AverageEpisodeLength | 986.19     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.112     |
| TotalNEpisodes       | 23135      |
| TotalNSamples        | 6.1913e+06 |
| ExplainedVariance    | 3.3541e-10 |
-------------------------------------
[2018-01-21 15:46:03.678630 UTC] Saving snapshot
[2018-01-21 15:46:03.678896 UTC] Starting iteration 1238
[2018-01-21 15:46:03.679051 UTC] Start collecting samples
[2018-01-21 15:46:08.231703 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:08.372472 UTC] Performing policy update
[2018-01-21 15:46:08.373593 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:08.493860 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:09.949056 UTC] Performing line search
[2018-01-21 15:46:10.141021 UTC] Updating baseline
[2018-01-21 15:46:12.307541 UTC] Computing logging information
-------------------------------------
| Iteration            | 1238       |
| ExpectedImprovement  | 0.018445   |
| ActualImprovement    | 0.017941   |
| ImprovementRatio     | 0.97268    |
| MeanKL               | 0.0080179  |
| Entropy              | -1.488     |
| Perplexity           | 0.22582    |
| AveragePolicyStd     | 0.19109    |
| AveragePolicyStd[0]  | 0.20935    |
| AveragePolicyStd[1]  | 0.20597    |
| AveragePolicyStd[2]  | 0.14769    |
| AveragePolicyStd[3]  | 0.18905    |
| AveragePolicyStd[4]  | 0.16182    |
| AveragePolicyStd[5]  | 0.23265    |
| AverageReturn        | 1761.2     |
| MinReturn            | 277.46     |
| MaxReturn            | 1875.6     |
| StdReturn            | 189.26     |
| AverageEpisodeLength | 984.95     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.711     |
| TotalNEpisodes       | 23141      |
| TotalNSamples        | 6.1972e+06 |
| ExplainedVariance    | 0.099502   |
-------------------------------------
[2018-01-21 15:46:13.074634 UTC] Saving snapshot
[2018-01-21 15:46:13.074884 UTC] Starting iteration 1239
[2018-01-21 15:46:13.075030 UTC] Start collecting samples
[2018-01-21 15:46:17.650123 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:17.777473 UTC] Performing policy update
[2018-01-21 15:46:17.778642 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:17.898107 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:19.311248 UTC] Performing line search
[2018-01-21 15:46:19.497333 UTC] Updating baseline
[2018-01-21 15:46:22.304787 UTC] Computing logging information
-------------------------------------
| Iteration            | 1239       |
| ExpectedImprovement  | 0.017455   |
| ActualImprovement    | 0.016608   |
| ImprovementRatio     | 0.95149    |
| MeanKL               | 0.0077362  |
| Entropy              | -1.4784    |
| Perplexity           | 0.228      |
| AveragePolicyStd     | 0.19141    |
| AveragePolicyStd[0]  | 0.20916    |
| AveragePolicyStd[1]  | 0.20615    |
| AveragePolicyStd[2]  | 0.14796    |
| AveragePolicyStd[3]  | 0.18923    |
| AveragePolicyStd[4]  | 0.16211    |
| AveragePolicyStd[5]  | 0.23383    |
| AverageReturn        | 1764.6     |
| MinReturn            | 277.46     |
| MaxReturn            | 1882.1     |
| StdReturn            | 189.81     |
| AverageEpisodeLength | 984.95     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 98.711     |
| TotalNEpisodes       | 23145      |
| TotalNSamples        | 6.2012e+06 |
| ExplainedVariance    | -0.060429  |
-------------------------------------
[2018-01-21 15:46:23.161756 UTC] Saving snapshot
[2018-01-21 15:46:23.162014 UTC] Starting iteration 1240
[2018-01-21 15:46:23.162175 UTC] Start collecting samples
[2018-01-21 15:46:27.756161 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:27.888102 UTC] Performing policy update
[2018-01-21 15:46:27.889304 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:28.029917 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:29.408546 UTC] Performing line search
[2018-01-21 15:46:29.600979 UTC] Updating baseline
[2018-01-21 15:46:31.533286 UTC] Computing logging information
-------------------------------------
| Iteration            | 1240       |
| ExpectedImprovement  | 0.018582   |
| ActualImprovement    | 0.017994   |
| ImprovementRatio     | 0.96833    |
| MeanKL               | 0.0087121  |
| Entropy              | -1.474     |
| Perplexity           | 0.229      |
| AveragePolicyStd     | 0.19155    |
| AveragePolicyStd[0]  | 0.20951    |
| AveragePolicyStd[1]  | 0.20651    |
| AveragePolicyStd[2]  | 0.14785    |
| AveragePolicyStd[3]  | 0.18915    |
| AveragePolicyStd[4]  | 0.1624     |
| AveragePolicyStd[5]  | 0.23391    |
| AverageReturn        | 1756.2     |
| MinReturn            | 277.46     |
| MaxReturn            | 1882.1     |
| StdReturn            | 208.94     |
| AverageEpisodeLength | 980.05     |
| MinEpisodeLength     | 191        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.42     |
| TotalNEpisodes       | 23152      |
| TotalNSamples        | 6.2077e+06 |
| ExplainedVariance    | 0.11331    |
-------------------------------------
[2018-01-21 15:46:32.303688 UTC] Saving snapshot
[2018-01-21 15:46:32.311030 UTC] Starting iteration 1241
[2018-01-21 15:46:32.311263 UTC] Start collecting samples
[2018-01-21 15:46:36.767603 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:36.895183 UTC] Performing policy update
[2018-01-21 15:46:36.896020 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:37.024140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:38.446587 UTC] Performing line search
[2018-01-21 15:46:38.629514 UTC] Updating baseline
[2018-01-21 15:46:40.498137 UTC] Computing logging information
------------------------------------
| Iteration            | 1241      |
| ExpectedImprovement  | 0.019384  |
| ActualImprovement    | 0.018536  |
| ImprovementRatio     | 0.95625   |
| MeanKL               | 0.0077984 |
| Entropy              | -1.4725   |
| Perplexity           | 0.22935   |
| AveragePolicyStd     | 0.19159   |
| AveragePolicyStd[0]  | 0.20955   |
| AveragePolicyStd[1]  | 0.20691   |
| AveragePolicyStd[2]  | 0.14816   |
| AveragePolicyStd[3]  | 0.18893   |
| AveragePolicyStd[4]  | 0.16231   |
| AveragePolicyStd[5]  | 0.23371   |
| AverageReturn        | 1741.4    |
| MinReturn            | 238.19    |
| MaxReturn            | 1882.1    |
| StdReturn            | 277.05    |
| AverageEpisodeLength | 969.42    |
| MinEpisodeLength     | 167       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 146.75    |
| TotalNEpisodes       | 23160     |
| TotalNSamples        | 6.214e+06 |
| ExplainedVariance    | 0.099963  |
------------------------------------
[2018-01-21 15:46:41.332583 UTC] Saving snapshot
[2018-01-21 15:46:41.332824 UTC] Starting iteration 1242
[2018-01-21 15:46:41.332978 UTC] Start collecting samples
[2018-01-21 15:46:45.675076 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:45.819367 UTC] Performing policy update
[2018-01-21 15:46:45.819976 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:45.944007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:47.346952 UTC] Performing line search
[2018-01-21 15:46:47.555594 UTC] Updating baseline
[2018-01-21 15:46:50.441203 UTC] Computing logging information
-------------------------------------
| Iteration            | 1242       |
| ExpectedImprovement  | 0.019633   |
| ActualImprovement    | 0.018622   |
| ImprovementRatio     | 0.94848    |
| MeanKL               | 0.007464   |
| Entropy              | -1.4732    |
| Perplexity           | 0.2292     |
| AveragePolicyStd     | 0.19155    |
| AveragePolicyStd[0]  | 0.20889    |
| AveragePolicyStd[1]  | 0.20715    |
| AveragePolicyStd[2]  | 0.14829    |
| AveragePolicyStd[3]  | 0.18922    |
| AveragePolicyStd[4]  | 0.16225    |
| AveragePolicyStd[5]  | 0.23353    |
| AverageReturn        | 1731.6     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 296.15     |
| AverageEpisodeLength | 963.42     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.27     |
| TotalNEpisodes       | 23164      |
| TotalNSamples        | 6.2174e+06 |
| ExplainedVariance    | 0.12565    |
-------------------------------------
[2018-01-21 15:46:51.305557 UTC] Saving snapshot
[2018-01-21 15:46:51.305832 UTC] Starting iteration 1243
[2018-01-21 15:46:51.306023 UTC] Start collecting samples
[2018-01-21 15:46:56.170282 UTC] Computing input variables for policy optimization
[2018-01-21 15:46:56.316411 UTC] Performing policy update
[2018-01-21 15:46:56.317691 UTC] Computing gradient in Euclidean space
[2018-01-21 15:46:56.439603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:46:57.892471 UTC] Performing line search
[2018-01-21 15:46:58.086976 UTC] Updating baseline
[2018-01-21 15:46:59.948434 UTC] Computing logging information
-------------------------------------
| Iteration            | 1243       |
| ExpectedImprovement  | 0.019714   |
| ActualImprovement    | 0.018073   |
| ImprovementRatio     | 0.91676    |
| MeanKL               | 0.0079205  |
| Entropy              | -1.4706    |
| Perplexity           | 0.22979    |
| AveragePolicyStd     | 0.19165    |
| AveragePolicyStd[0]  | 0.20912    |
| AveragePolicyStd[1]  | 0.20729    |
| AveragePolicyStd[2]  | 0.14819    |
| AveragePolicyStd[3]  | 0.18946    |
| AveragePolicyStd[4]  | 0.16215    |
| AveragePolicyStd[5]  | 0.23371    |
| AverageReturn        | 1748.2     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 257.81     |
| AverageEpisodeLength | 971.51     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.8      |
| TotalNEpisodes       | 23167      |
| TotalNSamples        | 6.2204e+06 |
| ExplainedVariance    | -0.032915  |
-------------------------------------
[2018-01-21 15:47:00.755235 UTC] Saving snapshot
[2018-01-21 15:47:00.755504 UTC] Starting iteration 1244
[2018-01-21 15:47:00.755684 UTC] Start collecting samples
[2018-01-21 15:47:05.314580 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:05.449083 UTC] Performing policy update
[2018-01-21 15:47:05.449723 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:05.580286 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:07.006120 UTC] Performing line search
[2018-01-21 15:47:07.192179 UTC] Updating baseline
[2018-01-21 15:47:10.770337 UTC] Computing logging information
-------------------------------------
| Iteration            | 1244       |
| ExpectedImprovement  | 0.019277   |
| ActualImprovement    | 0.018842   |
| ImprovementRatio     | 0.97743    |
| MeanKL               | 0.0077529  |
| Entropy              | -1.4764    |
| Perplexity           | 0.22847    |
| AveragePolicyStd     | 0.19149    |
| AveragePolicyStd[0]  | 0.20951    |
| AveragePolicyStd[1]  | 0.20747    |
| AveragePolicyStd[2]  | 0.14797    |
| AveragePolicyStd[3]  | 0.18864    |
| AveragePolicyStd[4]  | 0.1619     |
| AveragePolicyStd[5]  | 0.23343    |
| AverageReturn        | 1751.3     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 258.35     |
| AverageEpisodeLength | 971.51     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.8      |
| TotalNEpisodes       | 23175      |
| TotalNSamples        | 6.2284e+06 |
| ExplainedVariance    | -0.0040636 |
-------------------------------------
[2018-01-21 15:47:11.556001 UTC] Saving snapshot
[2018-01-21 15:47:11.556291 UTC] Starting iteration 1245
[2018-01-21 15:47:11.556487 UTC] Start collecting samples
[2018-01-21 15:47:16.181250 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:16.308970 UTC] Performing policy update
[2018-01-21 15:47:16.309996 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:16.427123 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:17.828572 UTC] Performing line search
[2018-01-21 15:47:18.022004 UTC] Updating baseline
[2018-01-21 15:47:20.594772 UTC] Computing logging information
-------------------------------------
| Iteration            | 1245       |
| ExpectedImprovement  | 0.017522   |
| ActualImprovement    | 0.016458   |
| ImprovementRatio     | 0.9393     |
| MeanKL               | 0.0078097  |
| Entropy              | -1.4745    |
| Perplexity           | 0.2289     |
| AveragePolicyStd     | 0.19157    |
| AveragePolicyStd[0]  | 0.20956    |
| AveragePolicyStd[1]  | 0.20804    |
| AveragePolicyStd[2]  | 0.14776    |
| AveragePolicyStd[3]  | 0.18869    |
| AveragePolicyStd[4]  | 0.16181    |
| AveragePolicyStd[5]  | 0.2336     |
| AverageReturn        | 1753       |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 258.72     |
| AverageEpisodeLength | 971.51     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.8      |
| TotalNEpisodes       | 23180      |
| TotalNSamples        | 6.2334e+06 |
| ExplainedVariance    | -2.931e-10 |
-------------------------------------
[2018-01-21 15:47:21.357424 UTC] Saving snapshot
[2018-01-21 15:47:21.357673 UTC] Starting iteration 1246
[2018-01-21 15:47:21.357857 UTC] Start collecting samples
[2018-01-21 15:47:25.699095 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:25.857300 UTC] Performing policy update
[2018-01-21 15:47:25.857937 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:26.014276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:27.439334 UTC] Performing line search
[2018-01-21 15:47:27.640078 UTC] Updating baseline
[2018-01-21 15:47:29.846726 UTC] Computing logging information
--------------------------------------
| Iteration            | 1246        |
| ExpectedImprovement  | 0.016581    |
| ActualImprovement    | 0.015465    |
| ImprovementRatio     | 0.93266     |
| MeanKL               | 0.0080854   |
| Entropy              | -1.4734     |
| Perplexity           | 0.22915     |
| AveragePolicyStd     | 0.19162     |
| AveragePolicyStd[0]  | 0.20893     |
| AveragePolicyStd[1]  | 0.20852     |
| AveragePolicyStd[2]  | 0.14765     |
| AveragePolicyStd[3]  | 0.18884     |
| AveragePolicyStd[4]  | 0.16183     |
| AveragePolicyStd[5]  | 0.23396     |
| AverageReturn        | 1753.9      |
| MinReturn            | 238.19      |
| MaxReturn            | 1882.1      |
| StdReturn            | 258.9       |
| AverageEpisodeLength | 971.51      |
| MinEpisodeLength     | 167         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 136.8       |
| TotalNEpisodes       | 23183       |
| TotalNSamples        | 6.2364e+06  |
| ExplainedVariance    | -2.7162e-09 |
--------------------------------------
[2018-01-21 15:47:30.655701 UTC] Saving snapshot
[2018-01-21 15:47:30.656113 UTC] Starting iteration 1247
[2018-01-21 15:47:30.656404 UTC] Start collecting samples
[2018-01-21 15:47:35.234352 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:35.370015 UTC] Performing policy update
[2018-01-21 15:47:35.370625 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:35.492987 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:36.940740 UTC] Performing line search
[2018-01-21 15:47:37.129274 UTC] Updating baseline
[2018-01-21 15:47:38.807368 UTC] Computing logging information
-------------------------------------
| Iteration            | 1247       |
| ExpectedImprovement  | 0.018261   |
| ActualImprovement    | 0.017402   |
| ImprovementRatio     | 0.95295    |
| MeanKL               | 0.0083781  |
| Entropy              | -1.4683    |
| Perplexity           | 0.23032    |
| AveragePolicyStd     | 0.19178    |
| AveragePolicyStd[0]  | 0.20938    |
| AveragePolicyStd[1]  | 0.2078     |
| AveragePolicyStd[2]  | 0.14793    |
| AveragePolicyStd[3]  | 0.1894     |
| AveragePolicyStd[4]  | 0.16188    |
| AveragePolicyStd[5]  | 0.23426    |
| AverageReturn        | 1754.7     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 259.07     |
| AverageEpisodeLength | 971.51     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.8      |
| TotalNEpisodes       | 23191      |
| TotalNSamples        | 6.2444e+06 |
| ExplainedVariance    | 4.0986e-09 |
-------------------------------------
[2018-01-21 15:47:39.591577 UTC] Saving snapshot
[2018-01-21 15:47:39.592135 UTC] Starting iteration 1248
[2018-01-21 15:47:39.592356 UTC] Start collecting samples
[2018-01-21 15:47:44.080243 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:44.197373 UTC] Performing policy update
[2018-01-21 15:47:44.198540 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:44.318780 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:45.724583 UTC] Performing line search
[2018-01-21 15:47:45.918068 UTC] Updating baseline
[2018-01-21 15:47:47.536883 UTC] Computing logging information
--------------------------------------
| Iteration            | 1248        |
| ExpectedImprovement  | 0.017698    |
| ActualImprovement    | 0.016371    |
| ImprovementRatio     | 0.92505     |
| MeanKL               | 0.0088829   |
| Entropy              | -1.4668     |
| Perplexity           | 0.23067     |
| AveragePolicyStd     | 0.19181     |
| AveragePolicyStd[0]  | 0.20899     |
| AveragePolicyStd[1]  | 0.20796     |
| AveragePolicyStd[2]  | 0.14807     |
| AveragePolicyStd[3]  | 0.18937     |
| AveragePolicyStd[4]  | 0.1621      |
| AveragePolicyStd[5]  | 0.23436     |
| AverageReturn        | 1755.9      |
| MinReturn            | 238.19      |
| MaxReturn            | 1882.1      |
| StdReturn            | 259.39      |
| AverageEpisodeLength | 971.51      |
| MinEpisodeLength     | 167         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 136.8       |
| TotalNEpisodes       | 23194       |
| TotalNSamples        | 6.2474e+06  |
| ExplainedVariance    | -6.9174e-09 |
--------------------------------------
[2018-01-21 15:47:48.293346 UTC] Saving snapshot
[2018-01-21 15:47:48.293635 UTC] Starting iteration 1249
[2018-01-21 15:47:48.293862 UTC] Start collecting samples
[2018-01-21 15:47:52.844452 UTC] Computing input variables for policy optimization
[2018-01-21 15:47:52.993773 UTC] Performing policy update
[2018-01-21 15:47:52.994419 UTC] Computing gradient in Euclidean space
[2018-01-21 15:47:53.111240 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:47:54.594115 UTC] Performing line search
[2018-01-21 15:47:54.830077 UTC] Updating baseline
[2018-01-21 15:47:57.243892 UTC] Computing logging information
-------------------------------------
| Iteration            | 1249       |
| ExpectedImprovement  | 0.018157   |
| ActualImprovement    | 0.017      |
| ImprovementRatio     | 0.93632    |
| MeanKL               | 0.0075375  |
| Entropy              | -1.4704    |
| Perplexity           | 0.22983    |
| AveragePolicyStd     | 0.19169    |
| AveragePolicyStd[0]  | 0.20916    |
| AveragePolicyStd[1]  | 0.20732    |
| AveragePolicyStd[2]  | 0.14827    |
| AveragePolicyStd[3]  | 0.1893     |
| AveragePolicyStd[4]  | 0.16176    |
| AveragePolicyStd[5]  | 0.23432    |
| AverageReturn        | 1745.3     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 281.97     |
| AverageEpisodeLength | 965.35     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.73     |
| TotalNEpisodes       | 23198      |
| TotalNSamples        | 6.2508e+06 |
| ExplainedVariance    | 0.12883    |
-------------------------------------
[2018-01-21 15:47:58.096234 UTC] Saving snapshot
[2018-01-21 15:47:58.096513 UTC] Starting iteration 1250
[2018-01-21 15:47:58.096736 UTC] Start collecting samples
[2018-01-21 15:48:02.789973 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:02.948053 UTC] Performing policy update
[2018-01-21 15:48:02.948749 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:03.077539 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:04.514393 UTC] Performing line search
[2018-01-21 15:48:04.710610 UTC] Updating baseline
[2018-01-21 15:48:06.511732 UTC] Computing logging information
-------------------------------------
| Iteration            | 1250       |
| ExpectedImprovement  | 0.016793   |
| ActualImprovement    | 0.0162     |
| ImprovementRatio     | 0.96466    |
| MeanKL               | 0.0081151  |
| Entropy              | -1.4696    |
| Perplexity           | 0.23001    |
| AveragePolicyStd     | 0.19172    |
| AveragePolicyStd[0]  | 0.20887    |
| AveragePolicyStd[1]  | 0.2073     |
| AveragePolicyStd[2]  | 0.14845    |
| AveragePolicyStd[3]  | 0.18929    |
| AveragePolicyStd[4]  | 0.16156    |
| AveragePolicyStd[5]  | 0.23485    |
| AverageReturn        | 1743.9     |
| MinReturn            | 238.19     |
| MaxReturn            | 1882.1     |
| StdReturn            | 283.35     |
| AverageEpisodeLength | 963.67     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.27     |
| TotalNEpisodes       | 23205      |
| TotalNSamples        | 6.2577e+06 |
| ExplainedVariance    | 0.10472    |
-------------------------------------
[2018-01-21 15:48:07.303042 UTC] Saving snapshot
[2018-01-21 15:48:07.312936 UTC] Starting iteration 1251
[2018-01-21 15:48:07.313168 UTC] Start collecting samples
[2018-01-21 15:48:11.878881 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:12.005924 UTC] Performing policy update
[2018-01-21 15:48:12.007097 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:12.132479 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:13.555983 UTC] Performing line search
[2018-01-21 15:48:13.747878 UTC] Updating baseline
[2018-01-21 15:48:15.703057 UTC] Computing logging information
-------------------------------------
| Iteration            | 1251       |
| ExpectedImprovement  | 0.018484   |
| ActualImprovement    | 0.017285   |
| ImprovementRatio     | 0.93515    |
| MeanKL               | 0.0077788  |
| Entropy              | -1.4641    |
| Perplexity           | 0.23128    |
| AveragePolicyStd     | 0.19193    |
| AveragePolicyStd[0]  | 0.20833    |
| AveragePolicyStd[1]  | 0.20813    |
| AveragePolicyStd[2]  | 0.14802    |
| AveragePolicyStd[3]  | 0.19028    |
| AveragePolicyStd[4]  | 0.16158    |
| AveragePolicyStd[5]  | 0.23524    |
| AverageReturn        | 1738       |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 293.96     |
| AverageEpisodeLength | 959.24     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.61     |
| TotalNEpisodes       | 23209      |
| TotalNSamples        | 6.2612e+06 |
| ExplainedVariance    | 0.18154    |
-------------------------------------
[2018-01-21 15:48:16.499911 UTC] Saving snapshot
[2018-01-21 15:48:16.500199 UTC] Starting iteration 1252
[2018-01-21 15:48:16.500393 UTC] Start collecting samples
[2018-01-21 15:48:21.226553 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:21.349420 UTC] Performing policy update
[2018-01-21 15:48:21.350033 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:21.489034 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:22.970860 UTC] Performing line search
[2018-01-21 15:48:23.183214 UTC] Updating baseline
[2018-01-21 15:48:25.211083 UTC] Computing logging information
-------------------------------------
| Iteration            | 1252       |
| ExpectedImprovement  | 0.018395   |
| ActualImprovement    | 0.017227   |
| ImprovementRatio     | 0.93648    |
| MeanKL               | 0.0077059  |
| Entropy              | -1.4656    |
| Perplexity           | 0.23095    |
| AveragePolicyStd     | 0.19191    |
| AveragePolicyStd[0]  | 0.20784    |
| AveragePolicyStd[1]  | 0.20812    |
| AveragePolicyStd[2]  | 0.14812    |
| AveragePolicyStd[3]  | 0.19044    |
| AveragePolicyStd[4]  | 0.16111    |
| AveragePolicyStd[5]  | 0.2358     |
| AverageReturn        | 1739.3     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 294.11     |
| AverageEpisodeLength | 959.24     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.61     |
| TotalNEpisodes       | 23215      |
| TotalNSamples        | 6.2672e+06 |
| ExplainedVariance    | -0.034233  |
-------------------------------------
[2018-01-21 15:48:25.983063 UTC] Saving snapshot
[2018-01-21 15:48:25.983302 UTC] Starting iteration 1253
[2018-01-21 15:48:25.983453 UTC] Start collecting samples
[2018-01-21 15:48:30.463282 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:30.588075 UTC] Performing policy update
[2018-01-21 15:48:30.588813 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:30.717648 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:32.152681 UTC] Performing line search
[2018-01-21 15:48:32.339937 UTC] Updating baseline
[2018-01-21 15:48:35.247189 UTC] Computing logging information
-------------------------------------
| Iteration            | 1253       |
| ExpectedImprovement  | 0.017924   |
| ActualImprovement    | 0.017096   |
| ImprovementRatio     | 0.95384    |
| MeanKL               | 0.0077154  |
| Entropy              | -1.4675    |
| Perplexity           | 0.2305     |
| AveragePolicyStd     | 0.19181    |
| AveragePolicyStd[0]  | 0.20775    |
| AveragePolicyStd[1]  | 0.20771    |
| AveragePolicyStd[2]  | 0.14799    |
| AveragePolicyStd[3]  | 0.19079    |
| AveragePolicyStd[4]  | 0.16144    |
| AveragePolicyStd[5]  | 0.2352     |
| AverageReturn        | 1739.5     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 294.09     |
| AverageEpisodeLength | 959.24     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.61     |
| TotalNEpisodes       | 23220      |
| TotalNSamples        | 6.2722e+06 |
| ExplainedVariance    | -0.0035372 |
-------------------------------------
[2018-01-21 15:48:36.087748 UTC] Saving snapshot
[2018-01-21 15:48:36.087980 UTC] Starting iteration 1254
[2018-01-21 15:48:36.088124 UTC] Start collecting samples
[2018-01-21 15:48:40.459811 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:40.594565 UTC] Performing policy update
[2018-01-21 15:48:40.595507 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:40.713646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:42.180026 UTC] Performing line search
[2018-01-21 15:48:42.374209 UTC] Updating baseline
[2018-01-21 15:48:44.703867 UTC] Computing logging information
-------------------------------------
| Iteration            | 1254       |
| ExpectedImprovement  | 0.019897   |
| ActualImprovement    | 0.018414   |
| ImprovementRatio     | 0.92544    |
| MeanKL               | 0.0077194  |
| Entropy              | -1.4667    |
| Perplexity           | 0.23068    |
| AveragePolicyStd     | 0.19187    |
| AveragePolicyStd[0]  | 0.20792    |
| AveragePolicyStd[1]  | 0.2075     |
| AveragePolicyStd[2]  | 0.14747    |
| AveragePolicyStd[3]  | 0.19103    |
| AveragePolicyStd[4]  | 0.16166    |
| AveragePolicyStd[5]  | 0.23565    |
| AverageReturn        | 1732.3     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 304.43     |
| AverageEpisodeLength | 954.71     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.89     |
| TotalNEpisodes       | 23225      |
| TotalNSamples        | 6.2768e+06 |
| ExplainedVariance    | 0.10227    |
-------------------------------------
[2018-01-21 15:48:45.490182 UTC] Saving snapshot
[2018-01-21 15:48:45.490419 UTC] Starting iteration 1255
[2018-01-21 15:48:45.490590 UTC] Start collecting samples
[2018-01-21 15:48:50.057045 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:50.189079 UTC] Performing policy update
[2018-01-21 15:48:50.189914 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:50.306742 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:48:51.807231 UTC] Performing line search
[2018-01-21 15:48:52.020505 UTC] Updating baseline
[2018-01-21 15:48:54.096650 UTC] Computing logging information
-------------------------------------
| Iteration            | 1255       |
| ExpectedImprovement  | 0.018528   |
| ActualImprovement    | 0.016901   |
| ImprovementRatio     | 0.91218    |
| MeanKL               | 0.0079247  |
| Entropy              | -1.4671    |
| Perplexity           | 0.2306     |
| AveragePolicyStd     | 0.19185    |
| AveragePolicyStd[0]  | 0.20811    |
| AveragePolicyStd[1]  | 0.2077     |
| AveragePolicyStd[2]  | 0.14742    |
| AveragePolicyStd[3]  | 0.19048    |
| AveragePolicyStd[4]  | 0.16195    |
| AveragePolicyStd[5]  | 0.23546    |
| AverageReturn        | 1727.9     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 303.55     |
| AverageEpisodeLength | 954.71     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.89     |
| TotalNEpisodes       | 23231      |
| TotalNSamples        | 6.2828e+06 |
| ExplainedVariance    | -0.0018234 |
-------------------------------------
[2018-01-21 15:48:54.875570 UTC] Saving snapshot
[2018-01-21 15:48:54.875825 UTC] Starting iteration 1256
[2018-01-21 15:48:54.875987 UTC] Start collecting samples
[2018-01-21 15:48:59.369709 UTC] Computing input variables for policy optimization
[2018-01-21 15:48:59.488314 UTC] Performing policy update
[2018-01-21 15:48:59.489002 UTC] Computing gradient in Euclidean space
[2018-01-21 15:48:59.605359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:01.019628 UTC] Performing line search
[2018-01-21 15:49:01.205325 UTC] Updating baseline
[2018-01-21 15:49:03.237389 UTC] Computing logging information
-------------------------------------
| Iteration            | 1256       |
| ExpectedImprovement  | 0.018594   |
| ActualImprovement    | 0.016692   |
| ImprovementRatio     | 0.89769    |
| MeanKL               | 0.0076206  |
| Entropy              | -1.4727    |
| Perplexity           | 0.2293     |
| AveragePolicyStd     | 0.19171    |
| AveragePolicyStd[0]  | 0.20739    |
| AveragePolicyStd[1]  | 0.20841    |
| AveragePolicyStd[2]  | 0.14728    |
| AveragePolicyStd[3]  | 0.18987    |
| AveragePolicyStd[4]  | 0.16146    |
| AveragePolicyStd[5]  | 0.23582    |
| AverageReturn        | 1717.7     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 316.3      |
| AverageEpisodeLength | 949.63     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.31     |
| TotalNEpisodes       | 23236      |
| TotalNSamples        | 6.2873e+06 |
| ExplainedVariance    | 0.081528   |
-------------------------------------
[2018-01-21 15:49:04.027025 UTC] Saving snapshot
[2018-01-21 15:49:04.027284 UTC] Starting iteration 1257
[2018-01-21 15:49:04.027463 UTC] Start collecting samples
[2018-01-21 15:49:08.561430 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:08.677005 UTC] Performing policy update
[2018-01-21 15:49:08.677582 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:08.797503 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:10.205758 UTC] Performing line search
[2018-01-21 15:49:10.393607 UTC] Updating baseline
[2018-01-21 15:49:12.184091 UTC] Computing logging information
-------------------------------------
| Iteration            | 1257       |
| ExpectedImprovement  | 0.017917   |
| ActualImprovement    | 0.016878   |
| ImprovementRatio     | 0.94202    |
| MeanKL               | 0.0080767  |
| Entropy              | -1.4762    |
| Perplexity           | 0.2285     |
| AveragePolicyStd     | 0.19155    |
| AveragePolicyStd[0]  | 0.20745    |
| AveragePolicyStd[1]  | 0.20825    |
| AveragePolicyStd[2]  | 0.14762    |
| AveragePolicyStd[3]  | 0.18943    |
| AveragePolicyStd[4]  | 0.16159    |
| AveragePolicyStd[5]  | 0.23493    |
| AverageReturn        | 1718.8     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 315.78     |
| AverageEpisodeLength | 950.87     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.22     |
| TotalNEpisodes       | 23241      |
| TotalNSamples        | 6.2923e+06 |
| ExplainedVariance    | -0.018197  |
-------------------------------------
[2018-01-21 15:49:13.023049 UTC] Saving snapshot
[2018-01-21 15:49:13.023298 UTC] Starting iteration 1258
[2018-01-21 15:49:13.023477 UTC] Start collecting samples
[2018-01-21 15:49:17.841552 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:17.994598 UTC] Performing policy update
[2018-01-21 15:49:17.995343 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:18.116959 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:19.524642 UTC] Performing line search
[2018-01-21 15:49:19.712003 UTC] Updating baseline
[2018-01-21 15:49:21.882058 UTC] Computing logging information
-------------------------------------
| Iteration            | 1258       |
| ExpectedImprovement  | 0.020237   |
| ActualImprovement    | 0.018579   |
| ImprovementRatio     | 0.91809    |
| MeanKL               | 0.0072266  |
| Entropy              | -1.4826    |
| Perplexity           | 0.22705    |
| AveragePolicyStd     | 0.19132    |
| AveragePolicyStd[0]  | 0.2074     |
| AveragePolicyStd[1]  | 0.20765    |
| AveragePolicyStd[2]  | 0.14786    |
| AveragePolicyStd[3]  | 0.18876    |
| AveragePolicyStd[4]  | 0.16155    |
| AveragePolicyStd[5]  | 0.23469    |
| AverageReturn        | 1708.7     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 327.87     |
| AverageEpisodeLength | 945.66     |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.64     |
| TotalNEpisodes       | 23248      |
| TotalNSamples        | 6.2987e+06 |
| ExplainedVariance    | 0.11328    |
-------------------------------------
[2018-01-21 15:49:22.657422 UTC] Saving snapshot
[2018-01-21 15:49:22.657659 UTC] Starting iteration 1259
[2018-01-21 15:49:22.657808 UTC] Start collecting samples
[2018-01-21 15:49:27.143405 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:27.276193 UTC] Performing policy update
[2018-01-21 15:49:27.276816 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:27.395682 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:28.787228 UTC] Performing line search
[2018-01-21 15:49:28.990067 UTC] Updating baseline
[2018-01-21 15:49:31.074569 UTC] Computing logging information
-------------------------------------
| Iteration            | 1259       |
| ExpectedImprovement  | 0.018076   |
| ActualImprovement    | 0.017121   |
| ImprovementRatio     | 0.94717    |
| MeanKL               | 0.0082591  |
| Entropy              | -1.4841    |
| Perplexity           | 0.22671    |
| AveragePolicyStd     | 0.19128    |
| AveragePolicyStd[0]  | 0.20755    |
| AveragePolicyStd[1]  | 0.20816    |
| AveragePolicyStd[2]  | 0.14772    |
| AveragePolicyStd[3]  | 0.18845    |
| AveragePolicyStd[4]  | 0.16149    |
| AveragePolicyStd[5]  | 0.23429    |
| AverageReturn        | 1704.7     |
| MinReturn            | 238.19     |
| MaxReturn            | 1885.9     |
| StdReturn            | 330.37     |
| AverageEpisodeLength | 942.7      |
| MinEpisodeLength     | 167        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.22     |
| TotalNEpisodes       | 23251      |
| TotalNSamples        | 6.3014e+06 |
| ExplainedVariance    | 0.11249    |
-------------------------------------
[2018-01-21 15:49:31.845208 UTC] Saving snapshot
[2018-01-21 15:49:31.845457 UTC] Starting iteration 1260
[2018-01-21 15:49:31.845677 UTC] Start collecting samples
[2018-01-21 15:49:36.450612 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:36.578481 UTC] Performing policy update
[2018-01-21 15:49:36.579126 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:36.707634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:38.156535 UTC] Performing line search
[2018-01-21 15:49:38.344544 UTC] Updating baseline
[2018-01-21 15:49:40.581749 UTC] Computing logging information
-------------------------------------
| Iteration            | 1260       |
| ExpectedImprovement  | 0.019277   |
| ActualImprovement    | 0.017967   |
| ImprovementRatio     | 0.93208    |
| MeanKL               | 0.008012   |
| Entropy              | -1.4861    |
| Perplexity           | 0.22626    |
| AveragePolicyStd     | 0.1912     |
| AveragePolicyStd[0]  | 0.20766    |
| AveragePolicyStd[1]  | 0.20812    |
| AveragePolicyStd[2]  | 0.14791    |
| AveragePolicyStd[3]  | 0.1883     |
| AveragePolicyStd[4]  | 0.16132    |
| AveragePolicyStd[5]  | 0.23388    |
| AverageReturn        | 1744.9     |
| MinReturn            | 645.43     |
| MaxReturn            | 1885.9     |
| StdReturn            | 246.02     |
| AverageEpisodeLength | 963.95     |
| MinEpisodeLength     | 384        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.7      |
| TotalNEpisodes       | 23256      |
| TotalNSamples        | 6.3064e+06 |
| ExplainedVariance    | 0.080855   |
-------------------------------------
[2018-01-21 15:49:41.435233 UTC] Saving snapshot
[2018-01-21 15:49:41.444543 UTC] Starting iteration 1261
[2018-01-21 15:49:41.444768 UTC] Start collecting samples
[2018-01-21 15:49:46.108789 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:46.255303 UTC] Performing policy update
[2018-01-21 15:49:46.256466 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:46.397948 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:47.825606 UTC] Performing line search
[2018-01-21 15:49:48.026215 UTC] Updating baseline
[2018-01-21 15:49:49.965782 UTC] Computing logging information
-------------------------------------
| Iteration            | 1261       |
| ExpectedImprovement  | 0.018351   |
| ActualImprovement    | 0.017569   |
| ImprovementRatio     | 0.95737    |
| MeanKL               | 0.0079273  |
| Entropy              | -1.4913    |
| Perplexity           | 0.22508    |
| AveragePolicyStd     | 0.19106    |
| AveragePolicyStd[0]  | 0.20743    |
| AveragePolicyStd[1]  | 0.20805    |
| AveragePolicyStd[2]  | 0.14779    |
| AveragePolicyStd[3]  | 0.18842    |
| AveragePolicyStd[4]  | 0.16067    |
| AveragePolicyStd[5]  | 0.23396    |
| AverageReturn        | 1744.7     |
| MinReturn            | 645.43     |
| MaxReturn            | 1885.9     |
| StdReturn            | 246.47     |
| AverageEpisodeLength | 963.99     |
| MinEpisodeLength     | 384        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.52     |
| TotalNEpisodes       | 23264      |
| TotalNSamples        | 6.3138e+06 |
| ExplainedVariance    | 0.076471   |
-------------------------------------
[2018-01-21 15:49:50.760292 UTC] Saving snapshot
[2018-01-21 15:49:50.760589 UTC] Starting iteration 1262
[2018-01-21 15:49:50.760790 UTC] Start collecting samples
[2018-01-21 15:49:55.379510 UTC] Computing input variables for policy optimization
[2018-01-21 15:49:55.507884 UTC] Performing policy update
[2018-01-21 15:49:55.508974 UTC] Computing gradient in Euclidean space
[2018-01-21 15:49:55.644255 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:49:57.157218 UTC] Performing line search
[2018-01-21 15:49:57.352165 UTC] Updating baseline
[2018-01-21 15:49:59.121792 UTC] Computing logging information
-------------------------------------
| Iteration            | 1262       |
| ExpectedImprovement  | 0.018234   |
| ActualImprovement    | 0.017503   |
| ImprovementRatio     | 0.95993    |
| MeanKL               | 0.0077143  |
| Entropy              | -1.4924    |
| Perplexity           | 0.22484    |
| AveragePolicyStd     | 0.19102    |
| AveragePolicyStd[0]  | 0.20745    |
| AveragePolicyStd[1]  | 0.20821    |
| AveragePolicyStd[2]  | 0.14776    |
| AveragePolicyStd[3]  | 0.18842    |
| AveragePolicyStd[4]  | 0.16057    |
| AveragePolicyStd[5]  | 0.23371    |
| AverageReturn        | 1744.2     |
| MinReturn            | 645.43     |
| MaxReturn            | 1885.9     |
| StdReturn            | 246.4      |
| AverageEpisodeLength | 963.99     |
| MinEpisodeLength     | 384        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.52     |
| TotalNEpisodes       | 23267      |
| TotalNSamples        | 6.3168e+06 |
| ExplainedVariance    | -0.0076294 |
-------------------------------------
[2018-01-21 15:49:59.891901 UTC] Saving snapshot
[2018-01-21 15:49:59.892120 UTC] Starting iteration 1263
[2018-01-21 15:49:59.892287 UTC] Start collecting samples
[2018-01-21 15:50:04.288654 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:04.435580 UTC] Performing policy update
[2018-01-21 15:50:04.441049 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:04.564149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:06.004439 UTC] Performing line search
[2018-01-21 15:50:06.200894 UTC] Updating baseline
[2018-01-21 15:50:08.740740 UTC] Computing logging information
-------------------------------------
| Iteration            | 1263       |
| ExpectedImprovement  | 0.020472   |
| ActualImprovement    | 0.019004   |
| ImprovementRatio     | 0.92831    |
| MeanKL               | 0.0076887  |
| Entropy              | -1.4977    |
| Perplexity           | 0.22365    |
| AveragePolicyStd     | 0.19086    |
| AveragePolicyStd[0]  | 0.20728    |
| AveragePolicyStd[1]  | 0.20842    |
| AveragePolicyStd[2]  | 0.14763    |
| AveragePolicyStd[3]  | 0.18837    |
| AveragePolicyStd[4]  | 0.1601     |
| AveragePolicyStd[5]  | 0.23339    |
| AverageReturn        | 1740.7     |
| MinReturn            | 645.43     |
| MaxReturn            | 1885.9     |
| StdReturn            | 245.69     |
| AverageEpisodeLength | 963.99     |
| MinEpisodeLength     | 384        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.52     |
| TotalNEpisodes       | 23272      |
| TotalNSamples        | 6.3218e+06 |
| ExplainedVariance    | -0.024705  |
-------------------------------------
[2018-01-21 15:50:09.609326 UTC] Saving snapshot
[2018-01-21 15:50:09.609584 UTC] Starting iteration 1264
[2018-01-21 15:50:09.609766 UTC] Start collecting samples
[2018-01-21 15:50:14.087150 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:14.206756 UTC] Performing policy update
[2018-01-21 15:50:14.207383 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:14.325431 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:15.742136 UTC] Performing line search
[2018-01-21 15:50:15.944450 UTC] Updating baseline
[2018-01-21 15:50:18.497794 UTC] Computing logging information
--------------------------------------
| Iteration            | 1264        |
| ExpectedImprovement  | 0.018308    |
| ActualImprovement    | 0.017402    |
| ImprovementRatio     | 0.95053     |
| MeanKL               | 0.0086342   |
| Entropy              | -1.4976     |
| Perplexity           | 0.22366     |
| AveragePolicyStd     | 0.19088     |
| AveragePolicyStd[0]  | 0.20752     |
| AveragePolicyStd[1]  | 0.2084      |
| AveragePolicyStd[2]  | 0.1476      |
| AveragePolicyStd[3]  | 0.18871     |
| AveragePolicyStd[4]  | 0.1597      |
| AveragePolicyStd[5]  | 0.23336     |
| AverageReturn        | 1740.6      |
| MinReturn            | 645.43      |
| MaxReturn            | 1885.9      |
| StdReturn            | 245.71      |
| AverageEpisodeLength | 963.99      |
| MinEpisodeLength     | 384         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 128.52      |
| TotalNEpisodes       | 23277       |
| TotalNSamples        | 6.3268e+06  |
| ExplainedVariance    | -7.7797e-07 |
--------------------------------------
[2018-01-21 15:50:19.292572 UTC] Saving snapshot
[2018-01-21 15:50:19.292856 UTC] Starting iteration 1265
[2018-01-21 15:50:19.292981 UTC] Start collecting samples
[2018-01-21 15:50:23.879190 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:24.020258 UTC] Performing policy update
[2018-01-21 15:50:24.020981 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:24.140373 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:25.517744 UTC] Performing line search
[2018-01-21 15:50:25.704619 UTC] Updating baseline
[2018-01-21 15:50:27.336672 UTC] Computing logging information
--------------------------------------
| Iteration            | 1265        |
| ExpectedImprovement  | 0.017672    |
| ActualImprovement    | 0.01671     |
| ImprovementRatio     | 0.94558     |
| MeanKL               | 0.00851     |
| Entropy              | -1.4952     |
| Perplexity           | 0.22421     |
| AveragePolicyStd     | 0.19096     |
| AveragePolicyStd[0]  | 0.20758     |
| AveragePolicyStd[1]  | 0.20819     |
| AveragePolicyStd[2]  | 0.14754     |
| AveragePolicyStd[3]  | 0.18854     |
| AveragePolicyStd[4]  | 0.16014     |
| AveragePolicyStd[5]  | 0.23376     |
| AverageReturn        | 1738.9      |
| MinReturn            | 645.43      |
| MaxReturn            | 1885.9      |
| StdReturn            | 245.18      |
| AverageEpisodeLength | 963.99      |
| MinEpisodeLength     | 384         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 128.52      |
| TotalNEpisodes       | 23283       |
| TotalNSamples        | 6.3328e+06  |
| ExplainedVariance    | -1.5854e-09 |
--------------------------------------
[2018-01-21 15:50:28.209075 UTC] Saving snapshot
[2018-01-21 15:50:28.209285 UTC] Starting iteration 1266
[2018-01-21 15:50:28.209449 UTC] Start collecting samples
[2018-01-21 15:50:32.796508 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:32.935459 UTC] Performing policy update
[2018-01-21 15:50:32.936565 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:33.057359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:34.451088 UTC] Performing line search
[2018-01-21 15:50:34.652507 UTC] Updating baseline
[2018-01-21 15:50:36.510392 UTC] Computing logging information
--------------------------------------
| Iteration            | 1266        |
| ExpectedImprovement  | 0.017341    |
| ActualImprovement    | 0.016213    |
| ImprovementRatio     | 0.93494     |
| MeanKL               | 0.0072491   |
| Entropy              | -1.4933     |
| Perplexity           | 0.22464     |
| AveragePolicyStd     | 0.19102     |
| AveragePolicyStd[0]  | 0.20786     |
| AveragePolicyStd[1]  | 0.20813     |
| AveragePolicyStd[2]  | 0.14773     |
| AveragePolicyStd[3]  | 0.18843     |
| AveragePolicyStd[4]  | 0.16011     |
| AveragePolicyStd[5]  | 0.23384     |
| AverageReturn        | 1738.7      |
| MinReturn            | 645.43      |
| MaxReturn            | 1885.9      |
| StdReturn            | 245.12      |
| AverageEpisodeLength | 963.99      |
| MinEpisodeLength     | 384         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 128.52      |
| TotalNEpisodes       | 23287       |
| TotalNSamples        | 6.3368e+06  |
| ExplainedVariance    | -1.0918e-09 |
--------------------------------------
[2018-01-21 15:50:37.267922 UTC] Saving snapshot
[2018-01-21 15:50:37.268222 UTC] Starting iteration 1267
[2018-01-21 15:50:37.268435 UTC] Start collecting samples
[2018-01-21 15:50:41.882815 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:42.017088 UTC] Performing policy update
[2018-01-21 15:50:42.018263 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:42.137617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:43.531993 UTC] Performing line search
[2018-01-21 15:50:43.724996 UTC] Updating baseline
[2018-01-21 15:50:45.594029 UTC] Computing logging information
-------------------------------------
| Iteration            | 1267       |
| ExpectedImprovement  | 0.016591   |
| ActualImprovement    | 0.01585    |
| ImprovementRatio     | 0.95535    |
| MeanKL               | 0.0079642  |
| Entropy              | -1.495     |
| Perplexity           | 0.22424    |
| AveragePolicyStd     | 0.19099    |
| AveragePolicyStd[0]  | 0.20831    |
| AveragePolicyStd[1]  | 0.20771    |
| AveragePolicyStd[2]  | 0.1475     |
| AveragePolicyStd[3]  | 0.18892    |
| AveragePolicyStd[4]  | 0.15959    |
| AveragePolicyStd[5]  | 0.2339     |
| AverageReturn        | 1721       |
| MinReturn            | 111.17     |
| MaxReturn            | 1885.9     |
| StdReturn            | 293.35     |
| AverageEpisodeLength | 954.98     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.62     |
| TotalNEpisodes       | 23292      |
| TotalNSamples        | 6.3409e+06 |
| ExplainedVariance    | 0.1062     |
-------------------------------------
[2018-01-21 15:50:46.399876 UTC] Saving snapshot
[2018-01-21 15:50:46.400102 UTC] Starting iteration 1268
[2018-01-21 15:50:46.400317 UTC] Start collecting samples
[2018-01-21 15:50:51.105935 UTC] Computing input variables for policy optimization
[2018-01-21 15:50:51.230023 UTC] Performing policy update
[2018-01-21 15:50:51.230820 UTC] Computing gradient in Euclidean space
[2018-01-21 15:50:51.352360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:50:52.783858 UTC] Performing line search
[2018-01-21 15:50:52.979517 UTC] Updating baseline
[2018-01-21 15:50:56.237978 UTC] Computing logging information
-------------------------------------
| Iteration            | 1268       |
| ExpectedImprovement  | 0.017951   |
| ActualImprovement    | 0.017165   |
| ImprovementRatio     | 0.9562     |
| MeanKL               | 0.0082098  |
| Entropy              | -1.4984    |
| Perplexity           | 0.22348    |
| AveragePolicyStd     | 0.19085    |
| AveragePolicyStd[0]  | 0.20781    |
| AveragePolicyStd[1]  | 0.20757    |
| AveragePolicyStd[2]  | 0.14755    |
| AveragePolicyStd[3]  | 0.18891    |
| AveragePolicyStd[4]  | 0.15985    |
| AveragePolicyStd[5]  | 0.23339    |
| AverageReturn        | 1735       |
| MinReturn            | 111.17     |
| MaxReturn            | 1885.9     |
| StdReturn            | 273.54     |
| AverageEpisodeLength | 961.14     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.62     |
| TotalNEpisodes       | 23299      |
| TotalNSamples        | 6.3479e+06 |
| ExplainedVariance    | 0.00072753 |
-------------------------------------
[2018-01-21 15:50:57.008239 UTC] Saving snapshot
[2018-01-21 15:50:57.008446 UTC] Starting iteration 1269
[2018-01-21 15:50:57.008623 UTC] Start collecting samples
[2018-01-21 15:51:01.639121 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:01.786133 UTC] Performing policy update
[2018-01-21 15:51:01.786735 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:01.932333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:03.333458 UTC] Performing line search
[2018-01-21 15:51:03.519402 UTC] Updating baseline
[2018-01-21 15:51:06.194538 UTC] Computing logging information
-------------------------------------
| Iteration            | 1269       |
| ExpectedImprovement  | 0.018642   |
| ActualImprovement    | 0.01766    |
| ImprovementRatio     | 0.94731    |
| MeanKL               | 0.0077106  |
| Entropy              | -1.4916    |
| Perplexity           | 0.22502    |
| AveragePolicyStd     | 0.19105    |
| AveragePolicyStd[0]  | 0.20795    |
| AveragePolicyStd[1]  | 0.20761    |
| AveragePolicyStd[2]  | 0.14782    |
| AveragePolicyStd[3]  | 0.1892     |
| AveragePolicyStd[4]  | 0.1602     |
| AveragePolicyStd[5]  | 0.2335     |
| AverageReturn        | 1723.8     |
| MinReturn            | 111.17     |
| MaxReturn            | 1885.9     |
| StdReturn            | 289.51     |
| AverageEpisodeLength | 954.96     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.76     |
| TotalNEpisodes       | 23305      |
| TotalNSamples        | 6.3532e+06 |
| ExplainedVariance    | 0.13851    |
-------------------------------------
[2018-01-21 15:51:07.024106 UTC] Saving snapshot
[2018-01-21 15:51:07.024355 UTC] Starting iteration 1270
[2018-01-21 15:51:07.024506 UTC] Start collecting samples
[2018-01-21 15:51:11.362643 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:11.485695 UTC] Performing policy update
[2018-01-21 15:51:11.486376 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:11.601182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:13.027937 UTC] Performing line search
[2018-01-21 15:51:13.214412 UTC] Updating baseline
[2018-01-21 15:51:15.542289 UTC] Computing logging information
-------------------------------------
| Iteration            | 1270       |
| ExpectedImprovement  | 0.017686   |
| ActualImprovement    | 0.017189   |
| ImprovementRatio     | 0.97191    |
| MeanKL               | 0.0078441  |
| Entropy              | -1.4982    |
| Perplexity           | 0.22353    |
| AveragePolicyStd     | 0.19084    |
| AveragePolicyStd[0]  | 0.20727    |
| AveragePolicyStd[1]  | 0.20751    |
| AveragePolicyStd[2]  | 0.14743    |
| AveragePolicyStd[3]  | 0.18913    |
| AveragePolicyStd[4]  | 0.16022    |
| AveragePolicyStd[5]  | 0.23349    |
| AverageReturn        | 1722.6     |
| MinReturn            | 111.17     |
| MaxReturn            | 1876.8     |
| StdReturn            | 289.07     |
| AverageEpisodeLength | 954.96     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.76     |
| TotalNEpisodes       | 23308      |
| TotalNSamples        | 6.3562e+06 |
| ExplainedVariance    | -0.037844  |
-------------------------------------
[2018-01-21 15:51:16.365246 UTC] Saving snapshot
[2018-01-21 15:51:16.375182 UTC] Starting iteration 1271
[2018-01-21 15:51:16.375353 UTC] Start collecting samples
[2018-01-21 15:51:20.939734 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:21.068221 UTC] Performing policy update
[2018-01-21 15:51:21.068838 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:21.207085 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:22.653374 UTC] Performing line search
[2018-01-21 15:51:22.852745 UTC] Updating baseline
[2018-01-21 15:51:24.645224 UTC] Computing logging information
--------------------------------------
| Iteration            | 1271        |
| ExpectedImprovement  | 0.01783     |
| ActualImprovement    | 0.016855    |
| ImprovementRatio     | 0.94533     |
| MeanKL               | 0.0083783   |
| Entropy              | -1.4941     |
| Perplexity           | 0.22445     |
| AveragePolicyStd     | 0.19095     |
| AveragePolicyStd[0]  | 0.20721     |
| AveragePolicyStd[1]  | 0.20754     |
| AveragePolicyStd[2]  | 0.1478      |
| AveragePolicyStd[3]  | 0.18928     |
| AveragePolicyStd[4]  | 0.16039     |
| AveragePolicyStd[5]  | 0.23346     |
| AverageReturn        | 1731.5      |
| MinReturn            | 111.17      |
| MaxReturn            | 1876.8      |
| StdReturn            | 279.42      |
| AverageEpisodeLength | 959.39      |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 147.49      |
| TotalNEpisodes       | 23315       |
| TotalNSamples        | 6.3632e+06  |
| ExplainedVariance    | -2.7921e-05 |
--------------------------------------
[2018-01-21 15:51:25.405398 UTC] Saving snapshot
[2018-01-21 15:51:25.405698 UTC] Starting iteration 1272
[2018-01-21 15:51:25.405928 UTC] Start collecting samples
[2018-01-21 15:51:29.924669 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:30.062699 UTC] Performing policy update
[2018-01-21 15:51:30.063705 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:30.182267 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:31.603806 UTC] Performing line search
[2018-01-21 15:51:31.802128 UTC] Updating baseline
[2018-01-21 15:51:33.826063 UTC] Computing logging information
-------------------------------------
| Iteration            | 1272       |
| ExpectedImprovement  | 0.018068   |
| ActualImprovement    | 0.017369   |
| ImprovementRatio     | 0.96128    |
| MeanKL               | 0.0080478  |
| Entropy              | -1.4928    |
| Perplexity           | 0.22474    |
| AveragePolicyStd     | 0.19098    |
| AveragePolicyStd[0]  | 0.20735    |
| AveragePolicyStd[1]  | 0.20745    |
| AveragePolicyStd[2]  | 0.14805    |
| AveragePolicyStd[3]  | 0.18938    |
| AveragePolicyStd[4]  | 0.16024    |
| AveragePolicyStd[5]  | 0.23343    |
| AverageReturn        | 1732       |
| MinReturn            | 111.17     |
| MaxReturn            | 1876.8     |
| StdReturn            | 279.56     |
| AverageEpisodeLength | 959.39     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.49     |
| TotalNEpisodes       | 23320      |
| TotalNSamples        | 6.3682e+06 |
| ExplainedVariance    | -0.0014665 |
-------------------------------------
[2018-01-21 15:51:34.617102 UTC] Saving snapshot
[2018-01-21 15:51:34.617411 UTC] Starting iteration 1273
[2018-01-21 15:51:34.617640 UTC] Start collecting samples
[2018-01-21 15:51:39.180878 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:39.304987 UTC] Performing policy update
[2018-01-21 15:51:39.306055 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:39.425799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:40.872637 UTC] Performing line search
[2018-01-21 15:51:41.072446 UTC] Updating baseline
[2018-01-21 15:51:43.497014 UTC] Computing logging information
-------------------------------------
| Iteration            | 1273       |
| ExpectedImprovement  | 0.018138   |
| ActualImprovement    | 0.016304   |
| ImprovementRatio     | 0.8989     |
| MeanKL               | 0.0075625  |
| Entropy              | -1.4969    |
| Perplexity           | 0.22382    |
| AveragePolicyStd     | 0.19083    |
| AveragePolicyStd[0]  | 0.20704    |
| AveragePolicyStd[1]  | 0.20768    |
| AveragePolicyStd[2]  | 0.1483     |
| AveragePolicyStd[3]  | 0.18906    |
| AveragePolicyStd[4]  | 0.15995    |
| AveragePolicyStd[5]  | 0.23296    |
| AverageReturn        | 1740.9     |
| MinReturn            | 111.17     |
| MaxReturn            | 1871.2     |
| StdReturn            | 268.24     |
| AverageEpisodeLength | 963.92     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.59     |
| TotalNEpisodes       | 23324      |
| TotalNSamples        | 6.3722e+06 |
| ExplainedVariance    | 0.00083558 |
-------------------------------------
[2018-01-21 15:51:44.254058 UTC] Saving snapshot
[2018-01-21 15:51:44.254320 UTC] Starting iteration 1274
[2018-01-21 15:51:44.254523 UTC] Start collecting samples
[2018-01-21 15:51:48.919514 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:49.041225 UTC] Performing policy update
[2018-01-21 15:51:49.041879 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:49.161636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:51:50.574313 UTC] Performing line search
[2018-01-21 15:51:50.763189 UTC] Updating baseline
[2018-01-21 15:51:53.272818 UTC] Computing logging information
-------------------------------------
| Iteration            | 1274       |
| ExpectedImprovement  | 0.018161   |
| ActualImprovement    | 0.017452   |
| ImprovementRatio     | 0.96095    |
| MeanKL               | 0.0089001  |
| Entropy              | -1.5009    |
| Perplexity           | 0.22293    |
| AveragePolicyStd     | 0.19073    |
| AveragePolicyStd[0]  | 0.20703    |
| AveragePolicyStd[1]  | 0.20736    |
| AveragePolicyStd[2]  | 0.14804    |
| AveragePolicyStd[3]  | 0.18924    |
| AveragePolicyStd[4]  | 0.15963    |
| AveragePolicyStd[5]  | 0.23306    |
| AverageReturn        | 1732.6     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 295.26     |
| AverageEpisodeLength | 956.95     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.06     |
| TotalNEpisodes       | 23331      |
| TotalNSamples        | 6.3785e+06 |
| ExplainedVariance    | 0.071829   |
-------------------------------------
[2018-01-21 15:51:54.053552 UTC] Saving snapshot
[2018-01-21 15:51:54.053804 UTC] Starting iteration 1275
[2018-01-21 15:51:54.053988 UTC] Start collecting samples
[2018-01-21 15:51:58.809837 UTC] Computing input variables for policy optimization
[2018-01-21 15:51:58.939562 UTC] Performing policy update
[2018-01-21 15:51:58.940604 UTC] Computing gradient in Euclidean space
[2018-01-21 15:51:59.059769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:00.470584 UTC] Performing line search
[2018-01-21 15:52:00.663266 UTC] Updating baseline
[2018-01-21 15:52:03.664127 UTC] Computing logging information
-------------------------------------
| Iteration            | 1275       |
| ExpectedImprovement  | 0.017513   |
| ActualImprovement    | 0.016053   |
| ImprovementRatio     | 0.91666    |
| MeanKL               | 0.0076326  |
| Entropy              | -1.5036    |
| Perplexity           | 0.22232    |
| AveragePolicyStd     | 0.19064    |
| AveragePolicyStd[0]  | 0.20691    |
| AveragePolicyStd[1]  | 0.20688    |
| AveragePolicyStd[2]  | 0.14802    |
| AveragePolicyStd[3]  | 0.18922    |
| AveragePolicyStd[4]  | 0.15956    |
| AveragePolicyStd[5]  | 0.23326    |
| AverageReturn        | 1743.6     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 281.25     |
| AverageEpisodeLength | 962.03     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.95     |
| TotalNEpisodes       | 23334      |
| TotalNSamples        | 6.3815e+06 |
| ExplainedVariance    | -0.0014124 |
-------------------------------------
[2018-01-21 15:52:04.434486 UTC] Saving snapshot
[2018-01-21 15:52:04.434911 UTC] Starting iteration 1276
[2018-01-21 15:52:04.435228 UTC] Start collecting samples
[2018-01-21 15:52:09.120635 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:09.245682 UTC] Performing policy update
[2018-01-21 15:52:09.246838 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:09.380961 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:10.817194 UTC] Performing line search
[2018-01-21 15:52:11.016377 UTC] Updating baseline
[2018-01-21 15:52:13.232848 UTC] Computing logging information
-------------------------------------
| Iteration            | 1276       |
| ExpectedImprovement  | 0.016649   |
| ActualImprovement    | 0.015284   |
| ImprovementRatio     | 0.91803    |
| MeanKL               | 0.0077944  |
| Entropy              | -1.5013    |
| Perplexity           | 0.22284    |
| AveragePolicyStd     | 0.19072    |
| AveragePolicyStd[0]  | 0.20715    |
| AveragePolicyStd[1]  | 0.20689    |
| AveragePolicyStd[2]  | 0.14806    |
| AveragePolicyStd[3]  | 0.18954    |
| AveragePolicyStd[4]  | 0.15948    |
| AveragePolicyStd[5]  | 0.23319    |
| AverageReturn        | 1744.2     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 281.45     |
| AverageEpisodeLength | 962.03     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.95     |
| TotalNEpisodes       | 23339      |
| TotalNSamples        | 6.3865e+06 |
| ExplainedVariance    | -0.0045293 |
-------------------------------------
[2018-01-21 15:52:14.008174 UTC] Saving snapshot
[2018-01-21 15:52:14.008502 UTC] Starting iteration 1277
[2018-01-21 15:52:14.008747 UTC] Start collecting samples
[2018-01-21 15:52:18.350388 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:18.473616 UTC] Performing policy update
[2018-01-21 15:52:18.474468 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:18.597108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:20.013340 UTC] Performing line search
[2018-01-21 15:52:20.208162 UTC] Updating baseline
[2018-01-21 15:52:21.892664 UTC] Computing logging information
--------------------------------------
| Iteration            | 1277        |
| ExpectedImprovement  | 0.019051    |
| ActualImprovement    | 0.017921    |
| ImprovementRatio     | 0.94067     |
| MeanKL               | 0.0083549   |
| Entropy              | -1.5093     |
| Perplexity           | 0.22107     |
| AveragePolicyStd     | 0.19048     |
| AveragePolicyStd[0]  | 0.20657     |
| AveragePolicyStd[1]  | 0.20671     |
| AveragePolicyStd[2]  | 0.14771     |
| AveragePolicyStd[3]  | 0.18976     |
| AveragePolicyStd[4]  | 0.15908     |
| AveragePolicyStd[5]  | 0.23305     |
| AverageReturn        | 1745.1      |
| MinReturn            | 111.17      |
| MaxReturn            | 1901.4      |
| StdReturn            | 281.56      |
| AverageEpisodeLength | 962.03      |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 148.95      |
| TotalNEpisodes       | 23345       |
| TotalNSamples        | 6.3925e+06  |
| ExplainedVariance    | -0.00010422 |
--------------------------------------
[2018-01-21 15:52:22.702211 UTC] Saving snapshot
[2018-01-21 15:52:22.702460 UTC] Starting iteration 1278
[2018-01-21 15:52:22.702629 UTC] Start collecting samples
[2018-01-21 15:52:27.399304 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:27.543700 UTC] Performing policy update
[2018-01-21 15:52:27.544784 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:27.664851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:29.049824 UTC] Performing line search
[2018-01-21 15:52:29.236035 UTC] Updating baseline
[2018-01-21 15:52:31.911228 UTC] Computing logging information
--------------------------------------
| Iteration            | 1278        |
| ExpectedImprovement  | 0.017408    |
| ActualImprovement    | 0.016118    |
| ImprovementRatio     | 0.9259      |
| MeanKL               | 0.0075772   |
| Entropy              | -1.5087     |
| Perplexity           | 0.22119     |
| AveragePolicyStd     | 0.19051     |
| AveragePolicyStd[0]  | 0.20676     |
| AveragePolicyStd[1]  | 0.20694     |
| AveragePolicyStd[2]  | 0.1476      |
| AveragePolicyStd[3]  | 0.18941     |
| AveragePolicyStd[4]  | 0.15912     |
| AveragePolicyStd[5]  | 0.23323     |
| AverageReturn        | 1761.3      |
| MinReturn            | 111.17      |
| MaxReturn            | 1901.4      |
| StdReturn            | 262.04      |
| AverageEpisodeLength | 970.2       |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 138.38      |
| TotalNEpisodes       | 23349       |
| TotalNSamples        | 6.3965e+06  |
| ExplainedVariance    | -1.3474e-11 |
--------------------------------------
[2018-01-21 15:52:32.690348 UTC] Saving snapshot
[2018-01-21 15:52:32.690720 UTC] Starting iteration 1279
[2018-01-21 15:52:32.690914 UTC] Start collecting samples
[2018-01-21 15:52:37.139316 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:37.281099 UTC] Performing policy update
[2018-01-21 15:52:37.281678 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:37.397054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:38.804966 UTC] Performing line search
[2018-01-21 15:52:39.007522 UTC] Updating baseline
[2018-01-21 15:52:41.376305 UTC] Computing logging information
--------------------------------------
| Iteration            | 1279        |
| ExpectedImprovement  | 0.017867    |
| ActualImprovement    | 0.017145    |
| ImprovementRatio     | 0.95957     |
| MeanKL               | 0.0081402   |
| Entropy              | -1.5092     |
| Perplexity           | 0.22108     |
| AveragePolicyStd     | 0.19048     |
| AveragePolicyStd[0]  | 0.20703     |
| AveragePolicyStd[1]  | 0.20673     |
| AveragePolicyStd[2]  | 0.14782     |
| AveragePolicyStd[3]  | 0.18905     |
| AveragePolicyStd[4]  | 0.15925     |
| AveragePolicyStd[5]  | 0.23298     |
| AverageReturn        | 1760        |
| MinReturn            | 111.17      |
| MaxReturn            | 1901.4      |
| StdReturn            | 261.99      |
| AverageEpisodeLength | 970.2       |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 138.38      |
| TotalNEpisodes       | 23355       |
| TotalNSamples        | 6.4025e+06  |
| ExplainedVariance    | -2.9674e-07 |
--------------------------------------
[2018-01-21 15:52:42.151768 UTC] Saving snapshot
[2018-01-21 15:52:42.152005 UTC] Starting iteration 1280
[2018-01-21 15:52:42.152191 UTC] Start collecting samples
[2018-01-21 15:52:46.655395 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:46.795094 UTC] Performing policy update
[2018-01-21 15:52:46.795750 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:46.925030 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:48.284866 UTC] Performing line search
[2018-01-21 15:52:48.476606 UTC] Updating baseline
[2018-01-21 15:52:50.334847 UTC] Computing logging information
--------------------------------------
| Iteration            | 1280        |
| ExpectedImprovement  | 0.017663    |
| ActualImprovement    | 0.016341    |
| ImprovementRatio     | 0.92517     |
| MeanKL               | 0.0078057   |
| Entropy              | -1.5076     |
| Perplexity           | 0.22144     |
| AveragePolicyStd     | 0.19053     |
| AveragePolicyStd[0]  | 0.20764     |
| AveragePolicyStd[1]  | 0.20677     |
| AveragePolicyStd[2]  | 0.14791     |
| AveragePolicyStd[3]  | 0.18926     |
| AveragePolicyStd[4]  | 0.15899     |
| AveragePolicyStd[5]  | 0.2326      |
| AverageReturn        | 1771.2      |
| MinReturn            | 111.17      |
| MaxReturn            | 1901.4      |
| StdReturn            | 238.55      |
| AverageEpisodeLength | 976.16      |
| MinEpisodeLength     | 99          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 126.16      |
| TotalNEpisodes       | 23358       |
| TotalNSamples        | 6.4055e+06  |
| ExplainedVariance    | -1.1116e-09 |
--------------------------------------
[2018-01-21 15:52:51.154089 UTC] Saving snapshot
[2018-01-21 15:52:51.163676 UTC] Starting iteration 1281
[2018-01-21 15:52:51.163910 UTC] Start collecting samples
[2018-01-21 15:52:55.695271 UTC] Computing input variables for policy optimization
[2018-01-21 15:52:55.817792 UTC] Performing policy update
[2018-01-21 15:52:55.818481 UTC] Computing gradient in Euclidean space
[2018-01-21 15:52:55.937838 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:52:57.400146 UTC] Performing line search
[2018-01-21 15:52:57.591597 UTC] Updating baseline
[2018-01-21 15:52:59.566813 UTC] Computing logging information
-------------------------------------
| Iteration            | 1281       |
| ExpectedImprovement  | 0.01748    |
| ActualImprovement    | 0.016754   |
| ImprovementRatio     | 0.9585     |
| MeanKL               | 0.008575   |
| Entropy              | -1.5087    |
| Perplexity           | 0.22119    |
| AveragePolicyStd     | 0.19046    |
| AveragePolicyStd[0]  | 0.20751    |
| AveragePolicyStd[1]  | 0.20647    |
| AveragePolicyStd[2]  | 0.14801    |
| AveragePolicyStd[3]  | 0.18886    |
| AveragePolicyStd[4]  | 0.15946    |
| AveragePolicyStd[5]  | 0.23248    |
| AverageReturn        | 1766.6     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 248.05     |
| AverageEpisodeLength | 972.25     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.32     |
| TotalNEpisodes       | 23366      |
| TotalNSamples        | 6.4131e+06 |
| ExplainedVariance    | 0.093571   |
-------------------------------------
[2018-01-21 15:53:00.341038 UTC] Saving snapshot
[2018-01-21 15:53:00.341252 UTC] Starting iteration 1282
[2018-01-21 15:53:00.341375 UTC] Start collecting samples
[2018-01-21 15:53:04.824821 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:04.965703 UTC] Performing policy update
[2018-01-21 15:53:04.966762 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:05.093441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:06.480743 UTC] Performing line search
[2018-01-21 15:53:06.669712 UTC] Updating baseline
[2018-01-21 15:53:08.919294 UTC] Computing logging information
-------------------------------------
| Iteration            | 1282       |
| ExpectedImprovement  | 0.018325   |
| ActualImprovement    | 0.017019   |
| ImprovementRatio     | 0.92876    |
| MeanKL               | 0.0084676  |
| Entropy              | -1.5072    |
| Perplexity           | 0.22153    |
| AveragePolicyStd     | 0.19051    |
| AveragePolicyStd[0]  | 0.20752    |
| AveragePolicyStd[1]  | 0.20642    |
| AveragePolicyStd[2]  | 0.14826    |
| AveragePolicyStd[3]  | 0.18879    |
| AveragePolicyStd[4]  | 0.15934    |
| AveragePolicyStd[5]  | 0.23273    |
| AverageReturn        | 1761.9     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 260.93     |
| AverageEpisodeLength | 968.08     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.87     |
| TotalNEpisodes       | 23372      |
| TotalNSamples        | 6.4186e+06 |
| ExplainedVariance    | 0.11862    |
-------------------------------------
[2018-01-21 15:53:09.719019 UTC] Saving snapshot
[2018-01-21 15:53:09.719202 UTC] Starting iteration 1283
[2018-01-21 15:53:09.719306 UTC] Start collecting samples
[2018-01-21 15:53:14.287118 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:14.409220 UTC] Performing policy update
[2018-01-21 15:53:14.409912 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:14.525009 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:15.943089 UTC] Performing line search
[2018-01-21 15:53:16.135529 UTC] Updating baseline
[2018-01-21 15:53:17.977267 UTC] Computing logging information
-------------------------------------
| Iteration            | 1283       |
| ExpectedImprovement  | 0.021279   |
| ActualImprovement    | 0.019114   |
| ImprovementRatio     | 0.89825    |
| MeanKL               | 0.0087102  |
| Entropy              | -1.5046    |
| Perplexity           | 0.22211    |
| AveragePolicyStd     | 0.19059    |
| AveragePolicyStd[0]  | 0.20798    |
| AveragePolicyStd[1]  | 0.20672    |
| AveragePolicyStd[2]  | 0.14818    |
| AveragePolicyStd[3]  | 0.18869    |
| AveragePolicyStd[4]  | 0.15958    |
| AveragePolicyStd[5]  | 0.23241    |
| AverageReturn        | 1757.7     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 264.04     |
| AverageEpisodeLength | 965.52     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.64     |
| TotalNEpisodes       | 23374      |
| TotalNSamples        | 6.4204e+06 |
| ExplainedVariance    | 0.27586    |
-------------------------------------
[2018-01-21 15:53:18.773925 UTC] Saving snapshot
[2018-01-21 15:53:18.774233 UTC] Starting iteration 1284
[2018-01-21 15:53:18.774448 UTC] Start collecting samples
[2018-01-21 15:53:23.499232 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:23.663288 UTC] Performing policy update
[2018-01-21 15:53:23.663944 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:23.782333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:25.175155 UTC] Performing line search
[2018-01-21 15:53:25.382321 UTC] Updating baseline
[2018-01-21 15:53:27.162614 UTC] Computing logging information
-------------------------------------
| Iteration            | 1284       |
| ExpectedImprovement  | 0.019759   |
| ActualImprovement    | 0.019167   |
| ImprovementRatio     | 0.97006    |
| MeanKL               | 0.0087547  |
| Entropy              | -1.5088    |
| Perplexity           | 0.22118    |
| AveragePolicyStd     | 0.19046    |
| AveragePolicyStd[0]  | 0.20778    |
| AveragePolicyStd[1]  | 0.2068     |
| AveragePolicyStd[2]  | 0.14803    |
| AveragePolicyStd[3]  | 0.18863    |
| AveragePolicyStd[4]  | 0.15936    |
| AveragePolicyStd[5]  | 0.23218    |
| AverageReturn        | 1760.1     |
| MinReturn            | 111.17     |
| MaxReturn            | 1901.4     |
| StdReturn            | 264.64     |
| AverageEpisodeLength | 965.52     |
| MinEpisodeLength     | 99         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.64     |
| TotalNEpisodes       | 23381      |
| TotalNSamples        | 6.4274e+06 |
| ExplainedVariance    | -0.062047  |
-------------------------------------
[2018-01-21 15:53:27.953663 UTC] Saving snapshot
[2018-01-21 15:53:27.953909 UTC] Starting iteration 1285
[2018-01-21 15:53:27.954079 UTC] Start collecting samples
[2018-01-21 15:53:32.702314 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:32.829594 UTC] Performing policy update
[2018-01-21 15:53:32.830195 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:32.962270 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:34.409276 UTC] Performing line search
[2018-01-21 15:53:34.611802 UTC] Updating baseline
[2018-01-21 15:53:36.377409 UTC] Computing logging information
-------------------------------------
| Iteration            | 1285       |
| ExpectedImprovement  | 0.018591   |
| ActualImprovement    | 0.017381   |
| ImprovementRatio     | 0.93491    |
| MeanKL               | 0.0080613  |
| Entropy              | -1.5151    |
| Perplexity           | 0.2198     |
| AveragePolicyStd     | 0.19026    |
| AveragePolicyStd[0]  | 0.20696    |
| AveragePolicyStd[1]  | 0.2067     |
| AveragePolicyStd[2]  | 0.14811    |
| AveragePolicyStd[3]  | 0.18827    |
| AveragePolicyStd[4]  | 0.15922    |
| AveragePolicyStd[5]  | 0.23229    |
| AverageReturn        | 1749.8     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 291.41     |
| AverageEpisodeLength | 959.04     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.88     |
| TotalNEpisodes       | 23390      |
| TotalNSamples        | 6.4348e+06 |
| ExplainedVariance    | 0.17066    |
-------------------------------------
[2018-01-21 15:53:37.179620 UTC] Saving snapshot
[2018-01-21 15:53:37.179882 UTC] Starting iteration 1286
[2018-01-21 15:53:37.180114 UTC] Start collecting samples
[2018-01-21 15:53:41.552090 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:41.694198 UTC] Performing policy update
[2018-01-21 15:53:41.697843 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:41.833240 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:43.268507 UTC] Performing line search
[2018-01-21 15:53:43.471756 UTC] Updating baseline
[2018-01-21 15:53:45.197901 UTC] Computing logging information
-------------------------------------
| Iteration            | 1286       |
| ExpectedImprovement  | 0.018345   |
| ActualImprovement    | 0.017179   |
| ImprovementRatio     | 0.93646    |
| MeanKL               | 0.0078429  |
| Entropy              | -1.5233    |
| Perplexity           | 0.218      |
| AveragePolicyStd     | 0.19001    |
| AveragePolicyStd[0]  | 0.20659    |
| AveragePolicyStd[1]  | 0.20644    |
| AveragePolicyStd[2]  | 0.14763    |
| AveragePolicyStd[3]  | 0.18796    |
| AveragePolicyStd[4]  | 0.15921    |
| AveragePolicyStd[5]  | 0.23222    |
| AverageReturn        | 1738.1     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 313.54     |
| AverageEpisodeLength | 952.57     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.35     |
| TotalNEpisodes       | 23392      |
| TotalNSamples        | 6.4362e+06 |
| ExplainedVariance    | 0.28098    |
-------------------------------------
[2018-01-21 15:53:45.961440 UTC] Saving snapshot
[2018-01-21 15:53:45.961760 UTC] Starting iteration 1287
[2018-01-21 15:53:45.961918 UTC] Start collecting samples
[2018-01-21 15:53:50.499476 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:50.632909 UTC] Performing policy update
[2018-01-21 15:53:50.633529 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:50.750718 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:53:52.170704 UTC] Performing line search
[2018-01-21 15:53:52.363205 UTC] Updating baseline
[2018-01-21 15:53:54.273231 UTC] Computing logging information
-------------------------------------
| Iteration            | 1287       |
| ExpectedImprovement  | 0.019213   |
| ActualImprovement    | 0.018525   |
| ImprovementRatio     | 0.96419    |
| MeanKL               | 0.0079644  |
| Entropy              | -1.5303    |
| Perplexity           | 0.21648    |
| AveragePolicyStd     | 0.18983    |
| AveragePolicyStd[0]  | 0.20635    |
| AveragePolicyStd[1]  | 0.20663    |
| AveragePolicyStd[2]  | 0.14754    |
| AveragePolicyStd[3]  | 0.18764    |
| AveragePolicyStd[4]  | 0.15827    |
| AveragePolicyStd[5]  | 0.23258    |
| AverageReturn        | 1736.9     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 313.28     |
| AverageEpisodeLength | 952.57     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.35     |
| TotalNEpisodes       | 23399      |
| TotalNSamples        | 6.4432e+06 |
| ExplainedVariance    | -0.10486   |
-------------------------------------
[2018-01-21 15:53:55.055836 UTC] Saving snapshot
[2018-01-21 15:53:55.056048 UTC] Starting iteration 1288
[2018-01-21 15:53:55.056157 UTC] Start collecting samples
[2018-01-21 15:53:59.639347 UTC] Computing input variables for policy optimization
[2018-01-21 15:53:59.760288 UTC] Performing policy update
[2018-01-21 15:53:59.760953 UTC] Computing gradient in Euclidean space
[2018-01-21 15:53:59.877306 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:01.285230 UTC] Performing line search
[2018-01-21 15:54:01.473867 UTC] Updating baseline
[2018-01-21 15:54:03.325777 UTC] Computing logging information
-------------------------------------
| Iteration            | 1288       |
| ExpectedImprovement  | 0.016485   |
| ActualImprovement    | 0.015978   |
| ImprovementRatio     | 0.96924    |
| MeanKL               | 0.008091   |
| Entropy              | -1.5309    |
| Perplexity           | 0.21635    |
| AveragePolicyStd     | 0.18983    |
| AveragePolicyStd[0]  | 0.20669    |
| AveragePolicyStd[1]  | 0.20624    |
| AveragePolicyStd[2]  | 0.14767    |
| AveragePolicyStd[3]  | 0.18773    |
| AveragePolicyStd[4]  | 0.15778    |
| AveragePolicyStd[5]  | 0.23289    |
| AverageReturn        | 1748       |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 300.17     |
| AverageEpisodeLength | 957.95     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.79     |
| TotalNEpisodes       | 23404      |
| TotalNSamples        | 6.4482e+06 |
| ExplainedVariance    | 0.0055091  |
-------------------------------------
[2018-01-21 15:54:04.090217 UTC] Saving snapshot
[2018-01-21 15:54:04.090538 UTC] Starting iteration 1289
[2018-01-21 15:54:04.090741 UTC] Start collecting samples
[2018-01-21 15:54:08.521291 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:08.663393 UTC] Performing policy update
[2018-01-21 15:54:08.664100 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:08.780824 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:10.236864 UTC] Performing line search
[2018-01-21 15:54:10.434367 UTC] Updating baseline
[2018-01-21 15:54:13.282412 UTC] Computing logging information
-------------------------------------
| Iteration            | 1289       |
| ExpectedImprovement  | 0.018111   |
| ActualImprovement    | 0.016624   |
| ImprovementRatio     | 0.91792    |
| MeanKL               | 0.0082019  |
| Entropy              | -1.5307    |
| Perplexity           | 0.21639    |
| AveragePolicyStd     | 0.18981    |
| AveragePolicyStd[0]  | 0.20676    |
| AveragePolicyStd[1]  | 0.20607    |
| AveragePolicyStd[2]  | 0.14776    |
| AveragePolicyStd[3]  | 0.18776    |
| AveragePolicyStd[4]  | 0.15802    |
| AveragePolicyStd[5]  | 0.23252    |
| AverageReturn        | 1754.7     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 297.98     |
| AverageEpisodeLength | 960.43     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.46     |
| TotalNEpisodes       | 23407      |
| TotalNSamples        | 6.4512e+06 |
| ExplainedVariance    | -0.010972  |
-------------------------------------
[2018-01-21 15:54:14.155046 UTC] Saving snapshot
[2018-01-21 15:54:14.155280 UTC] Starting iteration 1290
[2018-01-21 15:54:14.155429 UTC] Start collecting samples
[2018-01-21 15:54:18.813497 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:18.945061 UTC] Performing policy update
[2018-01-21 15:54:18.945801 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:19.064465 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:20.441526 UTC] Performing line search
[2018-01-21 15:54:20.644043 UTC] Updating baseline
[2018-01-21 15:54:22.765364 UTC] Computing logging information
-------------------------------------
| Iteration            | 1290       |
| ExpectedImprovement  | 0.018359   |
| ActualImprovement    | 0.017552   |
| ImprovementRatio     | 0.95606    |
| MeanKL               | 0.0075839  |
| Entropy              | -1.5297    |
| Perplexity           | 0.2166     |
| AveragePolicyStd     | 0.18987    |
| AveragePolicyStd[0]  | 0.20705    |
| AveragePolicyStd[1]  | 0.20597    |
| AveragePolicyStd[2]  | 0.14765    |
| AveragePolicyStd[3]  | 0.18791    |
| AveragePolicyStd[4]  | 0.15783    |
| AveragePolicyStd[5]  | 0.23279    |
| AverageReturn        | 1755.1     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 298.16     |
| AverageEpisodeLength | 960.43     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.46     |
| TotalNEpisodes       | 23414      |
| TotalNSamples        | 6.4582e+06 |
| ExplainedVariance    | 0.040589   |
-------------------------------------
[2018-01-21 15:54:23.527723 UTC] Saving snapshot
[2018-01-21 15:54:23.534426 UTC] Starting iteration 1291
[2018-01-21 15:54:23.534630 UTC] Start collecting samples
[2018-01-21 15:54:28.151247 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:28.267674 UTC] Performing policy update
[2018-01-21 15:54:28.268318 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:28.386501 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:29.792812 UTC] Performing line search
[2018-01-21 15:54:29.986560 UTC] Updating baseline
[2018-01-21 15:54:32.703849 UTC] Computing logging information
-------------------------------------
| Iteration            | 1291       |
| ExpectedImprovement  | 0.018325   |
| ActualImprovement    | 0.017109   |
| ImprovementRatio     | 0.93367    |
| MeanKL               | 0.0078104  |
| Entropy              | -1.5377    |
| Perplexity           | 0.21488    |
| AveragePolicyStd     | 0.18961    |
| AveragePolicyStd[0]  | 0.20668    |
| AveragePolicyStd[1]  | 0.20509    |
| AveragePolicyStd[2]  | 0.1475     |
| AveragePolicyStd[3]  | 0.18819    |
| AveragePolicyStd[4]  | 0.15751    |
| AveragePolicyStd[5]  | 0.23272    |
| AverageReturn        | 1757.8     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 298.9      |
| AverageEpisodeLength | 960.43     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.46     |
| TotalNEpisodes       | 23418      |
| TotalNSamples        | 6.4622e+06 |
| ExplainedVariance    | -0.020016  |
-------------------------------------
[2018-01-21 15:54:33.470375 UTC] Saving snapshot
[2018-01-21 15:54:33.470710 UTC] Starting iteration 1292
[2018-01-21 15:54:33.470953 UTC] Start collecting samples
[2018-01-21 15:54:38.036176 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:38.176731 UTC] Performing policy update
[2018-01-21 15:54:38.177777 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:38.297999 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:39.685118 UTC] Performing line search
[2018-01-21 15:54:39.879460 UTC] Updating baseline
[2018-01-21 15:54:42.140627 UTC] Computing logging information
-------------------------------------
| Iteration            | 1292       |
| ExpectedImprovement  | 0.017905   |
| ActualImprovement    | 0.016564   |
| ImprovementRatio     | 0.92509    |
| MeanKL               | 0.0082893  |
| Entropy              | -1.5405    |
| Perplexity           | 0.21427    |
| AveragePolicyStd     | 0.1895     |
| AveragePolicyStd[0]  | 0.20636    |
| AveragePolicyStd[1]  | 0.2047     |
| AveragePolicyStd[2]  | 0.14757    |
| AveragePolicyStd[3]  | 0.18856    |
| AveragePolicyStd[4]  | 0.15751    |
| AveragePolicyStd[5]  | 0.2323     |
| AverageReturn        | 1751.9     |
| MinReturn            | 57.766     |
| MaxReturn            | 1901.4     |
| StdReturn            | 303.49     |
| AverageEpisodeLength | 957.29     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.8      |
| TotalNEpisodes       | 23423      |
| TotalNSamples        | 6.4669e+06 |
| ExplainedVariance    | 0.15441    |
-------------------------------------
[2018-01-21 15:54:42.958860 UTC] Saving snapshot
[2018-01-21 15:54:42.959095 UTC] Starting iteration 1293
[2018-01-21 15:54:42.959246 UTC] Start collecting samples
[2018-01-21 15:54:47.479792 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:47.620436 UTC] Performing policy update
[2018-01-21 15:54:47.621497 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:47.751116 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:49.173791 UTC] Performing line search
[2018-01-21 15:54:49.361732 UTC] Updating baseline
[2018-01-21 15:54:51.501048 UTC] Computing logging information
-------------------------------------
| Iteration            | 1293       |
| ExpectedImprovement  | 0.016378   |
| ActualImprovement    | 0.015364   |
| ImprovementRatio     | 0.93806    |
| MeanKL               | 0.0082427  |
| Entropy              | -1.5357    |
| Perplexity           | 0.21531    |
| AveragePolicyStd     | 0.18966    |
| AveragePolicyStd[0]  | 0.20654    |
| AveragePolicyStd[1]  | 0.20448    |
| AveragePolicyStd[2]  | 0.14752    |
| AveragePolicyStd[3]  | 0.18886    |
| AveragePolicyStd[4]  | 0.15777    |
| AveragePolicyStd[5]  | 0.23279    |
| AverageReturn        | 1752.1     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 303.53     |
| AverageEpisodeLength | 957.29     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.8      |
| TotalNEpisodes       | 23429      |
| TotalNSamples        | 6.4729e+06 |
| ExplainedVariance    | -0.003626  |
-------------------------------------
[2018-01-21 15:54:52.362083 UTC] Saving snapshot
[2018-01-21 15:54:52.362333 UTC] Starting iteration 1294
[2018-01-21 15:54:52.362518 UTC] Start collecting samples
[2018-01-21 15:54:57.194598 UTC] Computing input variables for policy optimization
[2018-01-21 15:54:57.354704 UTC] Performing policy update
[2018-01-21 15:54:57.355314 UTC] Computing gradient in Euclidean space
[2018-01-21 15:54:57.475537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:54:58.872401 UTC] Performing line search
[2018-01-21 15:54:59.061659 UTC] Updating baseline
[2018-01-21 15:55:00.808163 UTC] Computing logging information
-------------------------------------
| Iteration            | 1294       |
| ExpectedImprovement  | 0.018404   |
| ActualImprovement    | 0.017295   |
| ImprovementRatio     | 0.93976    |
| MeanKL               | 0.0081956  |
| Entropy              | -1.536     |
| Perplexity           | 0.21524    |
| AveragePolicyStd     | 0.18963    |
| AveragePolicyStd[0]  | 0.20653    |
| AveragePolicyStd[1]  | 0.20486    |
| AveragePolicyStd[2]  | 0.14759    |
| AveragePolicyStd[3]  | 0.18903    |
| AveragePolicyStd[4]  | 0.15775    |
| AveragePolicyStd[5]  | 0.23201    |
| AverageReturn        | 1765.3     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 277.36     |
| AverageEpisodeLength | 964.26     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.39     |
| TotalNEpisodes       | 23433      |
| TotalNSamples        | 6.4769e+06 |
| ExplainedVariance    | -0.0011712 |
-------------------------------------
[2018-01-21 15:55:01.646131 UTC] Saving snapshot
[2018-01-21 15:55:01.646365 UTC] Starting iteration 1295
[2018-01-21 15:55:01.646536 UTC] Start collecting samples
[2018-01-21 15:55:06.265294 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:06.418814 UTC] Performing policy update
[2018-01-21 15:55:06.419461 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:06.539882 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:07.932300 UTC] Performing line search
[2018-01-21 15:55:08.121389 UTC] Updating baseline
[2018-01-21 15:55:10.129805 UTC] Computing logging information
-------------------------------------
| Iteration            | 1295       |
| ExpectedImprovement  | 0.01792    |
| ActualImprovement    | 0.016865   |
| ImprovementRatio     | 0.94111    |
| MeanKL               | 0.0083294  |
| Entropy              | -1.5402    |
| Perplexity           | 0.21434    |
| AveragePolicyStd     | 0.1895     |
| AveragePolicyStd[0]  | 0.20607    |
| AveragePolicyStd[1]  | 0.20478    |
| AveragePolicyStd[2]  | 0.14759    |
| AveragePolicyStd[3]  | 0.18887    |
| AveragePolicyStd[4]  | 0.15755    |
| AveragePolicyStd[5]  | 0.23213    |
| AverageReturn        | 1750.8     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 316.08     |
| AverageEpisodeLength | 956.05     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.36     |
| TotalNEpisodes       | 23440      |
| TotalNSamples        | 6.4831e+06 |
| ExplainedVariance    | 0.089073   |
-------------------------------------
[2018-01-21 15:55:10.950598 UTC] Saving snapshot
[2018-01-21 15:55:10.950825 UTC] Starting iteration 1296
[2018-01-21 15:55:10.951004 UTC] Start collecting samples
[2018-01-21 15:55:15.622552 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:15.765156 UTC] Performing policy update
[2018-01-21 15:55:15.766292 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:15.886253 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:17.304142 UTC] Performing line search
[2018-01-21 15:55:17.493534 UTC] Updating baseline
[2018-01-21 15:55:20.426500 UTC] Computing logging information
-------------------------------------
| Iteration            | 1296       |
| ExpectedImprovement  | 0.01713    |
| ActualImprovement    | 0.015988   |
| ImprovementRatio     | 0.93331    |
| MeanKL               | 0.0077394  |
| Entropy              | -1.5506    |
| Perplexity           | 0.21212    |
| AveragePolicyStd     | 0.18917    |
| AveragePolicyStd[0]  | 0.20607    |
| AveragePolicyStd[1]  | 0.20384    |
| AveragePolicyStd[2]  | 0.14726    |
| AveragePolicyStd[3]  | 0.18861    |
| AveragePolicyStd[4]  | 0.15742    |
| AveragePolicyStd[5]  | 0.23183    |
| AverageReturn        | 1753.3     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 316.78     |
| AverageEpisodeLength | 956.05     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.36     |
| TotalNEpisodes       | 23444      |
| TotalNSamples        | 6.4871e+06 |
| ExplainedVariance    | -0.0012957 |
-------------------------------------
[2018-01-21 15:55:21.193018 UTC] Saving snapshot
[2018-01-21 15:55:21.193269 UTC] Starting iteration 1297
[2018-01-21 15:55:21.193450 UTC] Start collecting samples
[2018-01-21 15:55:25.668720 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:25.817997 UTC] Performing policy update
[2018-01-21 15:55:25.818638 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:25.940915 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:27.367443 UTC] Performing line search
[2018-01-21 15:55:27.557643 UTC] Updating baseline
[2018-01-21 15:55:30.038113 UTC] Computing logging information
-------------------------------------
| Iteration            | 1297       |
| ExpectedImprovement  | 0.01843    |
| ActualImprovement    | 0.017343   |
| ImprovementRatio     | 0.94103    |
| MeanKL               | 0.0076791  |
| Entropy              | -1.5494    |
| Perplexity           | 0.21237    |
| AveragePolicyStd     | 0.18919    |
| AveragePolicyStd[0]  | 0.20576    |
| AveragePolicyStd[1]  | 0.20406    |
| AveragePolicyStd[2]  | 0.14761    |
| AveragePolicyStd[3]  | 0.18846    |
| AveragePolicyStd[4]  | 0.15743    |
| AveragePolicyStd[5]  | 0.2318     |
| AverageReturn        | 1754.2     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 317.08     |
| AverageEpisodeLength | 956.05     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.36     |
| TotalNEpisodes       | 23449      |
| TotalNSamples        | 6.4921e+06 |
| ExplainedVariance    | 0.0017582  |
-------------------------------------
[2018-01-21 15:55:30.807073 UTC] Saving snapshot
[2018-01-21 15:55:30.807297 UTC] Starting iteration 1298
[2018-01-21 15:55:30.807416 UTC] Start collecting samples
[2018-01-21 15:55:35.404371 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:35.537711 UTC] Performing policy update
[2018-01-21 15:55:35.538330 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:35.654996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:37.048300 UTC] Performing line search
[2018-01-21 15:55:37.236547 UTC] Updating baseline
[2018-01-21 15:55:39.244926 UTC] Computing logging information
-------------------------------------
| Iteration            | 1298       |
| ExpectedImprovement  | 0.018435   |
| ActualImprovement    | 0.017899   |
| ImprovementRatio     | 0.97092    |
| MeanKL               | 0.0072274  |
| Entropy              | -1.5498    |
| Perplexity           | 0.2123     |
| AveragePolicyStd     | 0.18918    |
| AveragePolicyStd[0]  | 0.20553    |
| AveragePolicyStd[1]  | 0.20434    |
| AveragePolicyStd[2]  | 0.14728    |
| AveragePolicyStd[3]  | 0.18857    |
| AveragePolicyStd[4]  | 0.15769    |
| AveragePolicyStd[5]  | 0.23168    |
| AverageReturn        | 1744.3     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 331.19     |
| AverageEpisodeLength | 949.34     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.39     |
| TotalNEpisodes       | 23456      |
| TotalNSamples        | 6.4984e+06 |
| ExplainedVariance    | 0.21909    |
-------------------------------------
[2018-01-21 15:55:40.088571 UTC] Saving snapshot
[2018-01-21 15:55:40.088811 UTC] Starting iteration 1299
[2018-01-21 15:55:40.089080 UTC] Start collecting samples
[2018-01-21 15:55:44.649224 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:44.789821 UTC] Performing policy update
[2018-01-21 15:55:44.790499 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:44.918561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:46.348845 UTC] Performing line search
[2018-01-21 15:55:46.544496 UTC] Updating baseline
[2018-01-21 15:55:48.622417 UTC] Computing logging information
-------------------------------------
| Iteration            | 1299       |
| ExpectedImprovement  | 0.017968   |
| ActualImprovement    | 0.016966   |
| ImprovementRatio     | 0.94424    |
| MeanKL               | 0.008233   |
| Entropy              | -1.5474    |
| Perplexity           | 0.21279    |
| AveragePolicyStd     | 0.18925    |
| AveragePolicyStd[0]  | 0.20544    |
| AveragePolicyStd[1]  | 0.20438    |
| AveragePolicyStd[2]  | 0.14745    |
| AveragePolicyStd[3]  | 0.18821    |
| AveragePolicyStd[4]  | 0.15799    |
| AveragePolicyStd[5]  | 0.23201    |
| AverageReturn        | 1744.5     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 331.22     |
| AverageEpisodeLength | 949.34     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.39     |
| TotalNEpisodes       | 23459      |
| TotalNSamples        | 6.5014e+06 |
| ExplainedVariance    | -0.01297   |
-------------------------------------
[2018-01-21 15:55:49.483564 UTC] Saving snapshot
[2018-01-21 15:55:49.483883 UTC] Starting iteration 1300
[2018-01-21 15:55:49.484069 UTC] Start collecting samples
[2018-01-21 15:55:54.179595 UTC] Computing input variables for policy optimization
[2018-01-21 15:55:54.308690 UTC] Performing policy update
[2018-01-21 15:55:54.313639 UTC] Computing gradient in Euclidean space
[2018-01-21 15:55:54.431737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:55:55.802607 UTC] Performing line search
[2018-01-21 15:55:56.000072 UTC] Updating baseline
[2018-01-21 15:55:57.967747 UTC] Computing logging information
-------------------------------------
| Iteration            | 1300       |
| ExpectedImprovement  | 0.017401   |
| ActualImprovement    | 0.016753   |
| ImprovementRatio     | 0.96279    |
| MeanKL               | 0.0076613  |
| Entropy              | -1.5483    |
| Perplexity           | 0.21262    |
| AveragePolicyStd     | 0.18921    |
| AveragePolicyStd[0]  | 0.20561    |
| AveragePolicyStd[1]  | 0.20421    |
| AveragePolicyStd[2]  | 0.14759    |
| AveragePolicyStd[3]  | 0.18792    |
| AveragePolicyStd[4]  | 0.15807    |
| AveragePolicyStd[5]  | 0.23185    |
| AverageReturn        | 1751.1     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 324.93     |
| AverageEpisodeLength | 953.25     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.97     |
| TotalNEpisodes       | 23465      |
| TotalNSamples        | 6.5074e+06 |
| ExplainedVariance    | 0.0017745  |
-------------------------------------
[2018-01-21 15:55:58.783030 UTC] Saving snapshot
[2018-01-21 15:55:58.792305 UTC] Starting iteration 1301
[2018-01-21 15:55:58.792539 UTC] Start collecting samples
[2018-01-21 15:56:03.238991 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:03.372694 UTC] Performing policy update
[2018-01-21 15:56:03.373715 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:03.493489 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:05.008044 UTC] Performing line search
[2018-01-21 15:56:05.200392 UTC] Updating baseline
[2018-01-21 15:56:07.137856 UTC] Computing logging information
--------------------------------------
| Iteration            | 1301        |
| ExpectedImprovement  | 0.018233    |
| ActualImprovement    | 0.017712    |
| ImprovementRatio     | 0.97143     |
| MeanKL               | 0.0077343   |
| Entropy              | -1.5464     |
| Perplexity           | 0.21302     |
| AveragePolicyStd     | 0.18927     |
| AveragePolicyStd[0]  | 0.2053      |
| AveragePolicyStd[1]  | 0.20425     |
| AveragePolicyStd[2]  | 0.14773     |
| AveragePolicyStd[3]  | 0.1877      |
| AveragePolicyStd[4]  | 0.15817     |
| AveragePolicyStd[5]  | 0.23249     |
| AverageReturn        | 1760.8      |
| MinReturn            | 57.766      |
| MaxReturn            | 1900.1      |
| StdReturn            | 315.62      |
| AverageEpisodeLength | 957.42      |
| MinEpisodeLength     | 74          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 161.8       |
| TotalNEpisodes       | 23472       |
| TotalNSamples        | 6.5144e+06  |
| ExplainedVariance    | -0.00092149 |
--------------------------------------
[2018-01-21 15:56:08.009260 UTC] Saving snapshot
[2018-01-21 15:56:08.009505 UTC] Starting iteration 1302
[2018-01-21 15:56:08.009679 UTC] Start collecting samples
[2018-01-21 15:56:12.343800 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:12.463525 UTC] Performing policy update
[2018-01-21 15:56:12.464736 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:12.583881 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:13.982743 UTC] Performing line search
[2018-01-21 15:56:14.171095 UTC] Updating baseline
[2018-01-21 15:56:16.373090 UTC] Computing logging information
-------------------------------------
| Iteration            | 1302       |
| ExpectedImprovement  | 0.017596   |
| ActualImprovement    | 0.01639    |
| ImprovementRatio     | 0.93147    |
| MeanKL               | 0.0088735  |
| Entropy              | -1.5442    |
| Perplexity           | 0.21349    |
| AveragePolicyStd     | 0.18933    |
| AveragePolicyStd[0]  | 0.20461    |
| AveragePolicyStd[1]  | 0.20457    |
| AveragePolicyStd[2]  | 0.14785    |
| AveragePolicyStd[3]  | 0.18831    |
| AveragePolicyStd[4]  | 0.15812    |
| AveragePolicyStd[5]  | 0.23254    |
| AverageReturn        | 1765.3     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 312.94     |
| AverageEpisodeLength | 959.98     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.43     |
| TotalNEpisodes       | 23474      |
| TotalNSamples        | 6.5164e+06 |
| ExplainedVariance    | -0.0013677 |
-------------------------------------
[2018-01-21 15:56:17.158732 UTC] Saving snapshot
[2018-01-21 15:56:17.159056 UTC] Starting iteration 1303
[2018-01-21 15:56:17.159293 UTC] Start collecting samples
[2018-01-21 15:56:21.732760 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:21.876650 UTC] Performing policy update
[2018-01-21 15:56:21.877324 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:22.001006 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:23.417854 UTC] Performing line search
[2018-01-21 15:56:23.612644 UTC] Updating baseline
[2018-01-21 15:56:26.005944 UTC] Computing logging information
-------------------------------------
| Iteration            | 1303       |
| ExpectedImprovement  | 0.017162   |
| ActualImprovement    | 0.016183   |
| ImprovementRatio     | 0.94295    |
| MeanKL               | 0.0085178  |
| Entropy              | -1.544     |
| Perplexity           | 0.21352    |
| AveragePolicyStd     | 0.18933    |
| AveragePolicyStd[0]  | 0.20477    |
| AveragePolicyStd[1]  | 0.20456    |
| AveragePolicyStd[2]  | 0.14797    |
| AveragePolicyStd[3]  | 0.1883     |
| AveragePolicyStd[4]  | 0.15801    |
| AveragePolicyStd[5]  | 0.23238    |
| AverageReturn        | 1764.1     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 312.73     |
| AverageEpisodeLength | 959.98     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.43     |
| TotalNEpisodes       | 23480      |
| TotalNSamples        | 6.5224e+06 |
| ExplainedVariance    | 1.2816e-05 |
-------------------------------------
[2018-01-21 15:56:26.839228 UTC] Saving snapshot
[2018-01-21 15:56:26.839569 UTC] Starting iteration 1304
[2018-01-21 15:56:26.839825 UTC] Start collecting samples
[2018-01-21 15:56:31.293773 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:31.419331 UTC] Performing policy update
[2018-01-21 15:56:31.420292 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:31.553056 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:32.978973 UTC] Performing line search
[2018-01-21 15:56:33.173891 UTC] Updating baseline
[2018-01-21 15:56:35.338400 UTC] Computing logging information
-------------------------------------
| Iteration            | 1304       |
| ExpectedImprovement  | 0.018348   |
| ActualImprovement    | 0.017277   |
| ImprovementRatio     | 0.94161    |
| MeanKL               | 0.007953   |
| Entropy              | -1.5476    |
| Perplexity           | 0.21275    |
| AveragePolicyStd     | 0.18918    |
| AveragePolicyStd[0]  | 0.20504    |
| AveragePolicyStd[1]  | 0.20391    |
| AveragePolicyStd[2]  | 0.1482     |
| AveragePolicyStd[3]  | 0.18845    |
| AveragePolicyStd[4]  | 0.15788    |
| AveragePolicyStd[5]  | 0.23163    |
| AverageReturn        | 1774.8     |
| MinReturn            | 57.766     |
| MaxReturn            | 1900.1     |
| StdReturn            | 290.68     |
| AverageEpisodeLength | 966.21     |
| MinEpisodeLength     | 74         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.38     |
| TotalNEpisodes       | 23487      |
| TotalNSamples        | 6.5294e+06 |
| ExplainedVariance    | -0.013782  |
-------------------------------------
[2018-01-21 15:56:36.190766 UTC] Saving snapshot
[2018-01-21 15:56:36.191020 UTC] Starting iteration 1305
[2018-01-21 15:56:36.191193 UTC] Start collecting samples
[2018-01-21 15:56:40.775671 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:40.921847 UTC] Performing policy update
[2018-01-21 15:56:40.922537 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:41.045699 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:42.474625 UTC] Performing line search
[2018-01-21 15:56:42.676289 UTC] Updating baseline
[2018-01-21 15:56:45.349799 UTC] Computing logging information
-------------------------------------
| Iteration            | 1305       |
| ExpectedImprovement  | 0.020884   |
| ActualImprovement    | 0.019232   |
| ImprovementRatio     | 0.92091    |
| MeanKL               | 0.0075392  |
| Entropy              | -1.5504    |
| Perplexity           | 0.21216    |
| AveragePolicyStd     | 0.18911    |
| AveragePolicyStd[0]  | 0.20528    |
| AveragePolicyStd[1]  | 0.20393    |
| AveragePolicyStd[2]  | 0.14836    |
| AveragePolicyStd[3]  | 0.18836    |
| AveragePolicyStd[4]  | 0.15726    |
| AveragePolicyStd[5]  | 0.23147    |
| AverageReturn        | 1793.3     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 234.34     |
| AverageEpisodeLength | 975.42     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.76     |
| TotalNEpisodes       | 23491      |
| TotalNSamples        | 6.5327e+06 |
| ExplainedVariance    | 0.11641    |
-------------------------------------
[2018-01-21 15:56:46.199862 UTC] Saving snapshot
[2018-01-21 15:56:46.200178 UTC] Starting iteration 1306
[2018-01-21 15:56:46.200363 UTC] Start collecting samples
[2018-01-21 15:56:50.755839 UTC] Computing input variables for policy optimization
[2018-01-21 15:56:50.876279 UTC] Performing policy update
[2018-01-21 15:56:50.878525 UTC] Computing gradient in Euclidean space
[2018-01-21 15:56:51.018855 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:56:52.425172 UTC] Performing line search
[2018-01-21 15:56:52.615143 UTC] Updating baseline
[2018-01-21 15:56:54.923972 UTC] Computing logging information
-------------------------------------
| Iteration            | 1306       |
| ExpectedImprovement  | 0.016955   |
| ActualImprovement    | 0.015951   |
| ImprovementRatio     | 0.94074    |
| MeanKL               | 0.0078927  |
| Entropy              | -1.5505    |
| Perplexity           | 0.21214    |
| AveragePolicyStd     | 0.18912    |
| AveragePolicyStd[0]  | 0.20517    |
| AveragePolicyStd[1]  | 0.20408    |
| AveragePolicyStd[2]  | 0.14823    |
| AveragePolicyStd[3]  | 0.18815    |
| AveragePolicyStd[4]  | 0.15736    |
| AveragePolicyStd[5]  | 0.23172    |
| AverageReturn        | 1793.4     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 234.3      |
| AverageEpisodeLength | 975.42     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.76     |
| TotalNEpisodes       | 23496      |
| TotalNSamples        | 6.5377e+06 |
| ExplainedVariance    | -0.024503  |
-------------------------------------
[2018-01-21 15:56:55.675396 UTC] Saving snapshot
[2018-01-21 15:56:55.675654 UTC] Starting iteration 1307
[2018-01-21 15:56:55.675828 UTC] Start collecting samples
[2018-01-21 15:57:00.448873 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:00.593751 UTC] Performing policy update
[2018-01-21 15:57:00.594379 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:00.713567 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:02.158489 UTC] Performing line search
[2018-01-21 15:57:02.354602 UTC] Updating baseline
[2018-01-21 15:57:03.993739 UTC] Computing logging information
--------------------------------------
| Iteration            | 1307        |
| ExpectedImprovement  | 0.01891     |
| ActualImprovement    | 0.017705    |
| ImprovementRatio     | 0.93628     |
| MeanKL               | 0.0080693   |
| Entropy              | -1.5531     |
| Perplexity           | 0.21159     |
| AveragePolicyStd     | 0.18905     |
| AveragePolicyStd[0]  | 0.20473     |
| AveragePolicyStd[1]  | 0.20415     |
| AveragePolicyStd[2]  | 0.14804     |
| AveragePolicyStd[3]  | 0.18804     |
| AveragePolicyStd[4]  | 0.15737     |
| AveragePolicyStd[5]  | 0.23195     |
| AverageReturn        | 1794.2      |
| MinReturn            | 248.8       |
| MaxReturn            | 1900.1      |
| StdReturn            | 234.42      |
| AverageEpisodeLength | 975.42      |
| MinEpisodeLength     | 179         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 119.76      |
| TotalNEpisodes       | 23500       |
| TotalNSamples        | 6.5417e+06  |
| ExplainedVariance    | -3.4654e-09 |
--------------------------------------
[2018-01-21 15:57:04.842252 UTC] Saving snapshot
[2018-01-21 15:57:04.842517 UTC] Starting iteration 1308
[2018-01-21 15:57:04.842678 UTC] Start collecting samples
[2018-01-21 15:57:09.541676 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:09.662334 UTC] Performing policy update
[2018-01-21 15:57:09.662947 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:09.781297 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:11.212479 UTC] Performing line search
[2018-01-21 15:57:11.406893 UTC] Updating baseline
[2018-01-21 15:57:13.726060 UTC] Computing logging information
-------------------------------------
| Iteration            | 1308       |
| ExpectedImprovement  | 0.019322   |
| ActualImprovement    | 0.018043   |
| ImprovementRatio     | 0.93379    |
| MeanKL               | 0.0076645  |
| Entropy              | -1.5496    |
| Perplexity           | 0.21234    |
| AveragePolicyStd     | 0.18914    |
| AveragePolicyStd[0]  | 0.20512    |
| AveragePolicyStd[1]  | 0.20409    |
| AveragePolicyStd[2]  | 0.14808    |
| AveragePolicyStd[3]  | 0.18812    |
| AveragePolicyStd[4]  | 0.15772    |
| AveragePolicyStd[5]  | 0.23173    |
| AverageReturn        | 1794.1     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 234.23     |
| AverageEpisodeLength | 975.42     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.76     |
| TotalNEpisodes       | 23506      |
| TotalNSamples        | 6.5477e+06 |
| ExplainedVariance    | 0.013662   |
-------------------------------------
[2018-01-21 15:57:14.573073 UTC] Saving snapshot
[2018-01-21 15:57:14.573308 UTC] Starting iteration 1309
[2018-01-21 15:57:14.573457 UTC] Start collecting samples
[2018-01-21 15:57:19.168798 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:19.304315 UTC] Performing policy update
[2018-01-21 15:57:19.304959 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:19.425538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:20.824826 UTC] Performing line search
[2018-01-21 15:57:21.014413 UTC] Updating baseline
[2018-01-21 15:57:23.224824 UTC] Computing logging information
-------------------------------------
| Iteration            | 1309       |
| ExpectedImprovement  | 0.016637   |
| ActualImprovement    | 0.016194   |
| ImprovementRatio     | 0.97339    |
| MeanKL               | 0.0089488  |
| Entropy              | -1.5502    |
| Perplexity           | 0.21221    |
| AveragePolicyStd     | 0.18911    |
| AveragePolicyStd[0]  | 0.20478    |
| AveragePolicyStd[1]  | 0.20401    |
| AveragePolicyStd[2]  | 0.14821    |
| AveragePolicyStd[3]  | 0.18828    |
| AveragePolicyStd[4]  | 0.15766    |
| AveragePolicyStd[5]  | 0.23172    |
| AverageReturn        | 1787.8     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 239.37     |
| AverageEpisodeLength | 972.7      |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.23     |
| TotalNEpisodes       | 23512      |
| TotalNSamples        | 6.5535e+06 |
| ExplainedVariance    | 0.10356    |
-------------------------------------
[2018-01-21 15:57:23.991240 UTC] Saving snapshot
[2018-01-21 15:57:23.991447 UTC] Starting iteration 1310
[2018-01-21 15:57:23.991631 UTC] Start collecting samples
[2018-01-21 15:57:28.502610 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:28.624697 UTC] Performing policy update
[2018-01-21 15:57:28.625289 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:28.744973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:30.149633 UTC] Performing line search
[2018-01-21 15:57:30.336373 UTC] Updating baseline
[2018-01-21 15:57:34.116805 UTC] Computing logging information
-------------------------------------
| Iteration            | 1310       |
| ExpectedImprovement  | 0.017336   |
| ActualImprovement    | 0.016255   |
| ImprovementRatio     | 0.93761    |
| MeanKL               | 0.0079637  |
| Entropy              | -1.5482    |
| Perplexity           | 0.21264    |
| AveragePolicyStd     | 0.18915    |
| AveragePolicyStd[0]  | 0.20509    |
| AveragePolicyStd[1]  | 0.20383    |
| AveragePolicyStd[2]  | 0.14809    |
| AveragePolicyStd[3]  | 0.18862    |
| AveragePolicyStd[4]  | 0.15809    |
| AveragePolicyStd[5]  | 0.23121    |
| AverageReturn        | 1787.5     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 239.23     |
| AverageEpisodeLength | 972.7      |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.23     |
| TotalNEpisodes       | 23515      |
| TotalNSamples        | 6.5565e+06 |
| ExplainedVariance    | -0.0071547 |
-------------------------------------
[2018-01-21 15:57:34.979740 UTC] Saving snapshot
[2018-01-21 15:57:34.989987 UTC] Starting iteration 1311
[2018-01-21 15:57:34.990856 UTC] Start collecting samples
[2018-01-21 15:57:39.565956 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:39.692841 UTC] Performing policy update
[2018-01-21 15:57:39.693899 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:39.811990 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:41.230506 UTC] Performing line search
[2018-01-21 15:57:41.424166 UTC] Updating baseline
[2018-01-21 15:57:43.500437 UTC] Computing logging information
-------------------------------------
| Iteration            | 1311       |
| ExpectedImprovement  | 0.019874   |
| ActualImprovement    | 0.018755   |
| ImprovementRatio     | 0.94369    |
| MeanKL               | 0.0083243  |
| Entropy              | -1.5456    |
| Perplexity           | 0.21319    |
| AveragePolicyStd     | 0.18923    |
| AveragePolicyStd[0]  | 0.20509    |
| AveragePolicyStd[1]  | 0.20371    |
| AveragePolicyStd[2]  | 0.14827    |
| AveragePolicyStd[3]  | 0.18892    |
| AveragePolicyStd[4]  | 0.15813    |
| AveragePolicyStd[5]  | 0.23125    |
| AverageReturn        | 1787.2     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 239.17     |
| AverageEpisodeLength | 972.7      |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.23     |
| TotalNEpisodes       | 23521      |
| TotalNSamples        | 6.5625e+06 |
| ExplainedVariance    | 0.041932   |
-------------------------------------
[2018-01-21 15:57:44.280902 UTC] Saving snapshot
[2018-01-21 15:57:44.281165 UTC] Starting iteration 1312
[2018-01-21 15:57:44.281350 UTC] Start collecting samples
[2018-01-21 15:57:48.878794 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:49.005954 UTC] Performing policy update
[2018-01-21 15:57:49.007056 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:49.124166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:57:50.535906 UTC] Performing line search
[2018-01-21 15:57:50.726258 UTC] Updating baseline
[2018-01-21 15:57:52.819375 UTC] Computing logging information
-------------------------------------
| Iteration            | 1312       |
| ExpectedImprovement  | 0.017547   |
| ActualImprovement    | 0.016296   |
| ImprovementRatio     | 0.92871    |
| MeanKL               | 0.0076999  |
| Entropy              | -1.548     |
| Perplexity           | 0.21267    |
| AveragePolicyStd     | 0.18914    |
| AveragePolicyStd[0]  | 0.20498    |
| AveragePolicyStd[1]  | 0.20368    |
| AveragePolicyStd[2]  | 0.14835    |
| AveragePolicyStd[3]  | 0.18865    |
| AveragePolicyStd[4]  | 0.15814    |
| AveragePolicyStd[5]  | 0.23102    |
| AverageReturn        | 1792.8     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 233.04     |
| AverageEpisodeLength | 975.23     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.85     |
| TotalNEpisodes       | 23526      |
| TotalNSamples        | 6.5674e+06 |
| ExplainedVariance    | 0.10594    |
-------------------------------------
[2018-01-21 15:57:53.592784 UTC] Saving snapshot
[2018-01-21 15:57:53.593034 UTC] Starting iteration 1313
[2018-01-21 15:57:53.593218 UTC] Start collecting samples
[2018-01-21 15:57:58.331946 UTC] Computing input variables for policy optimization
[2018-01-21 15:57:58.470084 UTC] Performing policy update
[2018-01-21 15:57:58.471184 UTC] Computing gradient in Euclidean space
[2018-01-21 15:57:58.618529 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:00.052943 UTC] Performing line search
[2018-01-21 15:58:00.247037 UTC] Updating baseline
[2018-01-21 15:58:02.727362 UTC] Computing logging information
-------------------------------------
| Iteration            | 1313       |
| ExpectedImprovement  | 0.016992   |
| ActualImprovement    | 0.016602   |
| ImprovementRatio     | 0.97704    |
| MeanKL               | 0.0080189  |
| Entropy              | -1.5492    |
| Perplexity           | 0.21241    |
| AveragePolicyStd     | 0.18911    |
| AveragePolicyStd[0]  | 0.20479    |
| AveragePolicyStd[1]  | 0.20387    |
| AveragePolicyStd[2]  | 0.1484     |
| AveragePolicyStd[3]  | 0.18848    |
| AveragePolicyStd[4]  | 0.15787    |
| AveragePolicyStd[5]  | 0.23128    |
| AverageReturn        | 1791.7     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 232.88     |
| AverageEpisodeLength | 975.23     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.85     |
| TotalNEpisodes       | 23530      |
| TotalNSamples        | 6.5714e+06 |
| ExplainedVariance    | -0.064692  |
-------------------------------------
[2018-01-21 15:58:03.601481 UTC] Saving snapshot
[2018-01-21 15:58:03.601973 UTC] Starting iteration 1314
[2018-01-21 15:58:03.602350 UTC] Start collecting samples
[2018-01-21 15:58:08.068895 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:08.202675 UTC] Performing policy update
[2018-01-21 15:58:08.203829 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:08.319773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:09.703052 UTC] Performing line search
[2018-01-21 15:58:09.895028 UTC] Updating baseline
[2018-01-21 15:58:12.772564 UTC] Computing logging information
-------------------------------------
| Iteration            | 1314       |
| ExpectedImprovement  | 0.017812   |
| ActualImprovement    | 0.016755   |
| ImprovementRatio     | 0.94064    |
| MeanKL               | 0.0075307  |
| Entropy              | -1.5393    |
| Perplexity           | 0.21453    |
| AveragePolicyStd     | 0.18945    |
| AveragePolicyStd[0]  | 0.20518    |
| AveragePolicyStd[1]  | 0.2039     |
| AveragePolicyStd[2]  | 0.14898    |
| AveragePolicyStd[3]  | 0.18872    |
| AveragePolicyStd[4]  | 0.15758    |
| AveragePolicyStd[5]  | 0.23233    |
| AverageReturn        | 1791.3     |
| MinReturn            | 248.8      |
| MaxReturn            | 1900.1     |
| StdReturn            | 232.85     |
| AverageEpisodeLength | 975.23     |
| MinEpisodeLength     | 179        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.85     |
| TotalNEpisodes       | 23536      |
| TotalNSamples        | 6.5774e+06 |
| ExplainedVariance    | 9.9531e-10 |
-------------------------------------
[2018-01-21 15:58:13.554370 UTC] Saving snapshot
[2018-01-21 15:58:13.554621 UTC] Starting iteration 1315
[2018-01-21 15:58:13.554788 UTC] Start collecting samples
[2018-01-21 15:58:18.003245 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:18.127889 UTC] Performing policy update
[2018-01-21 15:58:18.128672 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:18.245723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:19.624971 UTC] Performing line search
[2018-01-21 15:58:19.814029 UTC] Updating baseline
[2018-01-21 15:58:22.572516 UTC] Computing logging information
-------------------------------------
| Iteration            | 1315       |
| ExpectedImprovement  | 0.017204   |
| ActualImprovement    | 0.016131   |
| ImprovementRatio     | 0.93764    |
| MeanKL               | 0.0085432  |
| Entropy              | -1.5373    |
| Perplexity           | 0.21497    |
| AveragePolicyStd     | 0.18954    |
| AveragePolicyStd[0]  | 0.20526    |
| AveragePolicyStd[1]  | 0.20416    |
| AveragePolicyStd[2]  | 0.1492     |
| AveragePolicyStd[3]  | 0.18835    |
| AveragePolicyStd[4]  | 0.15715    |
| AveragePolicyStd[5]  | 0.23315    |
| AverageReturn        | 1806       |
| MinReturn            | 581.52     |
| MaxReturn            | 1900.1     |
| StdReturn            | 173.43     |
| AverageEpisodeLength | 983.44     |
| MinEpisodeLength     | 348        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.882     |
| TotalNEpisodes       | 23540      |
| TotalNSamples        | 6.5814e+06 |
| ExplainedVariance    | -5.995e-05 |
-------------------------------------
[2018-01-21 15:58:23.331440 UTC] Saving snapshot
[2018-01-21 15:58:23.331720 UTC] Starting iteration 1316
[2018-01-21 15:58:23.331923 UTC] Start collecting samples
[2018-01-21 15:58:28.159119 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:28.287418 UTC] Performing policy update
[2018-01-21 15:58:28.288348 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:28.405305 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:29.772340 UTC] Performing line search
[2018-01-21 15:58:29.966734 UTC] Updating baseline
[2018-01-21 15:58:32.631874 UTC] Computing logging information
-------------------------------------
| Iteration            | 1316       |
| ExpectedImprovement  | 0.018424   |
| ActualImprovement    | 0.017226   |
| ImprovementRatio     | 0.93496    |
| MeanKL               | 0.0080061  |
| Entropy              | -1.5295    |
| Perplexity           | 0.21664    |
| AveragePolicyStd     | 0.1898     |
| AveragePolicyStd[0]  | 0.20556    |
| AveragePolicyStd[1]  | 0.20461    |
| AveragePolicyStd[2]  | 0.14929    |
| AveragePolicyStd[3]  | 0.18852    |
| AveragePolicyStd[4]  | 0.15729    |
| AveragePolicyStd[5]  | 0.23355    |
| AverageReturn        | 1804.6     |
| MinReturn            | 581.52     |
| MaxReturn            | 1900.1     |
| StdReturn            | 173.01     |
| AverageEpisodeLength | 983.44     |
| MinEpisodeLength     | 348        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.882     |
| TotalNEpisodes       | 23546      |
| TotalNSamples        | 6.5874e+06 |
| ExplainedVariance    | 0.00025738 |
-------------------------------------
[2018-01-21 15:58:33.442785 UTC] Saving snapshot
[2018-01-21 15:58:33.443048 UTC] Starting iteration 1317
[2018-01-21 15:58:33.443202 UTC] Start collecting samples
[2018-01-21 15:58:38.220632 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:38.346332 UTC] Performing policy update
[2018-01-21 15:58:38.347424 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:38.469864 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:39.864504 UTC] Performing line search
[2018-01-21 15:58:40.063283 UTC] Updating baseline
[2018-01-21 15:58:41.964061 UTC] Computing logging information
-------------------------------------
| Iteration            | 1317       |
| ExpectedImprovement  | 0.018377   |
| ActualImprovement    | 0.017305   |
| ImprovementRatio     | 0.94164    |
| MeanKL               | 0.0076504  |
| Entropy              | -1.5333    |
| Perplexity           | 0.21583    |
| AveragePolicyStd     | 0.18968    |
| AveragePolicyStd[0]  | 0.20552    |
| AveragePolicyStd[1]  | 0.20497    |
| AveragePolicyStd[2]  | 0.14884    |
| AveragePolicyStd[3]  | 0.18833    |
| AveragePolicyStd[4]  | 0.15748    |
| AveragePolicyStd[5]  | 0.23297    |
| AverageReturn        | 1806.2     |
| MinReturn            | 581.52     |
| MaxReturn            | 1900.1     |
| StdReturn            | 171.07     |
| AverageEpisodeLength | 984.9      |
| MinEpisodeLength     | 348        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 86.927     |
| TotalNEpisodes       | 23552      |
| TotalNSamples        | 6.5934e+06 |
| ExplainedVariance    | 0.0022404  |
-------------------------------------
[2018-01-21 15:58:42.789835 UTC] Saving snapshot
[2018-01-21 15:58:42.790073 UTC] Starting iteration 1318
[2018-01-21 15:58:42.790247 UTC] Start collecting samples
[2018-01-21 15:58:47.282821 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:47.415427 UTC] Performing policy update
[2018-01-21 15:58:47.416420 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:47.533035 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:48.943647 UTC] Performing line search
[2018-01-21 15:58:49.147875 UTC] Updating baseline
[2018-01-21 15:58:50.962654 UTC] Computing logging information
-------------------------------------
| Iteration            | 1318       |
| ExpectedImprovement  | 0.018172   |
| ActualImprovement    | 0.016653   |
| ImprovementRatio     | 0.9164     |
| MeanKL               | 0.0077057  |
| Entropy              | -1.5348    |
| Perplexity           | 0.21549    |
| AveragePolicyStd     | 0.18962    |
| AveragePolicyStd[0]  | 0.20545    |
| AveragePolicyStd[1]  | 0.20506    |
| AveragePolicyStd[2]  | 0.14884    |
| AveragePolicyStd[3]  | 0.18823    |
| AveragePolicyStd[4]  | 0.15751    |
| AveragePolicyStd[5]  | 0.23264    |
| AverageReturn        | 1816.5     |
| MinReturn            | 581.52     |
| MaxReturn            | 1900.1     |
| StdReturn            | 139.5      |
| AverageEpisodeLength | 990.15     |
| MinEpisodeLength     | 348        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 70.222     |
| TotalNEpisodes       | 23555      |
| TotalNSamples        | 6.5964e+06 |
| ExplainedVariance    | -0.0078079 |
-------------------------------------
[2018-01-21 15:58:51.845885 UTC] Saving snapshot
[2018-01-21 15:58:51.846160 UTC] Starting iteration 1319
[2018-01-21 15:58:51.846381 UTC] Start collecting samples
[2018-01-21 15:58:56.361404 UTC] Computing input variables for policy optimization
[2018-01-21 15:58:56.516074 UTC] Performing policy update
[2018-01-21 15:58:56.517096 UTC] Computing gradient in Euclidean space
[2018-01-21 15:58:56.633835 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:58:58.034854 UTC] Performing line search
[2018-01-21 15:58:58.231495 UTC] Updating baseline
[2018-01-21 15:59:00.092060 UTC] Computing logging information
-------------------------------------
| Iteration            | 1319       |
| ExpectedImprovement  | 0.01724    |
| ActualImprovement    | 0.016132   |
| ImprovementRatio     | 0.93575    |
| MeanKL               | 0.0074121  |
| Entropy              | -1.5409    |
| Perplexity           | 0.21418    |
| AveragePolicyStd     | 0.18942    |
| AveragePolicyStd[0]  | 0.20481    |
| AveragePolicyStd[1]  | 0.20481    |
| AveragePolicyStd[2]  | 0.14877    |
| AveragePolicyStd[3]  | 0.18831    |
| AveragePolicyStd[4]  | 0.15737    |
| AveragePolicyStd[5]  | 0.23246    |
| AverageReturn        | 1814       |
| MinReturn            | 581.52     |
| MaxReturn            | 1900.1     |
| StdReturn            | 139.66     |
| AverageEpisodeLength | 990.15     |
| MinEpisodeLength     | 348        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 70.222     |
| TotalNEpisodes       | 23562      |
| TotalNSamples        | 6.6034e+06 |
| ExplainedVariance    | 0.0189     |
-------------------------------------
[2018-01-21 15:59:00.945172 UTC] Saving snapshot
[2018-01-21 15:59:00.945474 UTC] Starting iteration 1320
[2018-01-21 15:59:00.945657 UTC] Start collecting samples
[2018-01-21 15:59:05.434656 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:05.570144 UTC] Performing policy update
[2018-01-21 15:59:05.571251 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:05.688530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:07.156336 UTC] Performing line search
[2018-01-21 15:59:07.354940 UTC] Updating baseline
[2018-01-21 15:59:09.235386 UTC] Computing logging information
-------------------------------------
| Iteration            | 1320       |
| ExpectedImprovement  | 0.01896    |
| ActualImprovement    | 0.018102   |
| ImprovementRatio     | 0.95476    |
| MeanKL               | 0.0077337  |
| Entropy              | -1.5454    |
| Perplexity           | 0.21323    |
| AveragePolicyStd     | 0.18926    |
| AveragePolicyStd[0]  | 0.20495    |
| AveragePolicyStd[1]  | 0.20422    |
| AveragePolicyStd[2]  | 0.14865    |
| AveragePolicyStd[3]  | 0.18839    |
| AveragePolicyStd[4]  | 0.15748    |
| AveragePolicyStd[5]  | 0.23186    |
| AverageReturn        | 1802.2     |
| MinReturn            | 531.04     |
| MaxReturn            | 1900.1     |
| StdReturn            | 189.33     |
| AverageEpisodeLength | 983.41     |
| MinEpisodeLength     | 326        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.414     |
| TotalNEpisodes       | 23568      |
| TotalNSamples        | 6.6087e+06 |
| ExplainedVariance    | 0.10687    |
-------------------------------------
[2018-01-21 15:59:10.056713 UTC] Saving snapshot
[2018-01-21 15:59:10.066922 UTC] Starting iteration 1321
[2018-01-21 15:59:10.067110 UTC] Start collecting samples
[2018-01-21 15:59:14.335280 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:14.493503 UTC] Performing policy update
[2018-01-21 15:59:14.494111 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:14.615655 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:16.035264 UTC] Performing line search
[2018-01-21 15:59:16.228016 UTC] Updating baseline
[2018-01-21 15:59:18.050619 UTC] Computing logging information
-------------------------------------
| Iteration            | 1321       |
| ExpectedImprovement  | 0.016921   |
| ActualImprovement    | 0.016207   |
| ImprovementRatio     | 0.95784    |
| MeanKL               | 0.0081513  |
| Entropy              | -1.5458    |
| Perplexity           | 0.21313    |
| AveragePolicyStd     | 0.18924    |
| AveragePolicyStd[0]  | 0.20527    |
| AveragePolicyStd[1]  | 0.20416    |
| AveragePolicyStd[2]  | 0.14866    |
| AveragePolicyStd[3]  | 0.18848    |
| AveragePolicyStd[4]  | 0.15743    |
| AveragePolicyStd[5]  | 0.23141    |
| AverageReturn        | 1801.5     |
| MinReturn            | 531.04     |
| MaxReturn            | 1889.7     |
| StdReturn            | 189.12     |
| AverageEpisodeLength | 983.41     |
| MinEpisodeLength     | 326        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.414     |
| TotalNEpisodes       | 23572      |
| TotalNSamples        | 6.6127e+06 |
| ExplainedVariance    | -0.030232  |
-------------------------------------
[2018-01-21 15:59:18.818210 UTC] Saving snapshot
[2018-01-21 15:59:18.818409 UTC] Starting iteration 1322
[2018-01-21 15:59:18.818547 UTC] Start collecting samples
[2018-01-21 15:59:23.373296 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:23.504766 UTC] Performing policy update
[2018-01-21 15:59:23.505468 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:23.630066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:25.050424 UTC] Performing line search
[2018-01-21 15:59:25.248207 UTC] Updating baseline
[2018-01-21 15:59:26.785177 UTC] Computing logging information
-------------------------------------
| Iteration            | 1322       |
| ExpectedImprovement  | 0.017756   |
| ActualImprovement    | 0.016803   |
| ImprovementRatio     | 0.94629    |
| MeanKL               | 0.0085389  |
| Entropy              | -1.5443    |
| Perplexity           | 0.21347    |
| AveragePolicyStd     | 0.18927    |
| AveragePolicyStd[0]  | 0.20479    |
| AveragePolicyStd[1]  | 0.20419    |
| AveragePolicyStd[2]  | 0.14901    |
| AveragePolicyStd[3]  | 0.18844    |
| AveragePolicyStd[4]  | 0.15748    |
| AveragePolicyStd[5]  | 0.2317     |
| AverageReturn        | 1802.5     |
| MinReturn            | 531.04     |
| MaxReturn            | 1889.7     |
| StdReturn            | 189.36     |
| AverageEpisodeLength | 983.41     |
| MinEpisodeLength     | 326        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.414     |
| TotalNEpisodes       | 23577      |
| TotalNSamples        | 6.6177e+06 |
| ExplainedVariance    | 1.0366e-10 |
-------------------------------------
[2018-01-21 15:59:27.572803 UTC] Saving snapshot
[2018-01-21 15:59:27.572991 UTC] Starting iteration 1323
[2018-01-21 15:59:27.573173 UTC] Start collecting samples
[2018-01-21 15:59:32.313503 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:32.437193 UTC] Performing policy update
[2018-01-21 15:59:32.438349 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:32.569073 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:33.982141 UTC] Performing line search
[2018-01-21 15:59:34.202795 UTC] Updating baseline
[2018-01-21 15:59:35.487413 UTC] Computing logging information
--------------------------------------
| Iteration            | 1323        |
| ExpectedImprovement  | 0.018126    |
| ActualImprovement    | 0.016679    |
| ImprovementRatio     | 0.92016     |
| MeanKL               | 0.0080479   |
| Entropy              | -1.5484     |
| Perplexity           | 0.2126      |
| AveragePolicyStd     | 0.18914     |
| AveragePolicyStd[0]  | 0.20448     |
| AveragePolicyStd[1]  | 0.20429     |
| AveragePolicyStd[2]  | 0.14915     |
| AveragePolicyStd[3]  | 0.18889     |
| AveragePolicyStd[4]  | 0.15678     |
| AveragePolicyStd[5]  | 0.23126     |
| AverageReturn        | 1802.7      |
| MinReturn            | 531.04      |
| MaxReturn            | 1889.7      |
| StdReturn            | 189.43      |
| AverageEpisodeLength | 983.41      |
| MinEpisodeLength     | 326         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 96.414      |
| TotalNEpisodes       | 23581       |
| TotalNSamples        | 6.6217e+06  |
| ExplainedVariance    | -4.0221e-09 |
--------------------------------------
[2018-01-21 15:59:36.266648 UTC] Saving snapshot
[2018-01-21 15:59:36.266889 UTC] Starting iteration 1324
[2018-01-21 15:59:36.267031 UTC] Start collecting samples
[2018-01-21 15:59:41.005681 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:41.132921 UTC] Performing policy update
[2018-01-21 15:59:41.133509 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:41.251331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:42.685719 UTC] Performing line search
[2018-01-21 15:59:42.882655 UTC] Updating baseline
[2018-01-21 15:59:45.005756 UTC] Computing logging information
------------------------------------
| Iteration            | 1324      |
| ExpectedImprovement  | 0.01855   |
| ActualImprovement    | 0.017574  |
| ImprovementRatio     | 0.94736   |
| MeanKL               | 0.0076612 |
| Entropy              | -1.5506   |
| Perplexity           | 0.21212   |
| AveragePolicyStd     | 0.18905   |
| AveragePolicyStd[0]  | 0.20457   |
| AveragePolicyStd[1]  | 0.2042    |
| AveragePolicyStd[2]  | 0.14933   |
| AveragePolicyStd[3]  | 0.18837   |
| AveragePolicyStd[4]  | 0.15692   |
| AveragePolicyStd[5]  | 0.23092   |
| AverageReturn        | 1789.6    |
| MinReturn            | 427       |
| MaxReturn            | 1890.8    |
| StdReturn            | 233.9     |
| AverageEpisodeLength | 976.05    |
| MinEpisodeLength     | 264       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 120.06    |
| TotalNEpisodes       | 23589     |
| TotalNSamples        | 6.629e+06 |
| ExplainedVariance    | 0.096827  |
------------------------------------
[2018-01-21 15:59:45.781775 UTC] Saving snapshot
[2018-01-21 15:59:45.781982 UTC] Starting iteration 1325
[2018-01-21 15:59:45.782121 UTC] Start collecting samples
[2018-01-21 15:59:50.223434 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:50.370874 UTC] Performing policy update
[2018-01-21 15:59:50.371704 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:50.490309 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 15:59:51.881304 UTC] Performing line search
[2018-01-21 15:59:52.075338 UTC] Updating baseline
[2018-01-21 15:59:53.883568 UTC] Computing logging information
------------------------------------
| Iteration            | 1325      |
| ExpectedImprovement  | 0.01882   |
| ActualImprovement    | 0.017792  |
| ImprovementRatio     | 0.94533   |
| MeanKL               | 0.0077058 |
| Entropy              | -1.5544   |
| Perplexity           | 0.21132   |
| AveragePolicyStd     | 0.18893   |
| AveragePolicyStd[0]  | 0.2043    |
| AveragePolicyStd[1]  | 0.204     |
| AveragePolicyStd[2]  | 0.14939   |
| AveragePolicyStd[3]  | 0.18822   |
| AveragePolicyStd[4]  | 0.15678   |
| AveragePolicyStd[5]  | 0.23087   |
| AverageReturn        | 1802.7    |
| MinReturn            | 427       |
| MaxReturn            | 1915.9    |
| StdReturn            | 200.34    |
| AverageEpisodeLength | 982.57    |
| MinEpisodeLength     | 264       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 102.14    |
| TotalNEpisodes       | 23593     |
| TotalNSamples        | 6.633e+06 |
| ExplainedVariance    | -0.028253 |
------------------------------------
[2018-01-21 15:59:54.664914 UTC] Saving snapshot
[2018-01-21 15:59:54.665152 UTC] Starting iteration 1326
[2018-01-21 15:59:54.665299 UTC] Start collecting samples
[2018-01-21 15:59:59.420062 UTC] Computing input variables for policy optimization
[2018-01-21 15:59:59.560914 UTC] Performing policy update
[2018-01-21 15:59:59.561980 UTC] Computing gradient in Euclidean space
[2018-01-21 15:59:59.676163 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:01.087565 UTC] Performing line search
[2018-01-21 16:00:01.281081 UTC] Updating baseline
[2018-01-21 16:00:03.096440 UTC] Computing logging information
-------------------------------------
| Iteration            | 1326       |
| ExpectedImprovement  | 0.01755    |
| ActualImprovement    | 0.01668    |
| ImprovementRatio     | 0.95043    |
| MeanKL               | 0.0075681  |
| Entropy              | -1.5578    |
| Perplexity           | 0.2106     |
| AveragePolicyStd     | 0.18881    |
| AveragePolicyStd[0]  | 0.20403    |
| AveragePolicyStd[1]  | 0.20382    |
| AveragePolicyStd[2]  | 0.14931    |
| AveragePolicyStd[3]  | 0.18807    |
| AveragePolicyStd[4]  | 0.15681    |
| AveragePolicyStd[5]  | 0.23084    |
| AverageReturn        | 1796.6     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 209.17     |
| AverageEpisodeLength | 979.42     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 106.33     |
| TotalNEpisodes       | 23598      |
| TotalNSamples        | 6.6377e+06 |
| ExplainedVariance    | 0.14801    |
-------------------------------------
[2018-01-21 16:00:03.911598 UTC] Saving snapshot
[2018-01-21 16:00:03.911918 UTC] Starting iteration 1327
[2018-01-21 16:00:03.912131 UTC] Start collecting samples
[2018-01-21 16:00:08.578276 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:08.723021 UTC] Performing policy update
[2018-01-21 16:00:08.724039 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:08.846941 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:10.242215 UTC] Performing line search
[2018-01-21 16:00:10.472983 UTC] Updating baseline
[2018-01-21 16:00:13.305510 UTC] Computing logging information
--------------------------------------
| Iteration            | 1327        |
| ExpectedImprovement  | 0.017176    |
| ActualImprovement    | 0.015948    |
| ImprovementRatio     | 0.92856     |
| MeanKL               | 0.0088919   |
| Entropy              | -1.5614     |
| Perplexity           | 0.20984     |
| AveragePolicyStd     | 0.18871     |
| AveragePolicyStd[0]  | 0.2036      |
| AveragePolicyStd[1]  | 0.20385     |
| AveragePolicyStd[2]  | 0.14936     |
| AveragePolicyStd[3]  | 0.18802     |
| AveragePolicyStd[4]  | 0.15644     |
| AveragePolicyStd[5]  | 0.23099     |
| AverageReturn        | 1796.2      |
| MinReturn            | 427         |
| MaxReturn            | 1915.9      |
| StdReturn            | 209.11      |
| AverageEpisodeLength | 979.42      |
| MinEpisodeLength     | 264         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 106.33      |
| TotalNEpisodes       | 23603       |
| TotalNSamples        | 6.6427e+06  |
| ExplainedVariance    | -1.4674e-05 |
--------------------------------------
[2018-01-21 16:00:14.065958 UTC] Saving snapshot
[2018-01-21 16:00:14.066281 UTC] Starting iteration 1328
[2018-01-21 16:00:14.066501 UTC] Start collecting samples
[2018-01-21 16:00:18.761322 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:18.881863 UTC] Performing policy update
[2018-01-21 16:00:18.882517 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:18.998225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:20.418861 UTC] Performing line search
[2018-01-21 16:00:20.612616 UTC] Updating baseline
[2018-01-21 16:00:22.549873 UTC] Computing logging information
-------------------------------------
| Iteration            | 1328       |
| ExpectedImprovement  | 0.018875   |
| ActualImprovement    | 0.017638   |
| ImprovementRatio     | 0.93443    |
| MeanKL               | 0.0082487  |
| Entropy              | -1.5631    |
| Perplexity           | 0.20948    |
| AveragePolicyStd     | 0.18866    |
| AveragePolicyStd[0]  | 0.20386    |
| AveragePolicyStd[1]  | 0.20342    |
| AveragePolicyStd[2]  | 0.14927    |
| AveragePolicyStd[3]  | 0.18761    |
| AveragePolicyStd[4]  | 0.15664    |
| AveragePolicyStd[5]  | 0.23114    |
| AverageReturn        | 1793.7     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 210.18     |
| AverageEpisodeLength | 978.08     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 106.9      |
| TotalNEpisodes       | 23609      |
| TotalNSamples        | 6.6485e+06 |
| ExplainedVariance    | 0.11594    |
-------------------------------------
[2018-01-21 16:00:23.324797 UTC] Saving snapshot
[2018-01-21 16:00:23.325084 UTC] Starting iteration 1329
[2018-01-21 16:00:23.325264 UTC] Start collecting samples
[2018-01-21 16:00:27.942506 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:28.079576 UTC] Performing policy update
[2018-01-21 16:00:28.080350 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:28.200675 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:29.603979 UTC] Performing line search
[2018-01-21 16:00:29.795879 UTC] Updating baseline
[2018-01-21 16:00:32.486956 UTC] Computing logging information
-------------------------------------
| Iteration            | 1329       |
| ExpectedImprovement  | 0.017801   |
| ActualImprovement    | 0.016983   |
| ImprovementRatio     | 0.95404    |
| MeanKL               | 0.0085335  |
| Entropy              | -1.5668    |
| Perplexity           | 0.20872    |
| AveragePolicyStd     | 0.18853    |
| AveragePolicyStd[0]  | 0.20375    |
| AveragePolicyStd[1]  | 0.20313    |
| AveragePolicyStd[2]  | 0.14908    |
| AveragePolicyStd[3]  | 0.18765    |
| AveragePolicyStd[4]  | 0.15682    |
| AveragePolicyStd[5]  | 0.23072    |
| AverageReturn        | 1800.6     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 204.33     |
| AverageEpisodeLength | 980.8      |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.92     |
| TotalNEpisodes       | 23612      |
| TotalNSamples        | 6.6515e+06 |
| ExplainedVariance    | -0.014706  |
-------------------------------------
[2018-01-21 16:00:33.360980 UTC] Saving snapshot
[2018-01-21 16:00:33.361254 UTC] Starting iteration 1330
[2018-01-21 16:00:33.361449 UTC] Start collecting samples
[2018-01-21 16:00:38.219955 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:38.347035 UTC] Performing policy update
[2018-01-21 16:00:38.347680 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:38.460702 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:39.869398 UTC] Performing line search
[2018-01-21 16:00:40.067629 UTC] Updating baseline
[2018-01-21 16:00:43.849520 UTC] Computing logging information
-------------------------------------
| Iteration            | 1330       |
| ExpectedImprovement  | 0.020193   |
| ActualImprovement    | 0.018595   |
| ImprovementRatio     | 0.92085    |
| MeanKL               | 0.0077887  |
| Entropy              | -1.569     |
| Perplexity           | 0.20826    |
| AveragePolicyStd     | 0.18845    |
| AveragePolicyStd[0]  | 0.204      |
| AveragePolicyStd[1]  | 0.20264    |
| AveragePolicyStd[2]  | 0.14937    |
| AveragePolicyStd[3]  | 0.18745    |
| AveragePolicyStd[4]  | 0.15657    |
| AveragePolicyStd[5]  | 0.23066    |
| AverageReturn        | 1798.7     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 204.1      |
| AverageEpisodeLength | 980.8      |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.92     |
| TotalNEpisodes       | 23617      |
| TotalNSamples        | 6.6565e+06 |
| ExplainedVariance    | 9.9664e-06 |
-------------------------------------
[2018-01-21 16:00:44.643982 UTC] Saving snapshot
[2018-01-21 16:00:44.651394 UTC] Starting iteration 1331
[2018-01-21 16:00:44.651574 UTC] Start collecting samples
[2018-01-21 16:00:49.278986 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:49.402218 UTC] Performing policy update
[2018-01-21 16:00:49.403345 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:49.527095 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:00:50.926478 UTC] Performing line search
[2018-01-21 16:00:51.111535 UTC] Updating baseline
[2018-01-21 16:00:53.089220 UTC] Computing logging information
-------------------------------------
| Iteration            | 1331       |
| ExpectedImprovement  | 0.017695   |
| ActualImprovement    | 0.016849   |
| ImprovementRatio     | 0.95218    |
| MeanKL               | 0.0076944  |
| Entropy              | -1.5784    |
| Perplexity           | 0.20631    |
| AveragePolicyStd     | 0.18819    |
| AveragePolicyStd[0]  | 0.20366    |
| AveragePolicyStd[1]  | 0.20286    |
| AveragePolicyStd[2]  | 0.14855    |
| AveragePolicyStd[3]  | 0.18743    |
| AveragePolicyStd[4]  | 0.15631    |
| AveragePolicyStd[5]  | 0.2303     |
| AverageReturn        | 1798.4     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 203.42     |
| AverageEpisodeLength | 981.41     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.86     |
| TotalNEpisodes       | 23623      |
| TotalNSamples        | 6.6625e+06 |
| ExplainedVariance    | 0.0041732  |
-------------------------------------
[2018-01-21 16:00:53.934017 UTC] Saving snapshot
[2018-01-21 16:00:53.934201 UTC] Starting iteration 1332
[2018-01-21 16:00:53.934328 UTC] Start collecting samples
[2018-01-21 16:00:58.491107 UTC] Computing input variables for policy optimization
[2018-01-21 16:00:58.621143 UTC] Performing policy update
[2018-01-21 16:00:58.621809 UTC] Computing gradient in Euclidean space
[2018-01-21 16:00:58.754246 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:00.150691 UTC] Performing line search
[2018-01-21 16:01:00.351306 UTC] Updating baseline
[2018-01-21 16:01:02.269635 UTC] Computing logging information
-------------------------------------
| Iteration            | 1332       |
| ExpectedImprovement  | 0.019572   |
| ActualImprovement    | 0.018358   |
| ImprovementRatio     | 0.93797    |
| MeanKL               | 0.007203   |
| Entropy              | -1.5765    |
| Perplexity           | 0.2067     |
| AveragePolicyStd     | 0.18825    |
| AveragePolicyStd[0]  | 0.20393    |
| AveragePolicyStd[1]  | 0.20285    |
| AveragePolicyStd[2]  | 0.14885    |
| AveragePolicyStd[3]  | 0.1874     |
| AveragePolicyStd[4]  | 0.15603    |
| AveragePolicyStd[5]  | 0.23042    |
| AverageReturn        | 1783.6     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 243.35     |
| AverageEpisodeLength | 974.31     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.55     |
| TotalNEpisodes       | 23629      |
| TotalNSamples        | 6.6678e+06 |
| ExplainedVariance    | 0.11489    |
-------------------------------------
[2018-01-21 16:01:03.151792 UTC] Saving snapshot
[2018-01-21 16:01:03.152094 UTC] Starting iteration 1333
[2018-01-21 16:01:03.152284 UTC] Start collecting samples
[2018-01-21 16:01:07.684724 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:07.813808 UTC] Performing policy update
[2018-01-21 16:01:07.814942 UTC] Computing gradient in Euclidean space
[2018-01-21 16:01:07.937296 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:09.359769 UTC] Performing line search
[2018-01-21 16:01:09.564786 UTC] Updating baseline
[2018-01-21 16:01:11.894651 UTC] Computing logging information
-------------------------------------
| Iteration            | 1333       |
| ExpectedImprovement  | 0.016731   |
| ActualImprovement    | 0.015994   |
| ImprovementRatio     | 0.95594    |
| MeanKL               | 0.0082528  |
| Entropy              | -1.5823    |
| Perplexity           | 0.20549    |
| AveragePolicyStd     | 0.18804    |
| AveragePolicyStd[0]  | 0.20324    |
| AveragePolicyStd[1]  | 0.2025     |
| AveragePolicyStd[2]  | 0.149      |
| AveragePolicyStd[3]  | 0.18744    |
| AveragePolicyStd[4]  | 0.15597    |
| AveragePolicyStd[5]  | 0.23007    |
| AverageReturn        | 1785.5     |
| MinReturn            | 427        |
| MaxReturn            | 1915.9     |
| StdReturn            | 243.72     |
| AverageEpisodeLength | 974.31     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.55     |
| TotalNEpisodes       | 23634      |
| TotalNSamples        | 6.6728e+06 |
| ExplainedVariance    | -0.006255  |
-------------------------------------
[2018-01-21 16:01:12.730226 UTC] Saving snapshot
[2018-01-21 16:01:12.730538 UTC] Starting iteration 1334
[2018-01-21 16:01:12.730728 UTC] Start collecting samples
[2018-01-21 16:01:17.140305 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:17.277470 UTC] Performing policy update
[2018-01-21 16:01:17.278478 UTC] Computing gradient in Euclidean space
[2018-01-21 16:01:17.400799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:18.831891 UTC] Performing line search
[2018-01-21 16:01:19.025655 UTC] Updating baseline
[2018-01-21 16:01:21.447897 UTC] Computing logging information
--------------------------------------
| Iteration            | 1334        |
| ExpectedImprovement  | 0.018469    |
| ActualImprovement    | 0.016648    |
| ImprovementRatio     | 0.90142     |
| MeanKL               | 0.0085162   |
| Entropy              | -1.5794     |
| Perplexity           | 0.2061      |
| AveragePolicyStd     | 0.18817     |
| AveragePolicyStd[0]  | 0.20333     |
| AveragePolicyStd[1]  | 0.2031      |
| AveragePolicyStd[2]  | 0.14875     |
| AveragePolicyStd[3]  | 0.18808     |
| AveragePolicyStd[4]  | 0.1555      |
| AveragePolicyStd[5]  | 0.23025     |
| AverageReturn        | 1786.8      |
| MinReturn            | 427         |
| MaxReturn            | 1915.9      |
| StdReturn            | 244         |
| AverageEpisodeLength | 974.31      |
| MinEpisodeLength     | 264         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 124.55      |
| TotalNEpisodes       | 23638       |
| TotalNSamples        | 6.6768e+06  |
| ExplainedVariance    | -1.9394e-09 |
--------------------------------------
[2018-01-21 16:01:22.267700 UTC] Saving snapshot
[2018-01-21 16:01:22.267979 UTC] Starting iteration 1335
[2018-01-21 16:01:22.268183 UTC] Start collecting samples
[2018-01-21 16:01:26.913449 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:27.041514 UTC] Performing policy update
[2018-01-21 16:01:27.042159 UTC] Computing gradient in Euclidean space
[2018-01-21 16:01:27.162079 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:28.607238 UTC] Performing line search
[2018-01-21 16:01:28.796217 UTC] Updating baseline
[2018-01-21 16:01:31.704012 UTC] Computing logging information
-------------------------------------
| Iteration            | 1335       |
| ExpectedImprovement  | 0.018896   |
| ActualImprovement    | 0.017897   |
| ImprovementRatio     | 0.94711    |
| MeanKL               | 0.0081196  |
| Entropy              | -1.5837    |
| Perplexity           | 0.20521    |
| AveragePolicyStd     | 0.18801    |
| AveragePolicyStd[0]  | 0.20326    |
| AveragePolicyStd[1]  | 0.20277    |
| AveragePolicyStd[2]  | 0.14896    |
| AveragePolicyStd[3]  | 0.18805    |
| AveragePolicyStd[4]  | 0.15529    |
| AveragePolicyStd[5]  | 0.22974    |
| AverageReturn        | 1786.9     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 244.26     |
| AverageEpisodeLength | 974.31     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.55     |
| TotalNEpisodes       | 23645      |
| TotalNSamples        | 6.6838e+06 |
| ExplainedVariance    | 7.1872e-05 |
-------------------------------------
[2018-01-21 16:01:32.538361 UTC] Saving snapshot
[2018-01-21 16:01:32.538619 UTC] Starting iteration 1336
[2018-01-21 16:01:32.538779 UTC] Start collecting samples
[2018-01-21 16:01:37.356169 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:37.495534 UTC] Performing policy update
[2018-01-21 16:01:37.496207 UTC] Computing gradient in Euclidean space
[2018-01-21 16:01:37.617139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:39.047663 UTC] Performing line search
[2018-01-21 16:01:39.233391 UTC] Updating baseline
[2018-01-21 16:01:41.579307 UTC] Computing logging information
-------------------------------------
| Iteration            | 1336       |
| ExpectedImprovement  | 0.016767   |
| ActualImprovement    | 0.016081   |
| ImprovementRatio     | 0.95908    |
| MeanKL               | 0.0087537  |
| Entropy              | -1.5889    |
| Perplexity           | 0.20416    |
| AveragePolicyStd     | 0.18786    |
| AveragePolicyStd[0]  | 0.20314    |
| AveragePolicyStd[1]  | 0.20317    |
| AveragePolicyStd[2]  | 0.14905    |
| AveragePolicyStd[3]  | 0.18734    |
| AveragePolicyStd[4]  | 0.15485    |
| AveragePolicyStd[5]  | 0.22961    |
| AverageReturn        | 1788.8     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 244.61     |
| AverageEpisodeLength | 974.31     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.55     |
| TotalNEpisodes       | 23649      |
| TotalNSamples        | 6.6878e+06 |
| ExplainedVariance    | -0.0053739 |
-------------------------------------
[2018-01-21 16:01:42.451462 UTC] Saving snapshot
[2018-01-21 16:01:42.451705 UTC] Starting iteration 1337
[2018-01-21 16:01:42.451880 UTC] Start collecting samples
[2018-01-21 16:01:46.956568 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:47.084508 UTC] Performing policy update
[2018-01-21 16:01:47.085592 UTC] Computing gradient in Euclidean space
[2018-01-21 16:01:47.205691 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:01:48.615660 UTC] Performing line search
[2018-01-21 16:01:48.808561 UTC] Updating baseline
[2018-01-21 16:01:54.450202 UTC] Computing logging information
--------------------------------------
| Iteration            | 1337        |
| ExpectedImprovement  | 0.018718    |
| ActualImprovement    | 0.017483    |
| ImprovementRatio     | 0.93397     |
| MeanKL               | 0.0078701   |
| Entropy              | -1.5823     |
| Perplexity           | 0.2055      |
| AveragePolicyStd     | 0.18804     |
| AveragePolicyStd[0]  | 0.2032      |
| AveragePolicyStd[1]  | 0.20329     |
| AveragePolicyStd[2]  | 0.1493      |
| AveragePolicyStd[3]  | 0.18737     |
| AveragePolicyStd[4]  | 0.15541     |
| AveragePolicyStd[5]  | 0.22968     |
| AverageReturn        | 1789.6      |
| MinReturn            | 427         |
| MaxReturn            | 1919.2      |
| StdReturn            | 244.82      |
| AverageEpisodeLength | 974.31      |
| MinEpisodeLength     | 264         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 124.55      |
| TotalNEpisodes       | 23654       |
| TotalNSamples        | 6.6928e+06  |
| ExplainedVariance    | -0.00013558 |
--------------------------------------
[2018-01-21 16:01:55.313163 UTC] Saving snapshot
[2018-01-21 16:01:55.313404 UTC] Starting iteration 1338
[2018-01-21 16:01:55.313571 UTC] Start collecting samples
[2018-01-21 16:01:59.817890 UTC] Computing input variables for policy optimization
[2018-01-21 16:01:59.952316 UTC] Performing policy update
[2018-01-21 16:01:59.953198 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:00.070261 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:01.464339 UTC] Performing line search
[2018-01-21 16:02:01.667719 UTC] Updating baseline
[2018-01-21 16:02:03.931152 UTC] Computing logging information
--------------------------------------
| Iteration            | 1338        |
| ExpectedImprovement  | 0.016986    |
| ActualImprovement    | 0.015634    |
| ImprovementRatio     | 0.92041     |
| MeanKL               | 0.007997    |
| Entropy              | -1.5894     |
| Perplexity           | 0.20404     |
| AveragePolicyStd     | 0.18779     |
| AveragePolicyStd[0]  | 0.20293     |
| AveragePolicyStd[1]  | 0.20355     |
| AveragePolicyStd[2]  | 0.14935     |
| AveragePolicyStd[3]  | 0.18671     |
| AveragePolicyStd[4]  | 0.15545     |
| AveragePolicyStd[5]  | 0.22873     |
| AverageReturn        | 1792.1      |
| MinReturn            | 427         |
| MaxReturn            | 1919.2      |
| StdReturn            | 245.08      |
| AverageEpisodeLength | 974.31      |
| MinEpisodeLength     | 264         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 124.55      |
| TotalNEpisodes       | 23659       |
| TotalNSamples        | 6.6978e+06  |
| ExplainedVariance    | -3.5847e-10 |
--------------------------------------
[2018-01-21 16:02:04.723706 UTC] Saving snapshot
[2018-01-21 16:02:04.723987 UTC] Starting iteration 1339
[2018-01-21 16:02:04.724202 UTC] Start collecting samples
[2018-01-21 16:02:09.274638 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:09.401458 UTC] Performing policy update
[2018-01-21 16:02:09.402296 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:09.521138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:10.970329 UTC] Performing line search
[2018-01-21 16:02:11.164480 UTC] Updating baseline
[2018-01-21 16:02:13.393101 UTC] Computing logging information
-------------------------------------
| Iteration            | 1339       |
| ExpectedImprovement  | 0.016963   |
| ActualImprovement    | 0.015953   |
| ImprovementRatio     | 0.94048    |
| MeanKL               | 0.0081837  |
| Entropy              | -1.5915    |
| Perplexity           | 0.20362    |
| AveragePolicyStd     | 0.18773    |
| AveragePolicyStd[0]  | 0.20279    |
| AveragePolicyStd[1]  | 0.20388    |
| AveragePolicyStd[2]  | 0.14914    |
| AveragePolicyStd[3]  | 0.18626    |
| AveragePolicyStd[4]  | 0.15554    |
| AveragePolicyStd[5]  | 0.22879    |
| AverageReturn        | 1793.1     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 245.25     |
| AverageEpisodeLength | 974.31     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.55     |
| TotalNEpisodes       | 23663      |
| TotalNSamples        | 6.7018e+06 |
| ExplainedVariance    | -0.0002541 |
-------------------------------------
[2018-01-21 16:02:14.251186 UTC] Saving snapshot
[2018-01-21 16:02:14.251431 UTC] Starting iteration 1340
[2018-01-21 16:02:14.251612 UTC] Start collecting samples
[2018-01-21 16:02:18.962379 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:19.099419 UTC] Performing policy update
[2018-01-21 16:02:19.100068 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:19.215167 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:20.621343 UTC] Performing line search
[2018-01-21 16:02:20.825841 UTC] Updating baseline
[2018-01-21 16:02:24.370954 UTC] Computing logging information
-------------------------------------
| Iteration            | 1340       |
| ExpectedImprovement  | 0.020956   |
| ActualImprovement    | 0.019284   |
| ImprovementRatio     | 0.92022    |
| MeanKL               | 0.0078606  |
| Entropy              | -1.5957    |
| Perplexity           | 0.20278    |
| AveragePolicyStd     | 0.18759    |
| AveragePolicyStd[0]  | 0.20275    |
| AveragePolicyStd[1]  | 0.20344    |
| AveragePolicyStd[2]  | 0.14893    |
| AveragePolicyStd[3]  | 0.18615    |
| AveragePolicyStd[4]  | 0.15569    |
| AveragePolicyStd[5]  | 0.2286     |
| AverageReturn        | 1802.8     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 211.65     |
| AverageEpisodeLength | 979.27     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.32     |
| TotalNEpisodes       | 23670      |
| TotalNSamples        | 6.7087e+06 |
| ExplainedVariance    | 0.075733   |
-------------------------------------
[2018-01-21 16:02:25.163683 UTC] Saving snapshot
[2018-01-21 16:02:25.169726 UTC] Starting iteration 1341
[2018-01-21 16:02:25.169907 UTC] Start collecting samples
[2018-01-21 16:02:29.793596 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:29.918956 UTC] Performing policy update
[2018-01-21 16:02:29.919979 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:30.040581 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:31.416791 UTC] Performing line search
[2018-01-21 16:02:31.613543 UTC] Updating baseline
[2018-01-21 16:02:34.534496 UTC] Computing logging information
-------------------------------------
| Iteration            | 1341       |
| ExpectedImprovement  | 0.01918    |
| ActualImprovement    | 0.017934   |
| ImprovementRatio     | 0.93507    |
| MeanKL               | 0.0085773  |
| Entropy              | -1.5965    |
| Perplexity           | 0.2026     |
| AveragePolicyStd     | 0.18756    |
| AveragePolicyStd[0]  | 0.20272    |
| AveragePolicyStd[1]  | 0.20327    |
| AveragePolicyStd[2]  | 0.14905    |
| AveragePolicyStd[3]  | 0.18617    |
| AveragePolicyStd[4]  | 0.15567    |
| AveragePolicyStd[5]  | 0.22846    |
| AverageReturn        | 1804       |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 212        |
| AverageEpisodeLength | 979.27     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.32     |
| TotalNEpisodes       | 23674      |
| TotalNSamples        | 6.7127e+06 |
| ExplainedVariance    | -0.0071863 |
-------------------------------------
[2018-01-21 16:02:35.365948 UTC] Saving snapshot
[2018-01-21 16:02:35.366193 UTC] Starting iteration 1342
[2018-01-21 16:02:35.366414 UTC] Start collecting samples
[2018-01-21 16:02:39.931584 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:40.054589 UTC] Performing policy update
[2018-01-21 16:02:40.055213 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:40.180642 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:41.592007 UTC] Performing line search
[2018-01-21 16:02:41.780843 UTC] Updating baseline
[2018-01-21 16:02:44.443392 UTC] Computing logging information
-------------------------------------
| Iteration            | 1342       |
| ExpectedImprovement  | 0.017239   |
| ActualImprovement    | 0.016359   |
| ImprovementRatio     | 0.94895    |
| MeanKL               | 0.008279   |
| Entropy              | -1.597     |
| Perplexity           | 0.20251    |
| AveragePolicyStd     | 0.18755    |
| AveragePolicyStd[0]  | 0.20258    |
| AveragePolicyStd[1]  | 0.20331    |
| AveragePolicyStd[2]  | 0.14869    |
| AveragePolicyStd[3]  | 0.18646    |
| AveragePolicyStd[4]  | 0.1558     |
| AveragePolicyStd[5]  | 0.22846    |
| AverageReturn        | 1806.5     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 212.56     |
| AverageEpisodeLength | 979.27     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.32     |
| TotalNEpisodes       | 23679      |
| TotalNSamples        | 6.7177e+06 |
| ExplainedVariance    | 3.9092e-05 |
-------------------------------------
[2018-01-21 16:02:45.229705 UTC] Saving snapshot
[2018-01-21 16:02:45.229975 UTC] Starting iteration 1343
[2018-01-21 16:02:45.230157 UTC] Start collecting samples
[2018-01-21 16:02:49.985322 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:50.133766 UTC] Performing policy update
[2018-01-21 16:02:50.134967 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:50.256587 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:02:51.731605 UTC] Performing line search
[2018-01-21 16:02:51.923807 UTC] Updating baseline
[2018-01-21 16:02:53.945891 UTC] Computing logging information
-------------------------------------
| Iteration            | 1343       |
| ExpectedImprovement  | 0.017853   |
| ActualImprovement    | 0.016727   |
| ImprovementRatio     | 0.9369     |
| MeanKL               | 0.0077855  |
| Entropy              | -1.595     |
| Perplexity           | 0.2029     |
| AveragePolicyStd     | 0.18761    |
| AveragePolicyStd[0]  | 0.20281    |
| AveragePolicyStd[1]  | 0.20356    |
| AveragePolicyStd[2]  | 0.14847    |
| AveragePolicyStd[3]  | 0.18653    |
| AveragePolicyStd[4]  | 0.15606    |
| AveragePolicyStd[5]  | 0.22824    |
| AverageReturn        | 1809.7     |
| MinReturn            | 427        |
| MaxReturn            | 1919.2     |
| StdReturn            | 213.14     |
| AverageEpisodeLength | 979.27     |
| MinEpisodeLength     | 264        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.32     |
| TotalNEpisodes       | 23685      |
| TotalNSamples        | 6.7237e+06 |
| ExplainedVariance    | 0.00017415 |
-------------------------------------
[2018-01-21 16:02:54.857337 UTC] Saving snapshot
[2018-01-21 16:02:54.857595 UTC] Starting iteration 1344
[2018-01-21 16:02:54.857773 UTC] Start collecting samples
[2018-01-21 16:02:59.552401 UTC] Computing input variables for policy optimization
[2018-01-21 16:02:59.701767 UTC] Performing policy update
[2018-01-21 16:02:59.702377 UTC] Computing gradient in Euclidean space
[2018-01-21 16:02:59.810649 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:01.221164 UTC] Performing line search
[2018-01-21 16:03:01.420664 UTC] Updating baseline
[2018-01-21 16:03:03.727928 UTC] Computing logging information
-------------------------------------
| Iteration            | 1344       |
| ExpectedImprovement  | 0.019383   |
| ActualImprovement    | 0.017988   |
| ImprovementRatio     | 0.92806    |
| MeanKL               | 0.0078027  |
| Entropy              | -1.5997    |
| Perplexity           | 0.20196    |
| AveragePolicyStd     | 0.18746    |
| AveragePolicyStd[0]  | 0.20277    |
| AveragePolicyStd[1]  | 0.20348    |
| AveragePolicyStd[2]  | 0.14822    |
| AveragePolicyStd[3]  | 0.18666    |
| AveragePolicyStd[4]  | 0.15597    |
| AveragePolicyStd[5]  | 0.22768    |
| AverageReturn        | 1817.6     |
| MinReturn            | 458.25     |
| MaxReturn            | 1919.9     |
| StdReturn            | 176.48     |
| AverageEpisodeLength | 982.89     |
| MinEpisodeLength     | 290        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 87.382     |
| TotalNEpisodes       | 23690      |
| TotalNSamples        | 6.7283e+06 |
| ExplainedVariance    | 0.11209    |
-------------------------------------
[2018-01-21 16:03:04.502014 UTC] Saving snapshot
[2018-01-21 16:03:04.502260 UTC] Starting iteration 1345
[2018-01-21 16:03:04.502415 UTC] Start collecting samples
[2018-01-21 16:03:08.967611 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:09.119459 UTC] Performing policy update
[2018-01-21 16:03:09.120177 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:09.242220 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:10.663357 UTC] Performing line search
[2018-01-21 16:03:10.850786 UTC] Updating baseline
[2018-01-21 16:03:13.020940 UTC] Computing logging information
-------------------------------------
| Iteration            | 1345       |
| ExpectedImprovement  | 0.017489   |
| ActualImprovement    | 0.016591   |
| ImprovementRatio     | 0.94864    |
| MeanKL               | 0.0076104  |
| Entropy              | -1.6011    |
| Perplexity           | 0.20168    |
| AveragePolicyStd     | 0.18745    |
| AveragePolicyStd[0]  | 0.20254    |
| AveragePolicyStd[1]  | 0.2037     |
| AveragePolicyStd[2]  | 0.14774    |
| AveragePolicyStd[3]  | 0.18667    |
| AveragePolicyStd[4]  | 0.15604    |
| AveragePolicyStd[5]  | 0.228      |
| AverageReturn        | 1822.4     |
| MinReturn            | 458.25     |
| MaxReturn            | 1919.9     |
| StdReturn            | 167.88     |
| AverageEpisodeLength | 984.5      |
| MinEpisodeLength     | 290        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 83.265     |
| TotalNEpisodes       | 23695      |
| TotalNSamples        | 6.7331e+06 |
| ExplainedVariance    | 0.15363    |
-------------------------------------
[2018-01-21 16:03:13.898042 UTC] Saving snapshot
[2018-01-21 16:03:13.898272 UTC] Starting iteration 1346
[2018-01-21 16:03:13.898415 UTC] Start collecting samples
[2018-01-21 16:03:18.443748 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:18.577606 UTC] Performing policy update
[2018-01-21 16:03:18.578249 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:18.699499 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:20.101945 UTC] Performing line search
[2018-01-21 16:03:20.303964 UTC] Updating baseline
[2018-01-21 16:03:22.712380 UTC] Computing logging information
-------------------------------------
| Iteration            | 1346       |
| ExpectedImprovement  | 0.017886   |
| ActualImprovement    | 0.016442   |
| ImprovementRatio     | 0.91927    |
| MeanKL               | 0.0074915  |
| Entropy              | -1.6019    |
| Perplexity           | 0.20152    |
| AveragePolicyStd     | 0.18742    |
| AveragePolicyStd[0]  | 0.20241    |
| AveragePolicyStd[1]  | 0.20427    |
| AveragePolicyStd[2]  | 0.14775    |
| AveragePolicyStd[3]  | 0.18685    |
| AveragePolicyStd[4]  | 0.15582    |
| AveragePolicyStd[5]  | 0.2274     |
| AverageReturn        | 1824       |
| MinReturn            | 458.25     |
| MaxReturn            | 1919.9     |
| StdReturn            | 168.16     |
| AverageEpisodeLength | 984.5      |
| MinEpisodeLength     | 290        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 83.265     |
| TotalNEpisodes       | 23699      |
| TotalNSamples        | 6.7371e+06 |
| ExplainedVariance    | -0.0037921 |
-------------------------------------
[2018-01-21 16:03:23.458322 UTC] Saving snapshot
[2018-01-21 16:03:23.458860 UTC] Starting iteration 1347
[2018-01-21 16:03:23.459239 UTC] Start collecting samples
[2018-01-21 16:03:28.039027 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:28.163799 UTC] Performing policy update
[2018-01-21 16:03:28.165003 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:28.285360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:29.710037 UTC] Performing line search
[2018-01-21 16:03:29.908475 UTC] Updating baseline
[2018-01-21 16:03:32.505727 UTC] Computing logging information
-------------------------------------
| Iteration            | 1347       |
| ExpectedImprovement  | 0.020037   |
| ActualImprovement    | 0.018898   |
| ImprovementRatio     | 0.94317    |
| MeanKL               | 0.0074499  |
| Entropy              | -1.603     |
| Perplexity           | 0.2013     |
| AveragePolicyStd     | 0.18741    |
| AveragePolicyStd[0]  | 0.20271    |
| AveragePolicyStd[1]  | 0.2047     |
| AveragePolicyStd[2]  | 0.14765    |
| AveragePolicyStd[3]  | 0.18671    |
| AveragePolicyStd[4]  | 0.1554     |
| AveragePolicyStd[5]  | 0.22727    |
| AverageReturn        | 1816.1     |
| MinReturn            | 458.25     |
| MaxReturn            | 1919.9     |
| StdReturn            | 181.84     |
| AverageEpisodeLength | 979.64     |
| MinEpisodeLength     | 290        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.768     |
| TotalNEpisodes       | 23705      |
| TotalNSamples        | 6.7426e+06 |
| ExplainedVariance    | 0.23974    |
-------------------------------------
[2018-01-21 16:03:33.371867 UTC] Saving snapshot
[2018-01-21 16:03:33.372095 UTC] Starting iteration 1348
[2018-01-21 16:03:33.372239 UTC] Start collecting samples
[2018-01-21 16:03:38.020180 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:38.168385 UTC] Performing policy update
[2018-01-21 16:03:38.169633 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:38.291509 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:39.721194 UTC] Performing line search
[2018-01-21 16:03:39.922591 UTC] Updating baseline
[2018-01-21 16:03:41.733305 UTC] Computing logging information
-------------------------------------
| Iteration            | 1348       |
| ExpectedImprovement  | 0.01767    |
| ActualImprovement    | 0.017108   |
| ImprovementRatio     | 0.96823    |
| MeanKL               | 0.008052   |
| Entropy              | -1.6062    |
| Perplexity           | 0.20065    |
| AveragePolicyStd     | 0.18729    |
| AveragePolicyStd[0]  | 0.20258    |
| AveragePolicyStd[1]  | 0.20409    |
| AveragePolicyStd[2]  | 0.1476     |
| AveragePolicyStd[3]  | 0.18715    |
| AveragePolicyStd[4]  | 0.1554     |
| AveragePolicyStd[5]  | 0.22692    |
| AverageReturn        | 1818.6     |
| MinReturn            | 458.25     |
| MaxReturn            | 1919.9     |
| StdReturn            | 180.25     |
| AverageEpisodeLength | 980.98     |
| MinEpisodeLength     | 290        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 89.059     |
| TotalNEpisodes       | 23710      |
| TotalNSamples        | 6.7476e+06 |
| ExplainedVariance    | -0.088472  |
-------------------------------------
[2018-01-21 16:03:42.622269 UTC] Saving snapshot
[2018-01-21 16:03:42.622574 UTC] Starting iteration 1349
[2018-01-21 16:03:42.622782 UTC] Start collecting samples
[2018-01-21 16:03:47.176224 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:47.330016 UTC] Performing policy update
[2018-01-21 16:03:47.330714 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:47.458289 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:48.890911 UTC] Performing line search
[2018-01-21 16:03:49.080354 UTC] Updating baseline
[2018-01-21 16:03:51.102960 UTC] Computing logging information
-------------------------------------
| Iteration            | 1349       |
| ExpectedImprovement  | 0.018947   |
| ActualImprovement    | 0.017975   |
| ImprovementRatio     | 0.9487     |
| MeanKL               | 0.0082213  |
| Entropy              | -1.6074    |
| Perplexity           | 0.2004     |
| AveragePolicyStd     | 0.18724    |
| AveragePolicyStd[0]  | 0.20247    |
| AveragePolicyStd[1]  | 0.20369    |
| AveragePolicyStd[2]  | 0.14769    |
| AveragePolicyStd[3]  | 0.18723    |
| AveragePolicyStd[4]  | 0.15545    |
| AveragePolicyStd[5]  | 0.2269     |
| AverageReturn        | 1799       |
| MinReturn            | 328.47     |
| MaxReturn            | 1919.9     |
| StdReturn            | 242.1      |
| AverageEpisodeLength | 969.7      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.05     |
| TotalNEpisodes       | 23717      |
| TotalNSamples        | 6.7535e+06 |
| ExplainedVariance    | 0.15737    |
-------------------------------------
[2018-01-21 16:03:51.964299 UTC] Saving snapshot
[2018-01-21 16:03:51.964570 UTC] Starting iteration 1350
[2018-01-21 16:03:51.964741 UTC] Start collecting samples
[2018-01-21 16:03:56.334878 UTC] Computing input variables for policy optimization
[2018-01-21 16:03:56.459915 UTC] Performing policy update
[2018-01-21 16:03:56.460788 UTC] Computing gradient in Euclidean space
[2018-01-21 16:03:56.579524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:03:58.009071 UTC] Performing line search
[2018-01-21 16:03:58.238487 UTC] Updating baseline
[2018-01-21 16:04:00.311508 UTC] Computing logging information
-------------------------------------
| Iteration            | 1350       |
| ExpectedImprovement  | 0.018204   |
| ActualImprovement    | 0.016985   |
| ImprovementRatio     | 0.93306    |
| MeanKL               | 0.0090484  |
| Entropy              | -1.6135    |
| Perplexity           | 0.1992     |
| AveragePolicyStd     | 0.18707    |
| AveragePolicyStd[0]  | 0.20242    |
| AveragePolicyStd[1]  | 0.20376    |
| AveragePolicyStd[2]  | 0.14758    |
| AveragePolicyStd[3]  | 0.18687    |
| AveragePolicyStd[4]  | 0.15501    |
| AveragePolicyStd[5]  | 0.22675    |
| AverageReturn        | 1800.3     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 242.46     |
| AverageEpisodeLength | 969.7      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.05     |
| TotalNEpisodes       | 23720      |
| TotalNSamples        | 6.7565e+06 |
| ExplainedVariance    | -0.036234  |
-------------------------------------
[2018-01-21 16:04:01.041367 UTC] Saving snapshot
[2018-01-21 16:04:01.050700 UTC] Starting iteration 1351
[2018-01-21 16:04:01.050928 UTC] Start collecting samples
[2018-01-21 16:04:05.580132 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:05.719418 UTC] Performing policy update
[2018-01-21 16:04:05.720071 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:05.843700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:07.279887 UTC] Performing line search
[2018-01-21 16:04:07.472124 UTC] Updating baseline
[2018-01-21 16:04:09.297623 UTC] Computing logging information
-------------------------------------
| Iteration            | 1351       |
| ExpectedImprovement  | 0.018315   |
| ActualImprovement    | 0.01711    |
| ImprovementRatio     | 0.93424    |
| MeanKL               | 0.0087469  |
| Entropy              | -1.6167    |
| Perplexity           | 0.19855    |
| AveragePolicyStd     | 0.18698    |
| AveragePolicyStd[0]  | 0.2024     |
| AveragePolicyStd[1]  | 0.2037     |
| AveragePolicyStd[2]  | 0.14738    |
| AveragePolicyStd[3]  | 0.1859     |
| AveragePolicyStd[4]  | 0.15521    |
| AveragePolicyStd[5]  | 0.2273     |
| AverageReturn        | 1789.2     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 277.57     |
| AverageEpisodeLength | 962.5      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.1      |
| TotalNEpisodes       | 23727      |
| TotalNSamples        | 6.7628e+06 |
| ExplainedVariance    | 0.088179   |
-------------------------------------
[2018-01-21 16:04:10.149465 UTC] Saving snapshot
[2018-01-21 16:04:10.149641 UTC] Starting iteration 1352
[2018-01-21 16:04:10.149743 UTC] Start collecting samples
[2018-01-21 16:04:14.596430 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:14.726271 UTC] Performing policy update
[2018-01-21 16:04:14.726896 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:14.845682 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:16.294292 UTC] Performing line search
[2018-01-21 16:04:16.541686 UTC] Updating baseline
[2018-01-21 16:04:18.603587 UTC] Computing logging information
-------------------------------------
| Iteration            | 1352       |
| ExpectedImprovement  | 0.019262   |
| ActualImprovement    | 0.017663   |
| ImprovementRatio     | 0.91696    |
| MeanKL               | 0.0074858  |
| Entropy              | -1.6166    |
| Perplexity           | 0.19857    |
| AveragePolicyStd     | 0.18698    |
| AveragePolicyStd[0]  | 0.20246    |
| AveragePolicyStd[1]  | 0.20347    |
| AveragePolicyStd[2]  | 0.14741    |
| AveragePolicyStd[3]  | 0.18579    |
| AveragePolicyStd[4]  | 0.1554     |
| AveragePolicyStd[5]  | 0.22733    |
| AverageReturn        | 1800.3     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 243.82     |
| AverageEpisodeLength | 967.98     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.82     |
| TotalNEpisodes       | 23733      |
| TotalNSamples        | 6.7686e+06 |
| ExplainedVariance    | 0.16288    |
-------------------------------------
[2018-01-21 16:04:19.463032 UTC] Saving snapshot
[2018-01-21 16:04:19.463269 UTC] Starting iteration 1353
[2018-01-21 16:04:19.463417 UTC] Start collecting samples
[2018-01-21 16:04:23.835651 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:23.967110 UTC] Performing policy update
[2018-01-21 16:04:23.967771 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:24.087393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:25.518903 UTC] Performing line search
[2018-01-21 16:04:25.710261 UTC] Updating baseline
[2018-01-21 16:04:28.011375 UTC] Computing logging information
-------------------------------------
| Iteration            | 1353       |
| ExpectedImprovement  | 0.017795   |
| ActualImprovement    | 0.016521   |
| ImprovementRatio     | 0.92841    |
| MeanKL               | 0.0075577  |
| Entropy              | -1.6168    |
| Perplexity           | 0.19853    |
| AveragePolicyStd     | 0.18696    |
| AveragePolicyStd[0]  | 0.20207    |
| AveragePolicyStd[1]  | 0.20317    |
| AveragePolicyStd[2]  | 0.14758    |
| AveragePolicyStd[3]  | 0.18617    |
| AveragePolicyStd[4]  | 0.15535    |
| AveragePolicyStd[5]  | 0.22739    |
| AverageReturn        | 1796.6     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 245.74     |
| AverageEpisodeLength | 966.27     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.56     |
| TotalNEpisodes       | 23737      |
| TotalNSamples        | 6.7725e+06 |
| ExplainedVariance    | 0.19152    |
-------------------------------------
[2018-01-21 16:04:28.794022 UTC] Saving snapshot
[2018-01-21 16:04:28.794342 UTC] Starting iteration 1354
[2018-01-21 16:04:28.794555 UTC] Start collecting samples
[2018-01-21 16:04:33.281014 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:33.419941 UTC] Performing policy update
[2018-01-21 16:04:33.420844 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:33.541833 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:34.965289 UTC] Performing line search
[2018-01-21 16:04:35.158651 UTC] Updating baseline
[2018-01-21 16:04:37.373084 UTC] Computing logging information
------------------------------------
| Iteration            | 1354      |
| ExpectedImprovement  | 0.017682  |
| ActualImprovement    | 0.016483  |
| ImprovementRatio     | 0.93218   |
| MeanKL               | 0.0081126 |
| Entropy              | -1.6155   |
| Perplexity           | 0.19879   |
| AveragePolicyStd     | 0.18697   |
| AveragePolicyStd[0]  | 0.20214   |
| AveragePolicyStd[1]  | 0.20356   |
| AveragePolicyStd[2]  | 0.14779   |
| AveragePolicyStd[3]  | 0.18658   |
| AveragePolicyStd[4]  | 0.15528   |
| AveragePolicyStd[5]  | 0.22647   |
| AverageReturn        | 1787.8    |
| MinReturn            | 328.47    |
| MaxReturn            | 1923.1    |
| StdReturn            | 257.32    |
| AverageEpisodeLength | 961.84    |
| MinEpisodeLength     | 222       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 129.09    |
| TotalNEpisodes       | 23743     |
| TotalNSamples        | 6.778e+06 |
| ExplainedVariance    | 0.093527  |
------------------------------------
[2018-01-21 16:04:38.242199 UTC] Saving snapshot
[2018-01-21 16:04:38.242442 UTC] Starting iteration 1355
[2018-01-21 16:04:38.242591 UTC] Start collecting samples
[2018-01-21 16:04:42.601894 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:42.744031 UTC] Performing policy update
[2018-01-21 16:04:42.745127 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:42.863581 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:44.261397 UTC] Performing line search
[2018-01-21 16:04:44.444250 UTC] Updating baseline
[2018-01-21 16:04:46.582418 UTC] Computing logging information
-------------------------------------
| Iteration            | 1355       |
| ExpectedImprovement  | 0.015993   |
| ActualImprovement    | 0.015385   |
| ImprovementRatio     | 0.96199    |
| MeanKL               | 0.0079993  |
| Entropy              | -1.6164    |
| Perplexity           | 0.19862    |
| AveragePolicyStd     | 0.18693    |
| AveragePolicyStd[0]  | 0.20252    |
| AveragePolicyStd[1]  | 0.20363    |
| AveragePolicyStd[2]  | 0.14768    |
| AveragePolicyStd[3]  | 0.18663    |
| AveragePolicyStd[4]  | 0.15538    |
| AveragePolicyStd[5]  | 0.22571    |
| AverageReturn        | 1774.5     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 287.96     |
| AverageEpisodeLength | 954.8      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.03     |
| TotalNEpisodes       | 23749      |
| TotalNSamples        | 6.7833e+06 |
| ExplainedVariance    | 0.097639   |
-------------------------------------
[2018-01-21 16:04:47.388677 UTC] Saving snapshot
[2018-01-21 16:04:47.388951 UTC] Starting iteration 1356
[2018-01-21 16:04:47.389121 UTC] Start collecting samples
[2018-01-21 16:04:51.784630 UTC] Computing input variables for policy optimization
[2018-01-21 16:04:51.907493 UTC] Performing policy update
[2018-01-21 16:04:51.908715 UTC] Computing gradient in Euclidean space
[2018-01-21 16:04:52.027981 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:04:53.423898 UTC] Performing line search
[2018-01-21 16:04:53.609779 UTC] Updating baseline
[2018-01-21 16:04:55.855149 UTC] Computing logging information
-------------------------------------
| Iteration            | 1356       |
| ExpectedImprovement  | 0.017024   |
| ActualImprovement    | 0.016669   |
| ImprovementRatio     | 0.97915    |
| MeanKL               | 0.0080566  |
| Entropy              | -1.6117    |
| Perplexity           | 0.19954    |
| AveragePolicyStd     | 0.1871     |
| AveragePolicyStd[0]  | 0.20303    |
| AveragePolicyStd[1]  | 0.20421    |
| AveragePolicyStd[2]  | 0.1478     |
| AveragePolicyStd[3]  | 0.18615    |
| AveragePolicyStd[4]  | 0.15531    |
| AveragePolicyStd[5]  | 0.22608    |
| AverageReturn        | 1774.8     |
| MinReturn            | 328.47     |
| MaxReturn            | 1923.1     |
| StdReturn            | 288.05     |
| AverageEpisodeLength | 954.8      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.03     |
| TotalNEpisodes       | 23753      |
| TotalNSamples        | 6.7873e+06 |
| ExplainedVariance    | -0.05253   |
-------------------------------------
[2018-01-21 16:04:56.726318 UTC] Saving snapshot
[2018-01-21 16:04:56.726630 UTC] Starting iteration 1357
[2018-01-21 16:04:56.726835 UTC] Start collecting samples
[2018-01-21 16:05:01.159357 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:01.303022 UTC] Performing policy update
[2018-01-21 16:05:01.304351 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:01.434125 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:02.831329 UTC] Performing line search
[2018-01-21 16:05:03.026042 UTC] Updating baseline
[2018-01-21 16:05:05.589930 UTC] Computing logging information
--------------------------------------
| Iteration            | 1357        |
| ExpectedImprovement  | 0.019154    |
| ActualImprovement    | 0.01768     |
| ImprovementRatio     | 0.92303     |
| MeanKL               | 0.0079401   |
| Entropy              | -1.6142     |
| Perplexity           | 0.19904     |
| AveragePolicyStd     | 0.18699     |
| AveragePolicyStd[0]  | 0.20326     |
| AveragePolicyStd[1]  | 0.20423     |
| AveragePolicyStd[2]  | 0.14787     |
| AveragePolicyStd[3]  | 0.18638     |
| AveragePolicyStd[4]  | 0.15518     |
| AveragePolicyStd[5]  | 0.22502     |
| AverageReturn        | 1775.8      |
| MinReturn            | 328.47      |
| MaxReturn            | 1924        |
| StdReturn            | 288.51      |
| AverageEpisodeLength | 954.8       |
| MinEpisodeLength     | 222         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 145.03      |
| TotalNEpisodes       | 23759       |
| TotalNSamples        | 6.7933e+06  |
| ExplainedVariance    | -0.00063491 |
--------------------------------------
[2018-01-21 16:05:06.392953 UTC] Saving snapshot
[2018-01-21 16:05:06.393183 UTC] Starting iteration 1358
[2018-01-21 16:05:06.393318 UTC] Start collecting samples
[2018-01-21 16:05:10.910779 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:11.034320 UTC] Performing policy update
[2018-01-21 16:05:11.034971 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:11.153800 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:12.537612 UTC] Performing line search
[2018-01-21 16:05:12.729794 UTC] Updating baseline
[2018-01-21 16:05:15.556187 UTC] Computing logging information
--------------------------------------
| Iteration            | 1358        |
| ExpectedImprovement  | 0.018238    |
| ActualImprovement    | 0.017128    |
| ImprovementRatio     | 0.93913     |
| MeanKL               | 0.0076286   |
| Entropy              | -1.6191     |
| Perplexity           | 0.19808     |
| AveragePolicyStd     | 0.18684     |
| AveragePolicyStd[0]  | 0.20325     |
| AveragePolicyStd[1]  | 0.20408     |
| AveragePolicyStd[2]  | 0.14768     |
| AveragePolicyStd[3]  | 0.1866      |
| AveragePolicyStd[4]  | 0.15492     |
| AveragePolicyStd[5]  | 0.22453     |
| AverageReturn        | 1779.6      |
| MinReturn            | 328.47      |
| MaxReturn            | 1924        |
| StdReturn            | 287.83      |
| AverageEpisodeLength | 956.58      |
| MinEpisodeLength     | 222         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 144.48      |
| TotalNEpisodes       | 23765       |
| TotalNSamples        | 6.7993e+06  |
| ExplainedVariance    | -0.00040095 |
--------------------------------------
[2018-01-21 16:05:16.360881 UTC] Saving snapshot
[2018-01-21 16:05:16.361212 UTC] Starting iteration 1359
[2018-01-21 16:05:16.361462 UTC] Start collecting samples
[2018-01-21 16:05:20.769291 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:20.904953 UTC] Performing policy update
[2018-01-21 16:05:20.905778 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:21.042196 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:22.463318 UTC] Performing line search
[2018-01-21 16:05:22.660420 UTC] Updating baseline
[2018-01-21 16:05:24.530311 UTC] Computing logging information
------------------------------------
| Iteration            | 1359      |
| ExpectedImprovement  | 0.016994  |
| ActualImprovement    | 0.016121  |
| ImprovementRatio     | 0.94865   |
| MeanKL               | 0.0073524 |
| Entropy              | -1.6208   |
| Perplexity           | 0.19774   |
| AveragePolicyStd     | 0.1868    |
| AveragePolicyStd[0]  | 0.20345   |
| AveragePolicyStd[1]  | 0.20404   |
| AveragePolicyStd[2]  | 0.1474    |
| AveragePolicyStd[3]  | 0.18648   |
| AveragePolicyStd[4]  | 0.15496   |
| AveragePolicyStd[5]  | 0.22448   |
| AverageReturn        | 1755      |
| MinReturn            | 328.47    |
| MaxReturn            | 1924      |
| StdReturn            | 332.54    |
| AverageEpisodeLength | 943.84    |
| MinEpisodeLength     | 222       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 166.81    |
| TotalNEpisodes       | 23770     |
| TotalNSamples        | 6.803e+06 |
| ExplainedVariance    | 0.22241   |
------------------------------------
[2018-01-21 16:05:25.329511 UTC] Saving snapshot
[2018-01-21 16:05:25.329800 UTC] Starting iteration 1360
[2018-01-21 16:05:25.330000 UTC] Start collecting samples
[2018-01-21 16:05:29.930179 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:30.065588 UTC] Performing policy update
[2018-01-21 16:05:30.066185 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:30.190792 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:31.590129 UTC] Performing line search
[2018-01-21 16:05:31.782583 UTC] Updating baseline
[2018-01-21 16:05:33.666768 UTC] Computing logging information
-------------------------------------
| Iteration            | 1360       |
| ExpectedImprovement  | 0.018932   |
| ActualImprovement    | 0.017966   |
| ImprovementRatio     | 0.949      |
| MeanKL               | 0.009265   |
| Entropy              | -1.6152    |
| Perplexity           | 0.19885    |
| AveragePolicyStd     | 0.18696    |
| AveragePolicyStd[0]  | 0.20344    |
| AveragePolicyStd[1]  | 0.20429    |
| AveragePolicyStd[2]  | 0.1478     |
| AveragePolicyStd[3]  | 0.18673    |
| AveragePolicyStd[4]  | 0.15498    |
| AveragePolicyStd[5]  | 0.22452    |
| AverageReturn        | 1752.1     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924       |
| StdReturn            | 331.92     |
| AverageEpisodeLength | 943.84     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.81     |
| TotalNEpisodes       | 23776      |
| TotalNSamples        | 6.809e+06  |
| ExplainedVariance    | -0.0043775 |
-------------------------------------
[2018-01-21 16:05:34.476843 UTC] Saving snapshot
[2018-01-21 16:05:34.486278 UTC] Starting iteration 1361
[2018-01-21 16:05:34.486542 UTC] Start collecting samples
[2018-01-21 16:05:38.924396 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:39.074785 UTC] Performing policy update
[2018-01-21 16:05:39.075433 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:39.204530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:40.581064 UTC] Performing line search
[2018-01-21 16:05:40.766454 UTC] Updating baseline
[2018-01-21 16:05:43.133380 UTC] Computing logging information
--------------------------------------
| Iteration            | 1361        |
| ExpectedImprovement  | 0.018024    |
| ActualImprovement    | 0.016856    |
| ImprovementRatio     | 0.93519     |
| MeanKL               | 0.0085363   |
| Entropy              | -1.617      |
| Perplexity           | 0.1985      |
| AveragePolicyStd     | 0.18693     |
| AveragePolicyStd[0]  | 0.20369     |
| AveragePolicyStd[1]  | 0.2043      |
| AveragePolicyStd[2]  | 0.14766     |
| AveragePolicyStd[3]  | 0.18652     |
| AveragePolicyStd[4]  | 0.15473     |
| AveragePolicyStd[5]  | 0.22467     |
| AverageReturn        | 1751.4      |
| MinReturn            | 328.47      |
| MaxReturn            | 1924        |
| StdReturn            | 331.64      |
| AverageEpisodeLength | 943.84      |
| MinEpisodeLength     | 222         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 166.81      |
| TotalNEpisodes       | 23780       |
| TotalNSamples        | 6.813e+06   |
| ExplainedVariance    | -0.00024578 |
--------------------------------------
[2018-01-21 16:05:43.945685 UTC] Saving snapshot
[2018-01-21 16:05:43.945916 UTC] Starting iteration 1362
[2018-01-21 16:05:43.946098 UTC] Start collecting samples
[2018-01-21 16:05:48.403611 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:48.550520 UTC] Performing policy update
[2018-01-21 16:05:48.551182 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:48.674751 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:05:50.099495 UTC] Performing line search
[2018-01-21 16:05:50.283758 UTC] Updating baseline
[2018-01-21 16:05:53.740366 UTC] Computing logging information
------------------------------------
| Iteration            | 1362      |
| ExpectedImprovement  | 0.018509  |
| ActualImprovement    | 0.017573  |
| ImprovementRatio     | 0.94945   |
| MeanKL               | 0.0080384 |
| Entropy              | -1.6118   |
| Perplexity           | 0.19953   |
| AveragePolicyStd     | 0.1871    |
| AveragePolicyStd[0]  | 0.20395   |
| AveragePolicyStd[1]  | 0.20446   |
| AveragePolicyStd[2]  | 0.14759   |
| AveragePolicyStd[3]  | 0.18703   |
| AveragePolicyStd[4]  | 0.15479   |
| AveragePolicyStd[5]  | 0.22478   |
| AverageReturn        | 1747.6    |
| MinReturn            | 328.47    |
| MaxReturn            | 1924      |
| StdReturn            | 330.44    |
| AverageEpisodeLength | 943.84    |
| MinEpisodeLength     | 222       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 166.81    |
| TotalNEpisodes       | 23785     |
| TotalNSamples        | 6.818e+06 |
| ExplainedVariance    | 0.075513  |
------------------------------------
[2018-01-21 16:05:54.577640 UTC] Saving snapshot
[2018-01-21 16:05:54.577880 UTC] Starting iteration 1363
[2018-01-21 16:05:54.578057 UTC] Start collecting samples
[2018-01-21 16:05:59.082649 UTC] Computing input variables for policy optimization
[2018-01-21 16:05:59.201349 UTC] Performing policy update
[2018-01-21 16:05:59.202134 UTC] Computing gradient in Euclidean space
[2018-01-21 16:05:59.341923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:00.763473 UTC] Performing line search
[2018-01-21 16:06:00.954774 UTC] Updating baseline
[2018-01-21 16:06:03.108901 UTC] Computing logging information
-------------------------------------
| Iteration            | 1363       |
| ExpectedImprovement  | 0.018584   |
| ActualImprovement    | 0.017826   |
| ImprovementRatio     | 0.95921    |
| MeanKL               | 0.0081752  |
| Entropy              | -1.6095    |
| Perplexity           | 0.19999    |
| AveragePolicyStd     | 0.18716    |
| AveragePolicyStd[0]  | 0.20424    |
| AveragePolicyStd[1]  | 0.20445    |
| AveragePolicyStd[2]  | 0.14762    |
| AveragePolicyStd[3]  | 0.18677    |
| AveragePolicyStd[4]  | 0.15509    |
| AveragePolicyStd[5]  | 0.22482    |
| AverageReturn        | 1739.8     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924       |
| StdReturn            | 333.79     |
| AverageEpisodeLength | 941        |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.92     |
| TotalNEpisodes       | 23791      |
| TotalNSamples        | 6.8234e+06 |
| ExplainedVariance    | 0.117      |
-------------------------------------
[2018-01-21 16:06:03.900844 UTC] Saving snapshot
[2018-01-21 16:06:03.901130 UTC] Starting iteration 1364
[2018-01-21 16:06:03.901310 UTC] Start collecting samples
[2018-01-21 16:06:08.106903 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:08.242652 UTC] Performing policy update
[2018-01-21 16:06:08.243801 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:08.368215 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:09.805131 UTC] Performing line search
[2018-01-21 16:06:10.010343 UTC] Updating baseline
[2018-01-21 16:06:13.037175 UTC] Computing logging information
-------------------------------------
| Iteration            | 1364       |
| ExpectedImprovement  | 0.017447   |
| ActualImprovement    | 0.016527   |
| ImprovementRatio     | 0.9473     |
| MeanKL               | 0.0081285  |
| Entropy              | -1.6127    |
| Perplexity           | 0.19936    |
| AveragePolicyStd     | 0.18707    |
| AveragePolicyStd[0]  | 0.20425    |
| AveragePolicyStd[1]  | 0.20411    |
| AveragePolicyStd[2]  | 0.14748    |
| AveragePolicyStd[3]  | 0.1868     |
| AveragePolicyStd[4]  | 0.15494    |
| AveragePolicyStd[5]  | 0.22486    |
| AverageReturn        | 1739.6     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924       |
| StdReturn            | 333.72     |
| AverageEpisodeLength | 941        |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.92     |
| TotalNEpisodes       | 23794      |
| TotalNSamples        | 6.8264e+06 |
| ExplainedVariance    | -0.01215   |
-------------------------------------
[2018-01-21 16:06:13.937427 UTC] Saving snapshot
[2018-01-21 16:06:13.937715 UTC] Starting iteration 1365
[2018-01-21 16:06:13.937929 UTC] Start collecting samples
[2018-01-21 16:06:18.481786 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:18.617390 UTC] Performing policy update
[2018-01-21 16:06:18.618082 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:18.739649 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:20.166924 UTC] Performing line search
[2018-01-21 16:06:20.369046 UTC] Updating baseline
[2018-01-21 16:06:22.067624 UTC] Computing logging information
-------------------------------------
| Iteration            | 1365       |
| ExpectedImprovement  | 0.01721    |
| ActualImprovement    | 0.016517   |
| ImprovementRatio     | 0.95971    |
| MeanKL               | 0.008234   |
| Entropy              | -1.6193    |
| Perplexity           | 0.19804    |
| AveragePolicyStd     | 0.18687    |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.20427    |
| AveragePolicyStd[2]  | 0.14721    |
| AveragePolicyStd[3]  | 0.18654    |
| AveragePolicyStd[4]  | 0.15485    |
| AveragePolicyStd[5]  | 0.22435    |
| AverageReturn        | 1743.8     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924       |
| StdReturn            | 333.59     |
| AverageEpisodeLength | 942.54     |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.75     |
| TotalNEpisodes       | 23800      |
| TotalNSamples        | 6.8324e+06 |
| ExplainedVariance    | 0.00068657 |
-------------------------------------
[2018-01-21 16:06:22.946511 UTC] Saving snapshot
[2018-01-21 16:06:22.947106 UTC] Starting iteration 1366
[2018-01-21 16:06:22.947588 UTC] Start collecting samples
[2018-01-21 16:06:27.490875 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:27.645951 UTC] Performing policy update
[2018-01-21 16:06:27.647150 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:27.767538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:29.222017 UTC] Performing line search
[2018-01-21 16:06:29.426706 UTC] Updating baseline
[2018-01-21 16:06:31.225230 UTC] Computing logging information
-------------------------------------
| Iteration            | 1366       |
| ExpectedImprovement  | 0.02033    |
| ActualImprovement    | 0.018219   |
| ImprovementRatio     | 0.89617    |
| MeanKL               | 0.0080602  |
| Entropy              | -1.6244    |
| Perplexity           | 0.19703    |
| AveragePolicyStd     | 0.18669    |
| AveragePolicyStd[0]  | 0.20405    |
| AveragePolicyStd[1]  | 0.20439    |
| AveragePolicyStd[2]  | 0.14728    |
| AveragePolicyStd[3]  | 0.18612    |
| AveragePolicyStd[4]  | 0.15469    |
| AveragePolicyStd[5]  | 0.22362    |
| AverageReturn        | 1754.9     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924.8     |
| StdReturn            | 329.12     |
| AverageEpisodeLength | 947.4      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.47     |
| TotalNEpisodes       | 23805      |
| TotalNSamples        | 6.8374e+06 |
| ExplainedVariance    | 0.0050218  |
-------------------------------------
[2018-01-21 16:06:32.053648 UTC] Saving snapshot
[2018-01-21 16:06:32.053888 UTC] Starting iteration 1367
[2018-01-21 16:06:32.054034 UTC] Start collecting samples
[2018-01-21 16:06:36.749393 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:36.888821 UTC] Performing policy update
[2018-01-21 16:06:36.889975 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:37.017836 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:38.481718 UTC] Performing line search
[2018-01-21 16:06:38.694656 UTC] Updating baseline
[2018-01-21 16:06:42.272764 UTC] Computing logging information
-------------------------------------
| Iteration            | 1367       |
| ExpectedImprovement  | 0.016775   |
| ActualImprovement    | 0.016316   |
| ImprovementRatio     | 0.97263    |
| MeanKL               | 0.0080917  |
| Entropy              | -1.6284    |
| Perplexity           | 0.19625    |
| AveragePolicyStd     | 0.18657    |
| AveragePolicyStd[0]  | 0.20373    |
| AveragePolicyStd[1]  | 0.20432    |
| AveragePolicyStd[2]  | 0.14704    |
| AveragePolicyStd[3]  | 0.18579    |
| AveragePolicyStd[4]  | 0.15481    |
| AveragePolicyStd[5]  | 0.22376    |
| AverageReturn        | 1755.8     |
| MinReturn            | 328.47     |
| MaxReturn            | 1924.8     |
| StdReturn            | 329.39     |
| AverageEpisodeLength | 947.4      |
| MinEpisodeLength     | 222        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.47     |
| TotalNEpisodes       | 23810      |
| TotalNSamples        | 6.8424e+06 |
| ExplainedVariance    | -0.015365  |
-------------------------------------
[2018-01-21 16:06:43.107350 UTC] Saving snapshot
[2018-01-21 16:06:43.107605 UTC] Starting iteration 1368
[2018-01-21 16:06:43.107876 UTC] Start collecting samples
[2018-01-21 16:06:47.700787 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:47.842506 UTC] Performing policy update
[2018-01-21 16:06:47.843547 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:47.977406 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:49.424735 UTC] Performing line search
[2018-01-21 16:06:49.619845 UTC] Updating baseline
[2018-01-21 16:06:51.664109 UTC] Computing logging information
------------------------------------
| Iteration            | 1368      |
| ExpectedImprovement  | 0.018763  |
| ActualImprovement    | 0.017782  |
| ImprovementRatio     | 0.94772   |
| MeanKL               | 0.00747   |
| Entropy              | -1.6242   |
| Perplexity           | 0.19706   |
| AveragePolicyStd     | 0.18672   |
| AveragePolicyStd[0]  | 0.20389   |
| AveragePolicyStd[1]  | 0.20439   |
| AveragePolicyStd[2]  | 0.14713   |
| AveragePolicyStd[3]  | 0.18583   |
| AveragePolicyStd[4]  | 0.15473   |
| AveragePolicyStd[5]  | 0.22436   |
| AverageReturn        | 1772      |
| MinReturn            | 443.29    |
| MaxReturn            | 1924.8    |
| StdReturn            | 293.53    |
| AverageEpisodeLength | 954.48    |
| MinEpisodeLength     | 280       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 148.15    |
| TotalNEpisodes       | 23816     |
| TotalNSamples        | 6.848e+06 |
| ExplainedVariance    | 0.22949   |
------------------------------------
[2018-01-21 16:06:52.446589 UTC] Saving snapshot
[2018-01-21 16:06:52.446847 UTC] Starting iteration 1369
[2018-01-21 16:06:52.447029 UTC] Start collecting samples
[2018-01-21 16:06:56.999481 UTC] Computing input variables for policy optimization
[2018-01-21 16:06:57.119865 UTC] Performing policy update
[2018-01-21 16:06:57.120799 UTC] Computing gradient in Euclidean space
[2018-01-21 16:06:57.237359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:06:58.647657 UTC] Performing line search
[2018-01-21 16:06:58.844690 UTC] Updating baseline
[2018-01-21 16:07:00.742961 UTC] Computing logging information
-------------------------------------
| Iteration            | 1369       |
| ExpectedImprovement  | 0.017082   |
| ActualImprovement    | 0.016622   |
| ImprovementRatio     | 0.97308    |
| MeanKL               | 0.0079453  |
| Entropy              | -1.6268    |
| Perplexity           | 0.19656    |
| AveragePolicyStd     | 0.18665    |
| AveragePolicyStd[0]  | 0.20363    |
| AveragePolicyStd[1]  | 0.20448    |
| AveragePolicyStd[2]  | 0.147      |
| AveragePolicyStd[3]  | 0.18583    |
| AveragePolicyStd[4]  | 0.15464    |
| AveragePolicyStd[5]  | 0.2243     |
| AverageReturn        | 1749.4     |
| MinReturn            | 417.34     |
| MaxReturn            | 1924.8     |
| StdReturn            | 328.92     |
| AverageEpisodeLength | 943.18     |
| MinEpisodeLength     | 267        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.55     |
| TotalNEpisodes       | 23823      |
| TotalNSamples        | 6.8538e+06 |
| ExplainedVariance    | 0.23313    |
-------------------------------------
[2018-01-21 16:07:01.539479 UTC] Saving snapshot
[2018-01-21 16:07:01.539777 UTC] Starting iteration 1370
[2018-01-21 16:07:01.539970 UTC] Start collecting samples
[2018-01-21 16:07:06.183468 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:06.317930 UTC] Performing policy update
[2018-01-21 16:07:06.322591 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:06.447949 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:07.909414 UTC] Performing line search
[2018-01-21 16:07:08.109925 UTC] Updating baseline
[2018-01-21 16:07:10.182035 UTC] Computing logging information
-------------------------------------
| Iteration            | 1370       |
| ExpectedImprovement  | 0.019377   |
| ActualImprovement    | 0.01775    |
| ImprovementRatio     | 0.91605    |
| MeanKL               | 0.0081902  |
| Entropy              | -1.6228    |
| Perplexity           | 0.19735    |
| AveragePolicyStd     | 0.18676    |
| AveragePolicyStd[0]  | 0.20391    |
| AveragePolicyStd[1]  | 0.20451    |
| AveragePolicyStd[2]  | 0.14709    |
| AveragePolicyStd[3]  | 0.18574    |
| AveragePolicyStd[4]  | 0.15496    |
| AveragePolicyStd[5]  | 0.22437    |
| AverageReturn        | 1731.8     |
| MinReturn            | 293.71     |
| MaxReturn            | 1929       |
| StdReturn            | 354.33     |
| AverageEpisodeLength | 933.4      |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.51     |
| TotalNEpisodes       | 23830      |
| TotalNSamples        | 6.8591e+06 |
| ExplainedVariance    | 0.27367    |
-------------------------------------
[2018-01-21 16:07:10.940827 UTC] Saving snapshot
[2018-01-21 16:07:10.947632 UTC] Starting iteration 1371
[2018-01-21 16:07:10.947807 UTC] Start collecting samples
[2018-01-21 16:07:15.645344 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:15.777144 UTC] Performing policy update
[2018-01-21 16:07:15.777795 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:15.917091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:17.341281 UTC] Performing line search
[2018-01-21 16:07:17.530327 UTC] Updating baseline
[2018-01-21 16:07:20.705940 UTC] Computing logging information
-------------------------------------
| Iteration            | 1371       |
| ExpectedImprovement  | 0.019973   |
| ActualImprovement    | 0.020768   |
| ImprovementRatio     | 1.0398     |
| MeanKL               | 0.0079862  |
| Entropy              | -1.6248    |
| Perplexity           | 0.19694    |
| AveragePolicyStd     | 0.18667    |
| AveragePolicyStd[0]  | 0.2038     |
| AveragePolicyStd[1]  | 0.20455    |
| AveragePolicyStd[2]  | 0.14716    |
| AveragePolicyStd[3]  | 0.18555    |
| AveragePolicyStd[4]  | 0.15514    |
| AveragePolicyStd[5]  | 0.22383    |
| AverageReturn        | 1734.5     |
| MinReturn            | 293.71     |
| MaxReturn            | 1929       |
| StdReturn            | 354.59     |
| AverageEpisodeLength | 934.53     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.57     |
| TotalNEpisodes       | 23832      |
| TotalNSamples        | 6.8611e+06 |
| ExplainedVariance    | -0.10162   |
-------------------------------------
[2018-01-21 16:07:21.517827 UTC] Saving snapshot
[2018-01-21 16:07:21.518055 UTC] Starting iteration 1372
[2018-01-21 16:07:21.518205 UTC] Start collecting samples
[2018-01-21 16:07:26.330540 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:26.458756 UTC] Performing policy update
[2018-01-21 16:07:26.459628 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:26.581236 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:28.007835 UTC] Performing line search
[2018-01-21 16:07:28.199554 UTC] Updating baseline
[2018-01-21 16:07:30.024454 UTC] Computing logging information
------------------------------------
| Iteration            | 1372      |
| ExpectedImprovement  | 0.017122  |
| ActualImprovement    | 0.016801  |
| ImprovementRatio     | 0.98127   |
| MeanKL               | 0.0080699 |
| Entropy              | -1.6302   |
| Perplexity           | 0.19588   |
| AveragePolicyStd     | 0.18649   |
| AveragePolicyStd[0]  | 0.2035    |
| AveragePolicyStd[1]  | 0.20459   |
| AveragePolicyStd[2]  | 0.14716   |
| AveragePolicyStd[3]  | 0.1855    |
| AveragePolicyStd[4]  | 0.15492   |
| AveragePolicyStd[5]  | 0.22331   |
| AverageReturn        | 1749.4    |
| MinReturn            | 293.71    |
| MaxReturn            | 1935      |
| StdReturn            | 347.93    |
| AverageEpisodeLength | 940.02    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 175.28    |
| TotalNEpisodes       | 23839     |
| TotalNSamples        | 6.868e+06 |
| ExplainedVariance    | 0.10734   |
------------------------------------
[2018-01-21 16:07:30.843388 UTC] Saving snapshot
[2018-01-21 16:07:30.843625 UTC] Starting iteration 1373
[2018-01-21 16:07:30.843773 UTC] Start collecting samples
[2018-01-21 16:07:36.102096 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:36.254779 UTC] Performing policy update
[2018-01-21 16:07:36.255526 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:36.379639 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:37.795312 UTC] Performing line search
[2018-01-21 16:07:37.987456 UTC] Updating baseline
[2018-01-21 16:07:40.632124 UTC] Computing logging information
------------------------------------
| Iteration            | 1373      |
| ExpectedImprovement  | 0.0161    |
| ActualImprovement    | 0.015163  |
| ImprovementRatio     | 0.94184   |
| MeanKL               | 0.008399  |
| Entropy              | -1.6365   |
| Perplexity           | 0.19467   |
| AveragePolicyStd     | 0.18628   |
| AveragePolicyStd[0]  | 0.20381   |
| AveragePolicyStd[1]  | 0.20419   |
| AveragePolicyStd[2]  | 0.14677   |
| AveragePolicyStd[3]  | 0.18534   |
| AveragePolicyStd[4]  | 0.15524   |
| AveragePolicyStd[5]  | 0.22233   |
| AverageReturn        | 1765.6    |
| MinReturn            | 293.71    |
| MaxReturn            | 1935      |
| StdReturn            | 325.05    |
| AverageEpisodeLength | 947.06    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 162.97    |
| TotalNEpisodes       | 23846     |
| TotalNSamples        | 6.875e+06 |
| ExplainedVariance    | 0.0015576 |
------------------------------------
[2018-01-21 16:07:41.415578 UTC] Saving snapshot
[2018-01-21 16:07:41.415908 UTC] Starting iteration 1374
[2018-01-21 16:07:41.416122 UTC] Start collecting samples
[2018-01-21 16:07:45.882303 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:46.004411 UTC] Performing policy update
[2018-01-21 16:07:46.005487 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:46.124715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:47.534881 UTC] Performing line search
[2018-01-21 16:07:47.720380 UTC] Updating baseline
[2018-01-21 16:07:49.804160 UTC] Computing logging information
-------------------------------------
| Iteration            | 1374       |
| ExpectedImprovement  | 0.020157   |
| ActualImprovement    | 0.019092   |
| ImprovementRatio     | 0.94714    |
| MeanKL               | 0.007984   |
| Entropy              | -1.6384    |
| Perplexity           | 0.19429    |
| AveragePolicyStd     | 0.18624    |
| AveragePolicyStd[0]  | 0.20387    |
| AveragePolicyStd[1]  | 0.20384    |
| AveragePolicyStd[2]  | 0.14706    |
| AveragePolicyStd[3]  | 0.1854     |
| AveragePolicyStd[4]  | 0.15456    |
| AveragePolicyStd[5]  | 0.22269    |
| AverageReturn        | 1753       |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 349.29     |
| AverageEpisodeLength | 940.14     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.84     |
| TotalNEpisodes       | 23849      |
| TotalNSamples        | 6.8773e+06 |
| ExplainedVariance    | 0.1561     |
-------------------------------------
[2018-01-21 16:07:50.608459 UTC] Saving snapshot
[2018-01-21 16:07:50.608710 UTC] Starting iteration 1375
[2018-01-21 16:07:50.608890 UTC] Start collecting samples
[2018-01-21 16:07:55.071911 UTC] Computing input variables for policy optimization
[2018-01-21 16:07:55.192699 UTC] Performing policy update
[2018-01-21 16:07:55.193848 UTC] Computing gradient in Euclidean space
[2018-01-21 16:07:55.324756 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:07:56.739851 UTC] Performing line search
[2018-01-21 16:07:56.941123 UTC] Updating baseline
[2018-01-21 16:07:58.881080 UTC] Computing logging information
-------------------------------------
| Iteration            | 1375       |
| ExpectedImprovement  | 0.018419   |
| ActualImprovement    | 0.01753    |
| ImprovementRatio     | 0.95174    |
| MeanKL               | 0.0083298  |
| Entropy              | -1.6406    |
| Perplexity           | 0.19385    |
| AveragePolicyStd     | 0.18612    |
| AveragePolicyStd[0]  | 0.20368    |
| AveragePolicyStd[1]  | 0.20352    |
| AveragePolicyStd[2]  | 0.14723    |
| AveragePolicyStd[3]  | 0.1854     |
| AveragePolicyStd[4]  | 0.15496    |
| AveragePolicyStd[5]  | 0.2219     |
| AverageReturn        | 1737       |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 358.21     |
| AverageEpisodeLength | 932.63     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.45     |
| TotalNEpisodes       | 23856      |
| TotalNSamples        | 6.8836e+06 |
| ExplainedVariance    | 0.34343    |
-------------------------------------
[2018-01-21 16:07:59.713130 UTC] Saving snapshot
[2018-01-21 16:07:59.713308 UTC] Starting iteration 1376
[2018-01-21 16:07:59.713410 UTC] Start collecting samples
[2018-01-21 16:08:04.471485 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:04.628132 UTC] Performing policy update
[2018-01-21 16:08:04.628695 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:04.752716 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:06.199907 UTC] Performing line search
[2018-01-21 16:08:06.397673 UTC] Updating baseline
[2018-01-21 16:08:08.297429 UTC] Computing logging information
------------------------------------
| Iteration            | 1376      |
| ExpectedImprovement  | 0.019267  |
| ActualImprovement    | 0.019057  |
| ImprovementRatio     | 0.98908   |
| MeanKL               | 0.0081885 |
| Entropy              | -1.6465   |
| Perplexity           | 0.19272   |
| AveragePolicyStd     | 0.18596   |
| AveragePolicyStd[0]  | 0.20385   |
| AveragePolicyStd[1]  | 0.20331   |
| AveragePolicyStd[2]  | 0.147     |
| AveragePolicyStd[3]  | 0.18478   |
| AveragePolicyStd[4]  | 0.15468   |
| AveragePolicyStd[5]  | 0.22213   |
| AverageReturn        | 1726.3    |
| MinReturn            | 293.71    |
| MaxReturn            | 1935      |
| StdReturn            | 372.63    |
| AverageEpisodeLength | 926.81    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.47    |
| TotalNEpisodes       | 23862     |
| TotalNSamples        | 6.889e+06 |
| ExplainedVariance    | 0.14983   |
------------------------------------
[2018-01-21 16:08:09.089779 UTC] Saving snapshot
[2018-01-21 16:08:09.089979 UTC] Starting iteration 1377
[2018-01-21 16:08:09.090096 UTC] Start collecting samples
[2018-01-21 16:08:13.737969 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:13.857091 UTC] Performing policy update
[2018-01-21 16:08:13.857873 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:13.977367 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:15.385240 UTC] Performing line search
[2018-01-21 16:08:15.574332 UTC] Updating baseline
[2018-01-21 16:08:17.605876 UTC] Computing logging information
-------------------------------------
| Iteration            | 1377       |
| ExpectedImprovement  | 0.020111   |
| ActualImprovement    | 0.018771   |
| ImprovementRatio     | 0.9334     |
| MeanKL               | 0.0077175  |
| Entropy              | -1.6482    |
| Perplexity           | 0.19239    |
| AveragePolicyStd     | 0.18589    |
| AveragePolicyStd[0]  | 0.20381    |
| AveragePolicyStd[1]  | 0.20301    |
| AveragePolicyStd[2]  | 0.147      |
| AveragePolicyStd[3]  | 0.18477    |
| AveragePolicyStd[4]  | 0.15488    |
| AveragePolicyStd[5]  | 0.22188    |
| AverageReturn        | 1730.4     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 363.3      |
| AverageEpisodeLength | 928.46     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.5      |
| TotalNEpisodes       | 23867      |
| TotalNSamples        | 6.8935e+06 |
| ExplainedVariance    | 0.17125    |
-------------------------------------
[2018-01-21 16:08:18.436626 UTC] Saving snapshot
[2018-01-21 16:08:18.436902 UTC] Starting iteration 1378
[2018-01-21 16:08:18.437055 UTC] Start collecting samples
[2018-01-21 16:08:23.106907 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:23.232508 UTC] Performing policy update
[2018-01-21 16:08:23.233247 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:23.367661 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:24.803375 UTC] Performing line search
[2018-01-21 16:08:24.991403 UTC] Updating baseline
[2018-01-21 16:08:26.926818 UTC] Computing logging information
-------------------------------------
| Iteration            | 1378       |
| ExpectedImprovement  | 0.017428   |
| ActualImprovement    | 0.017026   |
| ImprovementRatio     | 0.97695    |
| MeanKL               | 0.007688   |
| Entropy              | -1.6516    |
| Perplexity           | 0.19175    |
| AveragePolicyStd     | 0.18581    |
| AveragePolicyStd[0]  | 0.20403    |
| AveragePolicyStd[1]  | 0.20326    |
| AveragePolicyStd[2]  | 0.14692    |
| AveragePolicyStd[3]  | 0.1843     |
| AveragePolicyStd[4]  | 0.15442    |
| AveragePolicyStd[5]  | 0.22195    |
| AverageReturn        | 1735.6     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 355.03     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.72     |
| TotalNEpisodes       | 23874      |
| TotalNSamples        | 6.9001e+06 |
| ExplainedVariance    | 0.070015   |
-------------------------------------
[2018-01-21 16:08:27.804113 UTC] Saving snapshot
[2018-01-21 16:08:27.804344 UTC] Starting iteration 1379
[2018-01-21 16:08:27.804488 UTC] Start collecting samples
[2018-01-21 16:08:33.134909 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:33.278828 UTC] Performing policy update
[2018-01-21 16:08:33.279472 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:33.434966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:34.901100 UTC] Performing line search
[2018-01-21 16:08:35.096864 UTC] Updating baseline
[2018-01-21 16:08:36.900878 UTC] Computing logging information
-------------------------------------
| Iteration            | 1379       |
| ExpectedImprovement  | 0.019667   |
| ActualImprovement    | 0.018081   |
| ImprovementRatio     | 0.91932    |
| MeanKL               | 0.0078041  |
| Entropy              | -1.6514    |
| Perplexity           | 0.19178    |
| AveragePolicyStd     | 0.18582    |
| AveragePolicyStd[0]  | 0.20385    |
| AveragePolicyStd[1]  | 0.20317    |
| AveragePolicyStd[2]  | 0.1469     |
| AveragePolicyStd[3]  | 0.18462    |
| AveragePolicyStd[4]  | 0.15429    |
| AveragePolicyStd[5]  | 0.22213    |
| AverageReturn        | 1736.9     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 355.34     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.72     |
| TotalNEpisodes       | 23877      |
| TotalNSamples        | 6.9031e+06 |
| ExplainedVariance    | 0.094788   |
-------------------------------------
[2018-01-21 16:08:37.745572 UTC] Saving snapshot
[2018-01-21 16:08:37.745807 UTC] Starting iteration 1380
[2018-01-21 16:08:37.745952 UTC] Start collecting samples
[2018-01-21 16:08:42.287914 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:42.419982 UTC] Performing policy update
[2018-01-21 16:08:42.420582 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:42.545117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:43.919709 UTC] Performing line search
[2018-01-21 16:08:44.106571 UTC] Updating baseline
[2018-01-21 16:08:46.106368 UTC] Computing logging information
-------------------------------------
| Iteration            | 1380       |
| ExpectedImprovement  | 0.019802   |
| ActualImprovement    | 0.019371   |
| ImprovementRatio     | 0.97824    |
| MeanKL               | 0.0075231  |
| Entropy              | -1.65      |
| Perplexity           | 0.19204    |
| AveragePolicyStd     | 0.18587    |
| AveragePolicyStd[0]  | 0.20408    |
| AveragePolicyStd[1]  | 0.20307    |
| AveragePolicyStd[2]  | 0.14686    |
| AveragePolicyStd[3]  | 0.18439    |
| AveragePolicyStd[4]  | 0.15447    |
| AveragePolicyStd[5]  | 0.22234    |
| AverageReturn        | 1723.5     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 379.67     |
| AverageEpisodeLength | 922.63     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.94     |
| TotalNEpisodes       | 23880      |
| TotalNSamples        | 6.9053e+06 |
| ExplainedVariance    | 0.087126   |
-------------------------------------
[2018-01-21 16:08:46.923929 UTC] Saving snapshot
[2018-01-21 16:08:46.933623 UTC] Starting iteration 1381
[2018-01-21 16:08:46.933856 UTC] Start collecting samples
[2018-01-21 16:08:51.519071 UTC] Computing input variables for policy optimization
[2018-01-21 16:08:51.657624 UTC] Performing policy update
[2018-01-21 16:08:51.658362 UTC] Computing gradient in Euclidean space
[2018-01-21 16:08:51.780248 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:08:53.176921 UTC] Performing line search
[2018-01-21 16:08:53.363989 UTC] Updating baseline
[2018-01-21 16:08:55.306280 UTC] Computing logging information
-------------------------------------
| Iteration            | 1381       |
| ExpectedImprovement  | 0.016298   |
| ActualImprovement    | 0.015695   |
| ImprovementRatio     | 0.963      |
| MeanKL               | 0.0074668  |
| Entropy              | -1.6502    |
| Perplexity           | 0.19202    |
| AveragePolicyStd     | 0.18584    |
| AveragePolicyStd[0]  | 0.20395    |
| AveragePolicyStd[1]  | 0.20296    |
| AveragePolicyStd[2]  | 0.14686    |
| AveragePolicyStd[3]  | 0.18423    |
| AveragePolicyStd[4]  | 0.15492    |
| AveragePolicyStd[5]  | 0.22212    |
| AverageReturn        | 1732       |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 379.28     |
| AverageEpisodeLength | 924.76     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.6      |
| TotalNEpisodes       | 23888      |
| TotalNSamples        | 6.9133e+06 |
| ExplainedVariance    | -0.031265  |
-------------------------------------
[2018-01-21 16:08:56.124864 UTC] Saving snapshot
[2018-01-21 16:08:56.125141 UTC] Starting iteration 1382
[2018-01-21 16:08:56.125333 UTC] Start collecting samples
[2018-01-21 16:09:00.632909 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:00.768676 UTC] Performing policy update
[2018-01-21 16:09:00.769313 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:00.889891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:02.416267 UTC] Performing line search
[2018-01-21 16:09:02.618011 UTC] Updating baseline
[2018-01-21 16:09:05.065802 UTC] Computing logging information
------------------------------------
| Iteration            | 1382      |
| ExpectedImprovement  | 0.019894  |
| ActualImprovement    | 0.018822  |
| ImprovementRatio     | 0.94609   |
| MeanKL               | 0.0080016 |
| Entropy              | -1.6534   |
| Perplexity           | 0.1914    |
| AveragePolicyStd     | 0.18573   |
| AveragePolicyStd[0]  | 0.2039    |
| AveragePolicyStd[1]  | 0.20256   |
| AveragePolicyStd[2]  | 0.1469    |
| AveragePolicyStd[3]  | 0.18405   |
| AveragePolicyStd[4]  | 0.15491   |
| AveragePolicyStd[5]  | 0.2221    |
| AverageReturn        | 1735.3    |
| MinReturn            | 293.71    |
| MaxReturn            | 1935      |
| StdReturn            | 377.1     |
| AverageEpisodeLength | 925.79    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 187.86    |
| TotalNEpisodes       | 23894     |
| TotalNSamples        | 6.919e+06 |
| ExplainedVariance    | 0.14912   |
------------------------------------
[2018-01-21 16:09:05.939065 UTC] Saving snapshot
[2018-01-21 16:09:05.939355 UTC] Starting iteration 1383
[2018-01-21 16:09:05.939535 UTC] Start collecting samples
[2018-01-21 16:09:10.659728 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:10.797236 UTC] Performing policy update
[2018-01-21 16:09:10.797849 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:10.920752 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:12.345078 UTC] Performing line search
[2018-01-21 16:09:12.543727 UTC] Updating baseline
[2018-01-21 16:09:14.533185 UTC] Computing logging information
-------------------------------------
| Iteration            | 1383       |
| ExpectedImprovement  | 0.017683   |
| ActualImprovement    | 0.016517   |
| ImprovementRatio     | 0.93406    |
| MeanKL               | 0.0087056  |
| Entropy              | -1.657     |
| Perplexity           | 0.19071    |
| AveragePolicyStd     | 0.18563    |
| AveragePolicyStd[0]  | 0.2036     |
| AveragePolicyStd[1]  | 0.20278    |
| AveragePolicyStd[2]  | 0.14639    |
| AveragePolicyStd[3]  | 0.18423    |
| AveragePolicyStd[4]  | 0.15507    |
| AveragePolicyStd[5]  | 0.22168    |
| AverageReturn        | 1730.5     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 378.85     |
| AverageEpisodeLength | 923.13     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.67     |
| TotalNEpisodes       | 23897      |
| TotalNSamples        | 6.9217e+06 |
| ExplainedVariance    | 0.20001    |
-------------------------------------
[2018-01-21 16:09:15.442652 UTC] Saving snapshot
[2018-01-21 16:09:15.442884 UTC] Starting iteration 1384
[2018-01-21 16:09:15.443034 UTC] Start collecting samples
[2018-01-21 16:09:20.054318 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:20.200103 UTC] Performing policy update
[2018-01-21 16:09:20.201094 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:20.317537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:21.684354 UTC] Performing line search
[2018-01-21 16:09:21.871822 UTC] Updating baseline
[2018-01-21 16:09:23.740953 UTC] Computing logging information
------------------------------------
| Iteration            | 1384      |
| ExpectedImprovement  | 0.018551  |
| ActualImprovement    | 0.01755   |
| ImprovementRatio     | 0.94607   |
| MeanKL               | 0.0076354 |
| Entropy              | -1.6565   |
| Perplexity           | 0.19081   |
| AveragePolicyStd     | 0.18564   |
| AveragePolicyStd[0]  | 0.20388   |
| AveragePolicyStd[1]  | 0.20308   |
| AveragePolicyStd[2]  | 0.14644   |
| AveragePolicyStd[3]  | 0.18398   |
| AveragePolicyStd[4]  | 0.15506   |
| AveragePolicyStd[5]  | 0.22142   |
| AverageReturn        | 1715.4    |
| MinReturn            | 293.71    |
| MaxReturn            | 1935      |
| StdReturn            | 399.77    |
| AverageEpisodeLength | 915.81    |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 199.44    |
| TotalNEpisodes       | 23904     |
| TotalNSamples        | 6.928e+06 |
| ExplainedVariance    | 0.10511   |
------------------------------------
[2018-01-21 16:09:24.568503 UTC] Saving snapshot
[2018-01-21 16:09:24.568752 UTC] Starting iteration 1385
[2018-01-21 16:09:24.568925 UTC] Start collecting samples
[2018-01-21 16:09:29.387888 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:29.542576 UTC] Performing policy update
[2018-01-21 16:09:29.543254 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:29.664268 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:31.137486 UTC] Performing line search
[2018-01-21 16:09:31.331288 UTC] Updating baseline
[2018-01-21 16:09:33.623525 UTC] Computing logging information
-------------------------------------
| Iteration            | 1385       |
| ExpectedImprovement  | 0.018044   |
| ActualImprovement    | 0.016967   |
| ImprovementRatio     | 0.94032    |
| MeanKL               | 0.0078967  |
| Entropy              | -1.6595    |
| Perplexity           | 0.19024    |
| AveragePolicyStd     | 0.18555    |
| AveragePolicyStd[0]  | 0.20347    |
| AveragePolicyStd[1]  | 0.20346    |
| AveragePolicyStd[2]  | 0.14653    |
| AveragePolicyStd[3]  | 0.18382    |
| AveragePolicyStd[4]  | 0.15472    |
| AveragePolicyStd[5]  | 0.22131    |
| AverageReturn        | 1688.2     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 430.97     |
| AverageEpisodeLength | 902.25     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.35     |
| TotalNEpisodes       | 23912      |
| TotalNSamples        | 6.9346e+06 |
| ExplainedVariance    | 0.15768    |
-------------------------------------
[2018-01-21 16:09:34.391878 UTC] Saving snapshot
[2018-01-21 16:09:34.392174 UTC] Starting iteration 1386
[2018-01-21 16:09:34.392395 UTC] Start collecting samples
[2018-01-21 16:09:38.936359 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:39.059783 UTC] Performing policy update
[2018-01-21 16:09:39.060367 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:39.180229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:40.576134 UTC] Performing line search
[2018-01-21 16:09:40.768904 UTC] Updating baseline
[2018-01-21 16:09:42.887740 UTC] Computing logging information
-------------------------------------
| Iteration            | 1386       |
| ExpectedImprovement  | 0.017729   |
| ActualImprovement    | 0.017115   |
| ImprovementRatio     | 0.96536    |
| MeanKL               | 0.0081862  |
| Entropy              | -1.661     |
| Perplexity           | 0.18994    |
| AveragePolicyStd     | 0.1855     |
| AveragePolicyStd[0]  | 0.20346    |
| AveragePolicyStd[1]  | 0.2037     |
| AveragePolicyStd[2]  | 0.14696    |
| AveragePolicyStd[3]  | 0.18313    |
| AveragePolicyStd[4]  | 0.15442    |
| AveragePolicyStd[5]  | 0.22134    |
| AverageReturn        | 1681.5     |
| MinReturn            | 293.71     |
| MaxReturn            | 1935       |
| StdReturn            | 441.35     |
| AverageEpisodeLength | 899.07     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.88     |
| TotalNEpisodes       | 23916      |
| TotalNSamples        | 6.9379e+06 |
| ExplainedVariance    | 0.33311    |
-------------------------------------
[2018-01-21 16:09:43.712018 UTC] Saving snapshot
[2018-01-21 16:09:43.712270 UTC] Starting iteration 1387
[2018-01-21 16:09:43.712453 UTC] Start collecting samples
[2018-01-21 16:09:48.389389 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:48.520905 UTC] Performing policy update
[2018-01-21 16:09:48.521700 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:48.635559 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:50.058168 UTC] Performing line search
[2018-01-21 16:09:50.259570 UTC] Updating baseline
[2018-01-21 16:09:52.024080 UTC] Computing logging information
-------------------------------------
| Iteration            | 1387       |
| ExpectedImprovement  | 0.019776   |
| ActualImprovement    | 0.019871   |
| ImprovementRatio     | 1.0048     |
| MeanKL               | 0.0081799  |
| Entropy              | -1.6607    |
| Perplexity           | 0.19001    |
| AveragePolicyStd     | 0.18551    |
| AveragePolicyStd[0]  | 0.20347    |
| AveragePolicyStd[1]  | 0.2032     |
| AveragePolicyStd[2]  | 0.14719    |
| AveragePolicyStd[3]  | 0.18302    |
| AveragePolicyStd[4]  | 0.15443    |
| AveragePolicyStd[5]  | 0.22172    |
| AverageReturn        | 1672       |
| MinReturn            | 293.71     |
| MaxReturn            | 1938.4     |
| StdReturn            | 450.23     |
| AverageEpisodeLength | 893.52     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 226.36     |
| TotalNEpisodes       | 23921      |
| TotalNSamples        | 6.9419e+06 |
| ExplainedVariance    | 0.16966    |
-------------------------------------
[2018-01-21 16:09:52.875329 UTC] Saving snapshot
[2018-01-21 16:09:52.875569 UTC] Starting iteration 1388
[2018-01-21 16:09:52.875744 UTC] Start collecting samples
[2018-01-21 16:09:57.605866 UTC] Computing input variables for policy optimization
[2018-01-21 16:09:57.767694 UTC] Performing policy update
[2018-01-21 16:09:57.768480 UTC] Computing gradient in Euclidean space
[2018-01-21 16:09:57.880688 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:09:59.271120 UTC] Performing line search
[2018-01-21 16:09:59.462468 UTC] Updating baseline
[2018-01-21 16:10:01.634932 UTC] Computing logging information
-------------------------------------
| Iteration            | 1388       |
| ExpectedImprovement  | 0.016159   |
| ActualImprovement    | 0.016083   |
| ImprovementRatio     | 0.99531    |
| MeanKL               | 0.0090302  |
| Entropy              | -1.6572    |
| Perplexity           | 0.19067    |
| AveragePolicyStd     | 0.1856     |
| AveragePolicyStd[0]  | 0.20317    |
| AveragePolicyStd[1]  | 0.20337    |
| AveragePolicyStd[2]  | 0.14726    |
| AveragePolicyStd[3]  | 0.18339    |
| AveragePolicyStd[4]  | 0.1547     |
| AveragePolicyStd[5]  | 0.22171    |
| AverageReturn        | 1706.1     |
| MinReturn            | 293.71     |
| MaxReturn            | 1938.4     |
| StdReturn            | 420.02     |
| AverageEpisodeLength | 910.32     |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.34     |
| TotalNEpisodes       | 23929      |
| TotalNSamples        | 6.9499e+06 |
| ExplainedVariance    | -0.012069  |
-------------------------------------
[2018-01-21 16:10:02.426785 UTC] Saving snapshot
[2018-01-21 16:10:02.427022 UTC] Starting iteration 1389
[2018-01-21 16:10:02.427171 UTC] Start collecting samples
[2018-01-21 16:10:06.922216 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:07.072969 UTC] Performing policy update
[2018-01-21 16:10:07.073961 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:07.199087 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:08.601886 UTC] Performing line search
[2018-01-21 16:10:08.793493 UTC] Updating baseline
[2018-01-21 16:10:10.942530 UTC] Computing logging information
-------------------------------------
| Iteration            | 1389       |
| ExpectedImprovement  | 0.018851   |
| ActualImprovement    | 0.01802    |
| ImprovementRatio     | 0.95589    |
| MeanKL               | 0.0078228  |
| Entropy              | -1.6457    |
| Perplexity           | 0.19289    |
| AveragePolicyStd     | 0.18598    |
| AveragePolicyStd[0]  | 0.20419    |
| AveragePolicyStd[1]  | 0.20397    |
| AveragePolicyStd[2]  | 0.14741    |
| AveragePolicyStd[3]  | 0.18367    |
| AveragePolicyStd[4]  | 0.15468    |
| AveragePolicyStd[5]  | 0.22196    |
| AverageReturn        | 1692.8     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 418.73     |
| AverageEpisodeLength | 904.14     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.04     |
| TotalNEpisodes       | 23935      |
| TotalNSamples        | 6.9545e+06 |
| ExplainedVariance    | 0.27133    |
-------------------------------------
[2018-01-21 16:10:11.825497 UTC] Saving snapshot
[2018-01-21 16:10:11.825760 UTC] Starting iteration 1390
[2018-01-21 16:10:11.825931 UTC] Start collecting samples
[2018-01-21 16:10:16.374899 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:16.505546 UTC] Performing policy update
[2018-01-21 16:10:16.506166 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:16.629476 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:18.007562 UTC] Performing line search
[2018-01-21 16:10:18.217945 UTC] Updating baseline
[2018-01-21 16:10:20.594146 UTC] Computing logging information
-------------------------------------
| Iteration            | 1390       |
| ExpectedImprovement  | 0.017837   |
| ActualImprovement    | 0.016705   |
| ImprovementRatio     | 0.93651    |
| MeanKL               | 0.0077575  |
| Entropy              | -1.6448    |
| Perplexity           | 0.19305    |
| AveragePolicyStd     | 0.18604    |
| AveragePolicyStd[0]  | 0.20418    |
| AveragePolicyStd[1]  | 0.20425    |
| AveragePolicyStd[2]  | 0.14732    |
| AveragePolicyStd[3]  | 0.18363    |
| AveragePolicyStd[4]  | 0.15433    |
| AveragePolicyStd[5]  | 0.22255    |
| AverageReturn        | 1692.1     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 418.39     |
| AverageEpisodeLength | 903.88     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.93     |
| TotalNEpisodes       | 23938      |
| TotalNSamples        | 6.9575e+06 |
| ExplainedVariance    | 0.16222    |
-------------------------------------
[2018-01-21 16:10:21.412602 UTC] Saving snapshot
[2018-01-21 16:10:21.421744 UTC] Starting iteration 1391
[2018-01-21 16:10:21.421980 UTC] Start collecting samples
[2018-01-21 16:10:26.112491 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:26.242020 UTC] Performing policy update
[2018-01-21 16:10:26.242734 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:26.357165 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:27.735244 UTC] Performing line search
[2018-01-21 16:10:27.931615 UTC] Updating baseline
[2018-01-21 16:10:29.782467 UTC] Computing logging information
-------------------------------------
| Iteration            | 1391       |
| ExpectedImprovement  | 0.018125   |
| ActualImprovement    | 0.018092   |
| ImprovementRatio     | 0.99815    |
| MeanKL               | 0.007279   |
| Entropy              | -1.6425    |
| Perplexity           | 0.19349    |
| AveragePolicyStd     | 0.18612    |
| AveragePolicyStd[0]  | 0.20427    |
| AveragePolicyStd[1]  | 0.20473    |
| AveragePolicyStd[2]  | 0.14736    |
| AveragePolicyStd[3]  | 0.18333    |
| AveragePolicyStd[4]  | 0.15446    |
| AveragePolicyStd[5]  | 0.22255    |
| AverageReturn        | 1685.2     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 418.63     |
| AverageEpisodeLength | 901.95     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.52     |
| TotalNEpisodes       | 23946      |
| TotalNSamples        | 6.9652e+06 |
| ExplainedVariance    | 0.028525   |
-------------------------------------
[2018-01-21 16:10:30.554477 UTC] Saving snapshot
[2018-01-21 16:10:30.554716 UTC] Starting iteration 1392
[2018-01-21 16:10:30.554899 UTC] Start collecting samples
[2018-01-21 16:10:34.961933 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:35.116021 UTC] Performing policy update
[2018-01-21 16:10:35.116714 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:35.233007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:36.638371 UTC] Performing line search
[2018-01-21 16:10:36.816371 UTC] Updating baseline
[2018-01-21 16:10:39.395906 UTC] Computing logging information
-------------------------------------
| Iteration            | 1392       |
| ExpectedImprovement  | 0.018402   |
| ActualImprovement    | 0.017085   |
| ImprovementRatio     | 0.9284     |
| MeanKL               | 0.0072469  |
| Entropy              | -1.6453    |
| Perplexity           | 0.19296    |
| AveragePolicyStd     | 0.18605    |
| AveragePolicyStd[0]  | 0.20413    |
| AveragePolicyStd[1]  | 0.20453    |
| AveragePolicyStd[2]  | 0.14755    |
| AveragePolicyStd[3]  | 0.18311    |
| AveragePolicyStd[4]  | 0.15393    |
| AveragePolicyStd[5]  | 0.22305    |
| AverageReturn        | 1698.5     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 401.07     |
| AverageEpisodeLength | 908.87     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.13     |
| TotalNEpisodes       | 23949      |
| TotalNSamples        | 6.9682e+06 |
| ExplainedVariance    | -0.0064222 |
-------------------------------------
[2018-01-21 16:10:40.279839 UTC] Saving snapshot
[2018-01-21 16:10:40.280081 UTC] Starting iteration 1393
[2018-01-21 16:10:40.280260 UTC] Start collecting samples
[2018-01-21 16:10:44.816578 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:44.953834 UTC] Performing policy update
[2018-01-21 16:10:44.955028 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:45.074663 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:46.502664 UTC] Performing line search
[2018-01-21 16:10:46.712098 UTC] Updating baseline
[2018-01-21 16:10:49.540548 UTC] Computing logging information
-------------------------------------
| Iteration            | 1393       |
| ExpectedImprovement  | 0.017672   |
| ActualImprovement    | 0.01589    |
| ImprovementRatio     | 0.89921    |
| MeanKL               | 0.0076666  |
| Entropy              | -1.6465    |
| Perplexity           | 0.19272    |
| AveragePolicyStd     | 0.186      |
| AveragePolicyStd[0]  | 0.2043     |
| AveragePolicyStd[1]  | 0.20466    |
| AveragePolicyStd[2]  | 0.14751    |
| AveragePolicyStd[3]  | 0.18286    |
| AveragePolicyStd[4]  | 0.15406    |
| AveragePolicyStd[5]  | 0.22262    |
| AverageReturn        | 1702.5     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 401.81     |
| AverageEpisodeLength | 909.89     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.33     |
| TotalNEpisodes       | 23953      |
| TotalNSamples        | 6.9722e+06 |
| ExplainedVariance    | 1.0477e-05 |
-------------------------------------
[2018-01-21 16:10:50.326913 UTC] Saving snapshot
[2018-01-21 16:10:50.327181 UTC] Starting iteration 1394
[2018-01-21 16:10:50.327346 UTC] Start collecting samples
[2018-01-21 16:10:55.191023 UTC] Computing input variables for policy optimization
[2018-01-21 16:10:55.325851 UTC] Performing policy update
[2018-01-21 16:10:55.326803 UTC] Computing gradient in Euclidean space
[2018-01-21 16:10:55.453767 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:10:56.848228 UTC] Performing line search
[2018-01-21 16:10:57.055735 UTC] Updating baseline
[2018-01-21 16:10:58.811571 UTC] Computing logging information
-------------------------------------
| Iteration            | 1394       |
| ExpectedImprovement  | 0.018188   |
| ActualImprovement    | 0.017147   |
| ImprovementRatio     | 0.94273    |
| MeanKL               | 0.0079177  |
| Entropy              | -1.6468    |
| Perplexity           | 0.19267    |
| AveragePolicyStd     | 0.18599    |
| AveragePolicyStd[0]  | 0.20433    |
| AveragePolicyStd[1]  | 0.20478    |
| AveragePolicyStd[2]  | 0.14739    |
| AveragePolicyStd[3]  | 0.18312    |
| AveragePolicyStd[4]  | 0.15403    |
| AveragePolicyStd[5]  | 0.22232    |
| AverageReturn        | 1716.9     |
| MinReturn            | 399.33     |
| MaxReturn            | 1938.4     |
| StdReturn            | 387.81     |
| AverageEpisodeLength | 917.96     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.62     |
| TotalNEpisodes       | 23962      |
| TotalNSamples        | 6.9808e+06 |
| ExplainedVariance    | 0.052606   |
-------------------------------------
[2018-01-21 16:10:59.586456 UTC] Saving snapshot
[2018-01-21 16:10:59.586942 UTC] Starting iteration 1395
[2018-01-21 16:10:59.587326 UTC] Start collecting samples
[2018-01-21 16:11:04.062739 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:04.193584 UTC] Performing policy update
[2018-01-21 16:11:04.194266 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:04.322250 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:05.725042 UTC] Performing line search
[2018-01-21 16:11:05.923916 UTC] Updating baseline
[2018-01-21 16:11:08.969517 UTC] Computing logging information
-------------------------------------
| Iteration            | 1395       |
| ExpectedImprovement  | 0.018332   |
| ActualImprovement    | 0.017179   |
| ImprovementRatio     | 0.93713    |
| MeanKL               | 0.0082623  |
| Entropy              | -1.6431    |
| Perplexity           | 0.19338    |
| AveragePolicyStd     | 0.18613    |
| AveragePolicyStd[0]  | 0.20448    |
| AveragePolicyStd[1]  | 0.20486    |
| AveragePolicyStd[2]  | 0.14689    |
| AveragePolicyStd[3]  | 0.18381    |
| AveragePolicyStd[4]  | 0.1542     |
| AveragePolicyStd[5]  | 0.22255    |
| AverageReturn        | 1715.4     |
| MinReturn            | 399.33     |
| MaxReturn            | 1939       |
| StdReturn            | 393.98     |
| AverageEpisodeLength | 917.16     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.65     |
| TotalNEpisodes       | 23967      |
| TotalNSamples        | 6.9852e+06 |
| ExplainedVariance    | 0.12341    |
-------------------------------------
[2018-01-21 16:11:09.829679 UTC] Saving snapshot
[2018-01-21 16:11:09.829923 UTC] Starting iteration 1396
[2018-01-21 16:11:09.830079 UTC] Start collecting samples
[2018-01-21 16:11:14.226836 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:14.372178 UTC] Performing policy update
[2018-01-21 16:11:14.372895 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:14.490661 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:15.942100 UTC] Performing line search
[2018-01-21 16:11:16.147732 UTC] Updating baseline
[2018-01-21 16:11:18.354779 UTC] Computing logging information
-------------------------------------
| Iteration            | 1396       |
| ExpectedImprovement  | 0.018608   |
| ActualImprovement    | 0.016832   |
| ImprovementRatio     | 0.90453    |
| MeanKL               | 0.0084545  |
| Entropy              | -1.6436    |
| Perplexity           | 0.19329    |
| AveragePolicyStd     | 0.18614    |
| AveragePolicyStd[0]  | 0.20474    |
| AveragePolicyStd[1]  | 0.20496    |
| AveragePolicyStd[2]  | 0.14672    |
| AveragePolicyStd[3]  | 0.18382    |
| AveragePolicyStd[4]  | 0.15405    |
| AveragePolicyStd[5]  | 0.22253    |
| AverageReturn        | 1715.4     |
| MinReturn            | 399.33     |
| MaxReturn            | 1939       |
| StdReturn            | 393.98     |
| AverageEpisodeLength | 917.16     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.65     |
| TotalNEpisodes       | 23968      |
| TotalNSamples        | 6.9862e+06 |
| ExplainedVariance    | -0.047413  |
-------------------------------------
[2018-01-21 16:11:19.227137 UTC] Saving snapshot
[2018-01-21 16:11:19.227341 UTC] Starting iteration 1397
[2018-01-21 16:11:19.227499 UTC] Start collecting samples
[2018-01-21 16:11:23.747477 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:23.868729 UTC] Performing policy update
[2018-01-21 16:11:23.869381 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:23.999375 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:25.388376 UTC] Performing line search
[2018-01-21 16:11:25.585283 UTC] Updating baseline
[2018-01-21 16:11:27.302817 UTC] Computing logging information
-------------------------------------
| Iteration            | 1397       |
| ExpectedImprovement  | 0.017816   |
| ActualImprovement    | 0.017036   |
| ImprovementRatio     | 0.95622    |
| MeanKL               | 0.0078912  |
| Entropy              | -1.642     |
| Perplexity           | 0.19359    |
| AveragePolicyStd     | 0.18618    |
| AveragePolicyStd[0]  | 0.20405    |
| AveragePolicyStd[1]  | 0.2054     |
| AveragePolicyStd[2]  | 0.14686    |
| AveragePolicyStd[3]  | 0.18394    |
| AveragePolicyStd[4]  | 0.15409    |
| AveragePolicyStd[5]  | 0.22273    |
| AverageReturn        | 1710.1     |
| MinReturn            | 399.33     |
| MaxReturn            | 1939       |
| StdReturn            | 403.49     |
| AverageEpisodeLength | 914.48     |
| MinEpisodeLength     | 252        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.72     |
| TotalNEpisodes       | 23975      |
| TotalNSamples        | 6.9925e+06 |
| ExplainedVariance    | 0.18       |
-------------------------------------
[2018-01-21 16:11:28.080090 UTC] Saving snapshot
[2018-01-21 16:11:28.080367 UTC] Starting iteration 1398
[2018-01-21 16:11:28.080564 UTC] Start collecting samples
[2018-01-21 16:11:32.737188 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:32.893420 UTC] Performing policy update
[2018-01-21 16:11:32.894092 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:33.009650 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:34.395655 UTC] Performing line search
[2018-01-21 16:11:34.582546 UTC] Updating baseline
[2018-01-21 16:11:36.513766 UTC] Computing logging information
-------------------------------------
| Iteration            | 1398       |
| ExpectedImprovement  | 0.017713   |
| ActualImprovement    | 0.016708   |
| ImprovementRatio     | 0.94326    |
| MeanKL               | 0.0078691  |
| Entropy              | -1.6425    |
| Perplexity           | 0.19349    |
| AveragePolicyStd     | 0.18616    |
| AveragePolicyStd[0]  | 0.20354    |
| AveragePolicyStd[1]  | 0.20516    |
| AveragePolicyStd[2]  | 0.14692    |
| AveragePolicyStd[3]  | 0.18427    |
| AveragePolicyStd[4]  | 0.15405    |
| AveragePolicyStd[5]  | 0.223      |
| AverageReturn        | 1689.2     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 431.49     |
| AverageEpisodeLength | 903        |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.48     |
| TotalNEpisodes       | 23985      |
| TotalNSamples        | 7.0006e+06 |
| ExplainedVariance    | 0.32845    |
-------------------------------------
[2018-01-21 16:11:37.311385 UTC] Saving snapshot
[2018-01-21 16:11:37.311599 UTC] Starting iteration 1399
[2018-01-21 16:11:37.311781 UTC] Start collecting samples
[2018-01-21 16:11:42.087474 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:42.207204 UTC] Performing policy update
[2018-01-21 16:11:42.207866 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:42.321575 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:43.681012 UTC] Performing line search
[2018-01-21 16:11:43.883757 UTC] Updating baseline
[2018-01-21 16:11:46.281290 UTC] Computing logging information
-------------------------------------
| Iteration            | 1399       |
| ExpectedImprovement  | 0.018711   |
| ActualImprovement    | 0.017118   |
| ImprovementRatio     | 0.91489    |
| MeanKL               | 0.007427   |
| Entropy              | -1.6467    |
| Perplexity           | 0.19268    |
| AveragePolicyStd     | 0.18603    |
| AveragePolicyStd[0]  | 0.20349    |
| AveragePolicyStd[1]  | 0.205      |
| AveragePolicyStd[2]  | 0.14699    |
| AveragePolicyStd[3]  | 0.18411    |
| AveragePolicyStd[4]  | 0.15368    |
| AveragePolicyStd[5]  | 0.22291    |
| AverageReturn        | 1675.9     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 447.53     |
| AverageEpisodeLength | 895.83     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.82     |
| TotalNEpisodes       | 23987      |
| TotalNSamples        | 7.0019e+06 |
| ExplainedVariance    | 0.25648    |
-------------------------------------
[2018-01-21 16:11:47.109953 UTC] Saving snapshot
[2018-01-21 16:11:47.110263 UTC] Starting iteration 1400
[2018-01-21 16:11:47.110497 UTC] Start collecting samples
[2018-01-21 16:11:51.697016 UTC] Computing input variables for policy optimization
[2018-01-21 16:11:51.828272 UTC] Performing policy update
[2018-01-21 16:11:51.829353 UTC] Computing gradient in Euclidean space
[2018-01-21 16:11:51.969328 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:11:53.352822 UTC] Performing line search
[2018-01-21 16:11:53.545580 UTC] Updating baseline
[2018-01-21 16:11:56.237408 UTC] Computing logging information
-------------------------------------
| Iteration            | 1400       |
| ExpectedImprovement  | 0.020231   |
| ActualImprovement    | 0.019166   |
| ImprovementRatio     | 0.94736    |
| MeanKL               | 0.007251   |
| Entropy              | -1.657     |
| Perplexity           | 0.1907     |
| AveragePolicyStd     | 0.18569    |
| AveragePolicyStd[0]  | 0.20283    |
| AveragePolicyStd[1]  | 0.20495    |
| AveragePolicyStd[2]  | 0.14708    |
| AveragePolicyStd[3]  | 0.18402    |
| AveragePolicyStd[4]  | 0.15323    |
| AveragePolicyStd[5]  | 0.22202    |
| AverageReturn        | 1680.9     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 444.42     |
| AverageEpisodeLength | 899.25     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.78     |
| TotalNEpisodes       | 23991      |
| TotalNSamples        | 7.0059e+06 |
| ExplainedVariance    | -0.011285  |
-------------------------------------
[2018-01-21 16:11:57.058962 UTC] Saving snapshot
[2018-01-21 16:11:57.066032 UTC] Starting iteration 1401
[2018-01-21 16:11:57.066219 UTC] Start collecting samples
[2018-01-21 16:12:01.592836 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:01.728663 UTC] Performing policy update
[2018-01-21 16:12:01.729271 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:01.844116 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:03.198143 UTC] Performing line search
[2018-01-21 16:12:03.385897 UTC] Updating baseline
[2018-01-21 16:12:05.712031 UTC] Computing logging information
-------------------------------------
| Iteration            | 1401       |
| ExpectedImprovement  | 0.017608   |
| ActualImprovement    | 0.016917   |
| ImprovementRatio     | 0.96079    |
| MeanKL               | 0.0082081  |
| Entropy              | -1.6591    |
| Perplexity           | 0.19032    |
| AveragePolicyStd     | 0.18565    |
| AveragePolicyStd[0]  | 0.20298    |
| AveragePolicyStd[1]  | 0.20512    |
| AveragePolicyStd[2]  | 0.14675    |
| AveragePolicyStd[3]  | 0.18428    |
| AveragePolicyStd[4]  | 0.153      |
| AveragePolicyStd[5]  | 0.22175    |
| AverageReturn        | 1684.2     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 443.31     |
| AverageEpisodeLength | 901.91     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.38     |
| TotalNEpisodes       | 23999      |
| TotalNSamples        | 7.0139e+06 |
| ExplainedVariance    | 0.0013172  |
-------------------------------------
[2018-01-21 16:12:06.482451 UTC] Saving snapshot
[2018-01-21 16:12:06.482868 UTC] Starting iteration 1402
[2018-01-21 16:12:06.483159 UTC] Start collecting samples
[2018-01-21 16:12:10.985056 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:11.129088 UTC] Performing policy update
[2018-01-21 16:12:11.129730 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:11.249977 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:12.617799 UTC] Performing line search
[2018-01-21 16:12:12.806728 UTC] Updating baseline
[2018-01-21 16:12:14.944319 UTC] Computing logging information
-------------------------------------
| Iteration            | 1402       |
| ExpectedImprovement  | 0.017245   |
| ActualImprovement    | 0.016518   |
| ImprovementRatio     | 0.95786    |
| MeanKL               | 0.0078368  |
| Entropy              | -1.6585    |
| Perplexity           | 0.19042    |
| AveragePolicyStd     | 0.18564    |
| AveragePolicyStd[0]  | 0.20269    |
| AveragePolicyStd[1]  | 0.20488    |
| AveragePolicyStd[2]  | 0.14693    |
| AveragePolicyStd[3]  | 0.18414    |
| AveragePolicyStd[4]  | 0.15325    |
| AveragePolicyStd[5]  | 0.22198    |
| AverageReturn        | 1698.4     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 425.27     |
| AverageEpisodeLength | 909.23     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.34     |
| TotalNEpisodes       | 24003      |
| TotalNSamples        | 7.0179e+06 |
| ExplainedVariance    | 0.00031502 |
-------------------------------------
[2018-01-21 16:12:15.759403 UTC] Saving snapshot
[2018-01-21 16:12:15.759660 UTC] Starting iteration 1403
[2018-01-21 16:12:15.759835 UTC] Start collecting samples
[2018-01-21 16:12:20.323871 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:20.472496 UTC] Performing policy update
[2018-01-21 16:12:20.473254 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:20.602355 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:22.000276 UTC] Performing line search
[2018-01-21 16:12:22.191269 UTC] Updating baseline
[2018-01-21 16:12:24.174665 UTC] Computing logging information
-------------------------------------
| Iteration            | 1403       |
| ExpectedImprovement  | 0.018153   |
| ActualImprovement    | 0.01647    |
| ImprovementRatio     | 0.9073     |
| MeanKL               | 0.0078587  |
| Entropy              | -1.6478    |
| Perplexity           | 0.19247    |
| AveragePolicyStd     | 0.186      |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.20518    |
| AveragePolicyStd[2]  | 0.1472     |
| AveragePolicyStd[3]  | 0.18454    |
| AveragePolicyStd[4]  | 0.15309    |
| AveragePolicyStd[5]  | 0.22228    |
| AverageReturn        | 1697.6     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 424.97     |
| AverageEpisodeLength | 909.23     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.34     |
| TotalNEpisodes       | 24006      |
| TotalNSamples        | 7.0209e+06 |
| ExplainedVariance    | 0.00091401 |
-------------------------------------
[2018-01-21 16:12:24.988835 UTC] Saving snapshot
[2018-01-21 16:12:24.989062 UTC] Starting iteration 1404
[2018-01-21 16:12:24.989205 UTC] Start collecting samples
[2018-01-21 16:12:29.658709 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:29.800098 UTC] Performing policy update
[2018-01-21 16:12:29.800743 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:29.941697 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:31.410904 UTC] Performing line search
[2018-01-21 16:12:31.596923 UTC] Updating baseline
[2018-01-21 16:12:33.604961 UTC] Computing logging information
-------------------------------------
| Iteration            | 1404       |
| ExpectedImprovement  | 0.0162     |
| ActualImprovement    | 0.015953   |
| ImprovementRatio     | 0.98471    |
| MeanKL               | 0.0076404  |
| Entropy              | -1.6501    |
| Perplexity           | 0.19202    |
| AveragePolicyStd     | 0.18593    |
| AveragePolicyStd[0]  | 0.20369    |
| AveragePolicyStd[1]  | 0.20485    |
| AveragePolicyStd[2]  | 0.14698    |
| AveragePolicyStd[3]  | 0.18422    |
| AveragePolicyStd[4]  | 0.15325    |
| AveragePolicyStd[5]  | 0.22258    |
| AverageReturn        | 1724.9     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 391.97     |
| AverageEpisodeLength | 924.04     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.05     |
| TotalNEpisodes       | 24015      |
| TotalNSamples        | 7.0299e+06 |
| ExplainedVariance    | 0.0041394  |
-------------------------------------
[2018-01-21 16:12:34.401805 UTC] Saving snapshot
[2018-01-21 16:12:34.402062 UTC] Starting iteration 1405
[2018-01-21 16:12:34.402280 UTC] Start collecting samples
[2018-01-21 16:12:38.718103 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:38.853043 UTC] Performing policy update
[2018-01-21 16:12:38.857124 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:38.987394 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:40.382210 UTC] Performing line search
[2018-01-21 16:12:40.569105 UTC] Updating baseline
[2018-01-21 16:12:42.844692 UTC] Computing logging information
-------------------------------------
| Iteration            | 1405       |
| ExpectedImprovement  | 0.017272   |
| ActualImprovement    | 0.016479   |
| ImprovementRatio     | 0.95407    |
| MeanKL               | 0.0082875  |
| Entropy              | -1.6463    |
| Perplexity           | 0.19275    |
| AveragePolicyStd     | 0.18607    |
| AveragePolicyStd[0]  | 0.20379    |
| AveragePolicyStd[1]  | 0.20472    |
| AveragePolicyStd[2]  | 0.14712    |
| AveragePolicyStd[3]  | 0.18408    |
| AveragePolicyStd[4]  | 0.15323    |
| AveragePolicyStd[5]  | 0.22348    |
| AverageReturn        | 1735.4     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 378.41     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.78     |
| TotalNEpisodes       | 24020      |
| TotalNSamples        | 7.0345e+06 |
| ExplainedVariance    | 0.08247    |
-------------------------------------
[2018-01-21 16:12:43.640624 UTC] Saving snapshot
[2018-01-21 16:12:43.640803 UTC] Starting iteration 1406
[2018-01-21 16:12:43.640902 UTC] Start collecting samples
[2018-01-21 16:12:47.933797 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:48.081074 UTC] Performing policy update
[2018-01-21 16:12:48.082099 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:48.212214 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:49.796505 UTC] Performing line search
[2018-01-21 16:12:50.043192 UTC] Updating baseline
[2018-01-21 16:12:52.369786 UTC] Computing logging information
-------------------------------------
| Iteration            | 1406       |
| ExpectedImprovement  | 0.021433   |
| ActualImprovement    | 0.019876   |
| ImprovementRatio     | 0.92737    |
| MeanKL               | 0.0072031  |
| Entropy              | -1.6483    |
| Perplexity           | 0.19237    |
| AveragePolicyStd     | 0.18605    |
| AveragePolicyStd[0]  | 0.20375    |
| AveragePolicyStd[1]  | 0.20492    |
| AveragePolicyStd[2]  | 0.14685    |
| AveragePolicyStd[3]  | 0.1843     |
| AveragePolicyStd[4]  | 0.15271    |
| AveragePolicyStd[5]  | 0.22375    |
| AverageReturn        | 1732.4     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 382.9      |
| AverageEpisodeLength | 929.32     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.03     |
| TotalNEpisodes       | 24023      |
| TotalNSamples        | 7.0369e+06 |
| ExplainedVariance    | 0.20399    |
-------------------------------------
[2018-01-21 16:12:53.244787 UTC] Saving snapshot
[2018-01-21 16:12:53.245067 UTC] Starting iteration 1407
[2018-01-21 16:12:53.245314 UTC] Start collecting samples
[2018-01-21 16:12:57.844529 UTC] Computing input variables for policy optimization
[2018-01-21 16:12:57.986711 UTC] Performing policy update
[2018-01-21 16:12:57.987810 UTC] Computing gradient in Euclidean space
[2018-01-21 16:12:58.103773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:12:59.515202 UTC] Performing line search
[2018-01-21 16:12:59.705247 UTC] Updating baseline
[2018-01-21 16:13:01.317335 UTC] Computing logging information
-------------------------------------
| Iteration            | 1407       |
| ExpectedImprovement  | 0.016497   |
| ActualImprovement    | 0.015495   |
| ImprovementRatio     | 0.93927    |
| MeanKL               | 0.0081799  |
| Entropy              | -1.6487    |
| Perplexity           | 0.1923     |
| AveragePolicyStd     | 0.18604    |
| AveragePolicyStd[0]  | 0.20382    |
| AveragePolicyStd[1]  | 0.20516    |
| AveragePolicyStd[2]  | 0.14704    |
| AveragePolicyStd[3]  | 0.18374    |
| AveragePolicyStd[4]  | 0.15264    |
| AveragePolicyStd[5]  | 0.22383    |
| AverageReturn        | 1738.6     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 378.94     |
| AverageEpisodeLength | 932.85     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.05     |
| TotalNEpisodes       | 24032      |
| TotalNSamples        | 7.0459e+06 |
| ExplainedVariance    | -0.0092658 |
-------------------------------------
[2018-01-21 16:13:02.133334 UTC] Saving snapshot
[2018-01-21 16:13:02.133548 UTC] Starting iteration 1408
[2018-01-21 16:13:02.133705 UTC] Start collecting samples
[2018-01-21 16:13:06.665479 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:06.799791 UTC] Performing policy update
[2018-01-21 16:13:06.800732 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:06.929227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:08.318341 UTC] Performing line search
[2018-01-21 16:13:08.508367 UTC] Updating baseline
[2018-01-21 16:13:10.858524 UTC] Computing logging information
-------------------------------------
| Iteration            | 1408       |
| ExpectedImprovement  | 0.019221   |
| ActualImprovement    | 0.018291   |
| ImprovementRatio     | 0.95162    |
| MeanKL               | 0.0080431  |
| Entropy              | -1.653     |
| Perplexity           | 0.19148    |
| AveragePolicyStd     | 0.18591    |
| AveragePolicyStd[0]  | 0.20391    |
| AveragePolicyStd[1]  | 0.20486    |
| AveragePolicyStd[2]  | 0.14726    |
| AveragePolicyStd[3]  | 0.18331    |
| AveragePolicyStd[4]  | 0.15224    |
| AveragePolicyStd[5]  | 0.22386    |
| AverageReturn        | 1730.2     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 399.78     |
| AverageEpisodeLength | 928.91     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202        |
| TotalNEpisodes       | 24038      |
| TotalNSamples        | 7.0504e+06 |
| ExplainedVariance    | 0.15562    |
-------------------------------------
[2018-01-21 16:13:11.758219 UTC] Saving snapshot
[2018-01-21 16:13:11.758499 UTC] Starting iteration 1409
[2018-01-21 16:13:11.758683 UTC] Start collecting samples
[2018-01-21 16:13:16.199418 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:16.327627 UTC] Performing policy update
[2018-01-21 16:13:16.328226 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:16.442683 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:17.799522 UTC] Performing line search
[2018-01-21 16:13:17.989753 UTC] Updating baseline
[2018-01-21 16:13:21.145267 UTC] Computing logging information
-------------------------------------
| Iteration            | 1409       |
| ExpectedImprovement  | 0.019397   |
| ActualImprovement    | 0.018291   |
| ImprovementRatio     | 0.94296    |
| MeanKL               | 0.007844   |
| Entropy              | -1.6586    |
| Perplexity           | 0.1904     |
| AveragePolicyStd     | 0.18571    |
| AveragePolicyStd[0]  | 0.20416    |
| AveragePolicyStd[1]  | 0.20453    |
| AveragePolicyStd[2]  | 0.14748    |
| AveragePolicyStd[3]  | 0.18292    |
| AveragePolicyStd[4]  | 0.152      |
| AveragePolicyStd[5]  | 0.22319    |
| AverageReturn        | 1729.1     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 399.39     |
| AverageEpisodeLength | 928.91     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202        |
| TotalNEpisodes       | 24040      |
| TotalNSamples        | 7.0524e+06 |
| ExplainedVariance    | -0.0086913 |
-------------------------------------
[2018-01-21 16:13:21.933112 UTC] Saving snapshot
[2018-01-21 16:13:21.933329 UTC] Starting iteration 1410
[2018-01-21 16:13:21.933505 UTC] Start collecting samples
[2018-01-21 16:13:26.448476 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:26.581329 UTC] Performing policy update
[2018-01-21 16:13:26.582104 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:26.707246 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:28.124886 UTC] Performing line search
[2018-01-21 16:13:28.318746 UTC] Updating baseline
[2018-01-21 16:13:30.642314 UTC] Computing logging information
-------------------------------------
| Iteration            | 1410       |
| ExpectedImprovement  | 0.01726    |
| ActualImprovement    | 0.015978   |
| ImprovementRatio     | 0.92571    |
| MeanKL               | 0.008344   |
| Entropy              | -1.6599    |
| Perplexity           | 0.19015    |
| AveragePolicyStd     | 0.18568    |
| AveragePolicyStd[0]  | 0.20406    |
| AveragePolicyStd[1]  | 0.20474    |
| AveragePolicyStd[2]  | 0.14728    |
| AveragePolicyStd[3]  | 0.18283    |
| AveragePolicyStd[4]  | 0.152      |
| AveragePolicyStd[5]  | 0.2232     |
| AverageReturn        | 1736.2     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 398.29     |
| AverageEpisodeLength | 931.49     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.24     |
| TotalNEpisodes       | 24047      |
| TotalNSamples        | 7.0594e+06 |
| ExplainedVariance    | -0.0024166 |
-------------------------------------
[2018-01-21 16:13:31.577265 UTC] Saving snapshot
[2018-01-21 16:13:31.586578 UTC] Starting iteration 1411
[2018-01-21 16:13:31.586803 UTC] Start collecting samples
[2018-01-21 16:13:36.308680 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:36.457885 UTC] Performing policy update
[2018-01-21 16:13:36.459198 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:36.582157 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:37.943032 UTC] Performing line search
[2018-01-21 16:13:38.143020 UTC] Updating baseline
[2018-01-21 16:13:40.029252 UTC] Computing logging information
-------------------------------------
| Iteration            | 1411       |
| ExpectedImprovement  | 0.019259   |
| ActualImprovement    | 0.017719   |
| ImprovementRatio     | 0.92004    |
| MeanKL               | 0.0077073  |
| Entropy              | -1.6599    |
| Perplexity           | 0.19016    |
| AveragePolicyStd     | 0.1857     |
| AveragePolicyStd[0]  | 0.20365    |
| AveragePolicyStd[1]  | 0.20512    |
| AveragePolicyStd[2]  | 0.1471     |
| AveragePolicyStd[3]  | 0.18228    |
| AveragePolicyStd[4]  | 0.15229    |
| AveragePolicyStd[5]  | 0.22377    |
| AverageReturn        | 1733.9     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 397.8      |
| AverageEpisodeLength | 931.24     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.17     |
| TotalNEpisodes       | 24054      |
| TotalNSamples        | 7.0663e+06 |
| ExplainedVariance    | 0.051054   |
-------------------------------------
[2018-01-21 16:13:40.882302 UTC] Saving snapshot
[2018-01-21 16:13:40.882835 UTC] Starting iteration 1412
[2018-01-21 16:13:40.883240 UTC] Start collecting samples
[2018-01-21 16:13:45.424825 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:45.590795 UTC] Performing policy update
[2018-01-21 16:13:45.593961 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:45.706956 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:47.114860 UTC] Performing line search
[2018-01-21 16:13:47.307639 UTC] Updating baseline
[2018-01-21 16:13:49.515413 UTC] Computing logging information
-------------------------------------
| Iteration            | 1412       |
| ExpectedImprovement  | 0.018006   |
| ActualImprovement    | 0.018123   |
| ImprovementRatio     | 1.0065     |
| MeanKL               | 0.008037   |
| Entropy              | -1.6597    |
| Perplexity           | 0.1902     |
| AveragePolicyStd     | 0.18572    |
| AveragePolicyStd[0]  | 0.20397    |
| AveragePolicyStd[1]  | 0.20552    |
| AveragePolicyStd[2]  | 0.14732    |
| AveragePolicyStd[3]  | 0.18187    |
| AveragePolicyStd[4]  | 0.15193    |
| AveragePolicyStd[5]  | 0.22373    |
| AverageReturn        | 1722       |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 410.72     |
| AverageEpisodeLength | 925.27     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.79     |
| TotalNEpisodes       | 24056      |
| TotalNSamples        | 7.0677e+06 |
| ExplainedVariance    | 0.18079    |
-------------------------------------
[2018-01-21 16:13:50.391616 UTC] Saving snapshot
[2018-01-21 16:13:50.391849 UTC] Starting iteration 1413
[2018-01-21 16:13:50.392034 UTC] Start collecting samples
[2018-01-21 16:13:55.170893 UTC] Computing input variables for policy optimization
[2018-01-21 16:13:55.294458 UTC] Performing policy update
[2018-01-21 16:13:55.295041 UTC] Computing gradient in Euclidean space
[2018-01-21 16:13:55.411359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:13:56.807341 UTC] Performing line search
[2018-01-21 16:13:57.025179 UTC] Updating baseline
[2018-01-21 16:13:59.247274 UTC] Computing logging information
-------------------------------------
| Iteration            | 1413       |
| ExpectedImprovement  | 0.018565   |
| ActualImprovement    | 0.017391   |
| ImprovementRatio     | 0.93678    |
| MeanKL               | 0.0089798  |
| Entropy              | -1.6567    |
| Perplexity           | 0.19076    |
| AveragePolicyStd     | 0.18579    |
| AveragePolicyStd[0]  | 0.20391    |
| AveragePolicyStd[1]  | 0.20553    |
| AveragePolicyStd[2]  | 0.14747    |
| AveragePolicyStd[3]  | 0.18236    |
| AveragePolicyStd[4]  | 0.15205    |
| AveragePolicyStd[5]  | 0.22341    |
| AverageReturn        | 1723.9     |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 409.16     |
| AverageEpisodeLength | 925.9      |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.82     |
| TotalNEpisodes       | 24062      |
| TotalNSamples        | 7.0734e+06 |
| ExplainedVariance    | 0.088417   |
-------------------------------------
[2018-01-21 16:14:00.069984 UTC] Saving snapshot
[2018-01-21 16:14:00.070222 UTC] Starting iteration 1414
[2018-01-21 16:14:00.070391 UTC] Start collecting samples
[2018-01-21 16:14:04.412026 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:04.536829 UTC] Performing policy update
[2018-01-21 16:14:04.538336 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:04.656659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:06.128243 UTC] Performing line search
[2018-01-21 16:14:06.319268 UTC] Updating baseline
[2018-01-21 16:14:08.237980 UTC] Computing logging information
-------------------------------------
| Iteration            | 1414       |
| ExpectedImprovement  | 0.018206   |
| ActualImprovement    | 0.017265   |
| ImprovementRatio     | 0.94832    |
| MeanKL               | 0.0075106  |
| Entropy              | -1.6599    |
| Perplexity           | 0.19016    |
| AveragePolicyStd     | 0.18565    |
| AveragePolicyStd[0]  | 0.20385    |
| AveragePolicyStd[1]  | 0.20498    |
| AveragePolicyStd[2]  | 0.14767    |
| AveragePolicyStd[3]  | 0.18217    |
| AveragePolicyStd[4]  | 0.15232    |
| AveragePolicyStd[5]  | 0.22293    |
| AverageReturn        | 1736       |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 392.41     |
| AverageEpisodeLength | 932.14     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.42     |
| TotalNEpisodes       | 24068      |
| TotalNSamples        | 7.0794e+06 |
| ExplainedVariance    | 0.01458    |
-------------------------------------
[2018-01-21 16:14:09.027841 UTC] Saving snapshot
[2018-01-21 16:14:09.028073 UTC] Starting iteration 1415
[2018-01-21 16:14:09.028233 UTC] Start collecting samples
[2018-01-21 16:14:13.411734 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:13.528277 UTC] Performing policy update
[2018-01-21 16:14:13.529345 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:13.644654 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:14.985207 UTC] Performing line search
[2018-01-21 16:14:15.179351 UTC] Updating baseline
[2018-01-21 16:14:17.312222 UTC] Computing logging information
-------------------------------------
| Iteration            | 1415       |
| ExpectedImprovement  | 0.01705    |
| ActualImprovement    | 0.015338   |
| ImprovementRatio     | 0.89957    |
| MeanKL               | 0.0076946  |
| Entropy              | -1.66      |
| Perplexity           | 0.19014    |
| AveragePolicyStd     | 0.18563    |
| AveragePolicyStd[0]  | 0.20411    |
| AveragePolicyStd[1]  | 0.20496    |
| AveragePolicyStd[2]  | 0.14773    |
| AveragePolicyStd[3]  | 0.1821     |
| AveragePolicyStd[4]  | 0.1524     |
| AveragePolicyStd[5]  | 0.22251    |
| AverageReturn        | 1737       |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 392.76     |
| AverageEpisodeLength | 932.43     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.5      |
| TotalNEpisodes       | 24071      |
| TotalNSamples        | 7.0824e+06 |
| ExplainedVariance    | 0.02002    |
-------------------------------------
[2018-01-21 16:14:18.090257 UTC] Saving snapshot
[2018-01-21 16:14:18.090475 UTC] Starting iteration 1416
[2018-01-21 16:14:18.090663 UTC] Start collecting samples
[2018-01-21 16:14:22.653825 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:22.781401 UTC] Performing policy update
[2018-01-21 16:14:22.782032 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:22.907670 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:24.284886 UTC] Performing line search
[2018-01-21 16:14:24.479881 UTC] Updating baseline
[2018-01-21 16:14:26.492266 UTC] Computing logging information
-------------------------------------
| Iteration            | 1416       |
| ExpectedImprovement  | 0.018002   |
| ActualImprovement    | 0.017111   |
| ImprovementRatio     | 0.95051    |
| MeanKL               | 0.0081456  |
| Entropy              | -1.6633    |
| Perplexity           | 0.18952    |
| AveragePolicyStd     | 0.1855     |
| AveragePolicyStd[0]  | 0.20396    |
| AveragePolicyStd[1]  | 0.20495    |
| AveragePolicyStd[2]  | 0.14757    |
| AveragePolicyStd[3]  | 0.18166    |
| AveragePolicyStd[4]  | 0.15294    |
| AveragePolicyStd[5]  | 0.22194    |
| AverageReturn        | 1739       |
| MinReturn            | 201.47     |
| MaxReturn            | 1949.2     |
| StdReturn            | 387.57     |
| AverageEpisodeLength | 933.61     |
| MinEpisodeLength     | 150        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.44     |
| TotalNEpisodes       | 24076      |
| TotalNSamples        | 7.0869e+06 |
| ExplainedVariance    | 0.099371   |
-------------------------------------
[2018-01-21 16:14:27.318956 UTC] Saving snapshot
[2018-01-21 16:14:27.319209 UTC] Starting iteration 1417
[2018-01-21 16:14:27.319389 UTC] Start collecting samples
[2018-01-21 16:14:32.082424 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:32.212719 UTC] Performing policy update
[2018-01-21 16:14:32.213366 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:32.328435 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:33.728711 UTC] Performing line search
[2018-01-21 16:14:33.941175 UTC] Updating baseline
[2018-01-21 16:14:35.814179 UTC] Computing logging information
-------------------------------------
| Iteration            | 1417       |
| ExpectedImprovement  | 0.019337   |
| ActualImprovement    | 0.018819   |
| ImprovementRatio     | 0.97321    |
| MeanKL               | 0.0077785  |
| Entropy              | -1.666     |
| Perplexity           | 0.189      |
| AveragePolicyStd     | 0.1854     |
| AveragePolicyStd[0]  | 0.20406    |
| AveragePolicyStd[1]  | 0.20448    |
| AveragePolicyStd[2]  | 0.14762    |
| AveragePolicyStd[3]  | 0.18224    |
| AveragePolicyStd[4]  | 0.1527     |
| AveragePolicyStd[5]  | 0.2213     |
| AverageReturn        | 1757       |
| MinReturn            | 57.825     |
| MaxReturn            | 1940.1     |
| StdReturn            | 367.85     |
| AverageEpisodeLength | 943.29     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.83     |
| TotalNEpisodes       | 24084      |
| TotalNSamples        | 7.0939e+06 |
| ExplainedVariance    | 0.13859    |
-------------------------------------
[2018-01-21 16:14:36.709739 UTC] Saving snapshot
[2018-01-21 16:14:36.710046 UTC] Starting iteration 1418
[2018-01-21 16:14:36.710217 UTC] Start collecting samples
[2018-01-21 16:14:41.222833 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:41.362279 UTC] Performing policy update
[2018-01-21 16:14:41.363028 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:41.479916 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:42.878983 UTC] Performing line search
[2018-01-21 16:14:43.083459 UTC] Updating baseline
[2018-01-21 16:14:44.839065 UTC] Computing logging information
-------------------------------------
| Iteration            | 1418       |
| ExpectedImprovement  | 0.016089   |
| ActualImprovement    | 0.015399   |
| ImprovementRatio     | 0.95709    |
| MeanKL               | 0.0078703  |
| Entropy              | -1.6722    |
| Perplexity           | 0.18783    |
| AveragePolicyStd     | 0.18521    |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.2046     |
| AveragePolicyStd[2]  | 0.14743    |
| AveragePolicyStd[3]  | 0.18189    |
| AveragePolicyStd[4]  | 0.15257    |
| AveragePolicyStd[5]  | 0.22112    |
| AverageReturn        | 1754.2     |
| MinReturn            | 57.825     |
| MaxReturn            | 1940.1     |
| StdReturn            | 373.43     |
| AverageEpisodeLength | 942.52     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.69     |
| TotalNEpisodes       | 24088      |
| TotalNSamples        | 7.0971e+06 |
| ExplainedVariance    | -0.041186  |
-------------------------------------
[2018-01-21 16:14:45.642575 UTC] Saving snapshot
[2018-01-21 16:14:45.642827 UTC] Starting iteration 1419
[2018-01-21 16:14:45.643012 UTC] Start collecting samples
[2018-01-21 16:14:50.318361 UTC] Computing input variables for policy optimization
[2018-01-21 16:14:50.441375 UTC] Performing policy update
[2018-01-21 16:14:50.442541 UTC] Computing gradient in Euclidean space
[2018-01-21 16:14:50.568651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:14:51.943666 UTC] Performing line search
[2018-01-21 16:14:52.133187 UTC] Updating baseline
[2018-01-21 16:14:54.612425 UTC] Computing logging information
--------------------------------------
| Iteration            | 1419        |
| ExpectedImprovement  | 0.016866    |
| ActualImprovement    | 0.015934    |
| ImprovementRatio     | 0.94478     |
| MeanKL               | 0.0077603   |
| Entropy              | -1.6686     |
| Perplexity           | 0.18851     |
| AveragePolicyStd     | 0.18531     |
| AveragePolicyStd[0]  | 0.2042      |
| AveragePolicyStd[1]  | 0.20454     |
| AveragePolicyStd[2]  | 0.1477      |
| AveragePolicyStd[3]  | 0.18152     |
| AveragePolicyStd[4]  | 0.15291     |
| AveragePolicyStd[5]  | 0.22098     |
| AverageReturn        | 1756.2      |
| MinReturn            | 57.825      |
| MaxReturn            | 1940.1      |
| StdReturn            | 373.69      |
| AverageEpisodeLength | 942.52      |
| MinEpisodeLength     | 72          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 189.69      |
| TotalNEpisodes       | 24093       |
| TotalNSamples        | 7.1021e+06  |
| ExplainedVariance    | -0.00030447 |
--------------------------------------
[2018-01-21 16:14:55.511634 UTC] Saving snapshot
[2018-01-21 16:14:55.511877 UTC] Starting iteration 1420
[2018-01-21 16:14:55.512058 UTC] Start collecting samples
[2018-01-21 16:15:00.164807 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:00.316496 UTC] Performing policy update
[2018-01-21 16:15:00.317774 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:00.445345 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:01.830784 UTC] Performing line search
[2018-01-21 16:15:02.025555 UTC] Updating baseline
[2018-01-21 16:15:04.096308 UTC] Computing logging information
-------------------------------------
| Iteration            | 1420       |
| ExpectedImprovement  | 0.01835    |
| ActualImprovement    | 0.017509   |
| ImprovementRatio     | 0.95421    |
| MeanKL               | 0.0077099  |
| Entropy              | -1.6738    |
| Perplexity           | 0.18754    |
| AveragePolicyStd     | 0.18513    |
| AveragePolicyStd[0]  | 0.2045     |
| AveragePolicyStd[1]  | 0.20402    |
| AveragePolicyStd[2]  | 0.14751    |
| AveragePolicyStd[3]  | 0.18172    |
| AveragePolicyStd[4]  | 0.15283    |
| AveragePolicyStd[5]  | 0.22021    |
| AverageReturn        | 1737.4     |
| MinReturn            | 57.825     |
| MaxReturn            | 1940.1     |
| StdReturn            | 395.22     |
| AverageEpisodeLength | 933.56     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.07     |
| TotalNEpisodes       | 24102      |
| TotalNSamples        | 7.1102e+06 |
| ExplainedVariance    | 0.11256    |
-------------------------------------
[2018-01-21 16:15:04.939278 UTC] Saving snapshot
[2018-01-21 16:15:04.951325 UTC] Starting iteration 1421
[2018-01-21 16:15:04.951557 UTC] Start collecting samples
[2018-01-21 16:15:09.595963 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:09.720017 UTC] Performing policy update
[2018-01-21 16:15:09.721080 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:09.840698 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:11.205824 UTC] Performing line search
[2018-01-21 16:15:11.417246 UTC] Updating baseline
[2018-01-21 16:15:13.071545 UTC] Computing logging information
--------------------------------------
| Iteration            | 1421        |
| ExpectedImprovement  | 0.017983    |
| ActualImprovement    | 0.017189    |
| ImprovementRatio     | 0.95585     |
| MeanKL               | 0.0078659   |
| Entropy              | -1.6734     |
| Perplexity           | 0.1876      |
| AveragePolicyStd     | 0.18517     |
| AveragePolicyStd[0]  | 0.20468     |
| AveragePolicyStd[1]  | 0.2039      |
| AveragePolicyStd[2]  | 0.14734     |
| AveragePolicyStd[3]  | 0.18172     |
| AveragePolicyStd[4]  | 0.15272     |
| AveragePolicyStd[5]  | 0.22065     |
| AverageReturn        | 1736.7      |
| MinReturn            | 57.825      |
| MaxReturn            | 1940.1      |
| StdReturn            | 395.04      |
| AverageEpisodeLength | 933.56      |
| MinEpisodeLength     | 72          |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 200.07      |
| TotalNEpisodes       | 24105       |
| TotalNSamples        | 7.1132e+06  |
| ExplainedVariance    | -0.00039647 |
--------------------------------------
[2018-01-21 16:15:13.843118 UTC] Saving snapshot
[2018-01-21 16:15:13.843527 UTC] Starting iteration 1422
[2018-01-21 16:15:13.843724 UTC] Start collecting samples
[2018-01-21 16:15:18.462009 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:18.596633 UTC] Performing policy update
[2018-01-21 16:15:18.597770 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:18.736252 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:20.169853 UTC] Performing line search
[2018-01-21 16:15:20.373766 UTC] Updating baseline
[2018-01-21 16:15:22.406217 UTC] Computing logging information
-------------------------------------
| Iteration            | 1422       |
| ExpectedImprovement  | 0.019692   |
| ActualImprovement    | 0.017972   |
| ImprovementRatio     | 0.91264    |
| MeanKL               | 0.0080557  |
| Entropy              | -1.6695    |
| Perplexity           | 0.18835    |
| AveragePolicyStd     | 0.18531    |
| AveragePolicyStd[0]  | 0.20507    |
| AveragePolicyStd[1]  | 0.20387    |
| AveragePolicyStd[2]  | 0.14732    |
| AveragePolicyStd[3]  | 0.18192    |
| AveragePolicyStd[4]  | 0.15265    |
| AveragePolicyStd[5]  | 0.22103    |
| AverageReturn        | 1726.2     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 406.15     |
| AverageEpisodeLength | 928.1      |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.56     |
| TotalNEpisodes       | 24110      |
| TotalNSamples        | 7.1177e+06 |
| ExplainedVariance    | 0.12792    |
-------------------------------------
[2018-01-21 16:15:23.242099 UTC] Saving snapshot
[2018-01-21 16:15:23.242379 UTC] Starting iteration 1423
[2018-01-21 16:15:23.242599 UTC] Start collecting samples
[2018-01-21 16:15:27.814983 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:27.946060 UTC] Performing policy update
[2018-01-21 16:15:27.946742 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:28.063149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:29.463458 UTC] Performing line search
[2018-01-21 16:15:29.653669 UTC] Updating baseline
[2018-01-21 16:15:31.791751 UTC] Computing logging information
-------------------------------------
| Iteration            | 1423       |
| ExpectedImprovement  | 0.020091   |
| ActualImprovement    | 0.018843   |
| ImprovementRatio     | 0.93789    |
| MeanKL               | 0.0078291  |
| Entropy              | -1.6669    |
| Perplexity           | 0.18883    |
| AveragePolicyStd     | 0.18538    |
| AveragePolicyStd[0]  | 0.20517    |
| AveragePolicyStd[1]  | 0.20387    |
| AveragePolicyStd[2]  | 0.14756    |
| AveragePolicyStd[3]  | 0.1817     |
| AveragePolicyStd[4]  | 0.1528     |
| AveragePolicyStd[5]  | 0.22117    |
| AverageReturn        | 1734.7     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 402.11     |
| AverageEpisodeLength | 931.91     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203.31     |
| TotalNEpisodes       | 24118      |
| TotalNSamples        | 7.1257e+06 |
| ExplainedVariance    | -0.0029342 |
-------------------------------------
[2018-01-21 16:15:32.625540 UTC] Saving snapshot
[2018-01-21 16:15:32.625748 UTC] Starting iteration 1424
[2018-01-21 16:15:32.625877 UTC] Start collecting samples
[2018-01-21 16:15:37.022669 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:37.154565 UTC] Performing policy update
[2018-01-21 16:15:37.155383 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:37.272077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:38.692396 UTC] Performing line search
[2018-01-21 16:15:38.879764 UTC] Updating baseline
[2018-01-21 16:15:40.674181 UTC] Computing logging information
-------------------------------------
| Iteration            | 1424       |
| ExpectedImprovement  | 0.019642   |
| ActualImprovement    | 0.019353   |
| ImprovementRatio     | 0.98529    |
| MeanKL               | 0.0084627  |
| Entropy              | -1.6722    |
| Perplexity           | 0.18783    |
| AveragePolicyStd     | 0.1852     |
| AveragePolicyStd[0]  | 0.20469    |
| AveragePolicyStd[1]  | 0.20364    |
| AveragePolicyStd[2]  | 0.14743    |
| AveragePolicyStd[3]  | 0.18189    |
| AveragePolicyStd[4]  | 0.15273    |
| AveragePolicyStd[5]  | 0.22082    |
| AverageReturn        | 1745.6     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 385.08     |
| AverageEpisodeLength | 938.08     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.55     |
| TotalNEpisodes       | 24121      |
| TotalNSamples        | 7.1287e+06 |
| ExplainedVariance    | 0.065653   |
-------------------------------------
[2018-01-21 16:15:41.504024 UTC] Saving snapshot
[2018-01-21 16:15:41.504238 UTC] Starting iteration 1425
[2018-01-21 16:15:41.504417 UTC] Start collecting samples
[2018-01-21 16:15:45.944960 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:46.065340 UTC] Performing policy update
[2018-01-21 16:15:46.065996 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:46.182619 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:47.555116 UTC] Performing line search
[2018-01-21 16:15:47.749451 UTC] Updating baseline
[2018-01-21 16:15:49.931683 UTC] Computing logging information
-------------------------------------
| Iteration            | 1425       |
| ExpectedImprovement  | 0.015697   |
| ActualImprovement    | 0.015048   |
| ImprovementRatio     | 0.95866    |
| MeanKL               | 0.0089888  |
| Entropy              | -1.6785    |
| Perplexity           | 0.18665    |
| AveragePolicyStd     | 0.18502    |
| AveragePolicyStd[0]  | 0.20473    |
| AveragePolicyStd[1]  | 0.20319    |
| AveragePolicyStd[2]  | 0.14738    |
| AveragePolicyStd[3]  | 0.18121    |
| AveragePolicyStd[4]  | 0.15247    |
| AveragePolicyStd[5]  | 0.22116    |
| AverageReturn        | 1741.5     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 386.7      |
| AverageEpisodeLength | 935.84     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.11     |
| TotalNEpisodes       | 24127      |
| TotalNSamples        | 7.1344e+06 |
| ExplainedVariance    | 0.10249    |
-------------------------------------
[2018-01-21 16:15:50.780422 UTC] Saving snapshot
[2018-01-21 16:15:50.780673 UTC] Starting iteration 1426
[2018-01-21 16:15:50.780856 UTC] Start collecting samples
[2018-01-21 16:15:55.399860 UTC] Computing input variables for policy optimization
[2018-01-21 16:15:55.535840 UTC] Performing policy update
[2018-01-21 16:15:55.536478 UTC] Computing gradient in Euclidean space
[2018-01-21 16:15:55.652363 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:15:57.054726 UTC] Performing line search
[2018-01-21 16:15:57.249524 UTC] Updating baseline
[2018-01-21 16:15:59.520121 UTC] Computing logging information
-------------------------------------
| Iteration            | 1426       |
| ExpectedImprovement  | 0.018068   |
| ActualImprovement    | 0.017558   |
| ImprovementRatio     | 0.97176    |
| MeanKL               | 0.0079195  |
| Entropy              | -1.6778    |
| Perplexity           | 0.18679    |
| AveragePolicyStd     | 0.18508    |
| AveragePolicyStd[0]  | 0.20507    |
| AveragePolicyStd[1]  | 0.20336    |
| AveragePolicyStd[2]  | 0.14726    |
| AveragePolicyStd[3]  | 0.18092    |
| AveragePolicyStd[4]  | 0.1523     |
| AveragePolicyStd[5]  | 0.22157    |
| AverageReturn        | 1753.4     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 362.63     |
| AverageEpisodeLength | 943.09     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.88     |
| TotalNEpisodes       | 24134      |
| TotalNSamples        | 7.1414e+06 |
| ExplainedVariance    | 0.10287    |
-------------------------------------
[2018-01-21 16:16:00.297445 UTC] Saving snapshot
[2018-01-21 16:16:00.297682 UTC] Starting iteration 1427
[2018-01-21 16:16:00.297838 UTC] Start collecting samples
[2018-01-21 16:16:04.814386 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:04.941909 UTC] Performing policy update
[2018-01-21 16:16:04.942984 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:05.066453 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:06.428772 UTC] Performing line search
[2018-01-21 16:16:06.623739 UTC] Updating baseline
[2018-01-21 16:16:08.376426 UTC] Computing logging information
-------------------------------------
| Iteration            | 1427       |
| ExpectedImprovement  | 0.018103   |
| ActualImprovement    | 0.017161   |
| ImprovementRatio     | 0.948      |
| MeanKL               | 0.0088419  |
| Entropy              | -1.6808    |
| Perplexity           | 0.18622    |
| AveragePolicyStd     | 0.18498    |
| AveragePolicyStd[0]  | 0.20522    |
| AveragePolicyStd[1]  | 0.20363    |
| AveragePolicyStd[2]  | 0.14721    |
| AveragePolicyStd[3]  | 0.18105    |
| AveragePolicyStd[4]  | 0.15206    |
| AveragePolicyStd[5]  | 0.2207     |
| AverageReturn        | 1767.2     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 336.7      |
| AverageEpisodeLength | 950.52     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.45     |
| TotalNEpisodes       | 24137      |
| TotalNSamples        | 7.1444e+06 |
| ExplainedVariance    | -0.017028  |
-------------------------------------
[2018-01-21 16:16:09.166931 UTC] Saving snapshot
[2018-01-21 16:16:09.167195 UTC] Starting iteration 1428
[2018-01-21 16:16:09.167377 UTC] Start collecting samples
[2018-01-21 16:16:13.874773 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:14.029635 UTC] Performing policy update
[2018-01-21 16:16:14.030335 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:14.148015 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:15.526156 UTC] Performing line search
[2018-01-21 16:16:15.721596 UTC] Updating baseline
[2018-01-21 16:16:17.951299 UTC] Computing logging information
-------------------------------------
| Iteration            | 1428       |
| ExpectedImprovement  | 0.018481   |
| ActualImprovement    | 0.017536   |
| ImprovementRatio     | 0.94888    |
| MeanKL               | 0.0076756  |
| Entropy              | -1.6817    |
| Perplexity           | 0.18606    |
| AveragePolicyStd     | 0.18491    |
| AveragePolicyStd[0]  | 0.20471    |
| AveragePolicyStd[1]  | 0.20376    |
| AveragePolicyStd[2]  | 0.14749    |
| AveragePolicyStd[3]  | 0.181      |
| AveragePolicyStd[4]  | 0.15233    |
| AveragePolicyStd[5]  | 0.22017    |
| AverageReturn        | 1751       |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 351.84     |
| AverageEpisodeLength | 942.56     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.55     |
| TotalNEpisodes       | 24143      |
| TotalNSamples        | 7.1496e+06 |
| ExplainedVariance    | 0.12895    |
-------------------------------------
[2018-01-21 16:16:18.750494 UTC] Saving snapshot
[2018-01-21 16:16:18.750780 UTC] Starting iteration 1429
[2018-01-21 16:16:18.750965 UTC] Start collecting samples
[2018-01-21 16:16:23.253935 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:23.380344 UTC] Performing policy update
[2018-01-21 16:16:23.381446 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:23.497804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:24.866391 UTC] Performing line search
[2018-01-21 16:16:25.055018 UTC] Updating baseline
[2018-01-21 16:16:26.999673 UTC] Computing logging information
------------------------------------
| Iteration            | 1429      |
| ExpectedImprovement  | 0.02093   |
| ActualImprovement    | 0.019326  |
| ImprovementRatio     | 0.92336   |
| MeanKL               | 0.0087935 |
| Entropy              | -1.6867   |
| Perplexity           | 0.18513   |
| AveragePolicyStd     | 0.18476   |
| AveragePolicyStd[0]  | 0.20487   |
| AveragePolicyStd[1]  | 0.20301   |
| AveragePolicyStd[2]  | 0.14725   |
| AveragePolicyStd[3]  | 0.18091   |
| AveragePolicyStd[4]  | 0.15231   |
| AveragePolicyStd[5]  | 0.22019   |
| AverageReturn        | 1740.2    |
| MinReturn            | 57.825    |
| MaxReturn            | 1938.8    |
| StdReturn            | 357.8     |
| AverageEpisodeLength | 936.77    |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 180.44    |
| TotalNEpisodes       | 24147     |
| TotalNSamples        | 7.153e+06 |
| ExplainedVariance    | 0.26944   |
------------------------------------
[2018-01-21 16:16:27.860698 UTC] Saving snapshot
[2018-01-21 16:16:27.861064 UTC] Starting iteration 1430
[2018-01-21 16:16:27.861274 UTC] Start collecting samples
[2018-01-21 16:16:32.437446 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:32.581107 UTC] Performing policy update
[2018-01-21 16:16:32.582210 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:32.703915 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:34.174300 UTC] Performing line search
[2018-01-21 16:16:34.377260 UTC] Updating baseline
[2018-01-21 16:16:36.181503 UTC] Computing logging information
------------------------------------
| Iteration            | 1430      |
| ExpectedImprovement  | 0.018297  |
| ActualImprovement    | 0.01729   |
| ImprovementRatio     | 0.94496   |
| MeanKL               | 0.007666  |
| Entropy              | -1.6906   |
| Perplexity           | 0.18441   |
| AveragePolicyStd     | 0.18466   |
| AveragePolicyStd[0]  | 0.20467   |
| AveragePolicyStd[1]  | 0.20337   |
| AveragePolicyStd[2]  | 0.14721   |
| AveragePolicyStd[3]  | 0.18063   |
| AveragePolicyStd[4]  | 0.1519    |
| AveragePolicyStd[5]  | 0.22016   |
| AverageReturn        | 1742.5    |
| MinReturn            | 57.825    |
| MaxReturn            | 1938.8    |
| StdReturn            | 358.25    |
| AverageEpisodeLength | 937.02    |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 180.51    |
| TotalNEpisodes       | 24151     |
| TotalNSamples        | 7.157e+06 |
| ExplainedVariance    | -0.032042 |
------------------------------------
[2018-01-21 16:16:37.010849 UTC] Saving snapshot
[2018-01-21 16:16:37.023047 UTC] Starting iteration 1431
[2018-01-21 16:16:37.023369 UTC] Start collecting samples
[2018-01-21 16:16:41.730246 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:41.889352 UTC] Performing policy update
[2018-01-21 16:16:41.889969 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:42.018520 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:43.390960 UTC] Performing line search
[2018-01-21 16:16:43.584699 UTC] Updating baseline
[2018-01-21 16:16:46.182189 UTC] Computing logging information
-------------------------------------
| Iteration            | 1431       |
| ExpectedImprovement  | 0.017944   |
| ActualImprovement    | 0.017125   |
| ImprovementRatio     | 0.95436    |
| MeanKL               | 0.0078783  |
| Entropy              | -1.6955    |
| Perplexity           | 0.18351    |
| AveragePolicyStd     | 0.1845     |
| AveragePolicyStd[0]  | 0.20452    |
| AveragePolicyStd[1]  | 0.20324    |
| AveragePolicyStd[2]  | 0.14721    |
| AveragePolicyStd[3]  | 0.18033    |
| AveragePolicyStd[4]  | 0.15175    |
| AveragePolicyStd[5]  | 0.21996    |
| AverageReturn        | 1754.8     |
| MinReturn            | 57.825     |
| MaxReturn            | 1938.8     |
| StdReturn            | 342.84     |
| AverageEpisodeLength | 942.99     |
| MinEpisodeLength     | 72         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.45     |
| TotalNEpisodes       | 24157      |
| TotalNSamples        | 7.163e+06  |
| ExplainedVariance    | 2.9313e-06 |
-------------------------------------
[2018-01-21 16:16:47.106006 UTC] Saving snapshot
[2018-01-21 16:16:47.106554 UTC] Starting iteration 1432
[2018-01-21 16:16:47.106965 UTC] Start collecting samples
[2018-01-21 16:16:51.719282 UTC] Computing input variables for policy optimization
[2018-01-21 16:16:51.845145 UTC] Performing policy update
[2018-01-21 16:16:51.845850 UTC] Computing gradient in Euclidean space
[2018-01-21 16:16:51.976210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:16:53.330667 UTC] Performing line search
[2018-01-21 16:16:53.517834 UTC] Updating baseline
[2018-01-21 16:16:55.555352 UTC] Computing logging information
------------------------------------
| Iteration            | 1432      |
| ExpectedImprovement  | 0.016553  |
| ActualImprovement    | 0.015757  |
| ImprovementRatio     | 0.95191   |
| MeanKL               | 0.0080738 |
| Entropy              | -1.6941   |
| Perplexity           | 0.18377   |
| AveragePolicyStd     | 0.18456   |
| AveragePolicyStd[0]  | 0.20468   |
| AveragePolicyStd[1]  | 0.20294   |
| AveragePolicyStd[2]  | 0.14713   |
| AveragePolicyStd[3]  | 0.18005   |
| AveragePolicyStd[4]  | 0.15198   |
| AveragePolicyStd[5]  | 0.22057   |
| AverageReturn        | 1760.9    |
| MinReturn            | 57.825    |
| MaxReturn            | 1937.7    |
| StdReturn            | 336.68    |
| AverageEpisodeLength | 946.6     |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 169.8     |
| TotalNEpisodes       | 24162     |
| TotalNSamples        | 7.168e+06 |
| ExplainedVariance    | -0.012203 |
------------------------------------
[2018-01-21 16:16:56.423000 UTC] Saving snapshot
[2018-01-21 16:16:56.423371 UTC] Starting iteration 1433
[2018-01-21 16:16:56.423643 UTC] Start collecting samples
[2018-01-21 16:17:00.922666 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:01.055135 UTC] Performing policy update
[2018-01-21 16:17:01.055993 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:01.171767 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:02.538179 UTC] Performing line search
[2018-01-21 16:17:02.736196 UTC] Updating baseline
[2018-01-21 16:17:05.472014 UTC] Computing logging information
------------------------------------
| Iteration            | 1433      |
| ExpectedImprovement  | 0.017756  |
| ActualImprovement    | 0.016501  |
| ImprovementRatio     | 0.92931   |
| MeanKL               | 0.0078347 |
| Entropy              | -1.6895   |
| Perplexity           | 0.1846    |
| AveragePolicyStd     | 0.18471   |
| AveragePolicyStd[0]  | 0.205     |
| AveragePolicyStd[1]  | 0.20284   |
| AveragePolicyStd[2]  | 0.14744   |
| AveragePolicyStd[3]  | 0.18023   |
| AveragePolicyStd[4]  | 0.15171   |
| AveragePolicyStd[5]  | 0.22103   |
| AverageReturn        | 1760.8    |
| MinReturn            | 57.825    |
| MaxReturn            | 1937.7    |
| StdReturn            | 336.6     |
| AverageEpisodeLength | 946.6     |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 169.8     |
| TotalNEpisodes       | 24167     |
| TotalNSamples        | 7.173e+06 |
| ExplainedVariance    | 0.0023087 |
------------------------------------
[2018-01-21 16:17:06.279760 UTC] Saving snapshot
[2018-01-21 16:17:06.279971 UTC] Starting iteration 1434
[2018-01-21 16:17:06.280103 UTC] Start collecting samples
[2018-01-21 16:17:10.908209 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:11.040446 UTC] Performing policy update
[2018-01-21 16:17:11.041036 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:11.157409 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:12.543196 UTC] Performing line search
[2018-01-21 16:17:12.736016 UTC] Updating baseline
[2018-01-21 16:17:14.785257 UTC] Computing logging information
------------------------------------
| Iteration            | 1434      |
| ExpectedImprovement  | 0.021305  |
| ActualImprovement    | 0.019785  |
| ImprovementRatio     | 0.92866   |
| MeanKL               | 0.0080117 |
| Entropy              | -1.6894   |
| Perplexity           | 0.18464   |
| AveragePolicyStd     | 0.18468   |
| AveragePolicyStd[0]  | 0.20466   |
| AveragePolicyStd[1]  | 0.20275   |
| AveragePolicyStd[2]  | 0.14755   |
| AveragePolicyStd[3]  | 0.18024   |
| AveragePolicyStd[4]  | 0.15215   |
| AveragePolicyStd[5]  | 0.22071   |
| AverageReturn        | 1759.7    |
| MinReturn            | 57.825    |
| MaxReturn            | 1922.4    |
| StdReturn            | 336.21    |
| AverageEpisodeLength | 946.6     |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 169.8     |
| TotalNEpisodes       | 24172     |
| TotalNSamples        | 7.178e+06 |
| ExplainedVariance    | 0.0022511 |
------------------------------------
[2018-01-21 16:17:15.604113 UTC] Saving snapshot
[2018-01-21 16:17:15.604370 UTC] Starting iteration 1435
[2018-01-21 16:17:15.604528 UTC] Start collecting samples
[2018-01-21 16:17:20.055093 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:20.196714 UTC] Performing policy update
[2018-01-21 16:17:20.197821 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:20.322086 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:21.735986 UTC] Performing line search
[2018-01-21 16:17:21.945572 UTC] Updating baseline
[2018-01-21 16:17:24.187349 UTC] Computing logging information
------------------------------------
| Iteration            | 1435      |
| ExpectedImprovement  | 0.018666  |
| ActualImprovement    | 0.017441  |
| ImprovementRatio     | 0.9344    |
| MeanKL               | 0.0084674 |
| Entropy              | -1.6914   |
| Perplexity           | 0.18426   |
| AveragePolicyStd     | 0.18464   |
| AveragePolicyStd[0]  | 0.20411   |
| AveragePolicyStd[1]  | 0.20312   |
| AveragePolicyStd[2]  | 0.14731   |
| AveragePolicyStd[3]  | 0.17981   |
| AveragePolicyStd[4]  | 0.15223   |
| AveragePolicyStd[5]  | 0.22122   |
| AverageReturn        | 1769.8    |
| MinReturn            | 57.825    |
| MaxReturn            | 1922.4    |
| StdReturn            | 323.12    |
| AverageEpisodeLength | 951.81    |
| MinEpisodeLength     | 72        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 163.24    |
| TotalNEpisodes       | 24177     |
| TotalNSamples        | 7.183e+06 |
| ExplainedVariance    | -0.012378 |
------------------------------------
[2018-01-21 16:17:25.001078 UTC] Saving snapshot
[2018-01-21 16:17:25.001318 UTC] Starting iteration 1436
[2018-01-21 16:17:25.001466 UTC] Start collecting samples
[2018-01-21 16:17:29.773863 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:29.920028 UTC] Performing policy update
[2018-01-21 16:17:29.920702 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:30.052303 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:31.421845 UTC] Performing line search
[2018-01-21 16:17:31.615143 UTC] Updating baseline
[2018-01-21 16:17:33.761104 UTC] Computing logging information
-------------------------------------
| Iteration            | 1436       |
| ExpectedImprovement  | 0.019379   |
| ActualImprovement    | 0.018161   |
| ImprovementRatio     | 0.93716    |
| MeanKL               | 0.0078842  |
| Entropy              | -1.6926    |
| Perplexity           | 0.18403    |
| AveragePolicyStd     | 0.1846     |
| AveragePolicyStd[0]  | 0.20434    |
| AveragePolicyStd[1]  | 0.20298    |
| AveragePolicyStd[2]  | 0.14698    |
| AveragePolicyStd[3]  | 0.17983    |
| AveragePolicyStd[4]  | 0.15237    |
| AveragePolicyStd[5]  | 0.22113    |
| AverageReturn        | 1774.1     |
| MinReturn            | 323.49     |
| MaxReturn            | 1947.5     |
| StdReturn            | 308.9      |
| AverageEpisodeLength | 953.34     |
| MinEpisodeLength     | 206        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.52     |
| TotalNEpisodes       | 24184      |
| TotalNSamples        | 7.1893e+06 |
| ExplainedVariance    | 0.078601   |
-------------------------------------
[2018-01-21 16:17:34.648075 UTC] Saving snapshot
[2018-01-21 16:17:34.648323 UTC] Starting iteration 1437
[2018-01-21 16:17:34.648512 UTC] Start collecting samples
[2018-01-21 16:17:39.108370 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:39.230694 UTC] Performing policy update
[2018-01-21 16:17:39.231340 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:39.352404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:40.750872 UTC] Performing line search
[2018-01-21 16:17:40.953176 UTC] Updating baseline
[2018-01-21 16:17:43.014397 UTC] Computing logging information
-------------------------------------
| Iteration            | 1437       |
| ExpectedImprovement  | 0.017614   |
| ActualImprovement    | 0.016973   |
| ImprovementRatio     | 0.96364    |
| MeanKL               | 0.0083125  |
| Entropy              | -1.6911    |
| Perplexity           | 0.18433    |
| AveragePolicyStd     | 0.18467    |
| AveragePolicyStd[0]  | 0.20427    |
| AveragePolicyStd[1]  | 0.20301    |
| AveragePolicyStd[2]  | 0.14699    |
| AveragePolicyStd[3]  | 0.17973    |
| AveragePolicyStd[4]  | 0.15234    |
| AveragePolicyStd[5]  | 0.22167    |
| AverageReturn        | 1775.8     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 303.11     |
| AverageEpisodeLength | 954.05     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.24     |
| TotalNEpisodes       | 24189      |
| TotalNSamples        | 7.1935e+06 |
| ExplainedVariance    | 0.13506    |
-------------------------------------
[2018-01-21 16:17:43.809027 UTC] Saving snapshot
[2018-01-21 16:17:43.809349 UTC] Starting iteration 1438
[2018-01-21 16:17:43.809589 UTC] Start collecting samples
[2018-01-21 16:17:48.376953 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:48.520830 UTC] Performing policy update
[2018-01-21 16:17:48.521854 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:48.637947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:50.056219 UTC] Performing line search
[2018-01-21 16:17:50.242856 UTC] Updating baseline
[2018-01-21 16:17:52.233210 UTC] Computing logging information
-------------------------------------
| Iteration            | 1438       |
| ExpectedImprovement  | 0.017116   |
| ActualImprovement    | 0.016353   |
| ImprovementRatio     | 0.95542    |
| MeanKL               | 0.0076661  |
| Entropy              | -1.6895    |
| Perplexity           | 0.18461    |
| AveragePolicyStd     | 0.18469    |
| AveragePolicyStd[0]  | 0.20411    |
| AveragePolicyStd[1]  | 0.20304    |
| AveragePolicyStd[2]  | 0.14708    |
| AveragePolicyStd[3]  | 0.18018    |
| AveragePolicyStd[4]  | 0.15248    |
| AveragePolicyStd[5]  | 0.22127    |
| AverageReturn        | 1768.3     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 305.96     |
| AverageEpisodeLength | 951.21     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.99     |
| TotalNEpisodes       | 24195      |
| TotalNSamples        | 7.1993e+06 |
| ExplainedVariance    | 0.083002   |
-------------------------------------
[2018-01-21 16:17:53.026685 UTC] Saving snapshot
[2018-01-21 16:17:53.027121 UTC] Starting iteration 1439
[2018-01-21 16:17:53.027428 UTC] Start collecting samples
[2018-01-21 16:17:57.778640 UTC] Computing input variables for policy optimization
[2018-01-21 16:17:57.926265 UTC] Performing policy update
[2018-01-21 16:17:57.927425 UTC] Computing gradient in Euclidean space
[2018-01-21 16:17:58.057880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:17:59.437093 UTC] Performing line search
[2018-01-21 16:17:59.640696 UTC] Updating baseline
[2018-01-21 16:18:01.468666 UTC] Computing logging information
-------------------------------------
| Iteration            | 1439       |
| ExpectedImprovement  | 0.017448   |
| ActualImprovement    | 0.016897   |
| ImprovementRatio     | 0.96841    |
| MeanKL               | 0.0081588  |
| Entropy              | -1.6867    |
| Perplexity           | 0.18512    |
| AveragePolicyStd     | 0.18476    |
| AveragePolicyStd[0]  | 0.20413    |
| AveragePolicyStd[1]  | 0.20309    |
| AveragePolicyStd[2]  | 0.14738    |
| AveragePolicyStd[3]  | 0.18029    |
| AveragePolicyStd[4]  | 0.15251    |
| AveragePolicyStd[5]  | 0.22118    |
| AverageReturn        | 1773.2     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 289.98     |
| AverageEpisodeLength | 953.22     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.56     |
| TotalNEpisodes       | 24202      |
| TotalNSamples        | 7.2056e+06 |
| ExplainedVariance    | 0.21029    |
-------------------------------------
[2018-01-21 16:18:02.350491 UTC] Saving snapshot
[2018-01-21 16:18:02.350727 UTC] Starting iteration 1440
[2018-01-21 16:18:02.350901 UTC] Start collecting samples
[2018-01-21 16:18:06.805336 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:06.947161 UTC] Performing policy update
[2018-01-21 16:18:06.948212 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:07.065687 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:08.472052 UTC] Performing line search
[2018-01-21 16:18:08.662599 UTC] Updating baseline
[2018-01-21 16:18:10.479823 UTC] Computing logging information
-------------------------------------
| Iteration            | 1440       |
| ExpectedImprovement  | 0.018994   |
| ActualImprovement    | 0.018673   |
| ImprovementRatio     | 0.98313    |
| MeanKL               | 0.0078107  |
| Entropy              | -1.6855    |
| Perplexity           | 0.18536    |
| AveragePolicyStd     | 0.18478    |
| AveragePolicyStd[0]  | 0.20404    |
| AveragePolicyStd[1]  | 0.20273    |
| AveragePolicyStd[2]  | 0.14757    |
| AveragePolicyStd[3]  | 0.18074    |
| AveragePolicyStd[4]  | 0.15254    |
| AveragePolicyStd[5]  | 0.22106    |
| AverageReturn        | 1774.4     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 290.39     |
| AverageEpisodeLength | 953.22     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.56     |
| TotalNEpisodes       | 24205      |
| TotalNSamples        | 7.2086e+06 |
| ExplainedVariance    | 0.0083395  |
-------------------------------------
[2018-01-21 16:18:11.342563 UTC] Saving snapshot
[2018-01-21 16:18:11.352154 UTC] Starting iteration 1441
[2018-01-21 16:18:11.352385 UTC] Start collecting samples
[2018-01-21 16:18:15.835147 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:15.971024 UTC] Performing policy update
[2018-01-21 16:18:15.971724 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:16.088449 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:17.446499 UTC] Performing line search
[2018-01-21 16:18:17.637534 UTC] Updating baseline
[2018-01-21 16:18:20.340676 UTC] Computing logging information
--------------------------------------
| Iteration            | 1441        |
| ExpectedImprovement  | 0.016921    |
| ActualImprovement    | 0.016169    |
| ImprovementRatio     | 0.95552     |
| MeanKL               | 0.007651    |
| Entropy              | -1.6839     |
| Perplexity           | 0.18565     |
| AveragePolicyStd     | 0.18485     |
| AveragePolicyStd[0]  | 0.20409     |
| AveragePolicyStd[1]  | 0.20264     |
| AveragePolicyStd[2]  | 0.14741     |
| AveragePolicyStd[3]  | 0.18087     |
| AveragePolicyStd[4]  | 0.15245     |
| AveragePolicyStd[5]  | 0.22165     |
| AverageReturn        | 1784        |
| MinReturn            | 354.98      |
| MaxReturn            | 1947.5      |
| StdReturn            | 272.42      |
| AverageEpisodeLength | 958.68      |
| MinEpisodeLength     | 225         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 137.77      |
| TotalNEpisodes       | 24210       |
| TotalNSamples        | 7.2136e+06  |
| ExplainedVariance    | -0.00036232 |
--------------------------------------
[2018-01-21 16:18:21.212377 UTC] Saving snapshot
[2018-01-21 16:18:21.212701 UTC] Starting iteration 1442
[2018-01-21 16:18:21.212920 UTC] Start collecting samples
[2018-01-21 16:18:25.882750 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:26.052778 UTC] Performing policy update
[2018-01-21 16:18:26.053211 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:26.173302 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:27.544367 UTC] Performing line search
[2018-01-21 16:18:27.736526 UTC] Updating baseline
[2018-01-21 16:18:29.703237 UTC] Computing logging information
-------------------------------------
| Iteration            | 1442       |
| ExpectedImprovement  | 0.018089   |
| ActualImprovement    | 0.01726    |
| ImprovementRatio     | 0.95418    |
| MeanKL               | 0.0075279  |
| Entropy              | -1.686     |
| Perplexity           | 0.18525    |
| AveragePolicyStd     | 0.18479    |
| AveragePolicyStd[0]  | 0.2042     |
| AveragePolicyStd[1]  | 0.20262    |
| AveragePolicyStd[2]  | 0.14732    |
| AveragePolicyStd[3]  | 0.18113    |
| AveragePolicyStd[4]  | 0.15223    |
| AveragePolicyStd[5]  | 0.22123    |
| AverageReturn        | 1784.4     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 272.59     |
| AverageEpisodeLength | 958.68     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.77     |
| TotalNEpisodes       | 24216      |
| TotalNSamples        | 7.2196e+06 |
| ExplainedVariance    | 0.018987   |
-------------------------------------
[2018-01-21 16:18:30.580360 UTC] Saving snapshot
[2018-01-21 16:18:30.580591 UTC] Starting iteration 1443
[2018-01-21 16:18:30.580735 UTC] Start collecting samples
[2018-01-21 16:18:35.263604 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:35.417367 UTC] Performing policy update
[2018-01-21 16:18:35.417986 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:35.538091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:36.984990 UTC] Performing line search
[2018-01-21 16:18:37.184779 UTC] Updating baseline
[2018-01-21 16:18:39.323178 UTC] Computing logging information
-------------------------------------
| Iteration            | 1443       |
| ExpectedImprovement  | 0.019822   |
| ActualImprovement    | 0.018152   |
| ImprovementRatio     | 0.91576    |
| MeanKL               | 0.0074233  |
| Entropy              | -1.6884    |
| Perplexity           | 0.18481    |
| AveragePolicyStd     | 0.18471    |
| AveragePolicyStd[0]  | 0.20408    |
| AveragePolicyStd[1]  | 0.20234    |
| AveragePolicyStd[2]  | 0.14738    |
| AveragePolicyStd[3]  | 0.1807     |
| AveragePolicyStd[4]  | 0.1523     |
| AveragePolicyStd[5]  | 0.22148    |
| AverageReturn        | 1784.4     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 272.7      |
| AverageEpisodeLength | 958.68     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.77     |
| TotalNEpisodes       | 24220      |
| TotalNSamples        | 7.2236e+06 |
| ExplainedVariance    | 0.017848   |
-------------------------------------
[2018-01-21 16:18:40.111780 UTC] Saving snapshot
[2018-01-21 16:18:40.112030 UTC] Starting iteration 1444
[2018-01-21 16:18:40.112218 UTC] Start collecting samples
[2018-01-21 16:18:44.588884 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:44.727727 UTC] Performing policy update
[2018-01-21 16:18:44.728352 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:44.845585 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:46.236212 UTC] Performing line search
[2018-01-21 16:18:46.423089 UTC] Updating baseline
[2018-01-21 16:18:48.618144 UTC] Computing logging information
-------------------------------------
| Iteration            | 1444       |
| ExpectedImprovement  | 0.017023   |
| ActualImprovement    | 0.016145   |
| ImprovementRatio     | 0.9484     |
| MeanKL               | 0.0076955  |
| Entropy              | -1.6922    |
| Perplexity           | 0.18411    |
| AveragePolicyStd     | 0.18458    |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.20216    |
| AveragePolicyStd[2]  | 0.14755    |
| AveragePolicyStd[3]  | 0.1803     |
| AveragePolicyStd[4]  | 0.1523     |
| AveragePolicyStd[5]  | 0.22152    |
| AverageReturn        | 1791       |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 269.85     |
| AverageEpisodeLength | 961.31     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.65     |
| TotalNEpisodes       | 24226      |
| TotalNSamples        | 7.2296e+06 |
| ExplainedVariance    | 0.061323   |
-------------------------------------
[2018-01-21 16:18:49.482870 UTC] Saving snapshot
[2018-01-21 16:18:49.483112 UTC] Starting iteration 1445
[2018-01-21 16:18:49.483265 UTC] Start collecting samples
[2018-01-21 16:18:53.913614 UTC] Computing input variables for policy optimization
[2018-01-21 16:18:54.062356 UTC] Performing policy update
[2018-01-21 16:18:54.063059 UTC] Computing gradient in Euclidean space
[2018-01-21 16:18:54.177834 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:18:55.553016 UTC] Performing line search
[2018-01-21 16:18:55.738991 UTC] Updating baseline
[2018-01-21 16:18:57.900143 UTC] Computing logging information
-------------------------------------
| Iteration            | 1445       |
| ExpectedImprovement  | 0.018657   |
| ActualImprovement    | 0.017052   |
| ImprovementRatio     | 0.914      |
| MeanKL               | 0.0077506  |
| Entropy              | -1.6833    |
| Perplexity           | 0.18575    |
| AveragePolicyStd     | 0.18485    |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.20227    |
| AveragePolicyStd[2]  | 0.14802    |
| AveragePolicyStd[3]  | 0.18022    |
| AveragePolicyStd[4]  | 0.15248    |
| AveragePolicyStd[5]  | 0.22215    |
| AverageReturn        | 1784.9     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 275.71     |
| AverageEpisodeLength | 958.14     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.36     |
| TotalNEpisodes       | 24230      |
| TotalNSamples        | 7.2332e+06 |
| ExplainedVariance    | 0.072112   |
-------------------------------------
[2018-01-21 16:18:58.672672 UTC] Saving snapshot
[2018-01-21 16:18:58.672978 UTC] Starting iteration 1446
[2018-01-21 16:18:58.673200 UTC] Start collecting samples
[2018-01-21 16:19:03.351895 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:03.502730 UTC] Performing policy update
[2018-01-21 16:19:03.503895 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:03.636634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:05.011855 UTC] Performing line search
[2018-01-21 16:19:05.210556 UTC] Updating baseline
[2018-01-21 16:19:06.899044 UTC] Computing logging information
-------------------------------------
| Iteration            | 1446       |
| ExpectedImprovement  | 0.018744   |
| ActualImprovement    | 0.018041   |
| ImprovementRatio     | 0.96251    |
| MeanKL               | 0.007869   |
| Entropy              | -1.6853    |
| Perplexity           | 0.18539    |
| AveragePolicyStd     | 0.18481    |
| AveragePolicyStd[0]  | 0.2037     |
| AveragePolicyStd[1]  | 0.20255    |
| AveragePolicyStd[2]  | 0.14806    |
| AveragePolicyStd[3]  | 0.18013    |
| AveragePolicyStd[4]  | 0.15214    |
| AveragePolicyStd[5]  | 0.22226    |
| AverageReturn        | 1786.6     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 275.95     |
| AverageEpisodeLength | 958.31     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.4      |
| TotalNEpisodes       | 24235      |
| TotalNSamples        | 7.2382e+06 |
| ExplainedVariance    | 0.087095   |
-------------------------------------
[2018-01-21 16:19:07.812216 UTC] Saving snapshot
[2018-01-21 16:19:07.812491 UTC] Starting iteration 1447
[2018-01-21 16:19:07.812699 UTC] Start collecting samples
[2018-01-21 16:19:12.364322 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:12.512498 UTC] Performing policy update
[2018-01-21 16:19:12.513096 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:12.638119 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:14.065355 UTC] Performing line search
[2018-01-21 16:19:14.266439 UTC] Updating baseline
[2018-01-21 16:19:16.442944 UTC] Computing logging information
-------------------------------------
| Iteration            | 1447       |
| ExpectedImprovement  | 0.016973   |
| ActualImprovement    | 0.016188   |
| ImprovementRatio     | 0.95376    |
| MeanKL               | 0.0082009  |
| Entropy              | -1.6883    |
| Perplexity           | 0.18483    |
| AveragePolicyStd     | 0.1847     |
| AveragePolicyStd[0]  | 0.20342    |
| AveragePolicyStd[1]  | 0.20232    |
| AveragePolicyStd[2]  | 0.14808    |
| AveragePolicyStd[3]  | 0.18007    |
| AveragePolicyStd[4]  | 0.15214    |
| AveragePolicyStd[5]  | 0.22219    |
| AverageReturn        | 1801.6     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 253.72     |
| AverageEpisodeLength | 966.27     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.96     |
| TotalNEpisodes       | 24239      |
| TotalNSamples        | 7.2422e+06 |
| ExplainedVariance    | 0.011832   |
-------------------------------------
[2018-01-21 16:19:17.336348 UTC] Saving snapshot
[2018-01-21 16:19:17.336632 UTC] Starting iteration 1448
[2018-01-21 16:19:17.336893 UTC] Start collecting samples
[2018-01-21 16:19:21.868627 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:22.019091 UTC] Performing policy update
[2018-01-21 16:19:22.019771 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:22.145432 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:23.614520 UTC] Performing line search
[2018-01-21 16:19:23.803246 UTC] Updating baseline
[2018-01-21 16:19:25.589555 UTC] Computing logging information
-------------------------------------
| Iteration            | 1448       |
| ExpectedImprovement  | 0.018747   |
| ActualImprovement    | 0.017797   |
| ImprovementRatio     | 0.94935    |
| MeanKL               | 0.0084991  |
| Entropy              | -1.6874    |
| Perplexity           | 0.185      |
| AveragePolicyStd     | 0.18473    |
| AveragePolicyStd[0]  | 0.20368    |
| AveragePolicyStd[1]  | 0.2019     |
| AveragePolicyStd[2]  | 0.14833    |
| AveragePolicyStd[3]  | 0.17998    |
| AveragePolicyStd[4]  | 0.15212    |
| AveragePolicyStd[5]  | 0.22234    |
| AverageReturn        | 1806.2     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 250.04     |
| AverageEpisodeLength | 968.62     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.39     |
| TotalNEpisodes       | 24245      |
| TotalNSamples        | 7.2482e+06 |
| ExplainedVariance    | -0.047981  |
-------------------------------------
[2018-01-21 16:19:26.500117 UTC] Saving snapshot
[2018-01-21 16:19:26.500413 UTC] Starting iteration 1449
[2018-01-21 16:19:26.500607 UTC] Start collecting samples
[2018-01-21 16:19:30.863090 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:30.992014 UTC] Performing policy update
[2018-01-21 16:19:30.992667 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:31.109839 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:32.487440 UTC] Performing line search
[2018-01-21 16:19:32.680143 UTC] Updating baseline
[2018-01-21 16:19:34.711388 UTC] Computing logging information
-------------------------------------
| Iteration            | 1449       |
| ExpectedImprovement  | 0.017921   |
| ActualImprovement    | 0.017206   |
| ImprovementRatio     | 0.96011    |
| MeanKL               | 0.008729   |
| Entropy              | -1.6917    |
| Perplexity           | 0.1842     |
| AveragePolicyStd     | 0.1846     |
| AveragePolicyStd[0]  | 0.2037     |
| AveragePolicyStd[1]  | 0.20205    |
| AveragePolicyStd[2]  | 0.14799    |
| AveragePolicyStd[3]  | 0.17968    |
| AveragePolicyStd[4]  | 0.1521     |
| AveragePolicyStd[5]  | 0.2221     |
| AverageReturn        | 1812.2     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 242.45     |
| AverageEpisodeLength | 972.06     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.45     |
| TotalNEpisodes       | 24250      |
| TotalNSamples        | 7.2532e+06 |
| ExplainedVariance    | 0.0067106  |
-------------------------------------
[2018-01-21 16:19:35.623520 UTC] Saving snapshot
[2018-01-21 16:19:35.623756 UTC] Starting iteration 1450
[2018-01-21 16:19:35.623906 UTC] Start collecting samples
[2018-01-21 16:19:40.207682 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:40.335418 UTC] Performing policy update
[2018-01-21 16:19:40.336514 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:40.470296 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:41.869685 UTC] Performing line search
[2018-01-21 16:19:42.072260 UTC] Updating baseline
[2018-01-21 16:19:44.604654 UTC] Computing logging information
-------------------------------------
| Iteration            | 1450       |
| ExpectedImprovement  | 0.015367   |
| ActualImprovement    | 0.015076   |
| ImprovementRatio     | 0.98103    |
| MeanKL               | 0.0080917  |
| Entropy              | -1.6872    |
| Perplexity           | 0.18503    |
| AveragePolicyStd     | 0.18475    |
| AveragePolicyStd[0]  | 0.20423    |
| AveragePolicyStd[1]  | 0.20199    |
| AveragePolicyStd[2]  | 0.14817    |
| AveragePolicyStd[3]  | 0.1792     |
| AveragePolicyStd[4]  | 0.15228    |
| AveragePolicyStd[5]  | 0.22265    |
| AverageReturn        | 1812.2     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 242.39     |
| AverageEpisodeLength | 972.06     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.45     |
| TotalNEpisodes       | 24254      |
| TotalNSamples        | 7.2572e+06 |
| ExplainedVariance    | 0.10259    |
-------------------------------------
[2018-01-21 16:19:45.427054 UTC] Saving snapshot
[2018-01-21 16:19:45.436055 UTC] Starting iteration 1451
[2018-01-21 16:19:45.436288 UTC] Start collecting samples
[2018-01-21 16:19:50.193509 UTC] Computing input variables for policy optimization
[2018-01-21 16:19:50.315747 UTC] Performing policy update
[2018-01-21 16:19:50.316468 UTC] Computing gradient in Euclidean space
[2018-01-21 16:19:50.430802 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:19:51.798524 UTC] Performing line search
[2018-01-21 16:19:51.997619 UTC] Updating baseline
[2018-01-21 16:19:54.488786 UTC] Computing logging information
-------------------------------------
| Iteration            | 1451       |
| ExpectedImprovement  | 0.016638   |
| ActualImprovement    | 0.01613    |
| ImprovementRatio     | 0.96947    |
| MeanKL               | 0.0084205  |
| Entropy              | -1.6839    |
| Perplexity           | 0.18564    |
| AveragePolicyStd     | 0.18485    |
| AveragePolicyStd[0]  | 0.20426    |
| AveragePolicyStd[1]  | 0.20216    |
| AveragePolicyStd[2]  | 0.1484     |
| AveragePolicyStd[3]  | 0.17939    |
| AveragePolicyStd[4]  | 0.15217    |
| AveragePolicyStd[5]  | 0.22274    |
| AverageReturn        | 1797.7     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 279.47     |
| AverageEpisodeLength | 964.65     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.48     |
| TotalNEpisodes       | 24261      |
| TotalNSamples        | 7.2635e+06 |
| ExplainedVariance    | 0.02548    |
-------------------------------------
[2018-01-21 16:19:55.336731 UTC] Saving snapshot
[2018-01-21 16:19:55.337021 UTC] Starting iteration 1452
[2018-01-21 16:19:55.337264 UTC] Start collecting samples
[2018-01-21 16:19:59.929673 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:00.062658 UTC] Performing policy update
[2018-01-21 16:20:00.063874 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:00.181181 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:01.562490 UTC] Performing line search
[2018-01-21 16:20:01.748991 UTC] Updating baseline
[2018-01-21 16:20:03.872440 UTC] Computing logging information
-------------------------------------
| Iteration            | 1452       |
| ExpectedImprovement  | 0.01859    |
| ActualImprovement    | 0.017599   |
| ImprovementRatio     | 0.94667    |
| MeanKL               | 0.0083217  |
| Entropy              | -1.6831    |
| Perplexity           | 0.1858     |
| AveragePolicyStd     | 0.18486    |
| AveragePolicyStd[0]  | 0.20458    |
| AveragePolicyStd[1]  | 0.20176    |
| AveragePolicyStd[2]  | 0.14838    |
| AveragePolicyStd[3]  | 0.17952    |
| AveragePolicyStd[4]  | 0.15242    |
| AveragePolicyStd[5]  | 0.22253    |
| AverageReturn        | 1783.5     |
| MinReturn            | 354.98     |
| MaxReturn            | 1947.5     |
| StdReturn            | 310.27     |
| AverageEpisodeLength | 957.38     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.27     |
| TotalNEpisodes       | 24268      |
| TotalNSamples        | 7.2698e+06 |
| ExplainedVariance    | 0.083136   |
-------------------------------------
[2018-01-21 16:20:04.673254 UTC] Saving snapshot
[2018-01-21 16:20:04.673625 UTC] Starting iteration 1453
[2018-01-21 16:20:04.673842 UTC] Start collecting samples
[2018-01-21 16:20:09.140424 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:09.261472 UTC] Performing policy update
[2018-01-21 16:20:09.262548 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:09.379980 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:10.772740 UTC] Performing line search
[2018-01-21 16:20:10.974214 UTC] Updating baseline
[2018-01-21 16:20:13.053130 UTC] Computing logging information
--------------------------------------
| Iteration            | 1453        |
| ExpectedImprovement  | 0.01648     |
| ActualImprovement    | 0.015481    |
| ImprovementRatio     | 0.93942     |
| MeanKL               | 0.0081529   |
| Entropy              | -1.685      |
| Perplexity           | 0.18545     |
| AveragePolicyStd     | 0.18482     |
| AveragePolicyStd[0]  | 0.2045      |
| AveragePolicyStd[1]  | 0.2014      |
| AveragePolicyStd[2]  | 0.14827     |
| AveragePolicyStd[3]  | 0.17945     |
| AveragePolicyStd[4]  | 0.15239     |
| AveragePolicyStd[5]  | 0.22289     |
| AverageReturn        | 1783.1      |
| MinReturn            | 354.98      |
| MaxReturn            | 1947.5      |
| StdReturn            | 310.17      |
| AverageEpisodeLength | 957.38      |
| MinEpisodeLength     | 225         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 157.27      |
| TotalNEpisodes       | 24271       |
| TotalNSamples        | 7.2728e+06  |
| ExplainedVariance    | -0.00058561 |
--------------------------------------
[2018-01-21 16:20:13.897721 UTC] Saving snapshot
[2018-01-21 16:20:13.897989 UTC] Starting iteration 1454
[2018-01-21 16:20:13.898167 UTC] Start collecting samples
[2018-01-21 16:20:18.619382 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:18.771054 UTC] Performing policy update
[2018-01-21 16:20:18.771650 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:18.889750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:20.257835 UTC] Performing line search
[2018-01-21 16:20:20.465756 UTC] Updating baseline
[2018-01-21 16:20:22.227989 UTC] Computing logging information
--------------------------------------
| Iteration            | 1454        |
| ExpectedImprovement  | 0.016397    |
| ActualImprovement    | 0.015472    |
| ImprovementRatio     | 0.94356     |
| MeanKL               | 0.0081233   |
| Entropy              | -1.6935     |
| Perplexity           | 0.18387     |
| AveragePolicyStd     | 0.18458     |
| AveragePolicyStd[0]  | 0.20452     |
| AveragePolicyStd[1]  | 0.20141     |
| AveragePolicyStd[2]  | 0.14789     |
| AveragePolicyStd[3]  | 0.17957     |
| AveragePolicyStd[4]  | 0.15174     |
| AveragePolicyStd[5]  | 0.22235     |
| AverageReturn        | 1779.7      |
| MinReturn            | 354.98      |
| MaxReturn            | 1947.5      |
| StdReturn            | 309.28      |
| AverageEpisodeLength | 957.38      |
| MinEpisodeLength     | 225         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 157.27      |
| TotalNEpisodes       | 24278       |
| TotalNSamples        | 7.2798e+06  |
| ExplainedVariance    | -0.00050873 |
--------------------------------------
[2018-01-21 16:20:23.048517 UTC] Saving snapshot
[2018-01-21 16:20:23.048935 UTC] Starting iteration 1455
[2018-01-21 16:20:23.049279 UTC] Start collecting samples
[2018-01-21 16:20:27.682712 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:27.821720 UTC] Performing policy update
[2018-01-21 16:20:27.822384 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:27.939825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:29.291359 UTC] Performing line search
[2018-01-21 16:20:29.478956 UTC] Updating baseline
[2018-01-21 16:20:31.325967 UTC] Computing logging information
-------------------------------------
| Iteration            | 1455       |
| ExpectedImprovement  | 0.01684    |
| ActualImprovement    | 0.016647   |
| ImprovementRatio     | 0.98852    |
| MeanKL               | 0.0092089  |
| Entropy              | -1.6993    |
| Perplexity           | 0.18281    |
| AveragePolicyStd     | 0.18441    |
| AveragePolicyStd[0]  | 0.2044     |
| AveragePolicyStd[1]  | 0.20107    |
| AveragePolicyStd[2]  | 0.14808    |
| AveragePolicyStd[3]  | 0.17864    |
| AveragePolicyStd[4]  | 0.15149    |
| AveragePolicyStd[5]  | 0.22279    |
| AverageReturn        | 1792.9     |
| MinReturn            | 411.28     |
| MaxReturn            | 1934.6     |
| StdReturn            | 273.69     |
| AverageEpisodeLength | 965.13     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.03     |
| TotalNEpisodes       | 24283      |
| TotalNSamples        | 7.2848e+06 |
| ExplainedVariance    | 0.00010719 |
-------------------------------------
[2018-01-21 16:20:32.102440 UTC] Saving snapshot
[2018-01-21 16:20:32.102692 UTC] Starting iteration 1456
[2018-01-21 16:20:32.102840 UTC] Start collecting samples
[2018-01-21 16:20:36.448058 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:36.568255 UTC] Performing policy update
[2018-01-21 16:20:36.568976 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:36.707624 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:38.118115 UTC] Performing line search
[2018-01-21 16:20:38.322108 UTC] Updating baseline
[2018-01-21 16:20:41.426425 UTC] Computing logging information
-------------------------------------
| Iteration            | 1456       |
| ExpectedImprovement  | 0.017951   |
| ActualImprovement    | 0.016576   |
| ImprovementRatio     | 0.92342    |
| MeanKL               | 0.0086839  |
| Entropy              | -1.6991    |
| Perplexity           | 0.18284    |
| AveragePolicyStd     | 0.18444    |
| AveragePolicyStd[0]  | 0.2043     |
| AveragePolicyStd[1]  | 0.20163    |
| AveragePolicyStd[2]  | 0.14797    |
| AveragePolicyStd[3]  | 0.17888    |
| AveragePolicyStd[4]  | 0.15105    |
| AveragePolicyStd[5]  | 0.22284    |
| AverageReturn        | 1791.9     |
| MinReturn            | 411.28     |
| MaxReturn            | 1934.6     |
| StdReturn            | 273.49     |
| AverageEpisodeLength | 965.13     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 139.03     |
| TotalNEpisodes       | 24286      |
| TotalNSamples        | 7.2878e+06 |
| ExplainedVariance    | -0.0023317 |
-------------------------------------
[2018-01-21 16:20:42.330380 UTC] Saving snapshot
[2018-01-21 16:20:42.330635 UTC] Starting iteration 1457
[2018-01-21 16:20:42.330847 UTC] Start collecting samples
[2018-01-21 16:20:46.878841 UTC] Computing input variables for policy optimization
[2018-01-21 16:20:47.011368 UTC] Performing policy update
[2018-01-21 16:20:47.012447 UTC] Computing gradient in Euclidean space
[2018-01-21 16:20:47.128250 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:20:48.501799 UTC] Performing line search
[2018-01-21 16:20:48.695013 UTC] Updating baseline
[2018-01-21 16:20:55.522688 UTC] Computing logging information
-------------------------------------
| Iteration            | 1457       |
| ExpectedImprovement  | 0.019408   |
| ActualImprovement    | 0.01851    |
| ImprovementRatio     | 0.95373    |
| MeanKL               | 0.0071704  |
| Entropy              | -1.6997    |
| Perplexity           | 0.18274    |
| AveragePolicyStd     | 0.18442    |
| AveragePolicyStd[0]  | 0.20446    |
| AveragePolicyStd[1]  | 0.20125    |
| AveragePolicyStd[2]  | 0.14829    |
| AveragePolicyStd[3]  | 0.17829    |
| AveragePolicyStd[4]  | 0.15116    |
| AveragePolicyStd[5]  | 0.22305    |
| AverageReturn        | 1806.3     |
| MinReturn            | 411.28     |
| MaxReturn            | 1934.6     |
| StdReturn            | 238.28     |
| AverageEpisodeLength | 972.36     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.64     |
| TotalNEpisodes       | 24293      |
| TotalNSamples        | 7.2948e+06 |
| ExplainedVariance    | 9.7934e-05 |
-------------------------------------
[2018-01-21 16:20:56.364955 UTC] Saving snapshot
[2018-01-21 16:20:56.365191 UTC] Starting iteration 1458
[2018-01-21 16:20:56.365380 UTC] Start collecting samples
[2018-01-21 16:21:00.926999 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:01.060311 UTC] Performing policy update
[2018-01-21 16:21:01.061017 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:01.178730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:02.587833 UTC] Performing line search
[2018-01-21 16:21:02.777545 UTC] Updating baseline
[2018-01-21 16:21:04.651170 UTC] Computing logging information
-------------------------------------
| Iteration            | 1458       |
| ExpectedImprovement  | 0.018883   |
| ActualImprovement    | 0.016812   |
| ImprovementRatio     | 0.89033    |
| MeanKL               | 0.0080227  |
| Entropy              | -1.7061    |
| Perplexity           | 0.18158    |
| AveragePolicyStd     | 0.1842     |
| AveragePolicyStd[0]  | 0.20454    |
| AveragePolicyStd[1]  | 0.20122    |
| AveragePolicyStd[2]  | 0.14821    |
| AveragePolicyStd[3]  | 0.17844    |
| AveragePolicyStd[4]  | 0.15084    |
| AveragePolicyStd[5]  | 0.22198    |
| AverageReturn        | 1809.6     |
| MinReturn            | 411.28     |
| MaxReturn            | 1934.6     |
| StdReturn            | 232.55     |
| AverageEpisodeLength | 975.2      |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 117.88     |
| TotalNEpisodes       | 24297      |
| TotalNSamples        | 7.2988e+06 |
| ExplainedVariance    | 0.0001722  |
-------------------------------------
[2018-01-21 16:21:05.431811 UTC] Saving snapshot
[2018-01-21 16:21:05.431990 UTC] Starting iteration 1459
[2018-01-21 16:21:05.432092 UTC] Start collecting samples
[2018-01-21 16:21:10.209972 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:10.328457 UTC] Performing policy update
[2018-01-21 16:21:10.329732 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:10.451728 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:11.831876 UTC] Performing line search
[2018-01-21 16:21:12.040711 UTC] Updating baseline
[2018-01-21 16:21:14.368991 UTC] Computing logging information
-------------------------------------
| Iteration            | 1459       |
| ExpectedImprovement  | 0.017948   |
| ActualImprovement    | 0.016334   |
| ImprovementRatio     | 0.91005    |
| MeanKL               | 0.0086261  |
| Entropy              | -1.7102    |
| Perplexity           | 0.18082    |
| AveragePolicyStd     | 0.18405    |
| AveragePolicyStd[0]  | 0.20401    |
| AveragePolicyStd[1]  | 0.20011    |
| AveragePolicyStd[2]  | 0.14834    |
| AveragePolicyStd[3]  | 0.17881    |
| AveragePolicyStd[4]  | 0.15086    |
| AveragePolicyStd[5]  | 0.22216    |
| AverageReturn        | 1818.1     |
| MinReturn            | 411.28     |
| MaxReturn            | 1934.6     |
| StdReturn            | 215.44     |
| AverageEpisodeLength | 979.93     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.95     |
| TotalNEpisodes       | 24301      |
| TotalNSamples        | 7.3028e+06 |
| ExplainedVariance    | 0.00099939 |
-------------------------------------
[2018-01-21 16:21:15.213202 UTC] Saving snapshot
[2018-01-21 16:21:15.213437 UTC] Starting iteration 1460
[2018-01-21 16:21:15.213585 UTC] Start collecting samples
[2018-01-21 16:21:19.743045 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:19.888906 UTC] Performing policy update
[2018-01-21 16:21:19.889861 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:20.012350 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:21.404382 UTC] Performing line search
[2018-01-21 16:21:21.595249 UTC] Updating baseline
[2018-01-21 16:21:23.452402 UTC] Computing logging information
-------------------------------------
| Iteration            | 1460       |
| ExpectedImprovement  | 0.018161   |
| ActualImprovement    | 0.01718    |
| ImprovementRatio     | 0.946      |
| MeanKL               | 0.0073701  |
| Entropy              | -1.7085    |
| Perplexity           | 0.18114    |
| AveragePolicyStd     | 0.18413    |
| AveragePolicyStd[0]  | 0.20434    |
| AveragePolicyStd[1]  | 0.19993    |
| AveragePolicyStd[2]  | 0.14813    |
| AveragePolicyStd[3]  | 0.17918    |
| AveragePolicyStd[4]  | 0.15073    |
| AveragePolicyStd[5]  | 0.22245    |
| AverageReturn        | 1821.7     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 211.27     |
| AverageEpisodeLength | 982.15     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.06     |
| TotalNEpisodes       | 24309      |
| TotalNSamples        | 7.3108e+06 |
| ExplainedVariance    | 0.0083385  |
-------------------------------------
[2018-01-21 16:21:24.230616 UTC] Saving snapshot
[2018-01-21 16:21:24.236878 UTC] Starting iteration 1461
[2018-01-21 16:21:24.237077 UTC] Start collecting samples
[2018-01-21 16:21:28.611733 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:28.734047 UTC] Performing policy update
[2018-01-21 16:21:28.734667 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:28.874934 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:30.277495 UTC] Performing line search
[2018-01-21 16:21:30.476710 UTC] Updating baseline
[2018-01-21 16:21:32.308187 UTC] Computing logging information
-------------------------------------
| Iteration            | 1461       |
| ExpectedImprovement  | 0.018478   |
| ActualImprovement    | 0.017686   |
| ImprovementRatio     | 0.95713    |
| MeanKL               | 0.0073257  |
| Entropy              | -1.7107    |
| Perplexity           | 0.18074    |
| AveragePolicyStd     | 0.18408    |
| AveragePolicyStd[0]  | 0.20392    |
| AveragePolicyStd[1]  | 0.20031    |
| AveragePolicyStd[2]  | 0.14788    |
| AveragePolicyStd[3]  | 0.17919    |
| AveragePolicyStd[4]  | 0.15053    |
| AveragePolicyStd[5]  | 0.22265    |
| AverageReturn        | 1821       |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 211.38     |
| AverageEpisodeLength | 982.15     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 107.06     |
| TotalNEpisodes       | 24311      |
| TotalNSamples        | 7.3128e+06 |
| ExplainedVariance    | -0.0090068 |
-------------------------------------
[2018-01-21 16:21:33.194036 UTC] Saving snapshot
[2018-01-21 16:21:33.194342 UTC] Starting iteration 1462
[2018-01-21 16:21:33.194609 UTC] Start collecting samples
[2018-01-21 16:21:37.579173 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:37.724386 UTC] Performing policy update
[2018-01-21 16:21:37.724982 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:37.868031 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:39.299759 UTC] Performing line search
[2018-01-21 16:21:39.498526 UTC] Updating baseline
[2018-01-21 16:21:41.213397 UTC] Computing logging information
-------------------------------------
| Iteration            | 1462       |
| ExpectedImprovement  | 0.016442   |
| ActualImprovement    | 0.015794   |
| ImprovementRatio     | 0.96053    |
| MeanKL               | 0.0081952  |
| Entropy              | -1.712     |
| Perplexity           | 0.1805     |
| AveragePolicyStd     | 0.18404    |
| AveragePolicyStd[0]  | 0.20383    |
| AveragePolicyStd[1]  | 0.19983    |
| AveragePolicyStd[2]  | 0.14799    |
| AveragePolicyStd[3]  | 0.17935    |
| AveragePolicyStd[4]  | 0.15038    |
| AveragePolicyStd[5]  | 0.22287    |
| AverageReturn        | 1817.5     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 212.38     |
| AverageEpisodeLength | 980.41     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.17     |
| TotalNEpisodes       | 24318      |
| TotalNSamples        | 7.3196e+06 |
| ExplainedVariance    | 0.074692   |
-------------------------------------
[2018-01-21 16:21:41.995595 UTC] Saving snapshot
[2018-01-21 16:21:41.995778 UTC] Starting iteration 1463
[2018-01-21 16:21:41.995908 UTC] Start collecting samples
[2018-01-21 16:21:46.485718 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:46.620078 UTC] Performing policy update
[2018-01-21 16:21:46.621136 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:46.740691 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:48.122167 UTC] Performing line search
[2018-01-21 16:21:48.323331 UTC] Updating baseline
[2018-01-21 16:21:50.062207 UTC] Computing logging information
-------------------------------------
| Iteration            | 1463       |
| ExpectedImprovement  | 0.019743   |
| ActualImprovement    | 0.018319   |
| ImprovementRatio     | 0.92787    |
| MeanKL               | 0.0076934  |
| Entropy              | -1.7131    |
| Perplexity           | 0.1803     |
| AveragePolicyStd     | 0.18399    |
| AveragePolicyStd[0]  | 0.2039     |
| AveragePolicyStd[1]  | 0.19962    |
| AveragePolicyStd[2]  | 0.14847    |
| AveragePolicyStd[3]  | 0.17925    |
| AveragePolicyStd[4]  | 0.15007    |
| AveragePolicyStd[5]  | 0.22262    |
| AverageReturn        | 1817.1     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 212.29     |
| AverageEpisodeLength | 980.41     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.17     |
| TotalNEpisodes       | 24321      |
| TotalNSamples        | 7.3226e+06 |
| ExplainedVariance    | -0.015201  |
-------------------------------------
[2018-01-21 16:21:50.865079 UTC] Saving snapshot
[2018-01-21 16:21:50.865385 UTC] Starting iteration 1464
[2018-01-21 16:21:50.865589 UTC] Start collecting samples
[2018-01-21 16:21:55.591113 UTC] Computing input variables for policy optimization
[2018-01-21 16:21:55.718164 UTC] Performing policy update
[2018-01-21 16:21:55.719235 UTC] Computing gradient in Euclidean space
[2018-01-21 16:21:55.834599 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:21:57.226761 UTC] Performing line search
[2018-01-21 16:21:57.432655 UTC] Updating baseline
[2018-01-21 16:22:00.485412 UTC] Computing logging information
-------------------------------------
| Iteration            | 1464       |
| ExpectedImprovement  | 0.018526   |
| ActualImprovement    | 0.017647   |
| ImprovementRatio     | 0.95254    |
| MeanKL               | 0.008504   |
| Entropy              | -1.7182    |
| Perplexity           | 0.17939    |
| AveragePolicyStd     | 0.18382    |
| AveragePolicyStd[0]  | 0.20352    |
| AveragePolicyStd[1]  | 0.19928    |
| AveragePolicyStd[2]  | 0.14843    |
| AveragePolicyStd[3]  | 0.17923    |
| AveragePolicyStd[4]  | 0.15003    |
| AveragePolicyStd[5]  | 0.22242    |
| AverageReturn        | 1823.5     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 203.81     |
| AverageEpisodeLength | 983.58     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.97     |
| TotalNEpisodes       | 24327      |
| TotalNSamples        | 7.3286e+06 |
| ExplainedVariance    | 0.00043621 |
-------------------------------------
[2018-01-21 16:22:01.397860 UTC] Saving snapshot
[2018-01-21 16:22:01.398101 UTC] Starting iteration 1465
[2018-01-21 16:22:01.398278 UTC] Start collecting samples
[2018-01-21 16:22:05.930693 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:06.053407 UTC] Performing policy update
[2018-01-21 16:22:06.054006 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:06.170710 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:07.541729 UTC] Performing line search
[2018-01-21 16:22:07.739906 UTC] Updating baseline
[2018-01-21 16:22:10.378836 UTC] Computing logging information
-------------------------------------
| Iteration            | 1465       |
| ExpectedImprovement  | 0.018079   |
| ActualImprovement    | 0.01709    |
| ImprovementRatio     | 0.94532    |
| MeanKL               | 0.0093043  |
| Entropy              | -1.7154    |
| Perplexity           | 0.1799     |
| AveragePolicyStd     | 0.18392    |
| AveragePolicyStd[0]  | 0.20351    |
| AveragePolicyStd[1]  | 0.1998     |
| AveragePolicyStd[2]  | 0.14832    |
| AveragePolicyStd[3]  | 0.17925    |
| AveragePolicyStd[4]  | 0.15012    |
| AveragePolicyStd[5]  | 0.22248    |
| AverageReturn        | 1820.7     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 203.42     |
| AverageEpisodeLength | 983.58     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 103.97     |
| TotalNEpisodes       | 24332      |
| TotalNSamples        | 7.3336e+06 |
| ExplainedVariance    | 7.9123e-05 |
-------------------------------------
[2018-01-21 16:22:11.173030 UTC] Saving snapshot
[2018-01-21 16:22:11.173250 UTC] Starting iteration 1466
[2018-01-21 16:22:11.173442 UTC] Start collecting samples
[2018-01-21 16:22:15.605553 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:15.725254 UTC] Performing policy update
[2018-01-21 16:22:15.725917 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:15.842935 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:17.219183 UTC] Performing line search
[2018-01-21 16:22:17.425031 UTC] Updating baseline
[2018-01-21 16:22:19.999520 UTC] Computing logging information
-------------------------------------
| Iteration            | 1466       |
| ExpectedImprovement  | 0.017594   |
| ActualImprovement    | 0.01686    |
| ImprovementRatio     | 0.95827    |
| MeanKL               | 0.0076472  |
| Entropy              | -1.7209    |
| Perplexity           | 0.1789     |
| AveragePolicyStd     | 0.18375    |
| AveragePolicyStd[0]  | 0.20311    |
| AveragePolicyStd[1]  | 0.19958    |
| AveragePolicyStd[2]  | 0.14795    |
| AveragePolicyStd[3]  | 0.17921    |
| AveragePolicyStd[4]  | 0.15016    |
| AveragePolicyStd[5]  | 0.22248    |
| AverageReturn        | 1807.7     |
| MinReturn            | 411.28     |
| MaxReturn            | 1919       |
| StdReturn            | 239.96     |
| AverageEpisodeLength | 976.82     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.93     |
| TotalNEpisodes       | 24337      |
| TotalNSamples        | 7.3379e+06 |
| ExplainedVariance    | 0.098324   |
-------------------------------------
[2018-01-21 16:22:20.890943 UTC] Saving snapshot
[2018-01-21 16:22:20.891283 UTC] Starting iteration 1467
[2018-01-21 16:22:20.891457 UTC] Start collecting samples
[2018-01-21 16:22:25.600720 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:25.751369 UTC] Performing policy update
[2018-01-21 16:22:25.752467 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:25.871756 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:27.225345 UTC] Performing line search
[2018-01-21 16:22:27.409297 UTC] Updating baseline
[2018-01-21 16:22:30.619891 UTC] Computing logging information
-------------------------------------
| Iteration            | 1467       |
| ExpectedImprovement  | 0.018444   |
| ActualImprovement    | 0.017675   |
| ImprovementRatio     | 0.95832    |
| MeanKL               | 0.0086644  |
| Entropy              | -1.7156    |
| Perplexity           | 0.17986    |
| AveragePolicyStd     | 0.18394    |
| AveragePolicyStd[0]  | 0.20391    |
| AveragePolicyStd[1]  | 0.1999     |
| AveragePolicyStd[2]  | 0.14782    |
| AveragePolicyStd[3]  | 0.17927    |
| AveragePolicyStd[4]  | 0.15018    |
| AveragePolicyStd[5]  | 0.22255    |
| AverageReturn        | 1809.1     |
| MinReturn            | 411.28     |
| MaxReturn            | 1941.1     |
| StdReturn            | 240.49     |
| AverageEpisodeLength | 976.82     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.93     |
| TotalNEpisodes       | 24343      |
| TotalNSamples        | 7.3439e+06 |
| ExplainedVariance    | 0.00067615 |
-------------------------------------
[2018-01-21 16:22:31.404931 UTC] Saving snapshot
[2018-01-21 16:22:31.405108 UTC] Starting iteration 1468
[2018-01-21 16:22:31.405232 UTC] Start collecting samples
[2018-01-21 16:22:36.109355 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:36.245217 UTC] Performing policy update
[2018-01-21 16:22:36.246421 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:36.375119 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:37.769634 UTC] Performing line search
[2018-01-21 16:22:37.891481 UTC] Updating baseline
[2018-01-21 16:22:40.077433 UTC] Computing logging information
--------------------------------------
| Iteration            | 1468        |
| ExpectedImprovement  | 0.017814    |
| ActualImprovement    | 0.016566    |
| ImprovementRatio     | 0.92992     |
| MeanKL               | 0.0081549   |
| Entropy              | -1.7143     |
| Perplexity           | 0.1801      |
| AveragePolicyStd     | 0.18398     |
| AveragePolicyStd[0]  | 0.20388     |
| AveragePolicyStd[1]  | 0.19998     |
| AveragePolicyStd[2]  | 0.14785     |
| AveragePolicyStd[3]  | 0.17933     |
| AveragePolicyStd[4]  | 0.15021     |
| AveragePolicyStd[5]  | 0.22262     |
| AverageReturn        | 1808.3      |
| MinReturn            | 411.28      |
| MaxReturn            | 1941.1      |
| StdReturn            | 240.33      |
| AverageEpisodeLength | 976.82      |
| MinEpisodeLength     | 259         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 122.93      |
| TotalNEpisodes       | 24348       |
| TotalNSamples        | 7.3489e+06  |
| ExplainedVariance    | -0.00053699 |
--------------------------------------
[2018-01-21 16:22:41.020504 UTC] Saving snapshot
[2018-01-21 16:22:41.020744 UTC] Starting iteration 1469
[2018-01-21 16:22:41.020932 UTC] Start collecting samples
[2018-01-21 16:22:45.769726 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:45.891768 UTC] Performing policy update
[2018-01-21 16:22:45.892591 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:46.012240 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:47.407709 UTC] Performing line search
[2018-01-21 16:22:47.607928 UTC] Updating baseline
[2018-01-21 16:22:49.312024 UTC] Computing logging information
-------------------------------------
| Iteration            | 1469       |
| ExpectedImprovement  | 0.019048   |
| ActualImprovement    | 0.017256   |
| ImprovementRatio     | 0.90592    |
| MeanKL               | 0.0077268  |
| Entropy              | -1.7135    |
| Perplexity           | 0.18023    |
| AveragePolicyStd     | 0.184      |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.1998     |
| AveragePolicyStd[2]  | 0.14779    |
| AveragePolicyStd[3]  | 0.1794     |
| AveragePolicyStd[4]  | 0.15027    |
| AveragePolicyStd[5]  | 0.22278    |
| AverageReturn        | 1808.3     |
| MinReturn            | 411.28     |
| MaxReturn            | 1941.1     |
| StdReturn            | 240.34     |
| AverageEpisodeLength | 976.82     |
| MinEpisodeLength     | 259        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 122.93     |
| TotalNEpisodes       | 24352      |
| TotalNSamples        | 7.3529e+06 |
| ExplainedVariance    | -5.447e-05 |
-------------------------------------
[2018-01-21 16:22:50.119983 UTC] Saving snapshot
[2018-01-21 16:22:50.120250 UTC] Starting iteration 1470
[2018-01-21 16:22:50.120419 UTC] Start collecting samples
[2018-01-21 16:22:54.789936 UTC] Computing input variables for policy optimization
[2018-01-21 16:22:54.948884 UTC] Performing policy update
[2018-01-21 16:22:54.950069 UTC] Computing gradient in Euclidean space
[2018-01-21 16:22:55.076590 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:22:56.508233 UTC] Performing line search
[2018-01-21 16:22:56.710346 UTC] Updating baseline
[2018-01-21 16:22:58.711896 UTC] Computing logging information
-------------------------------------
| Iteration            | 1470       |
| ExpectedImprovement  | 0.018532   |
| ActualImprovement    | 0.017713   |
| ImprovementRatio     | 0.95579    |
| MeanKL               | 0.008089   |
| Entropy              | -1.7124    |
| Perplexity           | 0.18044    |
| AveragePolicyStd     | 0.18406    |
| AveragePolicyStd[0]  | 0.20435    |
| AveragePolicyStd[1]  | 0.19965    |
| AveragePolicyStd[2]  | 0.14783    |
| AveragePolicyStd[3]  | 0.17956    |
| AveragePolicyStd[4]  | 0.15       |
| AveragePolicyStd[5]  | 0.22295    |
| AverageReturn        | 1823.5     |
| MinReturn            | 440.87     |
| MaxReturn            | 1941.1     |
| StdReturn            | 195.3      |
| AverageEpisodeLength | 984.23     |
| MinEpisodeLength     | 273        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.545     |
| TotalNEpisodes       | 24359      |
| TotalNSamples        | 7.3599e+06 |
| ExplainedVariance    | 0.034271   |
-------------------------------------
[2018-01-21 16:22:59.520522 UTC] Saving snapshot
[2018-01-21 16:22:59.532260 UTC] Starting iteration 1471
[2018-01-21 16:22:59.532495 UTC] Start collecting samples
[2018-01-21 16:23:04.294924 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:04.419332 UTC] Performing policy update
[2018-01-21 16:23:04.420486 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:04.536583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:05.907446 UTC] Performing line search
[2018-01-21 16:23:06.099611 UTC] Updating baseline
[2018-01-21 16:23:08.206849 UTC] Computing logging information
-------------------------------------
| Iteration            | 1471       |
| ExpectedImprovement  | 0.018741   |
| ActualImprovement    | 0.017239   |
| ImprovementRatio     | 0.91987    |
| MeanKL               | 0.0087715  |
| Entropy              | -1.7085    |
| Perplexity           | 0.18114    |
| AveragePolicyStd     | 0.18421    |
| AveragePolicyStd[0]  | 0.20451    |
| AveragePolicyStd[1]  | 0.20004    |
| AveragePolicyStd[2]  | 0.14753    |
| AveragePolicyStd[3]  | 0.17956    |
| AveragePolicyStd[4]  | 0.15006    |
| AveragePolicyStd[5]  | 0.22356    |
| AverageReturn        | 1814.6     |
| MinReturn            | 440.87     |
| MaxReturn            | 1941.1     |
| StdReturn            | 212.35     |
| AverageEpisodeLength | 979.52     |
| MinEpisodeLength     | 273        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 109.35     |
| TotalNEpisodes       | 24364      |
| TotalNSamples        | 7.3645e+06 |
| ExplainedVariance    | 0.066305   |
-------------------------------------
[2018-01-21 16:23:09.016727 UTC] Saving snapshot
[2018-01-21 16:23:09.016911 UTC] Starting iteration 1472
[2018-01-21 16:23:09.017140 UTC] Start collecting samples
[2018-01-21 16:23:13.754277 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:13.896643 UTC] Performing policy update
[2018-01-21 16:23:13.897770 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:14.023905 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:15.381976 UTC] Performing line search
[2018-01-21 16:23:15.584328 UTC] Updating baseline
[2018-01-21 16:23:17.620775 UTC] Computing logging information
-------------------------------------
| Iteration            | 1472       |
| ExpectedImprovement  | 0.019133   |
| ActualImprovement    | 0.017813   |
| ImprovementRatio     | 0.93097    |
| MeanKL               | 0.0082496  |
| Entropy              | -1.7045    |
| Perplexity           | 0.18186    |
| AveragePolicyStd     | 0.18433    |
| AveragePolicyStd[0]  | 0.20458    |
| AveragePolicyStd[1]  | 0.20016    |
| AveragePolicyStd[2]  | 0.14769    |
| AveragePolicyStd[3]  | 0.1794     |
| AveragePolicyStd[4]  | 0.15035    |
| AveragePolicyStd[5]  | 0.22377    |
| AverageReturn        | 1823.6     |
| MinReturn            | 540.63     |
| MaxReturn            | 1941.1     |
| StdReturn            | 176.94     |
| AverageEpisodeLength | 982.98     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 90.845     |
| TotalNEpisodes       | 24369      |
| TotalNSamples        | 7.3691e+06 |
| ExplainedVariance    | 0.13064    |
-------------------------------------
[2018-01-21 16:23:18.504106 UTC] Saving snapshot
[2018-01-21 16:23:18.504336 UTC] Starting iteration 1473
[2018-01-21 16:23:18.504481 UTC] Start collecting samples
[2018-01-21 16:23:22.962590 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:23.094027 UTC] Performing policy update
[2018-01-21 16:23:23.095179 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:23.217701 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:24.571188 UTC] Performing line search
[2018-01-21 16:23:24.766641 UTC] Updating baseline
[2018-01-21 16:23:26.386047 UTC] Computing logging information
-------------------------------------
| Iteration            | 1473       |
| ExpectedImprovement  | 0.01666    |
| ActualImprovement    | 0.016135   |
| ImprovementRatio     | 0.96849    |
| MeanKL               | 0.0080682  |
| Entropy              | -1.7041    |
| Perplexity           | 0.18193    |
| AveragePolicyStd     | 0.18433    |
| AveragePolicyStd[0]  | 0.2049     |
| AveragePolicyStd[1]  | 0.19939    |
| AveragePolicyStd[2]  | 0.14805    |
| AveragePolicyStd[3]  | 0.17944    |
| AveragePolicyStd[4]  | 0.15016    |
| AveragePolicyStd[5]  | 0.22406    |
| AverageReturn        | 1816.3     |
| MinReturn            | 540.63     |
| MaxReturn            | 1941.1     |
| StdReturn            | 193.04     |
| AverageEpisodeLength | 978.82     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.119     |
| TotalNEpisodes       | 24375      |
| TotalNSamples        | 7.3747e+06 |
| ExplainedVariance    | 0.11719    |
-------------------------------------
[2018-01-21 16:23:27.295261 UTC] Saving snapshot
[2018-01-21 16:23:27.295513 UTC] Starting iteration 1474
[2018-01-21 16:23:27.295694 UTC] Start collecting samples
[2018-01-21 16:23:31.756828 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:31.882884 UTC] Performing policy update
[2018-01-21 16:23:31.883629 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:31.993987 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:33.421449 UTC] Performing line search
[2018-01-21 16:23:33.624132 UTC] Updating baseline
[2018-01-21 16:23:35.700232 UTC] Computing logging information
--------------------------------------
| Iteration            | 1474        |
| ExpectedImprovement  | 0.019711    |
| ActualImprovement    | 0.018487    |
| ImprovementRatio     | 0.93792     |
| MeanKL               | 0.0086587   |
| Entropy              | -1.6984     |
| Perplexity           | 0.18298     |
| AveragePolicyStd     | 0.18451     |
| AveragePolicyStd[0]  | 0.20524     |
| AveragePolicyStd[1]  | 0.19947     |
| AveragePolicyStd[2]  | 0.14819     |
| AveragePolicyStd[3]  | 0.17922     |
| AveragePolicyStd[4]  | 0.15056     |
| AveragePolicyStd[5]  | 0.22435     |
| AverageReturn        | 1818.3      |
| MinReturn            | 540.63      |
| MaxReturn            | 1941.1      |
| StdReturn            | 193.35      |
| AverageEpisodeLength | 978.82      |
| MinEpisodeLength     | 324         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 99.119      |
| TotalNEpisodes       | 24379       |
| TotalNSamples        | 7.3787e+06  |
| ExplainedVariance    | -1.9913e-05 |
--------------------------------------
[2018-01-21 16:23:36.482655 UTC] Saving snapshot
[2018-01-21 16:23:36.482895 UTC] Starting iteration 1475
[2018-01-21 16:23:36.483045 UTC] Start collecting samples
[2018-01-21 16:23:41.100439 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:41.244740 UTC] Performing policy update
[2018-01-21 16:23:41.245461 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:41.359793 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:42.717408 UTC] Performing line search
[2018-01-21 16:23:42.904314 UTC] Updating baseline
[2018-01-21 16:23:44.995372 UTC] Computing logging information
-------------------------------------
| Iteration            | 1475       |
| ExpectedImprovement  | 0.018902   |
| ActualImprovement    | 0.017512   |
| ImprovementRatio     | 0.92648    |
| MeanKL               | 0.0084512  |
| Entropy              | -1.7004    |
| Perplexity           | 0.18262    |
| AveragePolicyStd     | 0.18445    |
| AveragePolicyStd[0]  | 0.20495    |
| AveragePolicyStd[1]  | 0.19941    |
| AveragePolicyStd[2]  | 0.14822    |
| AveragePolicyStd[3]  | 0.17886    |
| AveragePolicyStd[4]  | 0.15054    |
| AveragePolicyStd[5]  | 0.22473    |
| AverageReturn        | 1820.8     |
| MinReturn            | 540.63     |
| MaxReturn            | 1941.1     |
| StdReturn            | 193.95     |
| AverageEpisodeLength | 978.82     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.119     |
| TotalNEpisodes       | 24384      |
| TotalNSamples        | 7.3837e+06 |
| ExplainedVariance    | 0.072742   |
-------------------------------------
[2018-01-21 16:23:45.797202 UTC] Saving snapshot
[2018-01-21 16:23:45.797541 UTC] Starting iteration 1476
[2018-01-21 16:23:45.797772 UTC] Start collecting samples
[2018-01-21 16:23:50.480137 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:50.627817 UTC] Performing policy update
[2018-01-21 16:23:50.628647 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:50.746781 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:23:52.241503 UTC] Performing line search
[2018-01-21 16:23:52.431518 UTC] Updating baseline
[2018-01-21 16:23:54.066201 UTC] Computing logging information
-------------------------------------
| Iteration            | 1476       |
| ExpectedImprovement  | 0.018132   |
| ActualImprovement    | 0.017408   |
| ImprovementRatio     | 0.96007    |
| MeanKL               | 0.0080882  |
| Entropy              | -1.6966    |
| Perplexity           | 0.18331    |
| AveragePolicyStd     | 0.18459    |
| AveragePolicyStd[0]  | 0.20506    |
| AveragePolicyStd[1]  | 0.19993    |
| AveragePolicyStd[2]  | 0.14827    |
| AveragePolicyStd[3]  | 0.17865    |
| AveragePolicyStd[4]  | 0.1505     |
| AveragePolicyStd[5]  | 0.22511    |
| AverageReturn        | 1824       |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 194.75     |
| AverageEpisodeLength | 978.82     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.119     |
| TotalNEpisodes       | 24391      |
| TotalNSamples        | 7.3907e+06 |
| ExplainedVariance    | -0.0021489 |
-------------------------------------
[2018-01-21 16:23:54.870712 UTC] Saving snapshot
[2018-01-21 16:23:54.870956 UTC] Starting iteration 1477
[2018-01-21 16:23:54.871128 UTC] Start collecting samples
[2018-01-21 16:23:59.444854 UTC] Computing input variables for policy optimization
[2018-01-21 16:23:59.568330 UTC] Performing policy update
[2018-01-21 16:23:59.569411 UTC] Computing gradient in Euclidean space
[2018-01-21 16:23:59.700578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:01.076241 UTC] Performing line search
[2018-01-21 16:24:01.263716 UTC] Updating baseline
[2018-01-21 16:24:03.675417 UTC] Computing logging information
-------------------------------------
| Iteration            | 1477       |
| ExpectedImprovement  | 0.018092   |
| ActualImprovement    | 0.017171   |
| ImprovementRatio     | 0.94911    |
| MeanKL               | 0.0079384  |
| Entropy              | -1.6896    |
| Perplexity           | 0.1846     |
| AveragePolicyStd     | 0.18483    |
| AveragePolicyStd[0]  | 0.20558    |
| AveragePolicyStd[1]  | 0.19991    |
| AveragePolicyStd[2]  | 0.14798    |
| AveragePolicyStd[3]  | 0.17907    |
| AveragePolicyStd[4]  | 0.15074    |
| AveragePolicyStd[5]  | 0.22571    |
| AverageReturn        | 1825.6     |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 195.14     |
| AverageEpisodeLength | 978.82     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 99.119     |
| TotalNEpisodes       | 24394      |
| TotalNSamples        | 7.3937e+06 |
| ExplainedVariance    | 0.0015907  |
-------------------------------------
[2018-01-21 16:24:04.547100 UTC] Saving snapshot
[2018-01-21 16:24:04.547369 UTC] Starting iteration 1478
[2018-01-21 16:24:04.547609 UTC] Start collecting samples
[2018-01-21 16:24:09.200804 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:09.347639 UTC] Performing policy update
[2018-01-21 16:24:09.348306 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:09.467368 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:10.861302 UTC] Performing line search
[2018-01-21 16:24:11.058788 UTC] Updating baseline
[2018-01-21 16:24:13.173453 UTC] Computing logging information
-------------------------------------
| Iteration            | 1478       |
| ExpectedImprovement  | 0.019743   |
| ActualImprovement    | 0.018428   |
| ImprovementRatio     | 0.93339    |
| MeanKL               | 0.0072338  |
| Entropy              | -1.6894    |
| Perplexity           | 0.18463    |
| AveragePolicyStd     | 0.18483    |
| AveragePolicyStd[0]  | 0.20509    |
| AveragePolicyStd[1]  | 0.20007    |
| AveragePolicyStd[2]  | 0.14787    |
| AveragePolicyStd[3]  | 0.17891    |
| AveragePolicyStd[4]  | 0.15104    |
| AveragePolicyStd[5]  | 0.226      |
| AverageReturn        | 1823.5     |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 197.56     |
| AverageEpisodeLength | 977.24     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.02     |
| TotalNEpisodes       | 24399      |
| TotalNSamples        | 7.3985e+06 |
| ExplainedVariance    | 0.096995   |
-------------------------------------
[2018-01-21 16:24:13.977826 UTC] Saving snapshot
[2018-01-21 16:24:13.978013 UTC] Starting iteration 1479
[2018-01-21 16:24:13.978170 UTC] Start collecting samples
[2018-01-21 16:24:18.485487 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:18.643928 UTC] Performing policy update
[2018-01-21 16:24:18.644870 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:18.771446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:20.166361 UTC] Performing line search
[2018-01-21 16:24:20.357571 UTC] Updating baseline
[2018-01-21 16:24:22.951410 UTC] Computing logging information
-------------------------------------
| Iteration            | 1479       |
| ExpectedImprovement  | 0.020016   |
| ActualImprovement    | 0.017993   |
| ImprovementRatio     | 0.89894    |
| MeanKL               | 0.0085409  |
| Entropy              | -1.6913    |
| Perplexity           | 0.18427    |
| AveragePolicyStd     | 0.18477    |
| AveragePolicyStd[0]  | 0.20485    |
| AveragePolicyStd[1]  | 0.19974    |
| AveragePolicyStd[2]  | 0.14788    |
| AveragePolicyStd[3]  | 0.17857    |
| AveragePolicyStd[4]  | 0.15117    |
| AveragePolicyStd[5]  | 0.22643    |
| AverageReturn        | 1825.9     |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 197.94     |
| AverageEpisodeLength | 977.24     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 100.02     |
| TotalNEpisodes       | 24403      |
| TotalNSamples        | 7.4025e+06 |
| ExplainedVariance    | -0.006266  |
-------------------------------------
[2018-01-21 16:24:23.850998 UTC] Saving snapshot
[2018-01-21 16:24:23.851231 UTC] Starting iteration 1480
[2018-01-21 16:24:23.851391 UTC] Start collecting samples
[2018-01-21 16:24:28.541844 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:28.675630 UTC] Performing policy update
[2018-01-21 16:24:28.676299 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:28.802112 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:30.184122 UTC] Performing line search
[2018-01-21 16:24:30.380399 UTC] Updating baseline
[2018-01-21 16:24:32.572815 UTC] Computing logging information
-------------------------------------
| Iteration            | 1480       |
| ExpectedImprovement  | 0.018251   |
| ActualImprovement    | 0.017208   |
| ImprovementRatio     | 0.94288    |
| MeanKL               | 0.0085195  |
| Entropy              | -1.6933    |
| Perplexity           | 0.18391    |
| AveragePolicyStd     | 0.18474    |
| AveragePolicyStd[0]  | 0.20509    |
| AveragePolicyStd[1]  | 0.1996     |
| AveragePolicyStd[2]  | 0.1477     |
| AveragePolicyStd[3]  | 0.1785     |
| AveragePolicyStd[4]  | 0.15096    |
| AveragePolicyStd[5]  | 0.22656    |
| AverageReturn        | 1824       |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 207.81     |
| AverageEpisodeLength | 973.85     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 104.82     |
| TotalNEpisodes       | 24411      |
| TotalNSamples        | 7.4102e+06 |
| ExplainedVariance    | 0.084471   |
-------------------------------------
[2018-01-21 16:24:33.407915 UTC] Saving snapshot
[2018-01-21 16:24:33.419311 UTC] Starting iteration 1481
[2018-01-21 16:24:33.419550 UTC] Start collecting samples
[2018-01-21 16:24:37.876945 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:38.024765 UTC] Performing policy update
[2018-01-21 16:24:38.025829 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:38.147478 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:39.591508 UTC] Performing line search
[2018-01-21 16:24:39.800770 UTC] Updating baseline
[2018-01-21 16:24:41.864887 UTC] Computing logging information
-------------------------------------
| Iteration            | 1481       |
| ExpectedImprovement  | 0.018995   |
| ActualImprovement    | 0.017672   |
| ImprovementRatio     | 0.93031    |
| MeanKL               | 0.0084502  |
| Entropy              | -1.6921    |
| Perplexity           | 0.18412    |
| AveragePolicyStd     | 0.18477    |
| AveragePolicyStd[0]  | 0.2052     |
| AveragePolicyStd[1]  | 0.19949    |
| AveragePolicyStd[2]  | 0.14779    |
| AveragePolicyStd[3]  | 0.1785     |
| AveragePolicyStd[4]  | 0.15101    |
| AveragePolicyStd[5]  | 0.22664    |
| AverageReturn        | 1813.2     |
| MinReturn            | 540.63     |
| MaxReturn            | 1950.1     |
| StdReturn            | 234.87     |
| AverageEpisodeLength | 967.83     |
| MinEpisodeLength     | 324        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.42     |
| TotalNEpisodes       | 24415      |
| TotalNSamples        | 7.4136e+06 |
| ExplainedVariance    | 0.12816    |
-------------------------------------
[2018-01-21 16:24:42.770327 UTC] Saving snapshot
[2018-01-21 16:24:42.770547 UTC] Starting iteration 1482
[2018-01-21 16:24:42.770716 UTC] Start collecting samples
[2018-01-21 16:24:47.461026 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:47.583661 UTC] Performing policy update
[2018-01-21 16:24:47.584729 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:47.701979 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:49.092233 UTC] Performing line search
[2018-01-21 16:24:49.287989 UTC] Updating baseline
[2018-01-21 16:24:51.480796 UTC] Computing logging information
-------------------------------------
| Iteration            | 1482       |
| ExpectedImprovement  | 0.020415   |
| ActualImprovement    | 0.018842   |
| ImprovementRatio     | 0.92295    |
| MeanKL               | 0.0079118  |
| Entropy              | -1.6916    |
| Perplexity           | 0.18423    |
| AveragePolicyStd     | 0.18478    |
| AveragePolicyStd[0]  | 0.20479    |
| AveragePolicyStd[1]  | 0.19955    |
| AveragePolicyStd[2]  | 0.14767    |
| AveragePolicyStd[3]  | 0.17858    |
| AveragePolicyStd[4]  | 0.15134    |
| AveragePolicyStd[5]  | 0.22674    |
| AverageReturn        | 1803.3     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 271.83     |
| AverageEpisodeLength | 962.11     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.28     |
| TotalNEpisodes       | 24420      |
| TotalNSamples        | 7.4178e+06 |
| ExplainedVariance    | 0.11227    |
-------------------------------------
[2018-01-21 16:24:52.298818 UTC] Saving snapshot
[2018-01-21 16:24:52.299066 UTC] Starting iteration 1483
[2018-01-21 16:24:52.299297 UTC] Start collecting samples
[2018-01-21 16:24:56.880850 UTC] Computing input variables for policy optimization
[2018-01-21 16:24:57.035805 UTC] Performing policy update
[2018-01-21 16:24:57.036474 UTC] Computing gradient in Euclidean space
[2018-01-21 16:24:57.152828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:24:58.538500 UTC] Performing line search
[2018-01-21 16:24:58.736842 UTC] Updating baseline
[2018-01-21 16:25:00.657874 UTC] Computing logging information
-------------------------------------
| Iteration            | 1483       |
| ExpectedImprovement  | 0.016772   |
| ActualImprovement    | 0.016461   |
| ImprovementRatio     | 0.98148    |
| MeanKL               | 0.0076374  |
| Entropy              | -1.6918    |
| Perplexity           | 0.18418    |
| AveragePolicyStd     | 0.18475    |
| AveragePolicyStd[0]  | 0.20451    |
| AveragePolicyStd[1]  | 0.19948    |
| AveragePolicyStd[2]  | 0.14796    |
| AveragePolicyStd[3]  | 0.17815    |
| AveragePolicyStd[4]  | 0.15149    |
| AveragePolicyStd[5]  | 0.22693    |
| AverageReturn        | 1805.9     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 272.5      |
| AverageEpisodeLength | 962.11     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.28     |
| TotalNEpisodes       | 24427      |
| TotalNSamples        | 7.4248e+06 |
| ExplainedVariance    | -0.02367   |
-------------------------------------
[2018-01-21 16:25:01.486447 UTC] Saving snapshot
[2018-01-21 16:25:01.486668 UTC] Starting iteration 1484
[2018-01-21 16:25:01.486799 UTC] Start collecting samples
[2018-01-21 16:25:05.926518 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:06.047233 UTC] Performing policy update
[2018-01-21 16:25:06.047886 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:06.167143 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:07.553069 UTC] Performing line search
[2018-01-21 16:25:07.744664 UTC] Updating baseline
[2018-01-21 16:25:09.974607 UTC] Computing logging information
-------------------------------------
| Iteration            | 1484       |
| ExpectedImprovement  | 0.021142   |
| ActualImprovement    | 0.019784   |
| ImprovementRatio     | 0.93573    |
| MeanKL               | 0.0074259  |
| Entropy              | -1.6938    |
| Perplexity           | 0.18382    |
| AveragePolicyStd     | 0.18466    |
| AveragePolicyStd[0]  | 0.20406    |
| AveragePolicyStd[1]  | 0.19894    |
| AveragePolicyStd[2]  | 0.14782    |
| AveragePolicyStd[3]  | 0.17834    |
| AveragePolicyStd[4]  | 0.15198    |
| AveragePolicyStd[5]  | 0.22683    |
| AverageReturn        | 1808.7     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 273.17     |
| AverageEpisodeLength | 962.11     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.28     |
| TotalNEpisodes       | 24431      |
| TotalNSamples        | 7.4288e+06 |
| ExplainedVariance    | -0.0069617 |
-------------------------------------
[2018-01-21 16:25:10.770658 UTC] Saving snapshot
[2018-01-21 16:25:10.770896 UTC] Starting iteration 1485
[2018-01-21 16:25:10.771054 UTC] Start collecting samples
[2018-01-21 16:25:15.412186 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:15.521059 UTC] Performing policy update
[2018-01-21 16:25:15.521654 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:15.636947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:17.042388 UTC] Performing line search
[2018-01-21 16:25:17.234884 UTC] Updating baseline
[2018-01-21 16:25:19.509305 UTC] Computing logging information
-------------------------------------
| Iteration            | 1485       |
| ExpectedImprovement  | 0.018496   |
| ActualImprovement    | 0.017281   |
| ImprovementRatio     | 0.9343     |
| MeanKL               | 0.0073331  |
| Entropy              | -1.6989    |
| Perplexity           | 0.18288    |
| AveragePolicyStd     | 0.18455    |
| AveragePolicyStd[0]  | 0.20449    |
| AveragePolicyStd[1]  | 0.1987     |
| AveragePolicyStd[2]  | 0.14699    |
| AveragePolicyStd[3]  | 0.17848    |
| AveragePolicyStd[4]  | 0.15176    |
| AveragePolicyStd[5]  | 0.22691    |
| AverageReturn        | 1815.2     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 255.25     |
| AverageEpisodeLength | 964.56     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.8      |
| TotalNEpisodes       | 24436      |
| TotalNSamples        | 7.4334e+06 |
| ExplainedVariance    | 0.11184    |
-------------------------------------
[2018-01-21 16:25:20.416224 UTC] Saving snapshot
[2018-01-21 16:25:20.416455 UTC] Starting iteration 1486
[2018-01-21 16:25:20.416597 UTC] Start collecting samples
[2018-01-21 16:25:25.157966 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:25.308194 UTC] Performing policy update
[2018-01-21 16:25:25.309308 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:25.433196 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:26.844464 UTC] Performing line search
[2018-01-21 16:25:27.066552 UTC] Updating baseline
[2018-01-21 16:25:28.951611 UTC] Computing logging information
-------------------------------------
| Iteration            | 1486       |
| ExpectedImprovement  | 0.018525   |
| ActualImprovement    | 0.01786    |
| ImprovementRatio     | 0.96409    |
| MeanKL               | 0.0075689  |
| Entropy              | -1.7051    |
| Perplexity           | 0.18175    |
| AveragePolicyStd     | 0.18431    |
| AveragePolicyStd[0]  | 0.20366    |
| AveragePolicyStd[1]  | 0.19829    |
| AveragePolicyStd[2]  | 0.14717    |
| AveragePolicyStd[3]  | 0.17839    |
| AveragePolicyStd[4]  | 0.15199    |
| AveragePolicyStd[5]  | 0.22639    |
| AverageReturn        | 1800.8     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 285.99     |
| AverageEpisodeLength | 955.83     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.31     |
| TotalNEpisodes       | 24445      |
| TotalNSamples        | 7.4415e+06 |
| ExplainedVariance    | 0.12737    |
-------------------------------------
[2018-01-21 16:25:29.839020 UTC] Saving snapshot
[2018-01-21 16:25:29.839320 UTC] Starting iteration 1487
[2018-01-21 16:25:29.839485 UTC] Start collecting samples
[2018-01-21 16:25:34.475854 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:34.632002 UTC] Performing policy update
[2018-01-21 16:25:34.632687 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:34.750452 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:36.156713 UTC] Performing line search
[2018-01-21 16:25:36.366133 UTC] Updating baseline
[2018-01-21 16:25:38.655217 UTC] Computing logging information
-------------------------------------
| Iteration            | 1487       |
| ExpectedImprovement  | 0.018937   |
| ActualImprovement    | 0.017521   |
| ImprovementRatio     | 0.92525    |
| MeanKL               | 0.0085012  |
| Entropy              | -1.7096    |
| Perplexity           | 0.18094    |
| AveragePolicyStd     | 0.18415    |
| AveragePolicyStd[0]  | 0.20306    |
| AveragePolicyStd[1]  | 0.19846    |
| AveragePolicyStd[2]  | 0.14715    |
| AveragePolicyStd[3]  | 0.17867    |
| AveragePolicyStd[4]  | 0.15186    |
| AveragePolicyStd[5]  | 0.2257     |
| AverageReturn        | 1802.4     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 286.47     |
| AverageEpisodeLength | 955.83     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.31     |
| TotalNEpisodes       | 24448      |
| TotalNSamples        | 7.4445e+06 |
| ExplainedVariance    | -0.0027736 |
-------------------------------------
[2018-01-21 16:25:39.514783 UTC] Saving snapshot
[2018-01-21 16:25:39.515014 UTC] Starting iteration 1488
[2018-01-21 16:25:39.515229 UTC] Start collecting samples
[2018-01-21 16:25:43.992790 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:44.120803 UTC] Performing policy update
[2018-01-21 16:25:44.121453 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:44.238832 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:45.618837 UTC] Performing line search
[2018-01-21 16:25:45.805272 UTC] Updating baseline
[2018-01-21 16:25:47.395703 UTC] Computing logging information
-------------------------------------
| Iteration            | 1488       |
| ExpectedImprovement  | 0.019732   |
| ActualImprovement    | 0.018223   |
| ImprovementRatio     | 0.92352    |
| MeanKL               | 0.0080805  |
| Entropy              | -1.7083    |
| Perplexity           | 0.18118    |
| AveragePolicyStd     | 0.18419    |
| AveragePolicyStd[0]  | 0.20323    |
| AveragePolicyStd[1]  | 0.1984     |
| AveragePolicyStd[2]  | 0.14724    |
| AveragePolicyStd[3]  | 0.17848    |
| AveragePolicyStd[4]  | 0.15197    |
| AveragePolicyStd[5]  | 0.22584    |
| AverageReturn        | 1803.6     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 286.86     |
| AverageEpisodeLength | 955.83     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.31     |
| TotalNEpisodes       | 24451      |
| TotalNSamples        | 7.4475e+06 |
| ExplainedVariance    | 0.0024145  |
-------------------------------------
[2018-01-21 16:25:48.278774 UTC] Saving snapshot
[2018-01-21 16:25:48.279008 UTC] Starting iteration 1489
[2018-01-21 16:25:48.279187 UTC] Start collecting samples
[2018-01-21 16:25:52.931608 UTC] Computing input variables for policy optimization
[2018-01-21 16:25:53.067384 UTC] Performing policy update
[2018-01-21 16:25:53.068414 UTC] Computing gradient in Euclidean space
[2018-01-21 16:25:53.210533 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:25:54.600856 UTC] Performing line search
[2018-01-21 16:25:54.789986 UTC] Updating baseline
[2018-01-21 16:25:56.557073 UTC] Computing logging information
-------------------------------------
| Iteration            | 1489       |
| ExpectedImprovement  | 0.017171   |
| ActualImprovement    | 0.015951   |
| ImprovementRatio     | 0.92899    |
| MeanKL               | 0.0085358  |
| Entropy              | -1.7114    |
| Perplexity           | 0.18061    |
| AveragePolicyStd     | 0.18409    |
| AveragePolicyStd[0]  | 0.20294    |
| AveragePolicyStd[1]  | 0.19821    |
| AveragePolicyStd[2]  | 0.14742    |
| AveragePolicyStd[3]  | 0.17821    |
| AveragePolicyStd[4]  | 0.15189    |
| AveragePolicyStd[5]  | 0.22585    |
| AverageReturn        | 1805.1     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 287.34     |
| AverageEpisodeLength | 955.83     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.31     |
| TotalNEpisodes       | 24459      |
| TotalNSamples        | 7.4555e+06 |
| ExplainedVariance    | -0.01031   |
-------------------------------------
[2018-01-21 16:25:57.360265 UTC] Saving snapshot
[2018-01-21 16:25:57.360526 UTC] Starting iteration 1490
[2018-01-21 16:25:57.360710 UTC] Start collecting samples
[2018-01-21 16:26:01.960894 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:02.093318 UTC] Performing policy update
[2018-01-21 16:26:02.094518 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:02.208833 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:03.570587 UTC] Performing line search
[2018-01-21 16:26:03.753583 UTC] Updating baseline
[2018-01-21 16:26:06.510813 UTC] Computing logging information
-------------------------------------
| Iteration            | 1490       |
| ExpectedImprovement  | 0.019287   |
| ActualImprovement    | 0.018331   |
| ImprovementRatio     | 0.95041    |
| MeanKL               | 0.0073785  |
| Entropy              | -1.7107    |
| Perplexity           | 0.18074    |
| AveragePolicyStd     | 0.18412    |
| AveragePolicyStd[0]  | 0.20323    |
| AveragePolicyStd[1]  | 0.19776    |
| AveragePolicyStd[2]  | 0.14742    |
| AveragePolicyStd[3]  | 0.17858    |
| AveragePolicyStd[4]  | 0.1517     |
| AveragePolicyStd[5]  | 0.22601    |
| AverageReturn        | 1815       |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 275.56     |
| AverageEpisodeLength | 960.54     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.84     |
| TotalNEpisodes       | 24463      |
| TotalNSamples        | 7.4595e+06 |
| ExplainedVariance    | 0.00098809 |
-------------------------------------
[2018-01-21 16:26:07.362188 UTC] Saving snapshot
[2018-01-21 16:26:07.368384 UTC] Starting iteration 1491
[2018-01-21 16:26:07.368578 UTC] Start collecting samples
[2018-01-21 16:26:11.897434 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:12.019879 UTC] Performing policy update
[2018-01-21 16:26:12.020591 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:12.133645 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:13.534269 UTC] Performing line search
[2018-01-21 16:26:13.731260 UTC] Updating baseline
[2018-01-21 16:26:16.205734 UTC] Computing logging information
-------------------------------------
| Iteration            | 1491       |
| ExpectedImprovement  | 0.018236   |
| ActualImprovement    | 0.016682   |
| ImprovementRatio     | 0.91478    |
| MeanKL               | 0.0076959  |
| Entropy              | -1.7126    |
| Perplexity           | 0.1804     |
| AveragePolicyStd     | 0.18407    |
| AveragePolicyStd[0]  | 0.20312    |
| AveragePolicyStd[1]  | 0.19762    |
| AveragePolicyStd[2]  | 0.14719    |
| AveragePolicyStd[3]  | 0.17848    |
| AveragePolicyStd[4]  | 0.15181    |
| AveragePolicyStd[5]  | 0.22619    |
| AverageReturn        | 1823.6     |
| MinReturn            | 426.86     |
| MaxReturn            | 1950.1     |
| StdReturn            | 266.62     |
| AverageEpisodeLength | 964.35     |
| MinEpisodeLength     | 254        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133.55     |
| TotalNEpisodes       | 24466      |
| TotalNSamples        | 7.4625e+06 |
| ExplainedVariance    | 0.0010537  |
-------------------------------------
[2018-01-21 16:26:17.003591 UTC] Saving snapshot
[2018-01-21 16:26:17.003827 UTC] Starting iteration 1492
[2018-01-21 16:26:17.003975 UTC] Start collecting samples
[2018-01-21 16:26:21.578038 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:21.715800 UTC] Performing policy update
[2018-01-21 16:26:21.716665 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:21.837758 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:23.225421 UTC] Performing line search
[2018-01-21 16:26:23.411676 UTC] Updating baseline
[2018-01-21 16:26:25.921594 UTC] Computing logging information
--------------------------------------
| Iteration            | 1492        |
| ExpectedImprovement  | 0.019446    |
| ActualImprovement    | 0.018325    |
| ImprovementRatio     | 0.94236     |
| MeanKL               | 0.0085459   |
| Entropy              | -1.7098     |
| Perplexity           | 0.1809      |
| AveragePolicyStd     | 0.18418     |
| AveragePolicyStd[0]  | 0.20379     |
| AveragePolicyStd[1]  | 0.19722     |
| AveragePolicyStd[2]  | 0.14674     |
| AveragePolicyStd[3]  | 0.17845     |
| AveragePolicyStd[4]  | 0.15222     |
| AveragePolicyStd[5]  | 0.22665     |
| AverageReturn        | 1834.9      |
| MinReturn            | 426.86      |
| MaxReturn            | 1950.1      |
| StdReturn            | 256.01      |
| AverageEpisodeLength | 968.51      |
| MinEpisodeLength     | 254         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 128         |
| TotalNEpisodes       | 24475       |
| TotalNSamples        | 7.4715e+06  |
| ExplainedVariance    | -0.00048788 |
--------------------------------------
[2018-01-21 16:26:26.778918 UTC] Saving snapshot
[2018-01-21 16:26:26.779140 UTC] Starting iteration 1493
[2018-01-21 16:26:26.779306 UTC] Start collecting samples
[2018-01-21 16:26:31.384972 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:31.517470 UTC] Performing policy update
[2018-01-21 16:26:31.518092 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:31.636591 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:33.040876 UTC] Performing line search
[2018-01-21 16:26:33.226524 UTC] Updating baseline
[2018-01-21 16:26:35.139872 UTC] Computing logging information
-------------------------------------
| Iteration            | 1493       |
| ExpectedImprovement  | 0.019052   |
| ActualImprovement    | 0.01769    |
| ImprovementRatio     | 0.92851    |
| MeanKL               | 0.0079293  |
| Entropy              | -1.7155    |
| Perplexity           | 0.17988    |
| AveragePolicyStd     | 0.18397    |
| AveragePolicyStd[0]  | 0.20359    |
| AveragePolicyStd[1]  | 0.19689    |
| AveragePolicyStd[2]  | 0.1469     |
| AveragePolicyStd[3]  | 0.17813    |
| AveragePolicyStd[4]  | 0.15224    |
| AveragePolicyStd[5]  | 0.22609    |
| AverageReturn        | 1820.6     |
| MinReturn            | 346.77     |
| MaxReturn            | 1951       |
| StdReturn            | 295.97     |
| AverageEpisodeLength | 960.76     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.79     |
| TotalNEpisodes       | 24478      |
| TotalNSamples        | 7.4737e+06 |
| ExplainedVariance    | 0.16877    |
-------------------------------------
[2018-01-21 16:26:36.008231 UTC] Saving snapshot
[2018-01-21 16:26:36.008479 UTC] Starting iteration 1494
[2018-01-21 16:26:36.008609 UTC] Start collecting samples
[2018-01-21 16:26:40.724207 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:40.846498 UTC] Performing policy update
[2018-01-21 16:26:40.847921 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:40.973730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:42.362642 UTC] Performing line search
[2018-01-21 16:26:42.559167 UTC] Updating baseline
[2018-01-21 16:26:44.337403 UTC] Computing logging information
-------------------------------------
| Iteration            | 1494       |
| ExpectedImprovement  | 0.020252   |
| ActualImprovement    | 0.019961   |
| ImprovementRatio     | 0.9856     |
| MeanKL               | 0.0076959  |
| Entropy              | -1.7175    |
| Perplexity           | 0.17952    |
| AveragePolicyStd     | 0.18392    |
| AveragePolicyStd[0]  | 0.20395    |
| AveragePolicyStd[1]  | 0.1967     |
| AveragePolicyStd[2]  | 0.14671    |
| AveragePolicyStd[3]  | 0.1782     |
| AveragePolicyStd[4]  | 0.15211    |
| AveragePolicyStd[5]  | 0.22586    |
| AverageReturn        | 1812.5     |
| MinReturn            | 346.77     |
| MaxReturn            | 1951       |
| StdReturn            | 304.15     |
| AverageEpisodeLength | 956.92     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.66     |
| TotalNEpisodes       | 24483      |
| TotalNSamples        | 7.4784e+06 |
| ExplainedVariance    | 0.18185    |
-------------------------------------
[2018-01-21 16:26:45.201250 UTC] Saving snapshot
[2018-01-21 16:26:45.201517 UTC] Starting iteration 1495
[2018-01-21 16:26:45.201679 UTC] Start collecting samples
[2018-01-21 16:26:49.951297 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:50.072601 UTC] Performing policy update
[2018-01-21 16:26:50.073224 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:50.190474 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:26:51.566982 UTC] Performing line search
[2018-01-21 16:26:51.756664 UTC] Updating baseline
[2018-01-21 16:26:54.214589 UTC] Computing logging information
-------------------------------------
| Iteration            | 1495       |
| ExpectedImprovement  | 0.015662   |
| ActualImprovement    | 0.015333   |
| ImprovementRatio     | 0.979      |
| MeanKL               | 0.0080832  |
| Entropy              | -1.7189    |
| Perplexity           | 0.17926    |
| AveragePolicyStd     | 0.18389    |
| AveragePolicyStd[0]  | 0.2039     |
| AveragePolicyStd[1]  | 0.19651    |
| AveragePolicyStd[2]  | 0.14667    |
| AveragePolicyStd[3]  | 0.17835    |
| AveragePolicyStd[4]  | 0.15181    |
| AveragePolicyStd[5]  | 0.22613    |
| AverageReturn        | 1815.7     |
| MinReturn            | 346.77     |
| MaxReturn            | 1951       |
| StdReturn            | 304.96     |
| AverageEpisodeLength | 956.92     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.66     |
| TotalNEpisodes       | 24488      |
| TotalNSamples        | 7.4834e+06 |
| ExplainedVariance    | -0.010284  |
-------------------------------------
[2018-01-21 16:26:55.113508 UTC] Saving snapshot
[2018-01-21 16:26:55.113744 UTC] Starting iteration 1496
[2018-01-21 16:26:55.113915 UTC] Start collecting samples
[2018-01-21 16:26:59.724296 UTC] Computing input variables for policy optimization
[2018-01-21 16:26:59.848366 UTC] Performing policy update
[2018-01-21 16:26:59.848862 UTC] Computing gradient in Euclidean space
[2018-01-21 16:26:59.963535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:01.347929 UTC] Performing line search
[2018-01-21 16:27:01.543787 UTC] Updating baseline
[2018-01-21 16:27:04.028059 UTC] Computing logging information
-------------------------------------
| Iteration            | 1496       |
| ExpectedImprovement  | 0.015956   |
| ActualImprovement    | 0.015098   |
| ImprovementRatio     | 0.94625    |
| MeanKL               | 0.0081048  |
| Entropy              | -1.7162    |
| Perplexity           | 0.17975    |
| AveragePolicyStd     | 0.18398    |
| AveragePolicyStd[0]  | 0.20381    |
| AveragePolicyStd[1]  | 0.19666    |
| AveragePolicyStd[2]  | 0.14678    |
| AveragePolicyStd[3]  | 0.1785     |
| AveragePolicyStd[4]  | 0.15184    |
| AveragePolicyStd[5]  | 0.22626    |
| AverageReturn        | 1814       |
| MinReturn            | 346.77     |
| MaxReturn            | 1951       |
| StdReturn            | 306.53     |
| AverageEpisodeLength | 955.05     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.27     |
| TotalNEpisodes       | 24495      |
| TotalNSamples        | 7.4902e+06 |
| ExplainedVariance    | 0.11774    |
-------------------------------------
[2018-01-21 16:27:04.823584 UTC] Saving snapshot
[2018-01-21 16:27:04.823909 UTC] Starting iteration 1497
[2018-01-21 16:27:04.824135 UTC] Start collecting samples
[2018-01-21 16:27:09.321923 UTC] Computing input variables for policy optimization
[2018-01-21 16:27:09.471164 UTC] Performing policy update
[2018-01-21 16:27:09.471815 UTC] Computing gradient in Euclidean space
[2018-01-21 16:27:09.599815 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:11.019465 UTC] Performing line search
[2018-01-21 16:27:11.222276 UTC] Updating baseline
[2018-01-21 16:27:13.074576 UTC] Computing logging information
-------------------------------------
| Iteration            | 1497       |
| ExpectedImprovement  | 0.016675   |
| ActualImprovement    | 0.015983   |
| ImprovementRatio     | 0.9585     |
| MeanKL               | 0.0078046  |
| Entropy              | -1.7113    |
| Perplexity           | 0.18062    |
| AveragePolicyStd     | 0.18412    |
| AveragePolicyStd[0]  | 0.20391    |
| AveragePolicyStd[1]  | 0.19718    |
| AveragePolicyStd[2]  | 0.14691    |
| AveragePolicyStd[3]  | 0.17897    |
| AveragePolicyStd[4]  | 0.15174    |
| AveragePolicyStd[5]  | 0.22599    |
| AverageReturn        | 1819.9     |
| MinReturn            | 346.77     |
| MaxReturn            | 1969.9     |
| StdReturn            | 305.96     |
| AverageEpisodeLength | 956.63     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.9      |
| TotalNEpisodes       | 24498      |
| TotalNSamples        | 7.4932e+06 |
| ExplainedVariance    | -0.12283   |
-------------------------------------
[2018-01-21 16:27:13.877245 UTC] Saving snapshot
[2018-01-21 16:27:13.877622 UTC] Starting iteration 1498
[2018-01-21 16:27:13.877858 UTC] Start collecting samples
[2018-01-21 16:27:18.465375 UTC] Computing input variables for policy optimization
[2018-01-21 16:27:18.596901 UTC] Performing policy update
[2018-01-21 16:27:18.597649 UTC] Computing gradient in Euclidean space
[2018-01-21 16:27:18.718830 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:20.127620 UTC] Performing line search
[2018-01-21 16:27:20.321004 UTC] Updating baseline
[2018-01-21 16:27:22.045566 UTC] Computing logging information
-------------------------------------
| Iteration            | 1498       |
| ExpectedImprovement  | 0.020426   |
| ActualImprovement    | 0.019204   |
| ImprovementRatio     | 0.94017    |
| MeanKL               | 0.0078717  |
| Entropy              | -1.7071    |
| Perplexity           | 0.18138    |
| AveragePolicyStd     | 0.18426    |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.1972     |
| AveragePolicyStd[2]  | 0.14717    |
| AveragePolicyStd[3]  | 0.17914    |
| AveragePolicyStd[4]  | 0.15161    |
| AveragePolicyStd[5]  | 0.22644    |
| AverageReturn        | 1806.2     |
| MinReturn            | 346.77     |
| MaxReturn            | 1969.9     |
| StdReturn            | 331.51     |
| AverageEpisodeLength | 949.67     |
| MinEpisodeLength     | 225        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.13     |
| TotalNEpisodes       | 24503      |
| TotalNSamples        | 7.4975e+06 |
| ExplainedVariance    | 0.13548    |
-------------------------------------
[2018-01-21 16:27:22.926859 UTC] Saving snapshot
[2018-01-21 16:27:22.927124 UTC] Starting iteration 1499
[2018-01-21 16:27:22.927316 UTC] Start collecting samples
[2018-01-21 16:27:27.620734 UTC] Computing input variables for policy optimization
[2018-01-21 16:27:27.749615 UTC] Performing policy update
[2018-01-21 16:27:27.750208 UTC] Computing gradient in Euclidean space
[2018-01-21 16:27:27.865627 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:29.240451 UTC] Performing line search
[2018-01-21 16:27:29.428303 UTC] Updating baseline
[2018-01-21 16:27:31.405307 UTC] Computing logging information
------------------------------------
| Iteration            | 1499      |
| ExpectedImprovement  | 0.019351  |
| ActualImprovement    | 0.018503  |
| ImprovementRatio     | 0.95616   |
| MeanKL               | 0.0077522 |
| Entropy              | -1.7122   |
| Perplexity           | 0.18047   |
| AveragePolicyStd     | 0.18411   |
| AveragePolicyStd[0]  | 0.20351   |
| AveragePolicyStd[1]  | 0.19713   |
| AveragePolicyStd[2]  | 0.14659   |
| AveragePolicyStd[3]  | 0.17884   |
| AveragePolicyStd[4]  | 0.15198   |
| AveragePolicyStd[5]  | 0.22661   |
| AverageReturn        | 1797.4    |
| MinReturn            | 346.77    |
| MaxReturn            | 2000.1    |
| StdReturn            | 365       |
| AverageEpisodeLength | 944.19    |
| MinEpisodeLength     | 225       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 181.65    |
| TotalNEpisodes       | 24514     |
| TotalNSamples        | 7.507e+06 |
| ExplainedVariance    | 0.13302   |
------------------------------------
[2018-01-21 16:27:32.226377 UTC] Saving snapshot
[2018-01-21 16:27:32.226650 UTC] Starting iteration 1500
[2018-01-21 16:27:32.226830 UTC] Start collecting samples
[2018-01-21 16:27:36.636601 UTC] Computing input variables for policy optimization
[2018-01-21 16:27:36.761002 UTC] Performing policy update
[2018-01-21 16:27:36.761593 UTC] Computing gradient in Euclidean space
[2018-01-21 16:27:36.884857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:38.626837 UTC] Performing line search
[2018-01-21 16:27:38.925869 UTC] Updating baseline
[2018-01-21 16:27:42.458028 UTC] Computing logging information
-------------------------------------
| Iteration            | 1500       |
| ExpectedImprovement  | 0.018377   |
| ActualImprovement    | 0.017738   |
| ImprovementRatio     | 0.96523    |
| MeanKL               | 0.0074836  |
| Entropy              | -1.7192    |
| Perplexity           | 0.1792     |
| AveragePolicyStd     | 0.18388    |
| AveragePolicyStd[0]  | 0.20343    |
| AveragePolicyStd[1]  | 0.19637    |
| AveragePolicyStd[2]  | 0.1465     |
| AveragePolicyStd[3]  | 0.1793     |
| AveragePolicyStd[4]  | 0.15159    |
| AveragePolicyStd[5]  | 0.22612    |
| AverageReturn        | 1796.5     |
| MinReturn            | 235.2      |
| MaxReturn            | 2000.1     |
| StdReturn            | 373.02     |
| AverageEpisodeLength | 943.22     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.55     |
| TotalNEpisodes       | 24517      |
| TotalNSamples        | 7.5091e+06 |
| ExplainedVariance    | 0.06835    |
-------------------------------------
[2018-01-21 16:27:43.769219 UTC] Saving snapshot
[2018-01-21 16:27:43.786647 UTC] Starting iteration 1501
[2018-01-21 16:27:43.786963 UTC] Start collecting samples
[2018-01-21 16:27:50.607946 UTC] Computing input variables for policy optimization
[2018-01-21 16:27:50.741828 UTC] Performing policy update
[2018-01-21 16:27:50.742936 UTC] Computing gradient in Euclidean space
[2018-01-21 16:27:50.858174 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:27:52.225617 UTC] Performing line search
[2018-01-21 16:27:52.416314 UTC] Updating baseline
[2018-01-21 16:27:55.545035 UTC] Computing logging information
-------------------------------------
| Iteration            | 1501       |
| ExpectedImprovement  | 0.01884    |
| ActualImprovement    | 0.017647   |
| ImprovementRatio     | 0.93667    |
| MeanKL               | 0.0077972  |
| Entropy              | -1.7167    |
| Perplexity           | 0.17965    |
| AveragePolicyStd     | 0.18397    |
| AveragePolicyStd[0]  | 0.20389    |
| AveragePolicyStd[1]  | 0.1964     |
| AveragePolicyStd[2]  | 0.14657    |
| AveragePolicyStd[3]  | 0.17929    |
| AveragePolicyStd[4]  | 0.15149    |
| AveragePolicyStd[5]  | 0.2262     |
| AverageReturn        | 1797.3     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 373.44     |
| AverageEpisodeLength | 943.22     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.55     |
| TotalNEpisodes       | 24520      |
| TotalNSamples        | 7.5121e+06 |
| ExplainedVariance    | 0.0017842  |
-------------------------------------
[2018-01-21 16:27:56.343425 UTC] Saving snapshot
[2018-01-21 16:27:56.343686 UTC] Starting iteration 1502
[2018-01-21 16:27:56.343851 UTC] Start collecting samples
[2018-01-21 16:28:01.597095 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:01.726389 UTC] Performing policy update
[2018-01-21 16:28:01.727104 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:01.849463 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:03.258731 UTC] Performing line search
[2018-01-21 16:28:03.472403 UTC] Updating baseline
[2018-01-21 16:28:05.138750 UTC] Computing logging information
-------------------------------------
| Iteration            | 1502       |
| ExpectedImprovement  | 0.018322   |
| ActualImprovement    | 0.017601   |
| ImprovementRatio     | 0.96063    |
| MeanKL               | 0.0072385  |
| Entropy              | -1.7143    |
| Perplexity           | 0.18009    |
| AveragePolicyStd     | 0.18405    |
| AveragePolicyStd[0]  | 0.20431    |
| AveragePolicyStd[1]  | 0.19642    |
| AveragePolicyStd[2]  | 0.1467     |
| AveragePolicyStd[3]  | 0.17943    |
| AveragePolicyStd[4]  | 0.15137    |
| AveragePolicyStd[5]  | 0.22607    |
| AverageReturn        | 1799.5     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 374.13     |
| AverageEpisodeLength | 943.08     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.52     |
| TotalNEpisodes       | 24529      |
| TotalNSamples        | 7.5211e+06 |
| ExplainedVariance    | 0.082567   |
-------------------------------------
[2018-01-21 16:28:05.970381 UTC] Saving snapshot
[2018-01-21 16:28:05.970693 UTC] Starting iteration 1503
[2018-01-21 16:28:05.970849 UTC] Start collecting samples
[2018-01-21 16:28:10.595593 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:10.724487 UTC] Performing policy update
[2018-01-21 16:28:10.725550 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:10.843010 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:12.214226 UTC] Performing line search
[2018-01-21 16:28:12.406217 UTC] Updating baseline
[2018-01-21 16:28:14.118089 UTC] Computing logging information
-------------------------------------
| Iteration            | 1503       |
| ExpectedImprovement  | 0.01786    |
| ActualImprovement    | 0.01734    |
| ImprovementRatio     | 0.97088    |
| MeanKL               | 0.0071696  |
| Entropy              | -1.708     |
| Perplexity           | 0.18124    |
| AveragePolicyStd     | 0.18424    |
| AveragePolicyStd[0]  | 0.20496    |
| AveragePolicyStd[1]  | 0.19668    |
| AveragePolicyStd[2]  | 0.14685    |
| AveragePolicyStd[3]  | 0.17918    |
| AveragePolicyStd[4]  | 0.15169    |
| AveragePolicyStd[5]  | 0.22609    |
| AverageReturn        | 1797.7     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 375.28     |
| AverageEpisodeLength | 941.11     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.95     |
| TotalNEpisodes       | 24533      |
| TotalNSamples        | 7.5249e+06 |
| ExplainedVariance    | 0.12229    |
-------------------------------------
[2018-01-21 16:28:14.966539 UTC] Saving snapshot
[2018-01-21 16:28:14.966860 UTC] Starting iteration 1504
[2018-01-21 16:28:14.967050 UTC] Start collecting samples
[2018-01-21 16:28:19.410979 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:19.558395 UTC] Performing policy update
[2018-01-21 16:28:19.559008 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:19.683984 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:21.082926 UTC] Performing line search
[2018-01-21 16:28:21.282094 UTC] Updating baseline
[2018-01-21 16:28:23.298714 UTC] Computing logging information
-------------------------------------
| Iteration            | 1504       |
| ExpectedImprovement  | 0.019538   |
| ActualImprovement    | 0.018098   |
| ImprovementRatio     | 0.92631    |
| MeanKL               | 0.0081167  |
| Entropy              | -1.7114    |
| Perplexity           | 0.18062    |
| AveragePolicyStd     | 0.18415    |
| AveragePolicyStd[0]  | 0.20516    |
| AveragePolicyStd[1]  | 0.19713    |
| AveragePolicyStd[2]  | 0.14651    |
| AveragePolicyStd[3]  | 0.17881    |
| AveragePolicyStd[4]  | 0.15158    |
| AveragePolicyStd[5]  | 0.22572    |
| AverageReturn        | 1792.3     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 378.86     |
| AverageEpisodeLength | 937.67     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.06     |
| TotalNEpisodes       | 24538      |
| TotalNSamples        | 7.5292e+06 |
| ExplainedVariance    | 0.27305    |
-------------------------------------
[2018-01-21 16:28:24.162848 UTC] Saving snapshot
[2018-01-21 16:28:24.163154 UTC] Starting iteration 1505
[2018-01-21 16:28:24.163372 UTC] Start collecting samples
[2018-01-21 16:28:28.730132 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:28.873534 UTC] Performing policy update
[2018-01-21 16:28:28.874613 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:28.999104 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:30.387633 UTC] Performing line search
[2018-01-21 16:28:30.584191 UTC] Updating baseline
[2018-01-21 16:28:32.314231 UTC] Computing logging information
-------------------------------------
| Iteration            | 1505       |
| ExpectedImprovement  | 0.019261   |
| ActualImprovement    | 0.018104   |
| ImprovementRatio     | 0.93993    |
| MeanKL               | 0.0079334  |
| Entropy              | -1.7159    |
| Perplexity           | 0.17979    |
| AveragePolicyStd     | 0.18401    |
| AveragePolicyStd[0]  | 0.20508    |
| AveragePolicyStd[1]  | 0.19698    |
| AveragePolicyStd[2]  | 0.14639    |
| AveragePolicyStd[3]  | 0.17869    |
| AveragePolicyStd[4]  | 0.15145    |
| AveragePolicyStd[5]  | 0.22548    |
| AverageReturn        | 1808.3     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 357.49     |
| AverageEpisodeLength | 945.47     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.3      |
| TotalNEpisodes       | 24543      |
| TotalNSamples        | 7.5341e+06 |
| ExplainedVariance    | 0.11895    |
-------------------------------------
[2018-01-21 16:28:33.189295 UTC] Saving snapshot
[2018-01-21 16:28:33.189530 UTC] Starting iteration 1506
[2018-01-21 16:28:33.189679 UTC] Start collecting samples
[2018-01-21 16:28:37.725060 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:37.849600 UTC] Performing policy update
[2018-01-21 16:28:37.850197 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:37.982497 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:39.424602 UTC] Performing line search
[2018-01-21 16:28:39.620462 UTC] Updating baseline
[2018-01-21 16:28:41.649661 UTC] Computing logging information
-------------------------------------
| Iteration            | 1506       |
| ExpectedImprovement  | 0.018533   |
| ActualImprovement    | 0.017114   |
| ImprovementRatio     | 0.92346    |
| MeanKL               | 0.0083051  |
| Entropy              | -1.7204    |
| Perplexity           | 0.17899    |
| AveragePolicyStd     | 0.18389    |
| AveragePolicyStd[0]  | 0.20539    |
| AveragePolicyStd[1]  | 0.19641    |
| AveragePolicyStd[2]  | 0.14654    |
| AveragePolicyStd[3]  | 0.1784     |
| AveragePolicyStd[4]  | 0.15103    |
| AveragePolicyStd[5]  | 0.22554    |
| AverageReturn        | 1798       |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 371.77     |
| AverageEpisodeLength | 939.75     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.53     |
| TotalNEpisodes       | 24550      |
| TotalNSamples        | 7.5405e+06 |
| ExplainedVariance    | 0.071888   |
-------------------------------------
[2018-01-21 16:28:42.444594 UTC] Saving snapshot
[2018-01-21 16:28:42.444850 UTC] Starting iteration 1507
[2018-01-21 16:28:42.445029 UTC] Start collecting samples
[2018-01-21 16:28:46.931108 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:47.087243 UTC] Performing policy update
[2018-01-21 16:28:47.088215 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:47.208556 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:48.641278 UTC] Performing line search
[2018-01-21 16:28:48.848909 UTC] Updating baseline
[2018-01-21 16:28:51.127864 UTC] Computing logging information
-------------------------------------
| Iteration            | 1507       |
| ExpectedImprovement  | 0.020574   |
| ActualImprovement    | 0.018444   |
| ImprovementRatio     | 0.89645    |
| MeanKL               | 0.0074844  |
| Entropy              | -1.7236    |
| Perplexity           | 0.17841    |
| AveragePolicyStd     | 0.18378    |
| AveragePolicyStd[0]  | 0.20524    |
| AveragePolicyStd[1]  | 0.19654    |
| AveragePolicyStd[2]  | 0.14654    |
| AveragePolicyStd[3]  | 0.17804    |
| AveragePolicyStd[4]  | 0.15109    |
| AveragePolicyStd[5]  | 0.22521    |
| AverageReturn        | 1786.8     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 386.76     |
| AverageEpisodeLength | 933.72     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.15     |
| TotalNEpisodes       | 24555      |
| TotalNSamples        | 7.5449e+06 |
| ExplainedVariance    | 0.094229   |
-------------------------------------
[2018-01-21 16:28:51.936606 UTC] Saving snapshot
[2018-01-21 16:28:51.936964 UTC] Starting iteration 1508
[2018-01-21 16:28:51.937271 UTC] Start collecting samples
[2018-01-21 16:28:56.565132 UTC] Computing input variables for policy optimization
[2018-01-21 16:28:56.704887 UTC] Performing policy update
[2018-01-21 16:28:56.705500 UTC] Computing gradient in Euclidean space
[2018-01-21 16:28:56.824634 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:28:58.364511 UTC] Performing line search
[2018-01-21 16:28:58.565582 UTC] Updating baseline
[2018-01-21 16:29:00.357372 UTC] Computing logging information
-------------------------------------
| Iteration            | 1508       |
| ExpectedImprovement  | 0.019771   |
| ActualImprovement    | 0.0186     |
| ImprovementRatio     | 0.94079    |
| MeanKL               | 0.0080581  |
| Entropy              | -1.7283    |
| Perplexity           | 0.17758    |
| AveragePolicyStd     | 0.18369    |
| AveragePolicyStd[0]  | 0.20583    |
| AveragePolicyStd[1]  | 0.19603    |
| AveragePolicyStd[2]  | 0.14611    |
| AveragePolicyStd[3]  | 0.17776    |
| AveragePolicyStd[4]  | 0.15063    |
| AveragePolicyStd[5]  | 0.2258     |
| AverageReturn        | 1774.4     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 407.58     |
| AverageEpisodeLength | 926.62     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 202.42     |
| TotalNEpisodes       | 24560      |
| TotalNSamples        | 7.5492e+06 |
| ExplainedVariance    | 0.057898   |
-------------------------------------
[2018-01-21 16:29:01.243465 UTC] Saving snapshot
[2018-01-21 16:29:01.243711 UTC] Starting iteration 1509
[2018-01-21 16:29:01.243889 UTC] Start collecting samples
[2018-01-21 16:29:05.765809 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:05.891631 UTC] Performing policy update
[2018-01-21 16:29:05.892532 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:06.016383 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:07.453057 UTC] Performing line search
[2018-01-21 16:29:07.647914 UTC] Updating baseline
[2018-01-21 16:29:09.426145 UTC] Computing logging information
------------------------------------
| Iteration            | 1509      |
| ExpectedImprovement  | 0.017645  |
| ActualImprovement    | 0.016754  |
| ImprovementRatio     | 0.9495    |
| MeanKL               | 0.0075767 |
| Entropy              | -1.7327   |
| Perplexity           | 0.17681   |
| AveragePolicyStd     | 0.18355   |
| AveragePolicyStd[0]  | 0.20536   |
| AveragePolicyStd[1]  | 0.19593   |
| AveragePolicyStd[2]  | 0.14593   |
| AveragePolicyStd[3]  | 0.17731   |
| AveragePolicyStd[4]  | 0.15094   |
| AveragePolicyStd[5]  | 0.22583   |
| AverageReturn        | 1773.3    |
| MinReturn            | 235.2     |
| MaxReturn            | 2004.3    |
| StdReturn            | 408.08    |
| AverageEpisodeLength | 925.13    |
| MinEpisodeLength     | 157       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 202.42    |
| TotalNEpisodes       | 24567     |
| TotalNSamples        | 7.556e+06 |
| ExplainedVariance    | 0.072272  |
------------------------------------
[2018-01-21 16:29:10.367005 UTC] Saving snapshot
[2018-01-21 16:29:10.367203 UTC] Starting iteration 1510
[2018-01-21 16:29:10.367316 UTC] Start collecting samples
[2018-01-21 16:29:14.733925 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:14.861183 UTC] Performing policy update
[2018-01-21 16:29:14.861850 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:14.986910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:16.458232 UTC] Performing line search
[2018-01-21 16:29:16.649610 UTC] Updating baseline
[2018-01-21 16:29:18.558585 UTC] Computing logging information
------------------------------------
| Iteration            | 1510      |
| ExpectedImprovement  | 0.017258  |
| ActualImprovement    | 0.016075  |
| ImprovementRatio     | 0.93147   |
| MeanKL               | 0.0078198 |
| Entropy              | -1.731    |
| Perplexity           | 0.17711   |
| AveragePolicyStd     | 0.18359   |
| AveragePolicyStd[0]  | 0.20526   |
| AveragePolicyStd[1]  | 0.19586   |
| AveragePolicyStd[2]  | 0.14617   |
| AveragePolicyStd[3]  | 0.17724   |
| AveragePolicyStd[4]  | 0.1511    |
| AveragePolicyStd[5]  | 0.22589   |
| AverageReturn        | 1774.4    |
| MinReturn            | 235.2     |
| MaxReturn            | 2004.3    |
| StdReturn            | 408.48    |
| AverageEpisodeLength | 925.13    |
| MinEpisodeLength     | 157       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 202.42    |
| TotalNEpisodes       | 24570     |
| TotalNSamples        | 7.559e+06 |
| ExplainedVariance    | -0.01117  |
------------------------------------
[2018-01-21 16:29:19.450250 UTC] Saving snapshot
[2018-01-21 16:29:19.459698 UTC] Starting iteration 1511
[2018-01-21 16:29:19.459928 UTC] Start collecting samples
[2018-01-21 16:29:24.018332 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:24.156559 UTC] Performing policy update
[2018-01-21 16:29:24.157212 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:24.285551 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:25.713936 UTC] Performing line search
[2018-01-21 16:29:25.901428 UTC] Updating baseline
[2018-01-21 16:29:27.699638 UTC] Computing logging information
-------------------------------------
| Iteration            | 1511       |
| ExpectedImprovement  | 0.01875    |
| ActualImprovement    | 0.018344   |
| ImprovementRatio     | 0.97834    |
| MeanKL               | 0.0079745  |
| Entropy              | -1.7345    |
| Perplexity           | 0.17649    |
| AveragePolicyStd     | 0.18342    |
| AveragePolicyStd[0]  | 0.20503    |
| AveragePolicyStd[1]  | 0.195      |
| AveragePolicyStd[2]  | 0.14651    |
| AveragePolicyStd[3]  | 0.17717    |
| AveragePolicyStd[4]  | 0.15148    |
| AveragePolicyStd[5]  | 0.22534    |
| AverageReturn        | 1773.5     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 402.07     |
| AverageEpisodeLength | 923.51     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.1      |
| TotalNEpisodes       | 24578      |
| TotalNSamples        | 7.5661e+06 |
| ExplainedVariance    | 0.14161    |
-------------------------------------
[2018-01-21 16:29:28.478656 UTC] Saving snapshot
[2018-01-21 16:29:28.478959 UTC] Starting iteration 1512
[2018-01-21 16:29:28.479194 UTC] Start collecting samples
[2018-01-21 16:29:33.156912 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:33.310212 UTC] Performing policy update
[2018-01-21 16:29:33.310884 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:33.438957 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:34.928006 UTC] Performing line search
[2018-01-21 16:29:35.115499 UTC] Updating baseline
[2018-01-21 16:29:37.645176 UTC] Computing logging information
-------------------------------------
| Iteration            | 1512       |
| ExpectedImprovement  | 0.017632   |
| ActualImprovement    | 0.016781   |
| ImprovementRatio     | 0.95172    |
| MeanKL               | 0.008202   |
| Entropy              | -1.7334    |
| Perplexity           | 0.17669    |
| AveragePolicyStd     | 0.18347    |
| AveragePolicyStd[0]  | 0.20491    |
| AveragePolicyStd[1]  | 0.19506    |
| AveragePolicyStd[2]  | 0.14663    |
| AveragePolicyStd[3]  | 0.17713    |
| AveragePolicyStd[4]  | 0.15136    |
| AveragePolicyStd[5]  | 0.22571    |
| AverageReturn        | 1782.9     |
| MinReturn            | 235.2      |
| MaxReturn            | 2004.3     |
| StdReturn            | 397.14     |
| AverageEpisodeLength | 927.35     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.82     |
| TotalNEpisodes       | 24582      |
| TotalNSamples        | 7.5701e+06 |
| ExplainedVariance    | -0.0085655 |
-------------------------------------
[2018-01-21 16:29:38.570221 UTC] Saving snapshot
[2018-01-21 16:29:38.570467 UTC] Starting iteration 1513
[2018-01-21 16:29:38.570618 UTC] Start collecting samples
[2018-01-21 16:29:43.187951 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:43.328088 UTC] Performing policy update
[2018-01-21 16:29:43.329084 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:43.444190 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:44.915059 UTC] Performing line search
[2018-01-21 16:29:45.113291 UTC] Updating baseline
[2018-01-21 16:29:47.064240 UTC] Computing logging information
------------------------------------
| Iteration            | 1513      |
| ExpectedImprovement  | 0.021445  |
| ActualImprovement    | 0.019456  |
| ImprovementRatio     | 0.90722   |
| MeanKL               | 0.0071135 |
| Entropy              | -1.7334   |
| Perplexity           | 0.17667   |
| AveragePolicyStd     | 0.18345   |
| AveragePolicyStd[0]  | 0.20488   |
| AveragePolicyStd[1]  | 0.19548   |
| AveragePolicyStd[2]  | 0.14672   |
| AveragePolicyStd[3]  | 0.17699   |
| AveragePolicyStd[4]  | 0.15136   |
| AveragePolicyStd[5]  | 0.22527   |
| AverageReturn        | 1762.6    |
| MinReturn            | 235.2     |
| MaxReturn            | 2004.3    |
| StdReturn            | 421.08    |
| AverageEpisodeLength | 916.76    |
| MinEpisodeLength     | 157       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 208.39    |
| TotalNEpisodes       | 24586     |
| TotalNSamples        | 7.573e+06 |
| ExplainedVariance    | 0.2593    |
------------------------------------
[2018-01-21 16:29:47.986123 UTC] Saving snapshot
[2018-01-21 16:29:47.986355 UTC] Starting iteration 1514
[2018-01-21 16:29:47.986517 UTC] Start collecting samples
[2018-01-21 16:29:52.643611 UTC] Computing input variables for policy optimization
[2018-01-21 16:29:52.781389 UTC] Performing policy update
[2018-01-21 16:29:52.782050 UTC] Computing gradient in Euclidean space
[2018-01-21 16:29:52.927692 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:29:54.395877 UTC] Performing line search
[2018-01-21 16:29:54.583776 UTC] Updating baseline
[2018-01-21 16:29:56.651662 UTC] Computing logging information
-------------------------------------
| Iteration            | 1514       |
| ExpectedImprovement  | 0.017903   |
| ActualImprovement    | 0.017351   |
| ImprovementRatio     | 0.96917    |
| MeanKL               | 0.0081979  |
| Entropy              | -1.7325    |
| Perplexity           | 0.17683    |
| AveragePolicyStd     | 0.18347    |
| AveragePolicyStd[0]  | 0.20481    |
| AveragePolicyStd[1]  | 0.19539    |
| AveragePolicyStd[2]  | 0.14696    |
| AveragePolicyStd[3]  | 0.17695    |
| AveragePolicyStd[4]  | 0.15127    |
| AveragePolicyStd[5]  | 0.22547    |
| AverageReturn        | 1755       |
| MinReturn            | 235.2      |
| MaxReturn            | 2019.3     |
| StdReturn            | 427.62     |
| AverageEpisodeLength | 912.35     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.23     |
| TotalNEpisodes       | 24593      |
| TotalNSamples        | 7.5796e+06 |
| ExplainedVariance    | 0.065055   |
-------------------------------------
[2018-01-21 16:29:57.549200 UTC] Saving snapshot
[2018-01-21 16:29:57.549491 UTC] Starting iteration 1515
[2018-01-21 16:29:57.549691 UTC] Start collecting samples
[2018-01-21 16:30:02.192374 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:02.334637 UTC] Performing policy update
[2018-01-21 16:30:02.335403 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:02.455618 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:03.913624 UTC] Performing line search
[2018-01-21 16:30:04.108339 UTC] Updating baseline
[2018-01-21 16:30:06.849003 UTC] Computing logging information
-------------------------------------
| Iteration            | 1515       |
| ExpectedImprovement  | 0.019863   |
| ActualImprovement    | 0.018583   |
| ImprovementRatio     | 0.93557    |
| MeanKL               | 0.0074863  |
| Entropy              | -1.731     |
| Perplexity           | 0.17711    |
| AveragePolicyStd     | 0.18348    |
| AveragePolicyStd[0]  | 0.20496    |
| AveragePolicyStd[1]  | 0.19464    |
| AveragePolicyStd[2]  | 0.1471     |
| AveragePolicyStd[3]  | 0.1773     |
| AveragePolicyStd[4]  | 0.15175    |
| AveragePolicyStd[5]  | 0.22515    |
| AverageReturn        | 1759.1     |
| MinReturn            | 235.2      |
| MaxReturn            | 2019.3     |
| StdReturn            | 427.54     |
| AverageEpisodeLength | 914.22     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.17     |
| TotalNEpisodes       | 24598      |
| TotalNSamples        | 7.5846e+06 |
| ExplainedVariance    | -0.0045412 |
-------------------------------------
[2018-01-21 16:30:07.749426 UTC] Saving snapshot
[2018-01-21 16:30:07.749655 UTC] Starting iteration 1516
[2018-01-21 16:30:07.749810 UTC] Start collecting samples
[2018-01-21 16:30:12.387234 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:12.535463 UTC] Performing policy update
[2018-01-21 16:30:12.536457 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:12.660019 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:14.081317 UTC] Performing line search
[2018-01-21 16:30:14.281449 UTC] Updating baseline
[2018-01-21 16:30:16.338868 UTC] Computing logging information
-------------------------------------
| Iteration            | 1516       |
| ExpectedImprovement  | 0.019638   |
| ActualImprovement    | 0.018545   |
| ImprovementRatio     | 0.94434    |
| MeanKL               | 0.0074615  |
| Entropy              | -1.7359    |
| Perplexity           | 0.17624    |
| AveragePolicyStd     | 0.18333    |
| AveragePolicyStd[0]  | 0.2052     |
| AveragePolicyStd[1]  | 0.19445    |
| AveragePolicyStd[2]  | 0.14713    |
| AveragePolicyStd[3]  | 0.17718    |
| AveragePolicyStd[4]  | 0.15135    |
| AveragePolicyStd[5]  | 0.22469    |
| AverageReturn        | 1765.7     |
| MinReturn            | 235.2      |
| MaxReturn            | 2019.3     |
| StdReturn            | 417.05     |
| AverageEpisodeLength | 916.67     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.42     |
| TotalNEpisodes       | 24603      |
| TotalNSamples        | 7.5891e+06 |
| ExplainedVariance    | 0.10505    |
-------------------------------------
[2018-01-21 16:30:17.157211 UTC] Saving snapshot
[2018-01-21 16:30:17.157489 UTC] Starting iteration 1517
[2018-01-21 16:30:17.157689 UTC] Start collecting samples
[2018-01-21 16:30:21.694066 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:21.830006 UTC] Performing policy update
[2018-01-21 16:30:21.830657 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:21.957047 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:23.377668 UTC] Performing line search
[2018-01-21 16:30:23.584358 UTC] Updating baseline
[2018-01-21 16:30:25.312384 UTC] Computing logging information
-------------------------------------
| Iteration            | 1517       |
| ExpectedImprovement  | 0.017384   |
| ActualImprovement    | 0.016879   |
| ImprovementRatio     | 0.97095    |
| MeanKL               | 0.008945   |
| Entropy              | -1.7338    |
| Perplexity           | 0.17661    |
| AveragePolicyStd     | 0.18341    |
| AveragePolicyStd[0]  | 0.20558    |
| AveragePolicyStd[1]  | 0.19437    |
| AveragePolicyStd[2]  | 0.14699    |
| AveragePolicyStd[3]  | 0.17699    |
| AveragePolicyStd[4]  | 0.1516     |
| AveragePolicyStd[5]  | 0.22495    |
| AverageReturn        | 1773.9     |
| MinReturn            | 235.2      |
| MaxReturn            | 2019.3     |
| StdReturn            | 400.09     |
| AverageEpisodeLength | 920.41     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 196.61     |
| TotalNEpisodes       | 24610      |
| TotalNSamples        | 7.5958e+06 |
| ExplainedVariance    | 0.083783   |
-------------------------------------
[2018-01-21 16:30:26.198144 UTC] Saving snapshot
[2018-01-21 16:30:26.198319 UTC] Starting iteration 1518
[2018-01-21 16:30:26.198421 UTC] Start collecting samples
[2018-01-21 16:30:30.412329 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:30.542015 UTC] Performing policy update
[2018-01-21 16:30:30.542644 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:30.667636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:32.142249 UTC] Performing line search
[2018-01-21 16:30:32.338839 UTC] Updating baseline
[2018-01-21 16:30:34.189673 UTC] Computing logging information
-------------------------------------
| Iteration            | 1518       |
| ExpectedImprovement  | 0.018405   |
| ActualImprovement    | 0.017075   |
| ImprovementRatio     | 0.92774    |
| MeanKL               | 0.0078418  |
| Entropy              | -1.7369    |
| Perplexity           | 0.17607    |
| AveragePolicyStd     | 0.18332    |
| AveragePolicyStd[0]  | 0.20571    |
| AveragePolicyStd[1]  | 0.19425    |
| AveragePolicyStd[2]  | 0.14701    |
| AveragePolicyStd[3]  | 0.17669    |
| AveragePolicyStd[4]  | 0.15145    |
| AveragePolicyStd[5]  | 0.22481    |
| AverageReturn        | 1790       |
| MinReturn            | 235.2      |
| MaxReturn            | 2019.3     |
| StdReturn            | 376.94     |
| AverageEpisodeLength | 927.88     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.96     |
| TotalNEpisodes       | 24613      |
| TotalNSamples        | 7.5988e+06 |
| ExplainedVariance    | 0.003706   |
-------------------------------------
[2018-01-21 16:30:35.030718 UTC] Saving snapshot
[2018-01-21 16:30:35.030930 UTC] Starting iteration 1519
[2018-01-21 16:30:35.031104 UTC] Start collecting samples
[2018-01-21 16:30:39.477796 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:39.602197 UTC] Performing policy update
[2018-01-21 16:30:39.602919 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:39.723689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:41.161585 UTC] Performing line search
[2018-01-21 16:30:41.361302 UTC] Updating baseline
[2018-01-21 16:30:43.160435 UTC] Computing logging information
-------------------------------------
| Iteration            | 1519       |
| ExpectedImprovement  | 0.0194     |
| ActualImprovement    | 0.017886   |
| ImprovementRatio     | 0.92195    |
| MeanKL               | 0.0074835  |
| Entropy              | -1.7392    |
| Perplexity           | 0.17567    |
| AveragePolicyStd     | 0.18326    |
| AveragePolicyStd[0]  | 0.20554    |
| AveragePolicyStd[1]  | 0.19453    |
| AveragePolicyStd[2]  | 0.14686    |
| AveragePolicyStd[3]  | 0.17657    |
| AveragePolicyStd[4]  | 0.15132    |
| AveragePolicyStd[5]  | 0.22475    |
| AverageReturn        | 1807.1     |
| MinReturn            | 472.24     |
| MaxReturn            | 2019.3     |
| StdReturn            | 343.27     |
| AverageEpisodeLength | 936.31     |
| MinEpisodeLength     | 279        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.07     |
| TotalNEpisodes       | 24616      |
| TotalNSamples        | 7.6018e+06 |
| ExplainedVariance    | -0.042135  |
-------------------------------------
[2018-01-21 16:30:43.980386 UTC] Saving snapshot
[2018-01-21 16:30:43.980669 UTC] Starting iteration 1520
[2018-01-21 16:30:43.980862 UTC] Start collecting samples
[2018-01-21 16:30:48.627744 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:48.768904 UTC] Performing policy update
[2018-01-21 16:30:48.770014 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:48.893110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:50.326168 UTC] Performing line search
[2018-01-21 16:30:50.517266 UTC] Updating baseline
[2018-01-21 16:30:52.852494 UTC] Computing logging information
-------------------------------------
| Iteration            | 1520       |
| ExpectedImprovement  | 0.019412   |
| ActualImprovement    | 0.018548   |
| ImprovementRatio     | 0.95551    |
| MeanKL               | 0.0078217  |
| Entropy              | -1.7428    |
| Perplexity           | 0.17503    |
| AveragePolicyStd     | 0.1832     |
| AveragePolicyStd[0]  | 0.20568    |
| AveragePolicyStd[1]  | 0.19483    |
| AveragePolicyStd[2]  | 0.14669    |
| AveragePolicyStd[3]  | 0.17599    |
| AveragePolicyStd[4]  | 0.15079    |
| AveragePolicyStd[5]  | 0.22519    |
| AverageReturn        | 1779.1     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 390.55     |
| AverageEpisodeLength | 922.32     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.12     |
| TotalNEpisodes       | 24626      |
| TotalNSamples        | 7.6104e+06 |
| ExplainedVariance    | 0.1124     |
-------------------------------------
[2018-01-21 16:30:53.697462 UTC] Saving snapshot
[2018-01-21 16:30:53.706592 UTC] Starting iteration 1521
[2018-01-21 16:30:53.706830 UTC] Start collecting samples
[2018-01-21 16:30:58.227440 UTC] Computing input variables for policy optimization
[2018-01-21 16:30:58.361550 UTC] Performing policy update
[2018-01-21 16:30:58.362414 UTC] Computing gradient in Euclidean space
[2018-01-21 16:30:58.497349 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:30:59.932643 UTC] Performing line search
[2018-01-21 16:31:00.142215 UTC] Updating baseline
[2018-01-21 16:31:02.061080 UTC] Computing logging information
-------------------------------------
| Iteration            | 1521       |
| ExpectedImprovement  | 0.017378   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.96652    |
| MeanKL               | 0.0077094  |
| Entropy              | -1.741     |
| Perplexity           | 0.17534    |
| AveragePolicyStd     | 0.18323    |
| AveragePolicyStd[0]  | 0.20533    |
| AveragePolicyStd[1]  | 0.19438    |
| AveragePolicyStd[2]  | 0.1469     |
| AveragePolicyStd[3]  | 0.17652    |
| AveragePolicyStd[4]  | 0.15095    |
| AveragePolicyStd[5]  | 0.22527    |
| AverageReturn        | 1778.1     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 389.32     |
| AverageEpisodeLength | 923.22     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.77     |
| TotalNEpisodes       | 24631      |
| TotalNSamples        | 7.6153e+06 |
| ExplainedVariance    | 0.076481   |
-------------------------------------
[2018-01-21 16:31:02.904849 UTC] Saving snapshot
[2018-01-21 16:31:02.905163 UTC] Starting iteration 1522
[2018-01-21 16:31:02.905395 UTC] Start collecting samples
[2018-01-21 16:31:07.329876 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:07.458853 UTC] Performing policy update
[2018-01-21 16:31:07.460115 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:07.578865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:08.993042 UTC] Performing line search
[2018-01-21 16:31:09.186599 UTC] Updating baseline
[2018-01-21 16:31:11.055319 UTC] Computing logging information
-------------------------------------
| Iteration            | 1522       |
| ExpectedImprovement  | 0.018131   |
| ActualImprovement    | 0.0172     |
| ImprovementRatio     | 0.94864    |
| MeanKL               | 0.0089001  |
| Entropy              | -1.7374    |
| Perplexity           | 0.17597    |
| AveragePolicyStd     | 0.18336    |
| AveragePolicyStd[0]  | 0.20505    |
| AveragePolicyStd[1]  | 0.19481    |
| AveragePolicyStd[2]  | 0.14692    |
| AveragePolicyStd[3]  | 0.17663    |
| AveragePolicyStd[4]  | 0.15079    |
| AveragePolicyStd[5]  | 0.22596    |
| AverageReturn        | 1778.4     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 389.43     |
| AverageEpisodeLength | 923.22     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.77     |
| TotalNEpisodes       | 24633      |
| TotalNSamples        | 7.6173e+06 |
| ExplainedVariance    | 0.077034   |
-------------------------------------
[2018-01-21 16:31:11.882403 UTC] Saving snapshot
[2018-01-21 16:31:11.882712 UTC] Starting iteration 1523
[2018-01-21 16:31:11.882881 UTC] Start collecting samples
[2018-01-21 16:31:16.441535 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:16.567422 UTC] Performing policy update
[2018-01-21 16:31:16.570973 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:16.690275 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:18.100261 UTC] Performing line search
[2018-01-21 16:31:18.294504 UTC] Updating baseline
[2018-01-21 16:31:20.522490 UTC] Computing logging information
------------------------------------
| Iteration            | 1523      |
| ExpectedImprovement  | 0.017125  |
| ActualImprovement    | 0.016206  |
| ImprovementRatio     | 0.94631   |
| MeanKL               | 0.0079984 |
| Entropy              | -1.7372   |
| Perplexity           | 0.17601   |
| AveragePolicyStd     | 0.18338   |
| AveragePolicyStd[0]  | 0.2055    |
| AveragePolicyStd[1]  | 0.19473   |
| AveragePolicyStd[2]  | 0.14676   |
| AveragePolicyStd[3]  | 0.17661   |
| AveragePolicyStd[4]  | 0.15073   |
| AveragePolicyStd[5]  | 0.22597   |
| AverageReturn        | 1791.5    |
| MinReturn            | 257.14    |
| MaxReturn            | 2019.3    |
| StdReturn            | 379.93    |
| AverageEpisodeLength | 929.76    |
| MinEpisodeLength     | 176       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 186.17    |
| TotalNEpisodes       | 24641     |
| TotalNSamples        | 7.625e+06 |
| ExplainedVariance    | 0.089733  |
------------------------------------
[2018-01-21 16:31:21.369329 UTC] Saving snapshot
[2018-01-21 16:31:21.369688 UTC] Starting iteration 1524
[2018-01-21 16:31:21.369929 UTC] Start collecting samples
[2018-01-21 16:31:26.216823 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:26.361375 UTC] Performing policy update
[2018-01-21 16:31:26.362128 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:26.484650 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:27.928923 UTC] Performing line search
[2018-01-21 16:31:28.126094 UTC] Updating baseline
[2018-01-21 16:31:29.969649 UTC] Computing logging information
-------------------------------------
| Iteration            | 1524       |
| ExpectedImprovement  | 0.016072   |
| ActualImprovement    | 0.015393   |
| ImprovementRatio     | 0.95773    |
| MeanKL               | 0.0079144  |
| Entropy              | -1.7409    |
| Perplexity           | 0.17537    |
| AveragePolicyStd     | 0.18326    |
| AveragePolicyStd[0]  | 0.20579    |
| AveragePolicyStd[1]  | 0.19424    |
| AveragePolicyStd[2]  | 0.14667    |
| AveragePolicyStd[3]  | 0.17683    |
| AveragePolicyStd[4]  | 0.1506     |
| AveragePolicyStd[5]  | 0.22543    |
| AverageReturn        | 1780.3     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 388.76     |
| AverageEpisodeLength | 924.94     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.48     |
| TotalNEpisodes       | 24648      |
| TotalNSamples        | 7.6316e+06 |
| ExplainedVariance    | 0.0018928  |
-------------------------------------
[2018-01-21 16:31:30.752476 UTC] Saving snapshot
[2018-01-21 16:31:30.752671 UTC] Starting iteration 1525
[2018-01-21 16:31:30.752859 UTC] Start collecting samples
[2018-01-21 16:31:35.207239 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:35.325128 UTC] Performing policy update
[2018-01-21 16:31:35.325811 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:35.448152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:36.942534 UTC] Performing line search
[2018-01-21 16:31:37.136367 UTC] Updating baseline
[2018-01-21 16:31:38.937570 UTC] Computing logging information
------------------------------------
| Iteration            | 1525      |
| ExpectedImprovement  | 0.019602  |
| ActualImprovement    | 0.018407  |
| ImprovementRatio     | 0.93906   |
| MeanKL               | 0.0074841 |
| Entropy              | -1.7456   |
| Perplexity           | 0.17454   |
| AveragePolicyStd     | 0.1831    |
| AveragePolicyStd[0]  | 0.20497   |
| AveragePolicyStd[1]  | 0.19388   |
| AveragePolicyStd[2]  | 0.14677   |
| AveragePolicyStd[3]  | 0.1769    |
| AveragePolicyStd[4]  | 0.15057   |
| AveragePolicyStd[5]  | 0.22551   |
| AverageReturn        | 1791.6    |
| MinReturn            | 257.14    |
| MaxReturn            | 2019.3    |
| StdReturn            | 375.1     |
| AverageEpisodeLength | 931.06    |
| MinEpisodeLength     | 176       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 182.83    |
| TotalNEpisodes       | 24651     |
| TotalNSamples        | 7.634e+06 |
| ExplainedVariance    | 0.15911   |
------------------------------------
[2018-01-21 16:31:39.721338 UTC] Saving snapshot
[2018-01-21 16:31:39.721591 UTC] Starting iteration 1526
[2018-01-21 16:31:39.721762 UTC] Start collecting samples
[2018-01-21 16:31:44.235492 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:44.381363 UTC] Performing policy update
[2018-01-21 16:31:44.382123 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:44.504521 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:45.908000 UTC] Performing line search
[2018-01-21 16:31:46.104682 UTC] Updating baseline
[2018-01-21 16:31:48.311959 UTC] Computing logging information
------------------------------------
| Iteration            | 1526      |
| ExpectedImprovement  | 0.016978  |
| ActualImprovement    | 0.016405  |
| ImprovementRatio     | 0.96623   |
| MeanKL               | 0.0075195 |
| Entropy              | -1.7499   |
| Perplexity           | 0.17379   |
| AveragePolicyStd     | 0.18295   |
| AveragePolicyStd[0]  | 0.20443   |
| AveragePolicyStd[1]  | 0.19382   |
| AveragePolicyStd[2]  | 0.14674   |
| AveragePolicyStd[3]  | 0.17684   |
| AveragePolicyStd[4]  | 0.1506    |
| AveragePolicyStd[5]  | 0.22526   |
| AverageReturn        | 1790.4    |
| MinReturn            | 257.14    |
| MaxReturn            | 2019.3    |
| StdReturn            | 374.61    |
| AverageEpisodeLength | 931.06    |
| MinEpisodeLength     | 176       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 182.83    |
| TotalNEpisodes       | 24657     |
| TotalNSamples        | 7.64e+06  |
| ExplainedVariance    | 0.033272  |
------------------------------------
[2018-01-21 16:31:49.153048 UTC] Saving snapshot
[2018-01-21 16:31:49.153352 UTC] Starting iteration 1527
[2018-01-21 16:31:49.153575 UTC] Start collecting samples
[2018-01-21 16:31:53.705563 UTC] Computing input variables for policy optimization
[2018-01-21 16:31:53.837646 UTC] Performing policy update
[2018-01-21 16:31:53.838251 UTC] Computing gradient in Euclidean space
[2018-01-21 16:31:53.946531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:31:55.403113 UTC] Performing line search
[2018-01-21 16:31:55.594876 UTC] Updating baseline
[2018-01-21 16:31:57.726769 UTC] Computing logging information
-------------------------------------
| Iteration            | 1527       |
| ExpectedImprovement  | 0.016639   |
| ActualImprovement    | 0.015698   |
| ImprovementRatio     | 0.94345    |
| MeanKL               | 0.0086546  |
| Entropy              | -1.7472    |
| Perplexity           | 0.17427    |
| AveragePolicyStd     | 0.18304    |
| AveragePolicyStd[0]  | 0.20469    |
| AveragePolicyStd[1]  | 0.19427    |
| AveragePolicyStd[2]  | 0.14694    |
| AveragePolicyStd[3]  | 0.17703    |
| AveragePolicyStd[4]  | 0.1503     |
| AveragePolicyStd[5]  | 0.22499    |
| AverageReturn        | 1791.7     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 374.04     |
| AverageEpisodeLength | 932.91     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.54     |
| TotalNEpisodes       | 24664      |
| TotalNSamples        | 7.6463e+06 |
| ExplainedVariance    | 0.068951   |
-------------------------------------
[2018-01-21 16:31:58.542137 UTC] Saving snapshot
[2018-01-21 16:31:58.542491 UTC] Starting iteration 1528
[2018-01-21 16:31:58.542687 UTC] Start collecting samples
[2018-01-21 16:32:03.102950 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:03.239488 UTC] Performing policy update
[2018-01-21 16:32:03.240078 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:03.353913 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:04.767422 UTC] Performing line search
[2018-01-21 16:32:04.961131 UTC] Updating baseline
[2018-01-21 16:32:07.079042 UTC] Computing logging information
-------------------------------------
| Iteration            | 1528       |
| ExpectedImprovement  | 0.016304   |
| ActualImprovement    | 0.015535   |
| ImprovementRatio     | 0.95288    |
| MeanKL               | 0.0083573  |
| Entropy              | -1.7478    |
| Perplexity           | 0.17416    |
| AveragePolicyStd     | 0.18302    |
| AveragePolicyStd[0]  | 0.20472    |
| AveragePolicyStd[1]  | 0.19388    |
| AveragePolicyStd[2]  | 0.1469     |
| AveragePolicyStd[3]  | 0.17717    |
| AveragePolicyStd[4]  | 0.15032    |
| AveragePolicyStd[5]  | 0.2251     |
| AverageReturn        | 1781.3     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 380.1      |
| AverageEpisodeLength | 928.8      |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.6      |
| TotalNEpisodes       | 24669      |
| TotalNSamples        | 7.6509e+06 |
| ExplainedVariance    | 0.12893    |
-------------------------------------
[2018-01-21 16:32:07.924806 UTC] Saving snapshot
[2018-01-21 16:32:07.925038 UTC] Starting iteration 1529
[2018-01-21 16:32:07.925181 UTC] Start collecting samples
[2018-01-21 16:32:12.371783 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:12.517879 UTC] Performing policy update
[2018-01-21 16:32:12.519057 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:12.636273 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:14.087398 UTC] Performing line search
[2018-01-21 16:32:14.291130 UTC] Updating baseline
[2018-01-21 16:32:17.128290 UTC] Computing logging information
-------------------------------------
| Iteration            | 1529       |
| ExpectedImprovement  | 0.019147   |
| ActualImprovement    | 0.017484   |
| ImprovementRatio     | 0.91315    |
| MeanKL               | 0.0079397  |
| Entropy              | -1.7526    |
| Perplexity           | 0.17332    |
| AveragePolicyStd     | 0.18288    |
| AveragePolicyStd[0]  | 0.20473    |
| AveragePolicyStd[1]  | 0.19403    |
| AveragePolicyStd[2]  | 0.14615    |
| AveragePolicyStd[3]  | 0.17715    |
| AveragePolicyStd[4]  | 0.15055    |
| AveragePolicyStd[5]  | 0.22468    |
| AverageReturn        | 1780.1     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 379.76     |
| AverageEpisodeLength | 928.8      |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.6      |
| TotalNEpisodes       | 24671      |
| TotalNSamples        | 7.6529e+06 |
| ExplainedVariance    | 0.00195    |
-------------------------------------
[2018-01-21 16:32:17.947061 UTC] Saving snapshot
[2018-01-21 16:32:17.947296 UTC] Starting iteration 1530
[2018-01-21 16:32:17.947467 UTC] Start collecting samples
[2018-01-21 16:32:22.504481 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:22.654639 UTC] Performing policy update
[2018-01-21 16:32:22.655708 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:22.768209 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:24.197150 UTC] Performing line search
[2018-01-21 16:32:24.388128 UTC] Updating baseline
[2018-01-21 16:32:27.295387 UTC] Computing logging information
-------------------------------------
| Iteration            | 1530       |
| ExpectedImprovement  | 0.016258   |
| ActualImprovement    | 0.015816   |
| ImprovementRatio     | 0.97282    |
| MeanKL               | 0.0080578  |
| Entropy              | -1.7526    |
| Perplexity           | 0.17332    |
| AveragePolicyStd     | 0.18288    |
| AveragePolicyStd[0]  | 0.20513    |
| AveragePolicyStd[1]  | 0.19403    |
| AveragePolicyStd[2]  | 0.14597    |
| AveragePolicyStd[3]  | 0.17689    |
| AveragePolicyStd[4]  | 0.15086    |
| AveragePolicyStd[5]  | 0.22437    |
| AverageReturn        | 1795.9     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 358.66     |
| AverageEpisodeLength | 938.17     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.38     |
| TotalNEpisodes       | 24677      |
| TotalNSamples        | 7.6589e+06 |
| ExplainedVariance    | 0.0017921  |
-------------------------------------
[2018-01-21 16:32:28.080283 UTC] Saving snapshot
[2018-01-21 16:32:28.089759 UTC] Starting iteration 1531
[2018-01-21 16:32:28.089992 UTC] Start collecting samples
[2018-01-21 16:32:32.556949 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:32.690078 UTC] Performing policy update
[2018-01-21 16:32:32.690770 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:32.820360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:34.273427 UTC] Performing line search
[2018-01-21 16:32:34.473701 UTC] Updating baseline
[2018-01-21 16:32:36.306642 UTC] Computing logging information
-------------------------------------
| Iteration            | 1531       |
| ExpectedImprovement  | 0.018474   |
| ActualImprovement    | 0.01727    |
| ImprovementRatio     | 0.93483    |
| MeanKL               | 0.0073066  |
| Entropy              | -1.7537    |
| Perplexity           | 0.17313    |
| AveragePolicyStd     | 0.18283    |
| AveragePolicyStd[0]  | 0.20553    |
| AveragePolicyStd[1]  | 0.19412    |
| AveragePolicyStd[2]  | 0.14617    |
| AveragePolicyStd[3]  | 0.17656    |
| AveragePolicyStd[4]  | 0.1508     |
| AveragePolicyStd[5]  | 0.2238     |
| AverageReturn        | 1787.4     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 361.23     |
| AverageEpisodeLength | 935.17     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.86     |
| TotalNEpisodes       | 24684      |
| TotalNSamples        | 7.6656e+06 |
| ExplainedVariance    | 0.088061   |
-------------------------------------
[2018-01-21 16:32:37.186150 UTC] Saving snapshot
[2018-01-21 16:32:37.186383 UTC] Starting iteration 1532
[2018-01-21 16:32:37.186549 UTC] Start collecting samples
[2018-01-21 16:32:41.882218 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:42.028300 UTC] Performing policy update
[2018-01-21 16:32:42.028979 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:42.151443 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:43.565153 UTC] Performing line search
[2018-01-21 16:32:43.753105 UTC] Updating baseline
[2018-01-21 16:32:46.148859 UTC] Computing logging information
-------------------------------------
| Iteration            | 1532       |
| ExpectedImprovement  | 0.017744   |
| ActualImprovement    | 0.016966   |
| ImprovementRatio     | 0.95619    |
| MeanKL               | 0.0079191  |
| Entropy              | -1.7535    |
| Perplexity           | 0.17317    |
| AveragePolicyStd     | 0.18286    |
| AveragePolicyStd[0]  | 0.20604    |
| AveragePolicyStd[1]  | 0.19375    |
| AveragePolicyStd[2]  | 0.14641    |
| AveragePolicyStd[3]  | 0.17625    |
| AveragePolicyStd[4]  | 0.15048    |
| AveragePolicyStd[5]  | 0.22422    |
| AverageReturn        | 1808.2     |
| MinReturn            | 257.14     |
| MaxReturn            | 2019.3     |
| StdReturn            | 331.6      |
| AverageEpisodeLength | 945.76     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.77     |
| TotalNEpisodes       | 24687      |
| TotalNSamples        | 7.6686e+06 |
| ExplainedVariance    | -0.0038146 |
-------------------------------------
[2018-01-21 16:32:46.961530 UTC] Saving snapshot
[2018-01-21 16:32:46.961805 UTC] Starting iteration 1533
[2018-01-21 16:32:46.961984 UTC] Start collecting samples
[2018-01-21 16:32:51.320491 UTC] Computing input variables for policy optimization
[2018-01-21 16:32:51.454834 UTC] Performing policy update
[2018-01-21 16:32:51.455470 UTC] Computing gradient in Euclidean space
[2018-01-21 16:32:51.577297 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:32:53.037918 UTC] Performing line search
[2018-01-21 16:32:53.232255 UTC] Updating baseline
[2018-01-21 16:32:56.057342 UTC] Computing logging information
-------------------------------------
| Iteration            | 1533       |
| ExpectedImprovement  | 0.018374   |
| ActualImprovement    | 0.01698    |
| ImprovementRatio     | 0.9241     |
| MeanKL               | 0.0077573  |
| Entropy              | -1.7463    |
| Perplexity           | 0.17443    |
| AveragePolicyStd     | 0.18308    |
| AveragePolicyStd[0]  | 0.20647    |
| AveragePolicyStd[1]  | 0.19389    |
| AveragePolicyStd[2]  | 0.14661    |
| AveragePolicyStd[3]  | 0.17625    |
| AveragePolicyStd[4]  | 0.15078    |
| AveragePolicyStd[5]  | 0.22446    |
| AverageReturn        | 1806       |
| MinReturn            | 257.14     |
| MaxReturn            | 1982.2     |
| StdReturn            | 330.66     |
| AverageEpisodeLength | 945.76     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.77     |
| TotalNEpisodes       | 24692      |
| TotalNSamples        | 7.6736e+06 |
| ExplainedVariance    | -0.0020383 |
-------------------------------------
[2018-01-21 16:32:56.955394 UTC] Saving snapshot
[2018-01-21 16:32:56.955716 UTC] Starting iteration 1534
[2018-01-21 16:32:56.955965 UTC] Start collecting samples
[2018-01-21 16:33:01.705495 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:01.850618 UTC] Performing policy update
[2018-01-21 16:33:01.851272 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:01.980864 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:03.419080 UTC] Performing line search
[2018-01-21 16:33:03.614658 UTC] Updating baseline
[2018-01-21 16:33:06.328774 UTC] Computing logging information
-------------------------------------
| Iteration            | 1534       |
| ExpectedImprovement  | 0.016988   |
| ActualImprovement    | 0.016363   |
| ImprovementRatio     | 0.96322    |
| MeanKL               | 0.0079315  |
| Entropy              | -1.7463    |
| Perplexity           | 0.17441    |
| AveragePolicyStd     | 0.18309    |
| AveragePolicyStd[0]  | 0.20675    |
| AveragePolicyStd[1]  | 0.19378    |
| AveragePolicyStd[2]  | 0.14653    |
| AveragePolicyStd[3]  | 0.17622    |
| AveragePolicyStd[4]  | 0.15073    |
| AveragePolicyStd[5]  | 0.22451    |
| AverageReturn        | 1811.6     |
| MinReturn            | 257.14     |
| MaxReturn            | 1982.2     |
| StdReturn            | 320.49     |
| AverageEpisodeLength | 950.17     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.08     |
| TotalNEpisodes       | 24699      |
| TotalNSamples        | 7.6806e+06 |
| ExplainedVariance    | 0.0015501  |
-------------------------------------
[2018-01-21 16:33:07.165629 UTC] Saving snapshot
[2018-01-21 16:33:07.165865 UTC] Starting iteration 1535
[2018-01-21 16:33:07.166013 UTC] Start collecting samples
[2018-01-21 16:33:11.648065 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:11.770326 UTC] Performing policy update
[2018-01-21 16:33:11.771477 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:11.891842 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:13.337924 UTC] Performing line search
[2018-01-21 16:33:13.524524 UTC] Updating baseline
[2018-01-21 16:33:15.836902 UTC] Computing logging information
-------------------------------------
| Iteration            | 1535       |
| ExpectedImprovement  | 0.021834   |
| ActualImprovement    | 0.017644   |
| ImprovementRatio     | 0.80809    |
| MeanKL               | 0.0071293  |
| Entropy              | -1.7452    |
| Perplexity           | 0.17461    |
| AveragePolicyStd     | 0.18312    |
| AveragePolicyStd[0]  | 0.20687    |
| AveragePolicyStd[1]  | 0.19402    |
| AveragePolicyStd[2]  | 0.14672    |
| AveragePolicyStd[3]  | 0.17614    |
| AveragePolicyStd[4]  | 0.15062    |
| AveragePolicyStd[5]  | 0.22433    |
| AverageReturn        | 1794.1     |
| MinReturn            | 77.961     |
| MaxReturn            | 1982.2     |
| StdReturn            | 374.09     |
| AverageEpisodeLength | 942.82     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.78     |
| TotalNEpisodes       | 24704      |
| TotalNSamples        | 7.6841e+06 |
| ExplainedVariance    | 0.083233   |
-------------------------------------
[2018-01-21 16:33:16.735898 UTC] Saving snapshot
[2018-01-21 16:33:16.736175 UTC] Starting iteration 1536
[2018-01-21 16:33:16.736368 UTC] Start collecting samples
[2018-01-21 16:33:21.349677 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:21.480465 UTC] Performing policy update
[2018-01-21 16:33:21.481281 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:21.602416 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:23.014611 UTC] Performing line search
[2018-01-21 16:33:23.217417 UTC] Updating baseline
[2018-01-21 16:33:26.617129 UTC] Computing logging information
-------------------------------------
| Iteration            | 1536       |
| ExpectedImprovement  | 0.01674    |
| ActualImprovement    | 0.015572   |
| ImprovementRatio     | 0.93021    |
| MeanKL               | 0.00885    |
| Entropy              | -1.7468    |
| Perplexity           | 0.17433    |
| AveragePolicyStd     | 0.18307    |
| AveragePolicyStd[0]  | 0.20691    |
| AveragePolicyStd[1]  | 0.19414    |
| AveragePolicyStd[2]  | 0.14684    |
| AveragePolicyStd[3]  | 0.17612    |
| AveragePolicyStd[4]  | 0.15022    |
| AveragePolicyStd[5]  | 0.22423    |
| AverageReturn        | 1792.7     |
| MinReturn            | 77.961     |
| MaxReturn            | 1982.2     |
| StdReturn            | 373.65     |
| AverageEpisodeLength | 942.82     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.78     |
| TotalNEpisodes       | 24708      |
| TotalNSamples        | 7.6881e+06 |
| ExplainedVariance    | 0.0014489  |
-------------------------------------
[2018-01-21 16:33:27.522012 UTC] Saving snapshot
[2018-01-21 16:33:27.522269 UTC] Starting iteration 1537
[2018-01-21 16:33:27.522490 UTC] Start collecting samples
[2018-01-21 16:33:32.434052 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:32.570968 UTC] Performing policy update
[2018-01-21 16:33:32.571609 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:32.691766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:34.123422 UTC] Performing line search
[2018-01-21 16:33:34.317260 UTC] Updating baseline
[2018-01-21 16:33:36.304771 UTC] Computing logging information
--------------------------------------
| Iteration            | 1537        |
| ExpectedImprovement  | 0.018988    |
| ActualImprovement    | 0.017893    |
| ImprovementRatio     | 0.94236     |
| MeanKL               | 0.0090862   |
| Entropy              | -1.7506     |
| Perplexity           | 0.17367     |
| AveragePolicyStd     | 0.18297     |
| AveragePolicyStd[0]  | 0.207       |
| AveragePolicyStd[1]  | 0.19444     |
| AveragePolicyStd[2]  | 0.14662     |
| AveragePolicyStd[3]  | 0.1757      |
| AveragePolicyStd[4]  | 0.15008     |
| AveragePolicyStd[5]  | 0.22401     |
| AverageReturn        | 1791.1      |
| MinReturn            | 77.961      |
| MaxReturn            | 1985.8      |
| StdReturn            | 373.04      |
| AverageEpisodeLength | 942.82      |
| MinEpisodeLength     | 169         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 179.78      |
| TotalNEpisodes       | 24716       |
| TotalNSamples        | 7.6961e+06  |
| ExplainedVariance    | -0.00083634 |
--------------------------------------
[2018-01-21 16:33:37.141611 UTC] Saving snapshot
[2018-01-21 16:33:37.141847 UTC] Starting iteration 1538
[2018-01-21 16:33:37.142013 UTC] Start collecting samples
[2018-01-21 16:33:41.621775 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:41.753705 UTC] Performing policy update
[2018-01-21 16:33:41.754326 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:41.879925 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:43.298298 UTC] Performing line search
[2018-01-21 16:33:43.486168 UTC] Updating baseline
[2018-01-21 16:33:45.788666 UTC] Computing logging information
-------------------------------------
| Iteration            | 1538       |
| ExpectedImprovement  | 0.020702   |
| ActualImprovement    | 0.019707   |
| ImprovementRatio     | 0.95193    |
| MeanKL               | 0.0075511  |
| Entropy              | -1.7483    |
| Perplexity           | 0.17407    |
| AveragePolicyStd     | 0.18305    |
| AveragePolicyStd[0]  | 0.2069     |
| AveragePolicyStd[1]  | 0.19428    |
| AveragePolicyStd[2]  | 0.14644    |
| AveragePolicyStd[3]  | 0.17581    |
| AveragePolicyStd[4]  | 0.15044    |
| AveragePolicyStd[5]  | 0.22442    |
| AverageReturn        | 1779.3     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 385.36     |
| AverageEpisodeLength | 937.48     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.83     |
| TotalNEpisodes       | 24720      |
| TotalNSamples        | 7.6995e+06 |
| ExplainedVariance    | 0.22959    |
-------------------------------------
[2018-01-21 16:33:46.614289 UTC] Saving snapshot
[2018-01-21 16:33:46.614514 UTC] Starting iteration 1539
[2018-01-21 16:33:46.614670 UTC] Start collecting samples
[2018-01-21 16:33:51.076934 UTC] Computing input variables for policy optimization
[2018-01-21 16:33:51.216557 UTC] Performing policy update
[2018-01-21 16:33:51.217715 UTC] Computing gradient in Euclidean space
[2018-01-21 16:33:51.337869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:33:52.750166 UTC] Performing line search
[2018-01-21 16:33:52.939929 UTC] Updating baseline
[2018-01-21 16:33:55.059760 UTC] Computing logging information
-------------------------------------
| Iteration            | 1539       |
| ExpectedImprovement  | 0.020582   |
| ActualImprovement    | 0.019013   |
| ImprovementRatio     | 0.92378    |
| MeanKL               | 0.007541   |
| Entropy              | -1.7538    |
| Perplexity           | 0.17311    |
| AveragePolicyStd     | 0.18285    |
| AveragePolicyStd[0]  | 0.20654    |
| AveragePolicyStd[1]  | 0.19377    |
| AveragePolicyStd[2]  | 0.14642    |
| AveragePolicyStd[3]  | 0.17592    |
| AveragePolicyStd[4]  | 0.15053    |
| AveragePolicyStd[5]  | 0.2239     |
| AverageReturn        | 1809.2     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 337.86     |
| AverageEpisodeLength | 951.61     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.75     |
| TotalNEpisodes       | 24724      |
| TotalNSamples        | 7.7035e+06 |
| ExplainedVariance    | -0.1179    |
-------------------------------------
[2018-01-21 16:33:55.836715 UTC] Saving snapshot
[2018-01-21 16:33:55.836943 UTC] Starting iteration 1540
[2018-01-21 16:33:55.837124 UTC] Start collecting samples
[2018-01-21 16:34:00.344941 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:00.472150 UTC] Performing policy update
[2018-01-21 16:34:00.472796 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:00.594737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:01.984905 UTC] Performing line search
[2018-01-21 16:34:02.172092 UTC] Updating baseline
[2018-01-21 16:34:03.968444 UTC] Computing logging information
-------------------------------------
| Iteration            | 1540       |
| ExpectedImprovement  | 0.019117   |
| ActualImprovement    | 0.018036   |
| ImprovementRatio     | 0.94347    |
| MeanKL               | 0.0081761  |
| Entropy              | -1.7591    |
| Perplexity           | 0.1722     |
| AveragePolicyStd     | 0.18267    |
| AveragePolicyStd[0]  | 0.20605    |
| AveragePolicyStd[1]  | 0.19331    |
| AveragePolicyStd[2]  | 0.14622    |
| AveragePolicyStd[3]  | 0.17631    |
| AveragePolicyStd[4]  | 0.15059    |
| AveragePolicyStd[5]  | 0.22352    |
| AverageReturn        | 1807.6     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 340.41     |
| AverageEpisodeLength | 950.02     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.1      |
| TotalNEpisodes       | 24732      |
| TotalNSamples        | 7.7113e+06 |
| ExplainedVariance    | 0.052874   |
-------------------------------------
[2018-01-21 16:34:04.773499 UTC] Saving snapshot
[2018-01-21 16:34:04.783355 UTC] Starting iteration 1541
[2018-01-21 16:34:04.783597 UTC] Start collecting samples
[2018-01-21 16:34:09.237288 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:09.359347 UTC] Performing policy update
[2018-01-21 16:34:09.360415 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:09.482936 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:10.904877 UTC] Performing line search
[2018-01-21 16:34:11.098819 UTC] Updating baseline
[2018-01-21 16:34:13.031101 UTC] Computing logging information
-------------------------------------
| Iteration            | 1541       |
| ExpectedImprovement  | 0.020707   |
| ActualImprovement    | 0.018819   |
| ImprovementRatio     | 0.90883    |
| MeanKL               | 0.0077675  |
| Entropy              | -1.7608    |
| Perplexity           | 0.17191    |
| AveragePolicyStd     | 0.18261    |
| AveragePolicyStd[0]  | 0.20642    |
| AveragePolicyStd[1]  | 0.19309    |
| AveragePolicyStd[2]  | 0.14614    |
| AveragePolicyStd[3]  | 0.17636    |
| AveragePolicyStd[4]  | 0.15057    |
| AveragePolicyStd[5]  | 0.22306    |
| AverageReturn        | 1799.1     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 357.04     |
| AverageEpisodeLength | 945.9      |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.2      |
| TotalNEpisodes       | 24736      |
| TotalNSamples        | 7.7146e+06 |
| ExplainedVariance    | 0.16908    |
-------------------------------------
[2018-01-21 16:34:13.856047 UTC] Saving snapshot
[2018-01-21 16:34:13.856276 UTC] Starting iteration 1542
[2018-01-21 16:34:13.856454 UTC] Start collecting samples
[2018-01-21 16:34:18.466390 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:18.631705 UTC] Performing policy update
[2018-01-21 16:34:18.632330 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:18.774784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:20.167615 UTC] Performing line search
[2018-01-21 16:34:20.361008 UTC] Updating baseline
[2018-01-21 16:34:22.289999 UTC] Computing logging information
-------------------------------------
| Iteration            | 1542       |
| ExpectedImprovement  | 0.016718   |
| ActualImprovement    | 0.015878   |
| ImprovementRatio     | 0.94972    |
| MeanKL               | 0.0079597  |
| Entropy              | -1.7617    |
| Perplexity           | 0.17175    |
| AveragePolicyStd     | 0.18257    |
| AveragePolicyStd[0]  | 0.20642    |
| AveragePolicyStd[1]  | 0.19301    |
| AveragePolicyStd[2]  | 0.14637    |
| AveragePolicyStd[3]  | 0.17596    |
| AveragePolicyStd[4]  | 0.15066    |
| AveragePolicyStd[5]  | 0.22298    |
| AverageReturn        | 1799.1     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 357.02     |
| AverageEpisodeLength | 945.9      |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.2      |
| TotalNEpisodes       | 24741      |
| TotalNSamples        | 7.7196e+06 |
| ExplainedVariance    | -0.026479  |
-------------------------------------
[2018-01-21 16:34:23.136459 UTC] Saving snapshot
[2018-01-21 16:34:23.136708 UTC] Starting iteration 1543
[2018-01-21 16:34:23.136891 UTC] Start collecting samples
[2018-01-21 16:34:28.088443 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:28.218673 UTC] Performing policy update
[2018-01-21 16:34:28.219634 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:28.348502 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:29.765895 UTC] Performing line search
[2018-01-21 16:34:29.955684 UTC] Updating baseline
[2018-01-21 16:34:32.142823 UTC] Computing logging information
-------------------------------------
| Iteration            | 1543       |
| ExpectedImprovement  | 0.016763   |
| ActualImprovement    | 0.01568    |
| ImprovementRatio     | 0.93542    |
| MeanKL               | 0.0087635  |
| Entropy              | -1.7552    |
| Perplexity           | 0.17287    |
| AveragePolicyStd     | 0.18276    |
| AveragePolicyStd[0]  | 0.20666    |
| AveragePolicyStd[1]  | 0.19318    |
| AveragePolicyStd[2]  | 0.14676    |
| AveragePolicyStd[3]  | 0.17585    |
| AveragePolicyStd[4]  | 0.15079    |
| AveragePolicyStd[5]  | 0.22335    |
| AverageReturn        | 1800.6     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 357.5      |
| AverageEpisodeLength | 945.9      |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.2      |
| TotalNEpisodes       | 24746      |
| TotalNSamples        | 7.7246e+06 |
| ExplainedVariance    | 0.0017702  |
-------------------------------------
[2018-01-21 16:34:32.936677 UTC] Saving snapshot
[2018-01-21 16:34:32.936911 UTC] Starting iteration 1544
[2018-01-21 16:34:32.937062 UTC] Start collecting samples
[2018-01-21 16:34:37.561964 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:37.740808 UTC] Performing policy update
[2018-01-21 16:34:37.741464 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:37.898042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:39.327672 UTC] Performing line search
[2018-01-21 16:34:39.519685 UTC] Updating baseline
[2018-01-21 16:34:41.923924 UTC] Computing logging information
-------------------------------------
| Iteration            | 1544       |
| ExpectedImprovement  | 0.017188   |
| ActualImprovement    | 0.01617    |
| ImprovementRatio     | 0.94076    |
| MeanKL               | 0.0091805  |
| Entropy              | -1.7599    |
| Perplexity           | 0.17206    |
| AveragePolicyStd     | 0.18263    |
| AveragePolicyStd[0]  | 0.20664    |
| AveragePolicyStd[1]  | 0.19268    |
| AveragePolicyStd[2]  | 0.14666    |
| AveragePolicyStd[3]  | 0.17567    |
| AveragePolicyStd[4]  | 0.15068    |
| AveragePolicyStd[5]  | 0.22342    |
| AverageReturn        | 1814.2     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 340.5      |
| AverageEpisodeLength | 952.17     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.87     |
| TotalNEpisodes       | 24753      |
| TotalNSamples        | 7.7312e+06 |
| ExplainedVariance    | 0.095233   |
-------------------------------------
[2018-01-21 16:34:42.760780 UTC] Saving snapshot
[2018-01-21 16:34:42.761130 UTC] Starting iteration 1545
[2018-01-21 16:34:42.761375 UTC] Start collecting samples
[2018-01-21 16:34:47.315684 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:47.437541 UTC] Performing policy update
[2018-01-21 16:34:47.438611 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:47.559185 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:48.965070 UTC] Performing line search
[2018-01-21 16:34:49.154172 UTC] Updating baseline
[2018-01-21 16:34:51.217421 UTC] Computing logging information
-------------------------------------
| Iteration            | 1545       |
| ExpectedImprovement  | 0.018735   |
| ActualImprovement    | 0.017604   |
| ImprovementRatio     | 0.9396     |
| MeanKL               | 0.0083406  |
| Entropy              | -1.7546    |
| Perplexity           | 0.17298    |
| AveragePolicyStd     | 0.18278    |
| AveragePolicyStd[0]  | 0.20655    |
| AveragePolicyStd[1]  | 0.19334    |
| AveragePolicyStd[2]  | 0.14696    |
| AveragePolicyStd[3]  | 0.17516    |
| AveragePolicyStd[4]  | 0.15096    |
| AveragePolicyStd[5]  | 0.22371    |
| AverageReturn        | 1814.3     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 340.5      |
| AverageEpisodeLength | 952.17     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.87     |
| TotalNEpisodes       | 24756      |
| TotalNSamples        | 7.7342e+06 |
| ExplainedVariance    | -0.071307  |
-------------------------------------
[2018-01-21 16:34:52.093675 UTC] Saving snapshot
[2018-01-21 16:34:52.093930 UTC] Starting iteration 1546
[2018-01-21 16:34:52.094100 UTC] Start collecting samples
[2018-01-21 16:34:56.412007 UTC] Computing input variables for policy optimization
[2018-01-21 16:34:56.553925 UTC] Performing policy update
[2018-01-21 16:34:56.555077 UTC] Computing gradient in Euclidean space
[2018-01-21 16:34:56.668249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:34:58.146009 UTC] Performing line search
[2018-01-21 16:34:58.349328 UTC] Updating baseline
[2018-01-21 16:35:00.671192 UTC] Computing logging information
-------------------------------------
| Iteration            | 1546       |
| ExpectedImprovement  | 0.018482   |
| ActualImprovement    | 0.017065   |
| ImprovementRatio     | 0.92335    |
| MeanKL               | 0.0078045  |
| Entropy              | -1.7569    |
| Perplexity           | 0.17258    |
| AveragePolicyStd     | 0.18271    |
| AveragePolicyStd[0]  | 0.20637    |
| AveragePolicyStd[1]  | 0.19351    |
| AveragePolicyStd[2]  | 0.14689    |
| AveragePolicyStd[3]  | 0.17509    |
| AveragePolicyStd[4]  | 0.15094    |
| AveragePolicyStd[5]  | 0.22344    |
| AverageReturn        | 1815.3     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 340.74     |
| AverageEpisodeLength | 952.17     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.87     |
| TotalNEpisodes       | 24759      |
| TotalNSamples        | 7.7372e+06 |
| ExplainedVariance    | 0.0066872  |
-------------------------------------
[2018-01-21 16:35:01.481331 UTC] Saving snapshot
[2018-01-21 16:35:01.481526 UTC] Starting iteration 1547
[2018-01-21 16:35:01.481656 UTC] Start collecting samples
[2018-01-21 16:35:06.184751 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:06.308890 UTC] Performing policy update
[2018-01-21 16:35:06.309738 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:06.434423 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:07.847961 UTC] Performing line search
[2018-01-21 16:35:08.079122 UTC] Updating baseline
[2018-01-21 16:35:10.711904 UTC] Computing logging information
-------------------------------------
| Iteration            | 1547       |
| ExpectedImprovement  | 0.017923   |
| ActualImprovement    | 0.016584   |
| ImprovementRatio     | 0.92527    |
| MeanKL               | 0.0074425  |
| Entropy              | -1.7623    |
| Perplexity           | 0.17164    |
| AveragePolicyStd     | 0.18254    |
| AveragePolicyStd[0]  | 0.20607    |
| AveragePolicyStd[1]  | 0.19343    |
| AveragePolicyStd[2]  | 0.14687    |
| AveragePolicyStd[3]  | 0.17473    |
| AveragePolicyStd[4]  | 0.15075    |
| AveragePolicyStd[5]  | 0.22341    |
| AverageReturn        | 1838.7     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 305.17     |
| AverageEpisodeLength | 963.02     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.54     |
| TotalNEpisodes       | 24768      |
| TotalNSamples        | 7.7462e+06 |
| ExplainedVariance    | 0.0040673  |
-------------------------------------
[2018-01-21 16:35:11.517873 UTC] Saving snapshot
[2018-01-21 16:35:11.518063 UTC] Starting iteration 1548
[2018-01-21 16:35:11.518237 UTC] Start collecting samples
[2018-01-21 16:35:16.069143 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:16.194523 UTC] Performing policy update
[2018-01-21 16:35:16.195178 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:16.312631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:17.710668 UTC] Performing line search
[2018-01-21 16:35:17.899437 UTC] Updating baseline
[2018-01-21 16:35:19.790882 UTC] Computing logging information
-------------------------------------
| Iteration            | 1548       |
| ExpectedImprovement  | 0.01943    |
| ActualImprovement    | 0.018559   |
| ImprovementRatio     | 0.95518    |
| MeanKL               | 0.0085144  |
| Entropy              | -1.7594    |
| Perplexity           | 0.17215    |
| AveragePolicyStd     | 0.1826     |
| AveragePolicyStd[0]  | 0.20565    |
| AveragePolicyStd[1]  | 0.19374    |
| AveragePolicyStd[2]  | 0.14694    |
| AveragePolicyStd[3]  | 0.17464    |
| AveragePolicyStd[4]  | 0.15134    |
| AveragePolicyStd[5]  | 0.2233     |
| AverageReturn        | 1833.1     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 312.91     |
| AverageEpisodeLength | 959.25     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.39     |
| TotalNEpisodes       | 24773      |
| TotalNSamples        | 7.7508e+06 |
| ExplainedVariance    | 0.13771    |
-------------------------------------
[2018-01-21 16:35:20.640408 UTC] Saving snapshot
[2018-01-21 16:35:20.640707 UTC] Starting iteration 1549
[2018-01-21 16:35:20.640940 UTC] Start collecting samples
[2018-01-21 16:35:25.186603 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:25.324560 UTC] Performing policy update
[2018-01-21 16:35:25.325214 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:25.438561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:26.885011 UTC] Performing line search
[2018-01-21 16:35:27.108800 UTC] Updating baseline
[2018-01-21 16:35:29.263248 UTC] Computing logging information
-------------------------------------
| Iteration            | 1549       |
| ExpectedImprovement  | 0.020829   |
| ActualImprovement    | 0.019917   |
| ImprovementRatio     | 0.95622    |
| MeanKL               | 0.0074749  |
| Entropy              | -1.7653    |
| Perplexity           | 0.17113    |
| AveragePolicyStd     | 0.18245    |
| AveragePolicyStd[0]  | 0.20558    |
| AveragePolicyStd[1]  | 0.1941     |
| AveragePolicyStd[2]  | 0.14669    |
| AveragePolicyStd[3]  | 0.17449    |
| AveragePolicyStd[4]  | 0.15073    |
| AveragePolicyStd[5]  | 0.2231     |
| AverageReturn        | 1833.7     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 313.09     |
| AverageEpisodeLength | 959.25     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 148.39     |
| TotalNEpisodes       | 24774      |
| TotalNSamples        | 7.7518e+06 |
| ExplainedVariance    | -0.28928   |
-------------------------------------
[2018-01-21 16:35:30.120170 UTC] Saving snapshot
[2018-01-21 16:35:30.120456 UTC] Starting iteration 1550
[2018-01-21 16:35:30.120621 UTC] Start collecting samples
[2018-01-21 16:35:34.727791 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:34.858593 UTC] Performing policy update
[2018-01-21 16:35:34.859920 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:34.992436 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:36.440887 UTC] Performing line search
[2018-01-21 16:35:36.649634 UTC] Updating baseline
[2018-01-21 16:35:39.433835 UTC] Computing logging information
-------------------------------------
| Iteration            | 1550       |
| ExpectedImprovement  | 0.01818    |
| ActualImprovement    | 0.01746    |
| ImprovementRatio     | 0.96038    |
| MeanKL               | 0.0087187  |
| Entropy              | -1.7715    |
| Perplexity           | 0.17008    |
| AveragePolicyStd     | 0.18226    |
| AveragePolicyStd[0]  | 0.20531    |
| AveragePolicyStd[1]  | 0.19375    |
| AveragePolicyStd[2]  | 0.14645    |
| AveragePolicyStd[3]  | 0.17391    |
| AveragePolicyStd[4]  | 0.15099    |
| AveragePolicyStd[5]  | 0.22317    |
| AverageReturn        | 1841.9     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 308.8      |
| AverageEpisodeLength | 962.25     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.13     |
| TotalNEpisodes       | 24781      |
| TotalNSamples        | 7.7588e+06 |
| ExplainedVariance    | 0.0012357  |
-------------------------------------
[2018-01-21 16:35:40.336223 UTC] Saving snapshot
[2018-01-21 16:35:40.346150 UTC] Starting iteration 1551
[2018-01-21 16:35:40.346345 UTC] Start collecting samples
[2018-01-21 16:35:44.933573 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:45.073179 UTC] Performing policy update
[2018-01-21 16:35:45.073936 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:45.198894 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:46.608670 UTC] Performing line search
[2018-01-21 16:35:46.810223 UTC] Updating baseline
[2018-01-21 16:35:48.864984 UTC] Computing logging information
-------------------------------------
| Iteration            | 1551       |
| ExpectedImprovement  | 0.016912   |
| ActualImprovement    | 0.015747   |
| ImprovementRatio     | 0.93114    |
| MeanKL               | 0.0086004  |
| Entropy              | -1.7736    |
| Perplexity           | 0.16973    |
| AveragePolicyStd     | 0.18217    |
| AveragePolicyStd[0]  | 0.20535    |
| AveragePolicyStd[1]  | 0.19384    |
| AveragePolicyStd[2]  | 0.14646    |
| AveragePolicyStd[3]  | 0.17387    |
| AveragePolicyStd[4]  | 0.15113    |
| AveragePolicyStd[5]  | 0.22237    |
| AverageReturn        | 1842.9     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 309.01     |
| AverageEpisodeLength | 962.25     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.13     |
| TotalNEpisodes       | 24788      |
| TotalNSamples        | 7.7658e+06 |
| ExplainedVariance    | 0.0031575  |
-------------------------------------
[2018-01-21 16:35:49.721913 UTC] Saving snapshot
[2018-01-21 16:35:49.722126 UTC] Starting iteration 1552
[2018-01-21 16:35:49.722334 UTC] Start collecting samples
[2018-01-21 16:35:54.147012 UTC] Computing input variables for policy optimization
[2018-01-21 16:35:54.270203 UTC] Performing policy update
[2018-01-21 16:35:54.270859 UTC] Computing gradient in Euclidean space
[2018-01-21 16:35:54.387671 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:35:55.800721 UTC] Performing line search
[2018-01-21 16:35:55.997205 UTC] Updating baseline
[2018-01-21 16:35:58.071878 UTC] Computing logging information
-------------------------------------
| Iteration            | 1552       |
| ExpectedImprovement  | 0.019872   |
| ActualImprovement    | 0.018624   |
| ImprovementRatio     | 0.93719    |
| MeanKL               | 0.0078272  |
| Entropy              | -1.7781    |
| Perplexity           | 0.16896    |
| AveragePolicyStd     | 0.18202    |
| AveragePolicyStd[0]  | 0.20495    |
| AveragePolicyStd[1]  | 0.19331    |
| AveragePolicyStd[2]  | 0.14689    |
| AveragePolicyStd[3]  | 0.17341    |
| AveragePolicyStd[4]  | 0.151      |
| AveragePolicyStd[5]  | 0.22255    |
| AverageReturn        | 1824.2     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 333.32     |
| AverageEpisodeLength | 952.58     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.94     |
| TotalNEpisodes       | 24792      |
| TotalNSamples        | 7.7689e+06 |
| ExplainedVariance    | 0.27529    |
-------------------------------------
[2018-01-21 16:35:58.905916 UTC] Saving snapshot
[2018-01-21 16:35:58.906183 UTC] Starting iteration 1553
[2018-01-21 16:35:58.906303 UTC] Start collecting samples
[2018-01-21 16:36:03.352126 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:03.485328 UTC] Performing policy update
[2018-01-21 16:36:03.486039 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:03.604007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:05.028667 UTC] Performing line search
[2018-01-21 16:36:05.220078 UTC] Updating baseline
[2018-01-21 16:36:08.678223 UTC] Computing logging information
-------------------------------------
| Iteration            | 1553       |
| ExpectedImprovement  | 0.016904   |
| ActualImprovement    | 0.015814   |
| ImprovementRatio     | 0.93549    |
| MeanKL               | 0.007781   |
| Entropy              | -1.7817    |
| Perplexity           | 0.16836    |
| AveragePolicyStd     | 0.18194    |
| AveragePolicyStd[0]  | 0.20515    |
| AveragePolicyStd[1]  | 0.19321    |
| AveragePolicyStd[2]  | 0.14645    |
| AveragePolicyStd[3]  | 0.17288    |
| AveragePolicyStd[4]  | 0.15113    |
| AveragePolicyStd[5]  | 0.22279    |
| AverageReturn        | 1825.1     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 333.51     |
| AverageEpisodeLength | 952.58     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.94     |
| TotalNEpisodes       | 24797      |
| TotalNSamples        | 7.7739e+06 |
| ExplainedVariance    | 0.0034715  |
-------------------------------------
[2018-01-21 16:36:09.597068 UTC] Saving snapshot
[2018-01-21 16:36:09.597327 UTC] Starting iteration 1554
[2018-01-21 16:36:09.597473 UTC] Start collecting samples
[2018-01-21 16:36:14.116599 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:14.260756 UTC] Performing policy update
[2018-01-21 16:36:14.261390 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:14.386715 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:15.822650 UTC] Performing line search
[2018-01-21 16:36:16.012599 UTC] Updating baseline
[2018-01-21 16:36:18.413374 UTC] Computing logging information
-------------------------------------
| Iteration            | 1554       |
| ExpectedImprovement  | 0.017907   |
| ActualImprovement    | 0.016704   |
| ImprovementRatio     | 0.93279    |
| MeanKL               | 0.0082642  |
| Entropy              | -1.7805    |
| Perplexity           | 0.16856    |
| AveragePolicyStd     | 0.18195    |
| AveragePolicyStd[0]  | 0.20517    |
| AveragePolicyStd[1]  | 0.19318    |
| AveragePolicyStd[2]  | 0.14671    |
| AveragePolicyStd[3]  | 0.17298    |
| AveragePolicyStd[4]  | 0.15121    |
| AveragePolicyStd[5]  | 0.22246    |
| AverageReturn        | 1842.3     |
| MinReturn            | 77.961     |
| MaxReturn            | 1985.8     |
| StdReturn            | 304.56     |
| AverageEpisodeLength | 959.81     |
| MinEpisodeLength     | 169        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.77     |
| TotalNEpisodes       | 24802      |
| TotalNSamples        | 7.7789e+06 |
| ExplainedVariance    | -0.0054505 |
-------------------------------------
[2018-01-21 16:36:19.261946 UTC] Saving snapshot
[2018-01-21 16:36:19.262187 UTC] Starting iteration 1555
[2018-01-21 16:36:19.262370 UTC] Start collecting samples
[2018-01-21 16:36:23.753792 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:23.900302 UTC] Performing policy update
[2018-01-21 16:36:23.900994 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:24.026686 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:25.470137 UTC] Performing line search
[2018-01-21 16:36:25.675822 UTC] Updating baseline
[2018-01-21 16:36:28.242586 UTC] Computing logging information
-------------------------------------
| Iteration            | 1555       |
| ExpectedImprovement  | 0.016109   |
| ActualImprovement    | 0.015371   |
| ImprovementRatio     | 0.95418    |
| MeanKL               | 0.0087047  |
| Entropy              | -1.7929    |
| Perplexity           | 0.16648    |
| AveragePolicyStd     | 0.1816     |
| AveragePolicyStd[0]  | 0.20579    |
| AveragePolicyStd[1]  | 0.19219    |
| AveragePolicyStd[2]  | 0.14605    |
| AveragePolicyStd[3]  | 0.17236    |
| AveragePolicyStd[4]  | 0.15113    |
| AveragePolicyStd[5]  | 0.22207    |
| AverageReturn        | 1862.6     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.8     |
| StdReturn            | 248.12     |
| AverageEpisodeLength | 968.12     |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.84     |
| TotalNEpisodes       | 24808      |
| TotalNSamples        | 7.7849e+06 |
| ExplainedVariance    | 0.0025789  |
-------------------------------------
[2018-01-21 16:36:29.064753 UTC] Saving snapshot
[2018-01-21 16:36:29.065015 UTC] Starting iteration 1556
[2018-01-21 16:36:29.065188 UTC] Start collecting samples
[2018-01-21 16:36:33.821925 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:33.955606 UTC] Performing policy update
[2018-01-21 16:36:33.956273 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:34.075417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:35.471868 UTC] Performing line search
[2018-01-21 16:36:35.659829 UTC] Updating baseline
[2018-01-21 16:36:38.040847 UTC] Computing logging information
-------------------------------------
| Iteration            | 1556       |
| ExpectedImprovement  | 0.017133   |
| ActualImprovement    | 0.016174   |
| ImprovementRatio     | 0.94403    |
| MeanKL               | 0.0079193  |
| Entropy              | -1.7869    |
| Perplexity           | 0.16748    |
| AveragePolicyStd     | 0.18177    |
| AveragePolicyStd[0]  | 0.20589    |
| AveragePolicyStd[1]  | 0.19237    |
| AveragePolicyStd[2]  | 0.14632    |
| AveragePolicyStd[3]  | 0.17245    |
| AveragePolicyStd[4]  | 0.15129    |
| AveragePolicyStd[5]  | 0.22232    |
| AverageReturn        | 1863.7     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.8     |
| StdReturn            | 248.39     |
| AverageEpisodeLength | 968.12     |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.84     |
| TotalNEpisodes       | 24813      |
| TotalNSamples        | 7.7899e+06 |
| ExplainedVariance    | 0.0029291  |
-------------------------------------
[2018-01-21 16:36:38.944836 UTC] Saving snapshot
[2018-01-21 16:36:38.945141 UTC] Starting iteration 1557
[2018-01-21 16:36:38.945327 UTC] Start collecting samples
[2018-01-21 16:36:43.548103 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:43.680612 UTC] Performing policy update
[2018-01-21 16:36:43.681256 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:43.805773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:45.230181 UTC] Performing line search
[2018-01-21 16:36:45.420953 UTC] Updating baseline
[2018-01-21 16:36:48.644376 UTC] Computing logging information
-------------------------------------
| Iteration            | 1557       |
| ExpectedImprovement  | 0.017322   |
| ActualImprovement    | 0.015978   |
| ImprovementRatio     | 0.92238    |
| MeanKL               | 0.0082509  |
| Entropy              | -1.7843    |
| Perplexity           | 0.16792    |
| AveragePolicyStd     | 0.18184    |
| AveragePolicyStd[0]  | 0.20659    |
| AveragePolicyStd[1]  | 0.19179    |
| AveragePolicyStd[2]  | 0.1462     |
| AveragePolicyStd[3]  | 0.17244    |
| AveragePolicyStd[4]  | 0.15188    |
| AveragePolicyStd[5]  | 0.22215    |
| AverageReturn        | 1874.3     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.6     |
| StdReturn            | 224.04     |
| AverageEpisodeLength | 973.46     |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.73     |
| TotalNEpisodes       | 24817      |
| TotalNSamples        | 7.7939e+06 |
| ExplainedVariance    | 0.0059463  |
-------------------------------------
[2018-01-21 16:36:49.589397 UTC] Saving snapshot
[2018-01-21 16:36:49.589633 UTC] Starting iteration 1558
[2018-01-21 16:36:49.589778 UTC] Start collecting samples
[2018-01-21 16:36:54.211267 UTC] Computing input variables for policy optimization
[2018-01-21 16:36:54.370131 UTC] Performing policy update
[2018-01-21 16:36:54.370791 UTC] Computing gradient in Euclidean space
[2018-01-21 16:36:54.485755 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:36:55.878212 UTC] Performing line search
[2018-01-21 16:36:56.075675 UTC] Updating baseline
[2018-01-21 16:36:59.391955 UTC] Computing logging information
-------------------------------------
| Iteration            | 1558       |
| ExpectedImprovement  | 0.018981   |
| ActualImprovement    | 0.017619   |
| ImprovementRatio     | 0.92828    |
| MeanKL               | 0.0075538  |
| Entropy              | -1.7816    |
| Perplexity           | 0.16836    |
| AveragePolicyStd     | 0.18192    |
| AveragePolicyStd[0]  | 0.20658    |
| AveragePolicyStd[1]  | 0.19144    |
| AveragePolicyStd[2]  | 0.14625    |
| AveragePolicyStd[3]  | 0.17284    |
| AveragePolicyStd[4]  | 0.15197    |
| AveragePolicyStd[5]  | 0.22244    |
| AverageReturn        | 1852.3     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.6     |
| StdReturn            | 265.75     |
| AverageEpisodeLength | 962.9      |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.94     |
| TotalNEpisodes       | 24826      |
| TotalNSamples        | 7.8018e+06 |
| ExplainedVariance    | 0.092627   |
-------------------------------------
[2018-01-21 16:37:00.275914 UTC] Saving snapshot
[2018-01-21 16:37:00.276194 UTC] Starting iteration 1559
[2018-01-21 16:37:00.276415 UTC] Start collecting samples
[2018-01-21 16:37:05.052216 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:05.177667 UTC] Performing policy update
[2018-01-21 16:37:05.178811 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:05.301927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:06.705180 UTC] Performing line search
[2018-01-21 16:37:06.896709 UTC] Updating baseline
[2018-01-21 16:37:09.047173 UTC] Computing logging information
-------------------------------------
| Iteration            | 1559       |
| ExpectedImprovement  | 0.019635   |
| ActualImprovement    | 0.019452   |
| ImprovementRatio     | 0.99067    |
| MeanKL               | 0.0083131  |
| Entropy              | -1.7839    |
| Perplexity           | 0.16798    |
| AveragePolicyStd     | 0.18186    |
| AveragePolicyStd[0]  | 0.20653    |
| AveragePolicyStd[1]  | 0.19156    |
| AveragePolicyStd[2]  | 0.14576    |
| AveragePolicyStd[3]  | 0.17285    |
| AveragePolicyStd[4]  | 0.15213    |
| AveragePolicyStd[5]  | 0.22234    |
| AverageReturn        | 1856.9     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.6     |
| StdReturn            | 261.67     |
| AverageEpisodeLength | 965.56     |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.94     |
| TotalNEpisodes       | 24830      |
| TotalNSamples        | 7.8058e+06 |
| ExplainedVariance    | -0.098637  |
-------------------------------------
[2018-01-21 16:37:09.873988 UTC] Saving snapshot
[2018-01-21 16:37:09.874264 UTC] Starting iteration 1560
[2018-01-21 16:37:09.874467 UTC] Start collecting samples
[2018-01-21 16:37:14.444071 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:14.573888 UTC] Performing policy update
[2018-01-21 16:37:14.576567 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:14.696806 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:16.146843 UTC] Performing line search
[2018-01-21 16:37:16.335426 UTC] Updating baseline
[2018-01-21 16:37:18.702473 UTC] Computing logging information
-------------------------------------
| Iteration            | 1560       |
| ExpectedImprovement  | 0.019923   |
| ActualImprovement    | 0.017971   |
| ImprovementRatio     | 0.90202    |
| MeanKL               | 0.0077719  |
| Entropy              | -1.7864    |
| Perplexity           | 0.16755    |
| AveragePolicyStd     | 0.18178    |
| AveragePolicyStd[0]  | 0.20641    |
| AveragePolicyStd[1]  | 0.19083    |
| AveragePolicyStd[2]  | 0.14604    |
| AveragePolicyStd[3]  | 0.17325    |
| AveragePolicyStd[4]  | 0.1517     |
| AveragePolicyStd[5]  | 0.22244    |
| AverageReturn        | 1856.2     |
| MinReturn            | 664.68     |
| MaxReturn            | 1985.6     |
| StdReturn            | 261.62     |
| AverageEpisodeLength | 965.56     |
| MinEpisodeLength     | 374        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.94     |
| TotalNEpisodes       | 24833      |
| TotalNSamples        | 7.8088e+06 |
| ExplainedVariance    | 0.0024607  |
-------------------------------------
[2018-01-21 16:37:19.528252 UTC] Saving snapshot
[2018-01-21 16:37:19.534380 UTC] Starting iteration 1561
[2018-01-21 16:37:19.534600 UTC] Start collecting samples
[2018-01-21 16:37:24.173789 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:24.334776 UTC] Performing policy update
[2018-01-21 16:37:24.335434 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:24.455464 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:25.907165 UTC] Performing line search
[2018-01-21 16:37:26.096523 UTC] Updating baseline
[2018-01-21 16:37:28.028645 UTC] Computing logging information
-------------------------------------
| Iteration            | 1561       |
| ExpectedImprovement  | 0.020924   |
| ActualImprovement    | 0.019648   |
| ImprovementRatio     | 0.939      |
| MeanKL               | 0.0077769  |
| Entropy              | -1.7953    |
| Perplexity           | 0.16609    |
| AveragePolicyStd     | 0.18151    |
| AveragePolicyStd[0]  | 0.20647    |
| AveragePolicyStd[1]  | 0.19059    |
| AveragePolicyStd[2]  | 0.14594    |
| AveragePolicyStd[3]  | 0.17311    |
| AveragePolicyStd[4]  | 0.15127    |
| AveragePolicyStd[5]  | 0.22168    |
| AverageReturn        | 1859       |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 243.71     |
| AverageEpisodeLength | 966.45     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.36     |
| TotalNEpisodes       | 24843      |
| TotalNSamples        | 7.8183e+06 |
| ExplainedVariance    | 0.12041    |
-------------------------------------
[2018-01-21 16:37:28.872927 UTC] Saving snapshot
[2018-01-21 16:37:28.873199 UTC] Starting iteration 1562
[2018-01-21 16:37:28.873380 UTC] Start collecting samples
[2018-01-21 16:37:33.220325 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:33.341690 UTC] Performing policy update
[2018-01-21 16:37:33.342759 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:33.465649 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:34.904540 UTC] Performing line search
[2018-01-21 16:37:35.104060 UTC] Updating baseline
[2018-01-21 16:37:37.291772 UTC] Computing logging information
-------------------------------------
| Iteration            | 1562       |
| ExpectedImprovement  | 0.017193   |
| ActualImprovement    | 0.016314   |
| ImprovementRatio     | 0.94886    |
| MeanKL               | 0.0082419  |
| Entropy              | -1.8026    |
| Perplexity           | 0.16487    |
| AveragePolicyStd     | 0.18128    |
| AveragePolicyStd[0]  | 0.20581    |
| AveragePolicyStd[1]  | 0.19034    |
| AveragePolicyStd[2]  | 0.14571    |
| AveragePolicyStd[3]  | 0.17285    |
| AveragePolicyStd[4]  | 0.15131    |
| AveragePolicyStd[5]  | 0.22168    |
| AverageReturn        | 1859.4     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 243.84     |
| AverageEpisodeLength | 966.45     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 118.36     |
| TotalNEpisodes       | 24844      |
| TotalNSamples        | 7.8193e+06 |
| ExplainedVariance    | -0.015091  |
-------------------------------------
[2018-01-21 16:37:38.217608 UTC] Saving snapshot
[2018-01-21 16:37:38.217957 UTC] Starting iteration 1563
[2018-01-21 16:37:38.218210 UTC] Start collecting samples
[2018-01-21 16:37:42.863814 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:43.015887 UTC] Performing policy update
[2018-01-21 16:37:43.016528 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:43.135145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:44.605642 UTC] Performing line search
[2018-01-21 16:37:44.803017 UTC] Updating baseline
[2018-01-21 16:37:47.049816 UTC] Computing logging information
-------------------------------------
| Iteration            | 1563       |
| ExpectedImprovement  | 0.018434   |
| ActualImprovement    | 0.016769   |
| ImprovementRatio     | 0.90971    |
| MeanKL               | 0.007883   |
| Entropy              | -1.8023    |
| Perplexity           | 0.16492    |
| AveragePolicyStd     | 0.18129    |
| AveragePolicyStd[0]  | 0.20566    |
| AveragePolicyStd[1]  | 0.19038    |
| AveragePolicyStd[2]  | 0.14571    |
| AveragePolicyStd[3]  | 0.17294    |
| AveragePolicyStd[4]  | 0.15127    |
| AveragePolicyStd[5]  | 0.22181    |
| AverageReturn        | 1850.6     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 259.43     |
| AverageEpisodeLength | 961.7      |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.19     |
| TotalNEpisodes       | 24849      |
| TotalNSamples        | 7.8238e+06 |
| ExplainedVariance    | 0.1391     |
-------------------------------------
[2018-01-21 16:37:47.859148 UTC] Saving snapshot
[2018-01-21 16:37:47.859388 UTC] Starting iteration 1564
[2018-01-21 16:37:47.859552 UTC] Start collecting samples
[2018-01-21 16:37:52.382977 UTC] Computing input variables for policy optimization
[2018-01-21 16:37:52.530154 UTC] Performing policy update
[2018-01-21 16:37:52.531257 UTC] Computing gradient in Euclidean space
[2018-01-21 16:37:52.659859 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:37:54.100863 UTC] Performing line search
[2018-01-21 16:37:54.298881 UTC] Updating baseline
[2018-01-21 16:37:56.291100 UTC] Computing logging information
-------------------------------------
| Iteration            | 1564       |
| ExpectedImprovement  | 0.017911   |
| ActualImprovement    | 0.017355   |
| ImprovementRatio     | 0.96894    |
| MeanKL               | 0.0077587  |
| Entropy              | -1.8093    |
| Perplexity           | 0.16377    |
| AveragePolicyStd     | 0.18108    |
| AveragePolicyStd[0]  | 0.20535    |
| AveragePolicyStd[1]  | 0.19007    |
| AveragePolicyStd[2]  | 0.14564    |
| AveragePolicyStd[3]  | 0.17311    |
| AveragePolicyStd[4]  | 0.1508     |
| AveragePolicyStd[5]  | 0.2215     |
| AverageReturn        | 1858.7     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 245.18     |
| AverageEpisodeLength | 965.88     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.33     |
| TotalNEpisodes       | 24856      |
| TotalNSamples        | 7.8308e+06 |
| ExplainedVariance    | 0.0156     |
-------------------------------------
[2018-01-21 16:37:57.198146 UTC] Saving snapshot
[2018-01-21 16:37:57.198386 UTC] Starting iteration 1565
[2018-01-21 16:37:57.198583 UTC] Start collecting samples
[2018-01-21 16:38:01.597214 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:01.768010 UTC] Performing policy update
[2018-01-21 16:38:01.768684 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:01.897596 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:03.331110 UTC] Performing line search
[2018-01-21 16:38:03.538647 UTC] Updating baseline
[2018-01-21 16:38:06.557265 UTC] Computing logging information
-------------------------------------
| Iteration            | 1565       |
| ExpectedImprovement  | 0.020343   |
| ActualImprovement    | 0.018413   |
| ImprovementRatio     | 0.90514    |
| MeanKL               | 0.007483   |
| Entropy              | -1.8117    |
| Perplexity           | 0.16338    |
| AveragePolicyStd     | 0.18102    |
| AveragePolicyStd[0]  | 0.20511    |
| AveragePolicyStd[1]  | 0.19016    |
| AveragePolicyStd[2]  | 0.14528    |
| AveragePolicyStd[3]  | 0.17333    |
| AveragePolicyStd[4]  | 0.15072    |
| AveragePolicyStd[5]  | 0.22152    |
| AverageReturn        | 1849.6     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 263.65     |
| AverageEpisodeLength | 961.03     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.36     |
| TotalNEpisodes       | 24860      |
| TotalNSamples        | 7.8343e+06 |
| ExplainedVariance    | 0.22937    |
-------------------------------------
[2018-01-21 16:38:07.403785 UTC] Saving snapshot
[2018-01-21 16:38:07.404053 UTC] Starting iteration 1566
[2018-01-21 16:38:07.404240 UTC] Start collecting samples
[2018-01-21 16:38:11.916278 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:12.069298 UTC] Performing policy update
[2018-01-21 16:38:12.069925 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:12.190262 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:13.597925 UTC] Performing line search
[2018-01-21 16:38:13.790764 UTC] Updating baseline
[2018-01-21 16:38:15.884196 UTC] Computing logging information
-------------------------------------
| Iteration            | 1566       |
| ExpectedImprovement  | 0.019277   |
| ActualImprovement    | 0.017725   |
| ImprovementRatio     | 0.91948    |
| MeanKL               | 0.0080181  |
| Entropy              | -1.8163    |
| Perplexity           | 0.16263    |
| AveragePolicyStd     | 0.18086    |
| AveragePolicyStd[0]  | 0.20463    |
| AveragePolicyStd[1]  | 0.18981    |
| AveragePolicyStd[2]  | 0.14527    |
| AveragePolicyStd[3]  | 0.17335    |
| AveragePolicyStd[4]  | 0.15083    |
| AveragePolicyStd[5]  | 0.22123    |
| AverageReturn        | 1849.4     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 263.66     |
| AverageEpisodeLength | 961.03     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.36     |
| TotalNEpisodes       | 24864      |
| TotalNSamples        | 7.8383e+06 |
| ExplainedVariance    | -0.0035858 |
-------------------------------------
[2018-01-21 16:38:16.724124 UTC] Saving snapshot
[2018-01-21 16:38:16.724375 UTC] Starting iteration 1567
[2018-01-21 16:38:16.724534 UTC] Start collecting samples
[2018-01-21 16:38:21.179277 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:21.323658 UTC] Performing policy update
[2018-01-21 16:38:21.324269 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:21.445037 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:22.874107 UTC] Performing line search
[2018-01-21 16:38:23.075576 UTC] Updating baseline
[2018-01-21 16:38:25.285505 UTC] Computing logging information
-------------------------------------
| Iteration            | 1567       |
| ExpectedImprovement  | 0.016132   |
| ActualImprovement    | 0.015441   |
| ImprovementRatio     | 0.95716    |
| MeanKL               | 0.0078417  |
| Entropy              | -1.8188    |
| Perplexity           | 0.16223    |
| AveragePolicyStd     | 0.18078    |
| AveragePolicyStd[0]  | 0.20396    |
| AveragePolicyStd[1]  | 0.19008    |
| AveragePolicyStd[2]  | 0.14522    |
| AveragePolicyStd[3]  | 0.17349    |
| AveragePolicyStd[4]  | 0.15064    |
| AveragePolicyStd[5]  | 0.22129    |
| AverageReturn        | 1850       |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 263.91     |
| AverageEpisodeLength | 961.03     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.36     |
| TotalNEpisodes       | 24870      |
| TotalNSamples        | 7.8443e+06 |
| ExplainedVariance    | -0.001868  |
-------------------------------------
[2018-01-21 16:38:26.128223 UTC] Saving snapshot
[2018-01-21 16:38:26.128482 UTC] Starting iteration 1568
[2018-01-21 16:38:26.128641 UTC] Start collecting samples
[2018-01-21 16:38:30.523429 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:30.649845 UTC] Performing policy update
[2018-01-21 16:38:30.650935 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:30.777016 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:32.189934 UTC] Performing line search
[2018-01-21 16:38:32.376401 UTC] Updating baseline
[2018-01-21 16:38:34.325165 UTC] Computing logging information
-------------------------------------
| Iteration            | 1568       |
| ExpectedImprovement  | 0.020248   |
| ActualImprovement    | 0.019151   |
| ImprovementRatio     | 0.94582    |
| MeanKL               | 0.0083303  |
| Entropy              | -1.8094    |
| Perplexity           | 0.16376    |
| AveragePolicyStd     | 0.18106    |
| AveragePolicyStd[0]  | 0.20442    |
| AveragePolicyStd[1]  | 0.19087    |
| AveragePolicyStd[2]  | 0.14536    |
| AveragePolicyStd[3]  | 0.17399    |
| AveragePolicyStd[4]  | 0.15065    |
| AveragePolicyStd[5]  | 0.22109    |
| AverageReturn        | 1853       |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 255.74     |
| AverageEpisodeLength | 963.11     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 124.5      |
| TotalNEpisodes       | 24876      |
| TotalNSamples        | 7.8501e+06 |
| ExplainedVariance    | 0.093778   |
-------------------------------------
[2018-01-21 16:38:35.152927 UTC] Saving snapshot
[2018-01-21 16:38:35.153162 UTC] Starting iteration 1569
[2018-01-21 16:38:35.153311 UTC] Start collecting samples
[2018-01-21 16:38:39.559091 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:39.721100 UTC] Performing policy update
[2018-01-21 16:38:39.721755 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:39.850426 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:41.286559 UTC] Performing line search
[2018-01-21 16:38:41.478443 UTC] Updating baseline
[2018-01-21 16:38:43.563451 UTC] Computing logging information
-------------------------------------
| Iteration            | 1569       |
| ExpectedImprovement  | 0.016237   |
| ActualImprovement    | 0.01539    |
| ImprovementRatio     | 0.94782    |
| MeanKL               | 0.0082955  |
| Entropy              | -1.8125    |
| Perplexity           | 0.16325    |
| AveragePolicyStd     | 0.18096    |
| AveragePolicyStd[0]  | 0.20431    |
| AveragePolicyStd[1]  | 0.1909     |
| AveragePolicyStd[2]  | 0.14535    |
| AveragePolicyStd[3]  | 0.17385    |
| AveragePolicyStd[4]  | 0.15062    |
| AveragePolicyStd[5]  | 0.22073    |
| AverageReturn        | 1841.9     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 273.25     |
| AverageEpisodeLength | 958.02     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133        |
| TotalNEpisodes       | 24881      |
| TotalNSamples        | 7.8546e+06 |
| ExplainedVariance    | 0.10508    |
-------------------------------------
[2018-01-21 16:38:44.437904 UTC] Saving snapshot
[2018-01-21 16:38:44.438227 UTC] Starting iteration 1570
[2018-01-21 16:38:44.438461 UTC] Start collecting samples
[2018-01-21 16:38:49.082590 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:49.203065 UTC] Performing policy update
[2018-01-21 16:38:49.204022 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:49.329450 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:38:50.757629 UTC] Performing line search
[2018-01-21 16:38:50.948295 UTC] Updating baseline
[2018-01-21 16:38:52.975420 UTC] Computing logging information
-------------------------------------
| Iteration            | 1570       |
| ExpectedImprovement  | 0.015237   |
| ActualImprovement    | 0.014675   |
| ImprovementRatio     | 0.96309    |
| MeanKL               | 0.0082517  |
| Entropy              | -1.8069    |
| Perplexity           | 0.16417    |
| AveragePolicyStd     | 0.18112    |
| AveragePolicyStd[0]  | 0.20418    |
| AveragePolicyStd[1]  | 0.19111    |
| AveragePolicyStd[2]  | 0.14531    |
| AveragePolicyStd[3]  | 0.17456    |
| AveragePolicyStd[4]  | 0.15086    |
| AveragePolicyStd[5]  | 0.22068    |
| AverageReturn        | 1842.6     |
| MinReturn            | 697.66     |
| MaxReturn            | 1985.6     |
| StdReturn            | 273.53     |
| AverageEpisodeLength | 958.02     |
| MinEpisodeLength     | 383        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 133        |
| TotalNEpisodes       | 24885      |
| TotalNSamples        | 7.8586e+06 |
| ExplainedVariance    | 0.0057884  |
-------------------------------------
[2018-01-21 16:38:53.778451 UTC] Saving snapshot
[2018-01-21 16:38:53.786344 UTC] Starting iteration 1571
[2018-01-21 16:38:53.786551 UTC] Start collecting samples
[2018-01-21 16:38:58.378306 UTC] Computing input variables for policy optimization
[2018-01-21 16:38:58.582215 UTC] Performing policy update
[2018-01-21 16:38:58.586512 UTC] Computing gradient in Euclidean space
[2018-01-21 16:38:58.709916 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:00.124514 UTC] Performing line search
[2018-01-21 16:39:00.317706 UTC] Updating baseline
[2018-01-21 16:39:02.230051 UTC] Computing logging information
-------------------------------------
| Iteration            | 1571       |
| ExpectedImprovement  | 0.017963   |
| ActualImprovement    | 0.017463   |
| ImprovementRatio     | 0.97217    |
| MeanKL               | 0.0080888  |
| Entropy              | -1.8059    |
| Perplexity           | 0.16432    |
| AveragePolicyStd     | 0.18112    |
| AveragePolicyStd[0]  | 0.20373    |
| AveragePolicyStd[1]  | 0.19134    |
| AveragePolicyStd[2]  | 0.14548    |
| AveragePolicyStd[3]  | 0.17505    |
| AveragePolicyStd[4]  | 0.15076    |
| AveragePolicyStd[5]  | 0.22036    |
| AverageReturn        | 1841.9     |
| MinReturn            | 644.64     |
| MaxReturn            | 1985.6     |
| StdReturn            | 279.09     |
| AverageEpisodeLength | 957.51     |
| MinEpisodeLength     | 362        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.75     |
| TotalNEpisodes       | 24894      |
| TotalNSamples        | 7.8666e+06 |
| ExplainedVariance    | 0.1395     |
-------------------------------------
[2018-01-21 16:39:03.063051 UTC] Saving snapshot
[2018-01-21 16:39:03.063295 UTC] Starting iteration 1572
[2018-01-21 16:39:03.063448 UTC] Start collecting samples
[2018-01-21 16:39:07.939270 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:08.090939 UTC] Performing policy update
[2018-01-21 16:39:08.091548 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:08.206371 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:09.639419 UTC] Performing line search
[2018-01-21 16:39:09.834333 UTC] Updating baseline
[2018-01-21 16:39:11.861655 UTC] Computing logging information
-------------------------------------
| Iteration            | 1572       |
| ExpectedImprovement  | 0.017884   |
| ActualImprovement    | 0.01723    |
| ImprovementRatio     | 0.96338    |
| MeanKL               | 0.0086164  |
| Entropy              | -1.8042    |
| Perplexity           | 0.16461    |
| AveragePolicyStd     | 0.18119    |
| AveragePolicyStd[0]  | 0.20369    |
| AveragePolicyStd[1]  | 0.19127    |
| AveragePolicyStd[2]  | 0.1455     |
| AveragePolicyStd[3]  | 0.17536    |
| AveragePolicyStd[4]  | 0.15055    |
| AveragePolicyStd[5]  | 0.22077    |
| AverageReturn        | 1833.2     |
| MinReturn            | 644.64     |
| MaxReturn            | 1985.6     |
| StdReturn            | 288.23     |
| AverageEpisodeLength | 953.32     |
| MinEpisodeLength     | 362        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.75     |
| TotalNEpisodes       | 24898      |
| TotalNSamples        | 7.8702e+06 |
| ExplainedVariance    | 0.20812    |
-------------------------------------
[2018-01-21 16:39:12.676475 UTC] Saving snapshot
[2018-01-21 16:39:12.676988 UTC] Starting iteration 1573
[2018-01-21 16:39:12.677397 UTC] Start collecting samples
[2018-01-21 16:39:17.037577 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:17.164210 UTC] Performing policy update
[2018-01-21 16:39:17.164833 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:17.282891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:18.757263 UTC] Performing line search
[2018-01-21 16:39:18.968784 UTC] Updating baseline
[2018-01-21 16:39:21.163738 UTC] Computing logging information
-------------------------------------
| Iteration            | 1573       |
| ExpectedImprovement  | 0.018901   |
| ActualImprovement    | 0.017996   |
| ImprovementRatio     | 0.95215    |
| MeanKL               | 0.0074885  |
| Entropy              | -1.8028    |
| Perplexity           | 0.16484    |
| AveragePolicyStd     | 0.18125    |
| AveragePolicyStd[0]  | 0.20404    |
| AveragePolicyStd[1]  | 0.19129    |
| AveragePolicyStd[2]  | 0.14528    |
| AveragePolicyStd[3]  | 0.17518    |
| AveragePolicyStd[4]  | 0.15076    |
| AveragePolicyStd[5]  | 0.22093    |
| AverageReturn        | 1814.9     |
| MinReturn            | 644.64     |
| MaxReturn            | 1983.7     |
| StdReturn            | 307.13     |
| AverageEpisodeLength | 945.11     |
| MinEpisodeLength     | 362        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.15     |
| TotalNEpisodes       | 24901      |
| TotalNSamples        | 7.8724e+06 |
| ExplainedVariance    | 0.48798    |
-------------------------------------
[2018-01-21 16:39:22.065809 UTC] Saving snapshot
[2018-01-21 16:39:22.066044 UTC] Starting iteration 1574
[2018-01-21 16:39:22.066192 UTC] Start collecting samples
[2018-01-21 16:39:26.626873 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:26.764011 UTC] Performing policy update
[2018-01-21 16:39:26.764667 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:26.884725 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:28.336398 UTC] Performing line search
[2018-01-21 16:39:28.522046 UTC] Updating baseline
[2018-01-21 16:39:30.715832 UTC] Computing logging information
-------------------------------------
| Iteration            | 1574       |
| ExpectedImprovement  | 0.018779   |
| ActualImprovement    | 0.017726   |
| ImprovementRatio     | 0.94394    |
| MeanKL               | 0.0090767  |
| Entropy              | -1.8065    |
| Perplexity           | 0.16423    |
| AveragePolicyStd     | 0.18112    |
| AveragePolicyStd[0]  | 0.20392    |
| AveragePolicyStd[1]  | 0.191      |
| AveragePolicyStd[2]  | 0.14546    |
| AveragePolicyStd[3]  | 0.17491    |
| AveragePolicyStd[4]  | 0.1507     |
| AveragePolicyStd[5]  | 0.22072    |
| AverageReturn        | 1813.4     |
| MinReturn            | 644.64     |
| MaxReturn            | 1983.7     |
| StdReturn            | 306.56     |
| AverageEpisodeLength | 945.11     |
| MinEpisodeLength     | 362        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.15     |
| TotalNEpisodes       | 24910      |
| TotalNSamples        | 7.8814e+06 |
| ExplainedVariance    | -0.027373  |
-------------------------------------
[2018-01-21 16:39:31.552589 UTC] Saving snapshot
[2018-01-21 16:39:31.553094 UTC] Starting iteration 1575
[2018-01-21 16:39:31.553483 UTC] Start collecting samples
[2018-01-21 16:39:35.993183 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:36.128818 UTC] Performing policy update
[2018-01-21 16:39:36.129471 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:36.256835 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:37.686179 UTC] Performing line search
[2018-01-21 16:39:37.893640 UTC] Updating baseline
[2018-01-21 16:39:39.538032 UTC] Computing logging information
-------------------------------------
| Iteration            | 1575       |
| ExpectedImprovement  | 0.020111   |
| ActualImprovement    | 0.018629   |
| ImprovementRatio     | 0.92629    |
| MeanKL               | 0.0072423  |
| Entropy              | -1.8046    |
| Perplexity           | 0.16454    |
| AveragePolicyStd     | 0.18117    |
| AveragePolicyStd[0]  | 0.20444    |
| AveragePolicyStd[1]  | 0.19137    |
| AveragePolicyStd[2]  | 0.14568    |
| AveragePolicyStd[3]  | 0.17476    |
| AveragePolicyStd[4]  | 0.15063    |
| AveragePolicyStd[5]  | 0.22014    |
| AverageReturn        | 1783.9     |
| MinReturn            | 576.09     |
| MaxReturn            | 1983.7     |
| StdReturn            | 347.35     |
| AverageEpisodeLength | 930.59     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.16     |
| TotalNEpisodes       | 24915      |
| TotalNSamples        | 7.8849e+06 |
| ExplainedVariance    | 0.30893    |
-------------------------------------
[2018-01-21 16:39:40.344223 UTC] Saving snapshot
[2018-01-21 16:39:40.344500 UTC] Starting iteration 1576
[2018-01-21 16:39:40.344692 UTC] Start collecting samples
[2018-01-21 16:39:44.768011 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:44.927664 UTC] Performing policy update
[2018-01-21 16:39:44.928281 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:45.052924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:46.524986 UTC] Performing line search
[2018-01-21 16:39:46.717982 UTC] Updating baseline
[2018-01-21 16:39:48.520258 UTC] Computing logging information
-------------------------------------
| Iteration            | 1576       |
| ExpectedImprovement  | 0.017078   |
| ActualImprovement    | 0.016273   |
| ImprovementRatio     | 0.95287    |
| MeanKL               | 0.0081709  |
| Entropy              | -1.8054    |
| Perplexity           | 0.16441    |
| AveragePolicyStd     | 0.1811     |
| AveragePolicyStd[0]  | 0.2045     |
| AveragePolicyStd[1]  | 0.19127    |
| AveragePolicyStd[2]  | 0.14581    |
| AveragePolicyStd[3]  | 0.175      |
| AveragePolicyStd[4]  | 0.15083    |
| AveragePolicyStd[5]  | 0.21922    |
| AverageReturn        | 1784.3     |
| MinReturn            | 576.09     |
| MaxReturn            | 1983.7     |
| StdReturn            | 347.57     |
| AverageEpisodeLength | 930.59     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.16     |
| TotalNEpisodes       | 24919      |
| TotalNSamples        | 7.8889e+06 |
| ExplainedVariance    | -0.001411  |
-------------------------------------
[2018-01-21 16:39:49.332643 UTC] Saving snapshot
[2018-01-21 16:39:49.332879 UTC] Starting iteration 1577
[2018-01-21 16:39:49.333028 UTC] Start collecting samples
[2018-01-21 16:39:54.001328 UTC] Computing input variables for policy optimization
[2018-01-21 16:39:54.125024 UTC] Performing policy update
[2018-01-21 16:39:54.125650 UTC] Computing gradient in Euclidean space
[2018-01-21 16:39:54.246880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:39:55.624916 UTC] Performing line search
[2018-01-21 16:39:55.812711 UTC] Updating baseline
[2018-01-21 16:39:57.571591 UTC] Computing logging information
-------------------------------------
| Iteration            | 1577       |
| ExpectedImprovement  | 0.01793    |
| ActualImprovement    | 0.017143   |
| ImprovementRatio     | 0.9561     |
| MeanKL               | 0.0080201  |
| Entropy              | -1.8113    |
| Perplexity           | 0.16344    |
| AveragePolicyStd     | 0.18093    |
| AveragePolicyStd[0]  | 0.20457    |
| AveragePolicyStd[1]  | 0.19066    |
| AveragePolicyStd[2]  | 0.14594    |
| AveragePolicyStd[3]  | 0.17426    |
| AveragePolicyStd[4]  | 0.15073    |
| AveragePolicyStd[5]  | 0.21941    |
| AverageReturn        | 1804.5     |
| MinReturn            | 576.09     |
| MaxReturn            | 1983.7     |
| StdReturn            | 320.89     |
| AverageEpisodeLength | 940.73     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.66     |
| TotalNEpisodes       | 24926      |
| TotalNSamples        | 7.8959e+06 |
| ExplainedVariance    | 0.056599   |
-------------------------------------
[2018-01-21 16:39:58.491817 UTC] Saving snapshot
[2018-01-21 16:39:58.492133 UTC] Starting iteration 1578
[2018-01-21 16:39:58.492367 UTC] Start collecting samples
[2018-01-21 16:40:04.095126 UTC] Computing input variables for policy optimization
[2018-01-21 16:40:04.236757 UTC] Performing policy update
[2018-01-21 16:40:04.237979 UTC] Computing gradient in Euclidean space
[2018-01-21 16:40:04.362110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:40:05.979631 UTC] Performing line search
[2018-01-21 16:40:06.190634 UTC] Updating baseline
[2018-01-21 16:40:08.584570 UTC] Computing logging information
-------------------------------------
| Iteration            | 1578       |
| ExpectedImprovement  | 0.017827   |
| ActualImprovement    | 0.016796   |
| ImprovementRatio     | 0.94217    |
| MeanKL               | 0.0074789  |
| Entropy              | -1.8101    |
| Perplexity           | 0.16364    |
| AveragePolicyStd     | 0.18097    |
| AveragePolicyStd[0]  | 0.20489    |
| AveragePolicyStd[1]  | 0.19074    |
| AveragePolicyStd[2]  | 0.1458     |
| AveragePolicyStd[3]  | 0.1744     |
| AveragePolicyStd[4]  | 0.15077    |
| AveragePolicyStd[5]  | 0.21921    |
| AverageReturn        | 1804.8     |
| MinReturn            | 576.09     |
| MaxReturn            | 1983.7     |
| StdReturn            | 320.95     |
| AverageEpisodeLength | 940.73     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.66     |
| TotalNEpisodes       | 24930      |
| TotalNSamples        | 7.8999e+06 |
| ExplainedVariance    | -0.02944   |
-------------------------------------
[2018-01-21 16:40:09.494149 UTC] Saving snapshot
[2018-01-21 16:40:09.494387 UTC] Starting iteration 1579
[2018-01-21 16:40:09.494567 UTC] Start collecting samples
[2018-01-21 16:40:14.621853 UTC] Computing input variables for policy optimization
[2018-01-21 16:40:14.765219 UTC] Performing policy update
[2018-01-21 16:40:14.765819 UTC] Computing gradient in Euclidean space
[2018-01-21 16:40:14.899570 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:40:16.463997 UTC] Performing line search
[2018-01-21 16:40:16.731683 UTC] Updating baseline
[2018-01-21 16:40:18.900171 UTC] Computing logging information
-------------------------------------
| Iteration            | 1579       |
| ExpectedImprovement  | 0.01609    |
| ActualImprovement    | 0.014695   |
| ImprovementRatio     | 0.91334    |
| MeanKL               | 0.0082816  |
| Entropy              | -1.8133    |
| Perplexity           | 0.16312    |
| AveragePolicyStd     | 0.18091    |
| AveragePolicyStd[0]  | 0.20495    |
| AveragePolicyStd[1]  | 0.19077    |
| AveragePolicyStd[2]  | 0.14555    |
| AveragePolicyStd[3]  | 0.17418    |
| AveragePolicyStd[4]  | 0.1504     |
| AveragePolicyStd[5]  | 0.21962    |
| AverageReturn        | 1798       |
| MinReturn            | 576.09     |
| MaxReturn            | 1983.7     |
| StdReturn            | 334.84     |
| AverageEpisodeLength | 937.47     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.7      |
| TotalNEpisodes       | 24936      |
| TotalNSamples        | 7.9054e+06 |
| ExplainedVariance    | 0.12091    |
-------------------------------------
[2018-01-21 16:40:19.732763 UTC] Saving snapshot
[2018-01-21 16:40:19.733046 UTC] Starting iteration 1580
[2018-01-21 16:40:19.733234 UTC] Start collecting samples
[2018-01-21 16:40:26.361379 UTC] Computing input variables for policy optimization
[2018-01-21 16:40:26.519275 UTC] Performing policy update
[2018-01-21 16:40:26.519999 UTC] Computing gradient in Euclidean space
[2018-01-21 16:40:26.660980 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:40:28.478413 UTC] Performing line search
[2018-01-21 16:40:28.705533 UTC] Updating baseline
[2018-01-21 16:40:32.634575 UTC] Computing logging information
-------------------------------------
| Iteration            | 1580       |
| ExpectedImprovement  | 0.01735    |
| ActualImprovement    | 0.016471   |
| ImprovementRatio     | 0.94933    |
| MeanKL               | 0.0074269  |
| Entropy              | -1.8225    |
| Perplexity           | 0.16162    |
| AveragePolicyStd     | 0.18065    |
| AveragePolicyStd[0]  | 0.20447    |
| AveragePolicyStd[1]  | 0.19053    |
| AveragePolicyStd[2]  | 0.14522    |
| AveragePolicyStd[3]  | 0.17362    |
| AveragePolicyStd[4]  | 0.15024    |
| AveragePolicyStd[5]  | 0.21981    |
| AverageReturn        | 1798.6     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 335.09     |
| AverageEpisodeLength | 937.47     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.7      |
| TotalNEpisodes       | 24940      |
| TotalNSamples        | 7.9094e+06 |
| ExplainedVariance    | 0.0045238  |
-------------------------------------
[2018-01-21 16:40:33.725710 UTC] Saving snapshot
[2018-01-21 16:40:33.739916 UTC] Starting iteration 1581
[2018-01-21 16:40:33.740260 UTC] Start collecting samples
[2018-01-21 16:40:42.258320 UTC] Computing input variables for policy optimization
[2018-01-21 16:40:42.562388 UTC] Performing policy update
[2018-01-21 16:40:42.569186 UTC] Computing gradient in Euclidean space
[2018-01-21 16:40:42.783530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:40:44.924632 UTC] Performing line search
[2018-01-21 16:40:45.212363 UTC] Updating baseline
[2018-01-21 16:40:47.789941 UTC] Computing logging information
-------------------------------------
| Iteration            | 1581       |
| ExpectedImprovement  | 0.017525   |
| ActualImprovement    | 0.016255   |
| ImprovementRatio     | 0.92752    |
| MeanKL               | 0.0077895  |
| Entropy              | -1.8244    |
| Perplexity           | 0.16132    |
| AveragePolicyStd     | 0.18058    |
| AveragePolicyStd[0]  | 0.20489    |
| AveragePolicyStd[1]  | 0.19045    |
| AveragePolicyStd[2]  | 0.14504    |
| AveragePolicyStd[3]  | 0.17358    |
| AveragePolicyStd[4]  | 0.15046    |
| AveragePolicyStd[5]  | 0.21905    |
| AverageReturn        | 1798.2     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 332.14     |
| AverageEpisodeLength | 938.18     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.8      |
| TotalNEpisodes       | 24946      |
| TotalNSamples        | 7.9151e+06 |
| ExplainedVariance    | 0.095553   |
-------------------------------------
[2018-01-21 16:40:48.798706 UTC] Saving snapshot
[2018-01-21 16:40:48.799000 UTC] Starting iteration 1582
[2018-01-21 16:40:48.799182 UTC] Start collecting samples
[2018-01-21 16:40:57.206159 UTC] Computing input variables for policy optimization
[2018-01-21 16:40:57.400894 UTC] Performing policy update
[2018-01-21 16:40:57.401654 UTC] Computing gradient in Euclidean space
[2018-01-21 16:40:57.648919 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:40:59.577150 UTC] Performing line search
[2018-01-21 16:40:59.911563 UTC] Updating baseline
[2018-01-21 16:41:03.233196 UTC] Computing logging information
-------------------------------------
| Iteration            | 1582       |
| ExpectedImprovement  | 0.016251   |
| ActualImprovement    | 0.016265   |
| ImprovementRatio     | 1.0009     |
| MeanKL               | 0.007525   |
| Entropy              | -1.8215    |
| Perplexity           | 0.16178    |
| AveragePolicyStd     | 0.18065    |
| AveragePolicyStd[0]  | 0.20492    |
| AveragePolicyStd[1]  | 0.1908     |
| AveragePolicyStd[2]  | 0.14503    |
| AveragePolicyStd[3]  | 0.17387    |
| AveragePolicyStd[4]  | 0.15057    |
| AveragePolicyStd[5]  | 0.21872    |
| AverageReturn        | 1798.2     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 339.78     |
| AverageEpisodeLength | 936.97     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.3      |
| TotalNEpisodes       | 24953      |
| TotalNSamples        | 7.9215e+06 |
| ExplainedVariance    | 0.04757    |
-------------------------------------
[2018-01-21 16:41:04.318697 UTC] Saving snapshot
[2018-01-21 16:41:04.318940 UTC] Starting iteration 1583
[2018-01-21 16:41:04.319084 UTC] Start collecting samples
[2018-01-21 16:41:11.811035 UTC] Computing input variables for policy optimization
[2018-01-21 16:41:11.947349 UTC] Performing policy update
[2018-01-21 16:41:11.948030 UTC] Computing gradient in Euclidean space
[2018-01-21 16:41:12.071661 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:41:13.625452 UTC] Performing line search
[2018-01-21 16:41:13.900072 UTC] Updating baseline
[2018-01-21 16:41:16.884370 UTC] Computing logging information
-------------------------------------
| Iteration            | 1583       |
| ExpectedImprovement  | 0.017314   |
| ActualImprovement    | 0.015549   |
| ImprovementRatio     | 0.89808    |
| MeanKL               | 0.0083514  |
| Entropy              | -1.8185    |
| Perplexity           | 0.16226    |
| AveragePolicyStd     | 0.18075    |
| AveragePolicyStd[0]  | 0.20589    |
| AveragePolicyStd[1]  | 0.19032    |
| AveragePolicyStd[2]  | 0.14532    |
| AveragePolicyStd[3]  | 0.17439    |
| AveragePolicyStd[4]  | 0.15012    |
| AveragePolicyStd[5]  | 0.21846    |
| AverageReturn        | 1798.8     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 340.03     |
| AverageEpisodeLength | 936.97     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.3      |
| TotalNEpisodes       | 24955      |
| TotalNSamples        | 7.9235e+06 |
| ExplainedVariance    | -0.004887  |
-------------------------------------
[2018-01-21 16:41:17.990560 UTC] Saving snapshot
[2018-01-21 16:41:17.991101 UTC] Starting iteration 1584
[2018-01-21 16:41:17.991523 UTC] Start collecting samples
[2018-01-21 16:41:26.681504 UTC] Computing input variables for policy optimization
[2018-01-21 16:41:26.808899 UTC] Performing policy update
[2018-01-21 16:41:26.809580 UTC] Computing gradient in Euclidean space
[2018-01-21 16:41:26.934193 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:41:28.453603 UTC] Performing line search
[2018-01-21 16:41:28.703583 UTC] Updating baseline
[2018-01-21 16:41:32.289689 UTC] Computing logging information
-------------------------------------
| Iteration            | 1584       |
| ExpectedImprovement  | 0.017225   |
| ActualImprovement    | 0.016524   |
| ImprovementRatio     | 0.95932    |
| MeanKL               | 0.0081627  |
| Entropy              | -1.8255    |
| Perplexity           | 0.16113    |
| AveragePolicyStd     | 0.18055    |
| AveragePolicyStd[0]  | 0.20616    |
| AveragePolicyStd[1]  | 0.19022    |
| AveragePolicyStd[2]  | 0.14508    |
| AveragePolicyStd[3]  | 0.17397    |
| AveragePolicyStd[4]  | 0.14988    |
| AveragePolicyStd[5]  | 0.21801    |
| AverageReturn        | 1810.6     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 328.33     |
| AverageEpisodeLength | 941.82     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.87     |
| TotalNEpisodes       | 24960      |
| TotalNSamples        | 7.9285e+06 |
| ExplainedVariance    | 0.0061165  |
-------------------------------------
[2018-01-21 16:41:33.240592 UTC] Saving snapshot
[2018-01-21 16:41:33.240825 UTC] Starting iteration 1585
[2018-01-21 16:41:33.240994 UTC] Start collecting samples
[2018-01-21 16:41:39.082026 UTC] Computing input variables for policy optimization
[2018-01-21 16:41:39.220516 UTC] Performing policy update
[2018-01-21 16:41:39.221247 UTC] Computing gradient in Euclidean space
[2018-01-21 16:41:39.347746 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:41:40.846522 UTC] Performing line search
[2018-01-21 16:41:41.042382 UTC] Updating baseline
[2018-01-21 16:41:42.949567 UTC] Computing logging information
-------------------------------------
| Iteration            | 1585       |
| ExpectedImprovement  | 0.017849   |
| ActualImprovement    | 0.017085   |
| ImprovementRatio     | 0.95719    |
| MeanKL               | 0.0085069  |
| Entropy              | -1.8268    |
| Perplexity           | 0.16093    |
| AveragePolicyStd     | 0.18052    |
| AveragePolicyStd[0]  | 0.20667    |
| AveragePolicyStd[1]  | 0.19044    |
| AveragePolicyStd[2]  | 0.14519    |
| AveragePolicyStd[3]  | 0.17343    |
| AveragePolicyStd[4]  | 0.1497     |
| AveragePolicyStd[5]  | 0.2177     |
| AverageReturn        | 1799.4     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 343.14     |
| AverageEpisodeLength | 935.77     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.23     |
| TotalNEpisodes       | 24970      |
| TotalNSamples        | 7.9379e+06 |
| ExplainedVariance    | 0.13995    |
-------------------------------------
[2018-01-21 16:41:43.854831 UTC] Saving snapshot
[2018-01-21 16:41:43.855128 UTC] Starting iteration 1586
[2018-01-21 16:41:43.855321 UTC] Start collecting samples
[2018-01-21 16:41:49.371571 UTC] Computing input variables for policy optimization
[2018-01-21 16:41:49.508651 UTC] Performing policy update
[2018-01-21 16:41:49.509844 UTC] Computing gradient in Euclidean space
[2018-01-21 16:41:49.640465 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:41:51.116008 UTC] Performing line search
[2018-01-21 16:41:51.326287 UTC] Updating baseline
[2018-01-21 16:41:53.308075 UTC] Computing logging information
-------------------------------------
| Iteration            | 1586       |
| ExpectedImprovement  | 0.020713   |
| ActualImprovement    | 0.018631   |
| ImprovementRatio     | 0.8995     |
| MeanKL               | 0.007523   |
| Entropy              | -1.8219    |
| Perplexity           | 0.16172    |
| AveragePolicyStd     | 0.18069    |
| AveragePolicyStd[0]  | 0.20649    |
| AveragePolicyStd[1]  | 0.19057    |
| AveragePolicyStd[2]  | 0.14503    |
| AveragePolicyStd[3]  | 0.17364    |
| AveragePolicyStd[4]  | 0.14992    |
| AveragePolicyStd[5]  | 0.21848    |
| AverageReturn        | 1799.4     |
| MinReturn            | 576.09     |
| MaxReturn            | 1990.9     |
| StdReturn            | 343.14     |
| AverageEpisodeLength | 935.77     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.23     |
| TotalNEpisodes       | 24970      |
| TotalNSamples        | 7.9379e+06 |
| ExplainedVariance    | 0.26263    |
-------------------------------------
[2018-01-21 16:41:54.174590 UTC] Saving snapshot
[2018-01-21 16:41:54.174885 UTC] Starting iteration 1587
[2018-01-21 16:41:54.175099 UTC] Start collecting samples
[2018-01-21 16:42:01.186939 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:01.332463 UTC] Performing policy update
[2018-01-21 16:42:01.333640 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:01.459285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:42:02.928715 UTC] Performing line search
[2018-01-21 16:42:03.129018 UTC] Updating baseline
[2018-01-21 16:42:05.817340 UTC] Computing logging information
-------------------------------------
| Iteration            | 1587       |
| ExpectedImprovement  | 0.017117   |
| ActualImprovement    | 0.016852   |
| ImprovementRatio     | 0.98449    |
| MeanKL               | 0.0084493  |
| Entropy              | -1.8275    |
| Perplexity           | 0.16082    |
| AveragePolicyStd     | 0.1805     |
| AveragePolicyStd[0]  | 0.20615    |
| AveragePolicyStd[1]  | 0.19027    |
| AveragePolicyStd[2]  | 0.14488    |
| AveragePolicyStd[3]  | 0.17326    |
| AveragePolicyStd[4]  | 0.1502     |
| AveragePolicyStd[5]  | 0.21825    |
| AverageReturn        | 1792.6     |
| MinReturn            | 576.09     |
| MaxReturn            | 1995.3     |
| StdReturn            | 363.08     |
| AverageEpisodeLength | 930.91     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.98     |
| TotalNEpisodes       | 24976      |
| TotalNSamples        | 7.9432e+06 |
| ExplainedVariance    | 0.054424   |
-------------------------------------
[2018-01-21 16:42:06.687452 UTC] Saving snapshot
[2018-01-21 16:42:06.687644 UTC] Starting iteration 1588
[2018-01-21 16:42:06.687751 UTC] Start collecting samples
[2018-01-21 16:42:13.085041 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:13.237101 UTC] Performing policy update
[2018-01-21 16:42:13.237703 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:13.363092 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:42:14.960603 UTC] Performing line search
[2018-01-21 16:42:15.189245 UTC] Updating baseline
[2018-01-21 16:42:16.957433 UTC] Computing logging information
-------------------------------------
| Iteration            | 1588       |
| ExpectedImprovement  | 0.018261   |
| ActualImprovement    | 0.01697    |
| ImprovementRatio     | 0.9293     |
| MeanKL               | 0.0082363  |
| Entropy              | -1.8207    |
| Perplexity           | 0.16192    |
| AveragePolicyStd     | 0.18071    |
| AveragePolicyStd[0]  | 0.2063     |
| AveragePolicyStd[1]  | 0.19118    |
| AveragePolicyStd[2]  | 0.14464    |
| AveragePolicyStd[3]  | 0.17374    |
| AveragePolicyStd[4]  | 0.15045    |
| AveragePolicyStd[5]  | 0.21794    |
| AverageReturn        | 1805.7     |
| MinReturn            | 576.09     |
| MaxReturn            | 1995.9     |
| StdReturn            | 352.46     |
| AverageEpisodeLength | 936        |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.49     |
| TotalNEpisodes       | 24986      |
| TotalNSamples        | 7.9532e+06 |
| ExplainedVariance    | -0.004482  |
-------------------------------------
[2018-01-21 16:42:17.844812 UTC] Saving snapshot
[2018-01-21 16:42:17.845079 UTC] Starting iteration 1589
[2018-01-21 16:42:17.845256 UTC] Start collecting samples
[2018-01-21 16:42:25.380721 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:25.525272 UTC] Performing policy update
[2018-01-21 16:42:25.526077 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:25.643307 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:42:27.137547 UTC] Performing line search
[2018-01-21 16:42:27.323280 UTC] Updating baseline
[2018-01-21 16:42:31.221100 UTC] Computing logging information
-------------------------------------
| Iteration            | 1589       |
| ExpectedImprovement  | 0.019086   |
| ActualImprovement    | 0.018619   |
| ImprovementRatio     | 0.97553    |
| MeanKL               | 0.0079574  |
| Entropy              | -1.8217    |
| Perplexity           | 0.16176    |
| AveragePolicyStd     | 0.18069    |
| AveragePolicyStd[0]  | 0.20653    |
| AveragePolicyStd[1]  | 0.19079    |
| AveragePolicyStd[2]  | 0.14477    |
| AveragePolicyStd[3]  | 0.17362    |
| AveragePolicyStd[4]  | 0.15029    |
| AveragePolicyStd[5]  | 0.21811    |
| AverageReturn        | 1805.7     |
| MinReturn            | 576.09     |
| MaxReturn            | 1995.9     |
| StdReturn            | 352.46     |
| AverageEpisodeLength | 936        |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.49     |
| TotalNEpisodes       | 24986      |
| TotalNSamples        | 7.9532e+06 |
| ExplainedVariance    | 3.1509e-05 |
-------------------------------------
[2018-01-21 16:42:32.101979 UTC] Saving snapshot
[2018-01-21 16:42:32.102191 UTC] Starting iteration 1590
[2018-01-21 16:42:32.102356 UTC] Start collecting samples
[2018-01-21 16:42:37.512421 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:37.659968 UTC] Performing policy update
[2018-01-21 16:42:37.660629 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:37.782751 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:42:39.223866 UTC] Performing line search
[2018-01-21 16:42:39.415079 UTC] Updating baseline
[2018-01-21 16:42:41.910299 UTC] Computing logging information
-------------------------------------
| Iteration            | 1590       |
| ExpectedImprovement  | 0.018514   |
| ActualImprovement    | 0.017364   |
| ImprovementRatio     | 0.93786    |
| MeanKL               | 0.007877   |
| Entropy              | -1.8232    |
| Perplexity           | 0.1615     |
| AveragePolicyStd     | 0.18064    |
| AveragePolicyStd[0]  | 0.2063     |
| AveragePolicyStd[1]  | 0.19085    |
| AveragePolicyStd[2]  | 0.14477    |
| AveragePolicyStd[3]  | 0.17356    |
| AveragePolicyStd[4]  | 0.15016    |
| AveragePolicyStd[5]  | 0.21822    |
| AverageReturn        | 1814.5     |
| MinReturn            | 576.09     |
| MaxReturn            | 1995.9     |
| StdReturn            | 346.96     |
| AverageEpisodeLength | 939.8      |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.63     |
| TotalNEpisodes       | 24991      |
| TotalNSamples        | 7.9582e+06 |
| ExplainedVariance    | 0.0017637  |
-------------------------------------
[2018-01-21 16:42:42.816102 UTC] Saving snapshot
[2018-01-21 16:42:42.827436 UTC] Starting iteration 1591
[2018-01-21 16:42:42.827691 UTC] Start collecting samples
[2018-01-21 16:42:48.222379 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:48.417796 UTC] Performing policy update
[2018-01-21 16:42:48.418674 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:48.606028 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:42:50.115910 UTC] Performing line search
[2018-01-21 16:42:50.312315 UTC] Updating baseline
[2018-01-21 16:42:52.947156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1591       |
| ExpectedImprovement  | 0.018175   |
| ActualImprovement    | 0.017572   |
| ImprovementRatio     | 0.96681    |
| MeanKL               | 0.0081553  |
| Entropy              | -1.8252    |
| Perplexity           | 0.16118    |
| AveragePolicyStd     | 0.18058    |
| AveragePolicyStd[0]  | 0.20638    |
| AveragePolicyStd[1]  | 0.19073    |
| AveragePolicyStd[2]  | 0.14502    |
| AveragePolicyStd[3]  | 0.1733     |
| AveragePolicyStd[4]  | 0.14997    |
| AveragePolicyStd[5]  | 0.21805    |
| AverageReturn        | 1852.3     |
| MinReturn            | 576.09     |
| MaxReturn            | 1995.9     |
| StdReturn            | 299.38     |
| AverageEpisodeLength | 957.56     |
| MinEpisodeLength     | 335        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.76     |
| TotalNEpisodes       | 25000      |
| TotalNSamples        | 7.9671e+06 |
| ExplainedVariance    | 0.066744   |
-------------------------------------
[2018-01-21 16:42:53.854350 UTC] Saving snapshot
[2018-01-21 16:42:53.854692 UTC] Starting iteration 1592
[2018-01-21 16:42:53.854889 UTC] Start collecting samples
[2018-01-21 16:42:59.045486 UTC] Computing input variables for policy optimization
[2018-01-21 16:42:59.194125 UTC] Performing policy update
[2018-01-21 16:42:59.196753 UTC] Computing gradient in Euclidean space
[2018-01-21 16:42:59.331951 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:43:00.976872 UTC] Performing line search
[2018-01-21 16:43:01.273616 UTC] Updating baseline
[2018-01-21 16:43:03.450606 UTC] Computing logging information
-------------------------------------
| Iteration            | 1592       |
| ExpectedImprovement  | 0.01817    |
| ActualImprovement    | 0.017248   |
| ImprovementRatio     | 0.94931    |
| MeanKL               | 0.0079634  |
| Entropy              | -1.8242    |
| Perplexity           | 0.16134    |
| AveragePolicyStd     | 0.18063    |
| AveragePolicyStd[0]  | 0.20664    |
| AveragePolicyStd[1]  | 0.19054    |
| AveragePolicyStd[2]  | 0.14503    |
| AveragePolicyStd[3]  | 0.17312    |
| AveragePolicyStd[4]  | 0.1499     |
| AveragePolicyStd[5]  | 0.21854    |
| AverageReturn        | 1821.6     |
| MinReturn            | 222.5      |
| MaxReturn            | 2000.3     |
| StdReturn            | 365.5      |
| AverageEpisodeLength | 942.06     |
| MinEpisodeLength     | 160        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.47     |
| TotalNEpisodes       | 25004      |
| TotalNSamples        | 7.9696e+06 |
| ExplainedVariance    | 0.33616    |
-------------------------------------
[2018-01-21 16:43:04.340689 UTC] Saving snapshot
[2018-01-21 16:43:04.340945 UTC] Starting iteration 1593
[2018-01-21 16:43:04.341118 UTC] Start collecting samples
[2018-01-21 16:43:09.739131 UTC] Computing input variables for policy optimization
[2018-01-21 16:43:09.864601 UTC] Performing policy update
[2018-01-21 16:43:09.865630 UTC] Computing gradient in Euclidean space
[2018-01-21 16:43:09.991580 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:43:11.500322 UTC] Performing line search
[2018-01-21 16:43:11.705753 UTC] Updating baseline
[2018-01-21 16:43:13.875255 UTC] Computing logging information
-------------------------------------
| Iteration            | 1593       |
| ExpectedImprovement  | 0.020226   |
| ActualImprovement    | 0.018531   |
| ImprovementRatio     | 0.91619    |
| MeanKL               | 0.0080545  |
| Entropy              | -1.8226    |
| Perplexity           | 0.16161    |
| AveragePolicyStd     | 0.18067    |
| AveragePolicyStd[0]  | 0.20654    |
| AveragePolicyStd[1]  | 0.19046    |
| AveragePolicyStd[2]  | 0.14505    |
| AveragePolicyStd[3]  | 0.17292    |
| AveragePolicyStd[4]  | 0.15024    |
| AveragePolicyStd[5]  | 0.21882    |
| AverageReturn        | 1823.1     |
| MinReturn            | 222.5      |
| MaxReturn            | 2000.3     |
| StdReturn            | 366.01     |
| AverageEpisodeLength | 942.06     |
| MinEpisodeLength     | 160        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.47     |
| TotalNEpisodes       | 25009      |
| TotalNSamples        | 7.9746e+06 |
| ExplainedVariance    | -0.012766  |
-------------------------------------
[2018-01-21 16:43:14.725309 UTC] Saving snapshot
[2018-01-21 16:43:14.725662 UTC] Starting iteration 1594
[2018-01-21 16:43:14.725898 UTC] Start collecting samples
[2018-01-21 16:43:20.567814 UTC] Computing input variables for policy optimization
[2018-01-21 16:43:20.718897 UTC] Performing policy update
[2018-01-21 16:43:20.719534 UTC] Computing gradient in Euclidean space
[2018-01-21 16:43:20.872080 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:43:22.507061 UTC] Performing line search
[2018-01-21 16:43:22.705702 UTC] Updating baseline
[2018-01-21 16:43:25.438807 UTC] Computing logging information
-------------------------------------
| Iteration            | 1594       |
| ExpectedImprovement  | 0.017162   |
| ActualImprovement    | 0.016507   |
| ImprovementRatio     | 0.96181    |
| MeanKL               | 0.0081715  |
| Entropy              | -1.832     |
| Perplexity           | 0.16009    |
| AveragePolicyStd     | 0.18039    |
| AveragePolicyStd[0]  | 0.2058     |
| AveragePolicyStd[1]  | 0.19013    |
| AveragePolicyStd[2]  | 0.14516    |
| AveragePolicyStd[3]  | 0.17253    |
| AveragePolicyStd[4]  | 0.14972    |
| AveragePolicyStd[5]  | 0.21901    |
| AverageReturn        | 1854.3     |
| MinReturn            | 222.5      |
| MaxReturn            | 2000.3     |
| StdReturn            | 324.38     |
| AverageEpisodeLength | 956.58     |
| MinEpisodeLength     | 160        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.33     |
| TotalNEpisodes       | 25016      |
| TotalNSamples        | 7.9816e+06 |
| ExplainedVariance    | -0.0051626 |
-------------------------------------
[2018-01-21 16:43:26.415928 UTC] Saving snapshot
[2018-01-21 16:43:26.416403 UTC] Starting iteration 1595
[2018-01-21 16:43:26.416763 UTC] Start collecting samples
[2018-01-21 16:43:32.838760 UTC] Computing input variables for policy optimization
[2018-01-21 16:43:33.010629 UTC] Performing policy update
[2018-01-21 16:43:33.011524 UTC] Computing gradient in Euclidean space
[2018-01-21 16:43:33.137054 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:43:34.923243 UTC] Performing line search
[2018-01-21 16:43:35.132183 UTC] Updating baseline
[2018-01-21 16:43:37.566381 UTC] Computing logging information
-------------------------------------
| Iteration            | 1595       |
| ExpectedImprovement  | 0.017845   |
| ActualImprovement    | 0.016715   |
| ImprovementRatio     | 0.93668    |
| MeanKL               | 0.0076885  |
| Entropy              | -1.8316    |
| Perplexity           | 0.16016    |
| AveragePolicyStd     | 0.18039    |
| AveragePolicyStd[0]  | 0.20614    |
| AveragePolicyStd[1]  | 0.18973    |
| AveragePolicyStd[2]  | 0.14532    |
| AveragePolicyStd[3]  | 0.17235    |
| AveragePolicyStd[4]  | 0.1499     |
| AveragePolicyStd[5]  | 0.21892    |
| AverageReturn        | 1856.8     |
| MinReturn            | 222.5      |
| MaxReturn            | 2012       |
| StdReturn            | 325.12     |
| AverageEpisodeLength | 956.58     |
| MinEpisodeLength     | 160        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.33     |
| TotalNEpisodes       | 25020      |
| TotalNSamples        | 7.9856e+06 |
| ExplainedVariance    | 0.0066656  |
-------------------------------------
[2018-01-21 16:43:38.529547 UTC] Saving snapshot
[2018-01-21 16:43:38.529817 UTC] Starting iteration 1596
[2018-01-21 16:43:38.530001 UTC] Start collecting samples
[2018-01-21 16:43:46.336722 UTC] Computing input variables for policy optimization
[2018-01-21 16:43:46.546035 UTC] Performing policy update
[2018-01-21 16:43:46.546678 UTC] Computing gradient in Euclidean space
[2018-01-21 16:43:46.698669 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:43:48.618318 UTC] Performing line search
[2018-01-21 16:43:48.939250 UTC] Updating baseline
[2018-01-21 16:43:52.020171 UTC] Computing logging information
-------------------------------------
| Iteration            | 1596       |
| ExpectedImprovement  | 0.016928   |
| ActualImprovement    | 0.015699   |
| ImprovementRatio     | 0.92736    |
| MeanKL               | 0.0087127  |
| Entropy              | -1.8318    |
| Perplexity           | 0.16013    |
| AveragePolicyStd     | 0.18038    |
| AveragePolicyStd[0]  | 0.20578    |
| AveragePolicyStd[1]  | 0.19006    |
| AveragePolicyStd[2]  | 0.14556    |
| AveragePolicyStd[3]  | 0.17212    |
| AveragePolicyStd[4]  | 0.14986    |
| AveragePolicyStd[5]  | 0.21888    |
| AverageReturn        | 1858.2     |
| MinReturn            | 222.5      |
| MaxReturn            | 2012       |
| StdReturn            | 325.35     |
| AverageEpisodeLength | 957        |
| MinEpisodeLength     | 160        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.39     |
| TotalNEpisodes       | 25024      |
| TotalNSamples        | 7.9896e+06 |
| ExplainedVariance    | -0.02749   |
-------------------------------------
[2018-01-21 16:43:53.086674 UTC] Saving snapshot
[2018-01-21 16:43:53.087065 UTC] Starting iteration 1597
[2018-01-21 16:43:53.087357 UTC] Start collecting samples
[2018-01-21 16:43:59.093021 UTC] Computing input variables for policy optimization
[2018-01-21 16:43:59.253678 UTC] Performing policy update
[2018-01-21 16:43:59.254331 UTC] Computing gradient in Euclidean space
[2018-01-21 16:43:59.378990 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:01.056684 UTC] Performing line search
[2018-01-21 16:44:01.246953 UTC] Updating baseline
[2018-01-21 16:44:03.311408 UTC] Computing logging information
-------------------------------------
| Iteration            | 1597       |
| ExpectedImprovement  | 0.017637   |
| ActualImprovement    | 0.016633   |
| ImprovementRatio     | 0.94308    |
| MeanKL               | 0.007931   |
| Entropy              | -1.8254    |
| Perplexity           | 0.16116    |
| AveragePolicyStd     | 0.18059    |
| AveragePolicyStd[0]  | 0.20648    |
| AveragePolicyStd[1]  | 0.1901     |
| AveragePolicyStd[2]  | 0.14571    |
| AveragePolicyStd[3]  | 0.17243    |
| AveragePolicyStd[4]  | 0.14964    |
| AveragePolicyStd[5]  | 0.21919    |
| AverageReturn        | 1831.6     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 381.91     |
| AverageEpisodeLength | 942.87     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.43     |
| TotalNEpisodes       | 25031      |
| TotalNSamples        | 7.9952e+06 |
| ExplainedVariance    | 0.17573    |
-------------------------------------
[2018-01-21 16:44:04.229300 UTC] Saving snapshot
[2018-01-21 16:44:04.229543 UTC] Starting iteration 1598
[2018-01-21 16:44:04.229707 UTC] Start collecting samples
[2018-01-21 16:44:12.088351 UTC] Computing input variables for policy optimization
[2018-01-21 16:44:12.298114 UTC] Performing policy update
[2018-01-21 16:44:12.299620 UTC] Computing gradient in Euclidean space
[2018-01-21 16:44:12.443659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:14.686939 UTC] Performing line search
[2018-01-21 16:44:14.973160 UTC] Updating baseline
[2018-01-21 16:44:17.919847 UTC] Computing logging information
-------------------------------------
| Iteration            | 1598       |
| ExpectedImprovement  | 0.019559   |
| ActualImprovement    | 0.018006   |
| ImprovementRatio     | 0.92063    |
| MeanKL               | 0.0086888  |
| Entropy              | -1.8215    |
| Perplexity           | 0.16178    |
| AveragePolicyStd     | 0.18068    |
| AveragePolicyStd[0]  | 0.20627    |
| AveragePolicyStd[1]  | 0.19016    |
| AveragePolicyStd[2]  | 0.14601    |
| AveragePolicyStd[3]  | 0.17239    |
| AveragePolicyStd[4]  | 0.14999    |
| AveragePolicyStd[5]  | 0.21927    |
| AverageReturn        | 1843.5     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 368.11     |
| AverageEpisodeLength | 948.07     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.58     |
| TotalNEpisodes       | 25036      |
| TotalNSamples        | 8.0002e+06 |
| ExplainedVariance    | -0.0031363 |
-------------------------------------
[2018-01-21 16:44:18.949168 UTC] Saving snapshot
[2018-01-21 16:44:18.949447 UTC] Starting iteration 1599
[2018-01-21 16:44:18.949625 UTC] Start collecting samples
[2018-01-21 16:44:26.120495 UTC] Computing input variables for policy optimization
[2018-01-21 16:44:26.265219 UTC] Performing policy update
[2018-01-21 16:44:26.265851 UTC] Computing gradient in Euclidean space
[2018-01-21 16:44:26.390326 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:27.897950 UTC] Performing line search
[2018-01-21 16:44:28.100694 UTC] Updating baseline
[2018-01-21 16:44:30.335582 UTC] Computing logging information
-------------------------------------
| Iteration            | 1599       |
| ExpectedImprovement  | 0.017804   |
| ActualImprovement    | 0.016993   |
| ImprovementRatio     | 0.95443    |
| MeanKL               | 0.0083304  |
| Entropy              | -1.8269    |
| Perplexity           | 0.16091    |
| AveragePolicyStd     | 0.18051    |
| AveragePolicyStd[0]  | 0.20621    |
| AveragePolicyStd[1]  | 0.18991    |
| AveragePolicyStd[2]  | 0.14576    |
| AveragePolicyStd[3]  | 0.17213    |
| AveragePolicyStd[4]  | 0.15014    |
| AveragePolicyStd[5]  | 0.21893    |
| AverageReturn        | 1835.3     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 379.26     |
| AverageEpisodeLength | 943.06     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.97     |
| TotalNEpisodes       | 25041      |
| TotalNSamples        | 8.0047e+06 |
| ExplainedVariance    | 0.12123    |
-------------------------------------
[2018-01-21 16:44:31.212590 UTC] Saving snapshot
[2018-01-21 16:44:31.212778 UTC] Starting iteration 1600
[2018-01-21 16:44:31.212879 UTC] Start collecting samples
[2018-01-21 16:44:36.545025 UTC] Computing input variables for policy optimization
[2018-01-21 16:44:36.677213 UTC] Performing policy update
[2018-01-21 16:44:36.677901 UTC] Computing gradient in Euclidean space
[2018-01-21 16:44:36.827732 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:38.344447 UTC] Performing line search
[2018-01-21 16:44:38.540861 UTC] Updating baseline
[2018-01-21 16:44:40.584416 UTC] Computing logging information
-------------------------------------
| Iteration            | 1600       |
| ExpectedImprovement  | 0.018823   |
| ActualImprovement    | 0.01768    |
| ImprovementRatio     | 0.93928    |
| MeanKL               | 0.0077013  |
| Entropy              | -1.8259    |
| Perplexity           | 0.16107    |
| AveragePolicyStd     | 0.18055    |
| AveragePolicyStd[0]  | 0.20624    |
| AveragePolicyStd[1]  | 0.1906     |
| AveragePolicyStd[2]  | 0.14576    |
| AveragePolicyStd[3]  | 0.17237    |
| AveragePolicyStd[4]  | 0.14979    |
| AveragePolicyStd[5]  | 0.21853    |
| AverageReturn        | 1829.6     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 395.99     |
| AverageEpisodeLength | 939.85     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.65     |
| TotalNEpisodes       | 25048      |
| TotalNSamples        | 8.0105e+06 |
| ExplainedVariance    | 0.17532    |
-------------------------------------
[2018-01-21 16:44:41.501729 UTC] Saving snapshot
[2018-01-21 16:44:41.507986 UTC] Starting iteration 1601
[2018-01-21 16:44:41.508163 UTC] Start collecting samples
[2018-01-21 16:44:46.654563 UTC] Computing input variables for policy optimization
[2018-01-21 16:44:46.848593 UTC] Performing policy update
[2018-01-21 16:44:46.849297 UTC] Computing gradient in Euclidean space
[2018-01-21 16:44:46.977538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:48.576339 UTC] Performing line search
[2018-01-21 16:44:48.774417 UTC] Updating baseline
[2018-01-21 16:44:50.901171 UTC] Computing logging information
-------------------------------------
| Iteration            | 1601       |
| ExpectedImprovement  | 0.018176   |
| ActualImprovement    | 0.017758   |
| ImprovementRatio     | 0.97702    |
| MeanKL               | 0.0077026  |
| Entropy              | -1.828     |
| Perplexity           | 0.16073    |
| AveragePolicyStd     | 0.18049    |
| AveragePolicyStd[0]  | 0.20647    |
| AveragePolicyStd[1]  | 0.1906     |
| AveragePolicyStd[2]  | 0.14558    |
| AveragePolicyStd[3]  | 0.17227    |
| AveragePolicyStd[4]  | 0.14973    |
| AveragePolicyStd[5]  | 0.21831    |
| AverageReturn        | 1829.8     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 396.08     |
| AverageEpisodeLength | 939.85     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.65     |
| TotalNEpisodes       | 25053      |
| TotalNSamples        | 8.0155e+06 |
| ExplainedVariance    | 0.061654   |
-------------------------------------
[2018-01-21 16:44:51.857432 UTC] Saving snapshot
[2018-01-21 16:44:51.857664 UTC] Starting iteration 1602
[2018-01-21 16:44:51.857820 UTC] Start collecting samples
[2018-01-21 16:44:56.890233 UTC] Computing input variables for policy optimization
[2018-01-21 16:44:57.024303 UTC] Performing policy update
[2018-01-21 16:44:57.024821 UTC] Computing gradient in Euclidean space
[2018-01-21 16:44:57.145896 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:44:58.648668 UTC] Performing line search
[2018-01-21 16:44:58.838943 UTC] Updating baseline
[2018-01-21 16:45:00.670384 UTC] Computing logging information
-------------------------------------
| Iteration            | 1602       |
| ExpectedImprovement  | 0.017393   |
| ActualImprovement    | 0.016043   |
| ImprovementRatio     | 0.92237    |
| MeanKL               | 0.0086251  |
| Entropy              | -1.8308    |
| Perplexity           | 0.16029    |
| AveragePolicyStd     | 0.18039    |
| AveragePolicyStd[0]  | 0.20597    |
| AveragePolicyStd[1]  | 0.19037    |
| AveragePolicyStd[2]  | 0.14548    |
| AveragePolicyStd[3]  | 0.17266    |
| AveragePolicyStd[4]  | 0.1498     |
| AveragePolicyStd[5]  | 0.21805    |
| AverageReturn        | 1814.5     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 421.4      |
| AverageEpisodeLength | 932.23     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.81     |
| TotalNEpisodes       | 25056      |
| TotalNSamples        | 8.0177e+06 |
| ExplainedVariance    | 0.20953    |
-------------------------------------
[2018-01-21 16:45:01.580320 UTC] Saving snapshot
[2018-01-21 16:45:01.580578 UTC] Starting iteration 1603
[2018-01-21 16:45:01.580753 UTC] Start collecting samples
[2018-01-21 16:45:06.675242 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:06.831975 UTC] Performing policy update
[2018-01-21 16:45:06.832789 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:06.968974 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:08.440539 UTC] Performing line search
[2018-01-21 16:45:08.629763 UTC] Updating baseline
[2018-01-21 16:45:10.790591 UTC] Computing logging information
-------------------------------------
| Iteration            | 1603       |
| ExpectedImprovement  | 0.01829    |
| ActualImprovement    | 0.017362   |
| ImprovementRatio     | 0.94923    |
| MeanKL               | 0.0079497  |
| Entropy              | -1.8261    |
| Perplexity           | 0.16104    |
| AveragePolicyStd     | 0.18053    |
| AveragePolicyStd[0]  | 0.20569    |
| AveragePolicyStd[1]  | 0.19017    |
| AveragePolicyStd[2]  | 0.14557    |
| AveragePolicyStd[3]  | 0.17256    |
| AveragePolicyStd[4]  | 0.15023    |
| AveragePolicyStd[5]  | 0.21895    |
| AverageReturn        | 1808.3     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 424.27     |
| AverageEpisodeLength | 929.28     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.93     |
| TotalNEpisodes       | 25064      |
| TotalNSamples        | 8.0254e+06 |
| ExplainedVariance    | 0.10644    |
-------------------------------------
[2018-01-21 16:45:11.608687 UTC] Saving snapshot
[2018-01-21 16:45:11.608864 UTC] Starting iteration 1604
[2018-01-21 16:45:11.608967 UTC] Start collecting samples
[2018-01-21 16:45:16.724638 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:16.880797 UTC] Performing policy update
[2018-01-21 16:45:16.881497 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:17.018665 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:18.496630 UTC] Performing line search
[2018-01-21 16:45:18.694107 UTC] Updating baseline
[2018-01-21 16:45:21.052235 UTC] Computing logging information
-------------------------------------
| Iteration            | 1604       |
| ExpectedImprovement  | 0.018912   |
| ActualImprovement    | 0.01825    |
| ImprovementRatio     | 0.96504    |
| MeanKL               | 0.0076594  |
| Entropy              | -1.8261    |
| Perplexity           | 0.16103    |
| AveragePolicyStd     | 0.18055    |
| AveragePolicyStd[0]  | 0.20586    |
| AveragePolicyStd[1]  | 0.19028    |
| AveragePolicyStd[2]  | 0.1456     |
| AveragePolicyStd[3]  | 0.17254    |
| AveragePolicyStd[4]  | 0.14992    |
| AveragePolicyStd[5]  | 0.21909    |
| AverageReturn        | 1820.9     |
| MinReturn            | 51.772     |
| MaxReturn            | 2012       |
| StdReturn            | 412.4      |
| AverageEpisodeLength | 935.33     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.2      |
| TotalNEpisodes       | 25069      |
| TotalNSamples        | 8.0304e+06 |
| ExplainedVariance    | 0.01399    |
-------------------------------------
[2018-01-21 16:45:21.910090 UTC] Saving snapshot
[2018-01-21 16:45:21.910383 UTC] Starting iteration 1605
[2018-01-21 16:45:21.910573 UTC] Start collecting samples
[2018-01-21 16:45:26.852986 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:27.027586 UTC] Performing policy update
[2018-01-21 16:45:27.028304 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:27.154545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:28.614298 UTC] Performing line search
[2018-01-21 16:45:28.809996 UTC] Updating baseline
[2018-01-21 16:45:30.859627 UTC] Computing logging information
------------------------------------
| Iteration            | 1605      |
| ExpectedImprovement  | 0.019188  |
| ActualImprovement    | 0.016902  |
| ImprovementRatio     | 0.88087   |
| MeanKL               | 0.0076379 |
| Entropy              | -1.8227   |
| Perplexity           | 0.16159   |
| AveragePolicyStd     | 0.18064   |
| AveragePolicyStd[0]  | 0.20561   |
| AveragePolicyStd[1]  | 0.19036   |
| AveragePolicyStd[2]  | 0.14566   |
| AveragePolicyStd[3]  | 0.17278   |
| AveragePolicyStd[4]  | 0.15014   |
| AveragePolicyStd[5]  | 0.21931   |
| AverageReturn        | 1826.7    |
| MinReturn            | 51.772    |
| MaxReturn            | 2013      |
| StdReturn            | 401.68    |
| AverageEpisodeLength | 937.57    |
| MinEpisodeLength     | 65        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 194.76    |
| TotalNEpisodes       | 25072     |
| TotalNSamples        | 8.033e+06 |
| ExplainedVariance    | 0.21838   |
------------------------------------
[2018-01-21 16:45:31.782919 UTC] Saving snapshot
[2018-01-21 16:45:31.783138 UTC] Starting iteration 1606
[2018-01-21 16:45:31.783336 UTC] Start collecting samples
[2018-01-21 16:45:36.798637 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:36.941652 UTC] Performing policy update
[2018-01-21 16:45:36.942248 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:37.063531 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:38.484242 UTC] Performing line search
[2018-01-21 16:45:38.704252 UTC] Updating baseline
[2018-01-21 16:45:40.694865 UTC] Computing logging information
-------------------------------------
| Iteration            | 1606       |
| ExpectedImprovement  | 0.018389   |
| ActualImprovement    | 0.018148   |
| ImprovementRatio     | 0.98689    |
| MeanKL               | 0.0080336  |
| Entropy              | -1.8259    |
| Perplexity           | 0.16107    |
| AveragePolicyStd     | 0.18054    |
| AveragePolicyStd[0]  | 0.20537    |
| AveragePolicyStd[1]  | 0.18991    |
| AveragePolicyStd[2]  | 0.14589    |
| AveragePolicyStd[3]  | 0.17302    |
| AveragePolicyStd[4]  | 0.14968    |
| AveragePolicyStd[5]  | 0.2194     |
| AverageReturn        | 1807.8     |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 422.47     |
| AverageEpisodeLength | 928.34     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.98     |
| TotalNEpisodes       | 25080      |
| TotalNSamples        | 8.0401e+06 |
| ExplainedVariance    | 0.19669    |
-------------------------------------
[2018-01-21 16:45:41.538645 UTC] Saving snapshot
[2018-01-21 16:45:41.538862 UTC] Starting iteration 1607
[2018-01-21 16:45:41.539044 UTC] Start collecting samples
[2018-01-21 16:45:46.644192 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:46.774490 UTC] Performing policy update
[2018-01-21 16:45:46.775167 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:46.902051 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:48.360298 UTC] Performing line search
[2018-01-21 16:45:48.584327 UTC] Updating baseline
[2018-01-21 16:45:50.900517 UTC] Computing logging information
-------------------------------------
| Iteration            | 1607       |
| ExpectedImprovement  | 0.016622   |
| ActualImprovement    | 0.016348   |
| ImprovementRatio     | 0.98353    |
| MeanKL               | 0.008251   |
| Entropy              | -1.8258    |
| Perplexity           | 0.16109    |
| AveragePolicyStd     | 0.18053    |
| AveragePolicyStd[0]  | 0.20506    |
| AveragePolicyStd[1]  | 0.18984    |
| AveragePolicyStd[2]  | 0.14599    |
| AveragePolicyStd[3]  | 0.17305    |
| AveragePolicyStd[4]  | 0.14984    |
| AveragePolicyStd[5]  | 0.21942    |
| AverageReturn        | 1793.4     |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 444.96     |
| AverageEpisodeLength | 920.77     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.88     |
| TotalNEpisodes       | 25086      |
| TotalNSamples        | 8.0453e+06 |
| ExplainedVariance    | 0.011799   |
-------------------------------------
[2018-01-21 16:45:51.726197 UTC] Saving snapshot
[2018-01-21 16:45:51.726642 UTC] Starting iteration 1608
[2018-01-21 16:45:51.726854 UTC] Start collecting samples
[2018-01-21 16:45:57.041578 UTC] Computing input variables for policy optimization
[2018-01-21 16:45:57.177099 UTC] Performing policy update
[2018-01-21 16:45:57.177879 UTC] Computing gradient in Euclidean space
[2018-01-21 16:45:57.296951 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:45:58.721245 UTC] Performing line search
[2018-01-21 16:45:58.931014 UTC] Updating baseline
[2018-01-21 16:46:01.166529 UTC] Computing logging information
------------------------------------
| Iteration            | 1608      |
| ExpectedImprovement  | 0.019324  |
| ActualImprovement    | 0.017549  |
| ImprovementRatio     | 0.90814   |
| MeanKL               | 0.0072976 |
| Entropy              | -1.8249   |
| Perplexity           | 0.16123   |
| AveragePolicyStd     | 0.18055   |
| AveragePolicyStd[0]  | 0.20531   |
| AveragePolicyStd[1]  | 0.18997   |
| AveragePolicyStd[2]  | 0.14648   |
| AveragePolicyStd[3]  | 0.17262   |
| AveragePolicyStd[4]  | 0.1496    |
| AveragePolicyStd[5]  | 0.21935   |
| AverageReturn        | 1747.9    |
| MinReturn            | 51.772    |
| MaxReturn            | 2022.9    |
| StdReturn            | 488.46    |
| AverageEpisodeLength | 898.08    |
| MinEpisodeLength     | 65        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 237.56    |
| TotalNEpisodes       | 25093     |
| TotalNSamples        | 8.05e+06  |
| ExplainedVariance    | 0.32934   |
------------------------------------
[2018-01-21 16:46:02.126106 UTC] Saving snapshot
[2018-01-21 16:46:02.126341 UTC] Starting iteration 1609
[2018-01-21 16:46:02.126556 UTC] Start collecting samples
[2018-01-21 16:46:07.076964 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:07.237569 UTC] Performing policy update
[2018-01-21 16:46:07.249377 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:07.367009 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:08.859133 UTC] Performing line search
[2018-01-21 16:46:09.062102 UTC] Updating baseline
[2018-01-21 16:46:11.061320 UTC] Computing logging information
------------------------------------
| Iteration            | 1609      |
| ExpectedImprovement  | 0.019993  |
| ActualImprovement    | 0.01825   |
| ImprovementRatio     | 0.91281   |
| MeanKL               | 0.0083307 |
| Entropy              | -1.8259   |
| Perplexity           | 0.16107   |
| AveragePolicyStd     | 0.18051   |
| AveragePolicyStd[0]  | 0.20494   |
| AveragePolicyStd[1]  | 0.19006   |
| AveragePolicyStd[2]  | 0.14649   |
| AveragePolicyStd[3]  | 0.17263   |
| AveragePolicyStd[4]  | 0.14967   |
| AveragePolicyStd[5]  | 0.21928   |
| AverageReturn        | 1751.8    |
| MinReturn            | 51.772    |
| MaxReturn            | 2022.9    |
| StdReturn            | 489.68    |
| AverageEpisodeLength | 899.03    |
| MinEpisodeLength     | 65        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 237.78    |
| TotalNEpisodes       | 25098     |
| TotalNSamples        | 8.055e+06 |
| ExplainedVariance    | 0.059839  |
------------------------------------
[2018-01-21 16:46:11.921707 UTC] Saving snapshot
[2018-01-21 16:46:11.921973 UTC] Starting iteration 1610
[2018-01-21 16:46:11.922184 UTC] Start collecting samples
[2018-01-21 16:46:17.013161 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:17.141503 UTC] Performing policy update
[2018-01-21 16:46:17.142889 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:17.263561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:18.676013 UTC] Performing line search
[2018-01-21 16:46:18.946522 UTC] Updating baseline
[2018-01-21 16:46:20.894497 UTC] Computing logging information
------------------------------------
| Iteration            | 1610      |
| ExpectedImprovement  | 0.019307  |
| ActualImprovement    | 0.019169  |
| ImprovementRatio     | 0.99283   |
| MeanKL               | 0.0073615 |
| Entropy              | -1.8324   |
| Perplexity           | 0.16003   |
| AveragePolicyStd     | 0.18031   |
| AveragePolicyStd[0]  | 0.20452   |
| AveragePolicyStd[1]  | 0.18937   |
| AveragePolicyStd[2]  | 0.14634   |
| AveragePolicyStd[3]  | 0.17238   |
| AveragePolicyStd[4]  | 0.14982   |
| AveragePolicyStd[5]  | 0.21944   |
| AverageReturn        | 1769.7    |
| MinReturn            | 51.772    |
| MaxReturn            | 2022.9    |
| StdReturn            | 465.52    |
| AverageEpisodeLength | 907.1     |
| MinEpisodeLength     | 65        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 225.96    |
| TotalNEpisodes       | 25103     |
| TotalNSamples        | 8.06e+06  |
| ExplainedVariance    | 0.058725  |
------------------------------------
[2018-01-21 16:46:21.790519 UTC] Saving snapshot
[2018-01-21 16:46:21.800264 UTC] Starting iteration 1611
[2018-01-21 16:46:21.800503 UTC] Start collecting samples
[2018-01-21 16:46:26.741626 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:26.875242 UTC] Performing policy update
[2018-01-21 16:46:26.875889 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:26.992757 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:28.398897 UTC] Performing line search
[2018-01-21 16:46:28.591099 UTC] Updating baseline
[2018-01-21 16:46:30.658172 UTC] Computing logging information
-------------------------------------
| Iteration            | 1611       |
| ExpectedImprovement  | 0.019634   |
| ActualImprovement    | 0.0186     |
| ImprovementRatio     | 0.94733    |
| MeanKL               | 0.0075532  |
| Entropy              | -1.828     |
| Perplexity           | 0.16073    |
| AveragePolicyStd     | 0.18046    |
| AveragePolicyStd[0]  | 0.20478    |
| AveragePolicyStd[1]  | 0.18919    |
| AveragePolicyStd[2]  | 0.14649    |
| AveragePolicyStd[3]  | 0.1723     |
| AveragePolicyStd[4]  | 0.14987    |
| AveragePolicyStd[5]  | 0.22014    |
| AverageReturn        | 1779.8     |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 449.02     |
| AverageEpisodeLength | 911.86     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.77     |
| TotalNEpisodes       | 25108      |
| TotalNSamples        | 8.0648e+06 |
| ExplainedVariance    | 0.10225    |
-------------------------------------
[2018-01-21 16:46:31.541493 UTC] Saving snapshot
[2018-01-21 16:46:31.541704 UTC] Starting iteration 1612
[2018-01-21 16:46:31.541886 UTC] Start collecting samples
[2018-01-21 16:46:36.663224 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:36.792277 UTC] Performing policy update
[2018-01-21 16:46:36.792864 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:36.913268 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:38.435576 UTC] Performing line search
[2018-01-21 16:46:38.634656 UTC] Updating baseline
[2018-01-21 16:46:40.512103 UTC] Computing logging information
-------------------------------------
| Iteration            | 1612       |
| ExpectedImprovement  | 0.018986   |
| ActualImprovement    | 0.018064   |
| ImprovementRatio     | 0.95146    |
| MeanKL               | 0.0075543  |
| Entropy              | -1.8284    |
| Perplexity           | 0.16068    |
| AveragePolicyStd     | 0.18044    |
| AveragePolicyStd[0]  | 0.20466    |
| AveragePolicyStd[1]  | 0.18938    |
| AveragePolicyStd[2]  | 0.14671    |
| AveragePolicyStd[3]  | 0.17209    |
| AveragePolicyStd[4]  | 0.14982    |
| AveragePolicyStd[5]  | 0.21998    |
| AverageReturn        | 1782       |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 449.86     |
| AverageEpisodeLength | 911.86     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.77     |
| TotalNEpisodes       | 25114      |
| TotalNSamples        | 8.0708e+06 |
| ExplainedVariance    | -0.0070903 |
-------------------------------------
[2018-01-21 16:46:41.510220 UTC] Saving snapshot
[2018-01-21 16:46:41.510539 UTC] Starting iteration 1613
[2018-01-21 16:46:41.510767 UTC] Start collecting samples
[2018-01-21 16:46:46.476556 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:46.639731 UTC] Performing policy update
[2018-01-21 16:46:46.640667 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:46.770072 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:48.302497 UTC] Performing line search
[2018-01-21 16:46:48.506733 UTC] Updating baseline
[2018-01-21 16:46:50.349203 UTC] Computing logging information
-------------------------------------
| Iteration            | 1613       |
| ExpectedImprovement  | 0.020743   |
| ActualImprovement    | 0.019823   |
| ImprovementRatio     | 0.95565    |
| MeanKL               | 0.0076536  |
| Entropy              | -1.8341    |
| Perplexity           | 0.15975    |
| AveragePolicyStd     | 0.18025    |
| AveragePolicyStd[0]  | 0.20436    |
| AveragePolicyStd[1]  | 0.18934    |
| AveragePolicyStd[2]  | 0.14641    |
| AveragePolicyStd[3]  | 0.17202    |
| AveragePolicyStd[4]  | 0.14992    |
| AveragePolicyStd[5]  | 0.21947    |
| AverageReturn        | 1772.6     |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 450.83     |
| AverageEpisodeLength | 908.44     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.49     |
| TotalNEpisodes       | 25120      |
| TotalNSamples        | 8.0764e+06 |
| ExplainedVariance    | 0.18408    |
-------------------------------------
[2018-01-21 16:46:51.358878 UTC] Saving snapshot
[2018-01-21 16:46:51.359117 UTC] Starting iteration 1614
[2018-01-21 16:46:51.359313 UTC] Start collecting samples
[2018-01-21 16:46:56.370714 UTC] Computing input variables for policy optimization
[2018-01-21 16:46:56.527683 UTC] Performing policy update
[2018-01-21 16:46:56.528347 UTC] Computing gradient in Euclidean space
[2018-01-21 16:46:56.652114 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:46:58.207809 UTC] Performing line search
[2018-01-21 16:46:58.420048 UTC] Updating baseline
[2018-01-21 16:47:00.859004 UTC] Computing logging information
-------------------------------------
| Iteration            | 1614       |
| ExpectedImprovement  | 0.01969    |
| ActualImprovement    | 0.01848    |
| ImprovementRatio     | 0.93856    |
| MeanKL               | 0.008187   |
| Entropy              | -1.838     |
| Perplexity           | 0.15913    |
| AveragePolicyStd     | 0.18014    |
| AveragePolicyStd[0]  | 0.20396    |
| AveragePolicyStd[1]  | 0.1893     |
| AveragePolicyStd[2]  | 0.14633    |
| AveragePolicyStd[3]  | 0.17165    |
| AveragePolicyStd[4]  | 0.14989    |
| AveragePolicyStd[5]  | 0.21971    |
| AverageReturn        | 1754.5     |
| MinReturn            | 51.772     |
| MaxReturn            | 2022.9     |
| StdReturn            | 472.78     |
| AverageEpisodeLength | 898.54     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 228.96     |
| TotalNEpisodes       | 25126      |
| TotalNSamples        | 8.0814e+06 |
| ExplainedVariance    | 0.15843    |
-------------------------------------
[2018-01-21 16:47:01.714181 UTC] Saving snapshot
[2018-01-21 16:47:01.714482 UTC] Starting iteration 1615
[2018-01-21 16:47:01.714707 UTC] Start collecting samples
[2018-01-21 16:47:06.649790 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:06.816740 UTC] Performing policy update
[2018-01-21 16:47:06.817756 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:06.965769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:08.486423 UTC] Performing line search
[2018-01-21 16:47:08.671256 UTC] Updating baseline
[2018-01-21 16:47:10.841268 UTC] Computing logging information
-------------------------------------
| Iteration            | 1615       |
| ExpectedImprovement  | 0.018385   |
| ActualImprovement    | 0.017376   |
| ImprovementRatio     | 0.94508    |
| MeanKL               | 0.0073748  |
| Entropy              | -1.8423    |
| Perplexity           | 0.15845    |
| AveragePolicyStd     | 0.18003    |
| AveragePolicyStd[0]  | 0.20424    |
| AveragePolicyStd[1]  | 0.18918    |
| AveragePolicyStd[2]  | 0.14621    |
| AveragePolicyStd[3]  | 0.17143    |
| AveragePolicyStd[4]  | 0.14957    |
| AveragePolicyStd[5]  | 0.21955    |
| AverageReturn        | 1778.9     |
| MinReturn            | 336.53     |
| MaxReturn            | 2022.9     |
| StdReturn            | 434.74     |
| AverageEpisodeLength | 910.35     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 210.21     |
| TotalNEpisodes       | 25129      |
| TotalNSamples        | 8.0842e+06 |
| ExplainedVariance    | 0.23094    |
-------------------------------------
[2018-01-21 16:47:11.710839 UTC] Saving snapshot
[2018-01-21 16:47:11.711125 UTC] Starting iteration 1616
[2018-01-21 16:47:11.711318 UTC] Start collecting samples
[2018-01-21 16:47:16.967685 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:17.112174 UTC] Performing policy update
[2018-01-21 16:47:17.113304 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:17.238335 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:18.679905 UTC] Performing line search
[2018-01-21 16:47:18.879090 UTC] Updating baseline
[2018-01-21 16:47:21.283966 UTC] Computing logging information
-------------------------------------
| Iteration            | 1616       |
| ExpectedImprovement  | 0.016478   |
| ActualImprovement    | 0.016085   |
| ImprovementRatio     | 0.97616    |
| MeanKL               | 0.0081601  |
| Entropy              | -1.8415    |
| Perplexity           | 0.15858    |
| AveragePolicyStd     | 0.18004    |
| AveragePolicyStd[0]  | 0.20468    |
| AveragePolicyStd[1]  | 0.18902    |
| AveragePolicyStd[2]  | 0.14629    |
| AveragePolicyStd[3]  | 0.17126    |
| AveragePolicyStd[4]  | 0.14991    |
| AveragePolicyStd[5]  | 0.21907    |
| AverageReturn        | 1763.1     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 449.32     |
| AverageEpisodeLength | 902.12     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.77     |
| TotalNEpisodes       | 25137      |
| TotalNSamples        | 8.0914e+06 |
| ExplainedVariance    | 0.21223    |
-------------------------------------
[2018-01-21 16:47:22.136703 UTC] Saving snapshot
[2018-01-21 16:47:22.136940 UTC] Starting iteration 1617
[2018-01-21 16:47:22.137084 UTC] Start collecting samples
[2018-01-21 16:47:27.608122 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:27.754521 UTC] Performing policy update
[2018-01-21 16:47:27.755660 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:27.905606 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:29.369918 UTC] Performing line search
[2018-01-21 16:47:29.565443 UTC] Updating baseline
[2018-01-21 16:47:31.541778 UTC] Computing logging information
-------------------------------------
| Iteration            | 1617       |
| ExpectedImprovement  | 0.019419   |
| ActualImprovement    | 0.018017   |
| ImprovementRatio     | 0.92778    |
| MeanKL               | 0.0083758  |
| Entropy              | -1.8352    |
| Perplexity           | 0.15958    |
| AveragePolicyStd     | 0.18022    |
| AveragePolicyStd[0]  | 0.20431    |
| AveragePolicyStd[1]  | 0.18953    |
| AveragePolicyStd[2]  | 0.14654    |
| AveragePolicyStd[3]  | 0.17083    |
| AveragePolicyStd[4]  | 0.15032    |
| AveragePolicyStd[5]  | 0.21979    |
| AverageReturn        | 1773.9     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 442.3      |
| AverageEpisodeLength | 907.13     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.15     |
| TotalNEpisodes       | 25140      |
| TotalNSamples        | 8.0944e+06 |
| ExplainedVariance    | 0.0092834  |
-------------------------------------
[2018-01-21 16:47:32.438045 UTC] Saving snapshot
[2018-01-21 16:47:32.438282 UTC] Starting iteration 1618
[2018-01-21 16:47:32.438446 UTC] Start collecting samples
[2018-01-21 16:47:37.208751 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:37.349066 UTC] Performing policy update
[2018-01-21 16:47:37.349739 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:37.471507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:38.966212 UTC] Performing line search
[2018-01-21 16:47:39.184264 UTC] Updating baseline
[2018-01-21 16:47:41.242912 UTC] Computing logging information
-------------------------------------
| Iteration            | 1618       |
| ExpectedImprovement  | 0.019139   |
| ActualImprovement    | 0.018301   |
| ImprovementRatio     | 0.95623    |
| MeanKL               | 0.0087416  |
| Entropy              | -1.8361    |
| Perplexity           | 0.15943    |
| AveragePolicyStd     | 0.18019    |
| AveragePolicyStd[0]  | 0.20396    |
| AveragePolicyStd[1]  | 0.18955    |
| AveragePolicyStd[2]  | 0.14661    |
| AveragePolicyStd[3]  | 0.17093    |
| AveragePolicyStd[4]  | 0.15018    |
| AveragePolicyStd[5]  | 0.21993    |
| AverageReturn        | 1785.8     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 429.71     |
| AverageEpisodeLength | 912.68     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 207.08     |
| TotalNEpisodes       | 25145      |
| TotalNSamples        | 8.0987e+06 |
| ExplainedVariance    | 0.14201    |
-------------------------------------
[2018-01-21 16:47:42.219232 UTC] Saving snapshot
[2018-01-21 16:47:42.219483 UTC] Starting iteration 1619
[2018-01-21 16:47:42.219663 UTC] Start collecting samples
[2018-01-21 16:47:47.258379 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:47.393004 UTC] Performing policy update
[2018-01-21 16:47:47.393744 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:47.513145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:48.879022 UTC] Performing line search
[2018-01-21 16:47:49.073715 UTC] Updating baseline
[2018-01-21 16:47:50.957156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1619       |
| ExpectedImprovement  | 0.020176   |
| ActualImprovement    | 0.018894   |
| ImprovementRatio     | 0.93645    |
| MeanKL               | 0.0072915  |
| Entropy              | -1.8346    |
| Perplexity           | 0.15967    |
| AveragePolicyStd     | 0.18025    |
| AveragePolicyStd[0]  | 0.20384    |
| AveragePolicyStd[1]  | 0.1898     |
| AveragePolicyStd[2]  | 0.14667    |
| AveragePolicyStd[3]  | 0.17134    |
| AveragePolicyStd[4]  | 0.14982    |
| AveragePolicyStd[5]  | 0.22003    |
| AverageReturn        | 1775       |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 437.93     |
| AverageEpisodeLength | 907.57     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.13     |
| TotalNEpisodes       | 25153      |
| TotalNSamples        | 8.1062e+06 |
| ExplainedVariance    | 0.089024   |
-------------------------------------
[2018-01-21 16:47:51.800447 UTC] Saving snapshot
[2018-01-21 16:47:51.800697 UTC] Starting iteration 1620
[2018-01-21 16:47:51.800879 UTC] Start collecting samples
[2018-01-21 16:47:57.152554 UTC] Computing input variables for policy optimization
[2018-01-21 16:47:57.286179 UTC] Performing policy update
[2018-01-21 16:47:57.287371 UTC] Computing gradient in Euclidean space
[2018-01-21 16:47:57.409227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:47:58.830337 UTC] Performing line search
[2018-01-21 16:47:59.021757 UTC] Updating baseline
[2018-01-21 16:48:02.444840 UTC] Computing logging information
-------------------------------------
| Iteration            | 1620       |
| ExpectedImprovement  | 0.018402   |
| ActualImprovement    | 0.017591   |
| ImprovementRatio     | 0.95594    |
| MeanKL               | 0.0078988  |
| Entropy              | -1.8362    |
| Perplexity           | 0.15943    |
| AveragePolicyStd     | 0.1802     |
| AveragePolicyStd[0]  | 0.20424    |
| AveragePolicyStd[1]  | 0.18968    |
| AveragePolicyStd[2]  | 0.14636    |
| AveragePolicyStd[3]  | 0.17131    |
| AveragePolicyStd[4]  | 0.14997    |
| AveragePolicyStd[5]  | 0.21965    |
| AverageReturn        | 1790       |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 415        |
| AverageEpisodeLength | 915.19     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.3      |
| TotalNEpisodes       | 25157      |
| TotalNSamples        | 8.1102e+06 |
| ExplainedVariance    | -0.048462  |
-------------------------------------
[2018-01-21 16:48:03.354340 UTC] Saving snapshot
[2018-01-21 16:48:03.368710 UTC] Starting iteration 1621
[2018-01-21 16:48:03.369384 UTC] Start collecting samples
[2018-01-21 16:48:10.536805 UTC] Computing input variables for policy optimization
[2018-01-21 16:48:10.664013 UTC] Performing policy update
[2018-01-21 16:48:10.664665 UTC] Computing gradient in Euclidean space
[2018-01-21 16:48:10.792136 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:48:12.337417 UTC] Performing line search
[2018-01-21 16:48:12.561758 UTC] Updating baseline
[2018-01-21 16:48:15.305205 UTC] Computing logging information
-------------------------------------
| Iteration            | 1621       |
| ExpectedImprovement  | 0.018074   |
| ActualImprovement    | 0.0168     |
| ImprovementRatio     | 0.92949    |
| MeanKL               | 0.0075115  |
| Entropy              | -1.8388    |
| Perplexity           | 0.15901    |
| AveragePolicyStd     | 0.18012    |
| AveragePolicyStd[0]  | 0.20408    |
| AveragePolicyStd[1]  | 0.18926    |
| AveragePolicyStd[2]  | 0.14633    |
| AveragePolicyStd[3]  | 0.17174    |
| AveragePolicyStd[4]  | 0.14979    |
| AveragePolicyStd[5]  | 0.21952    |
| AverageReturn        | 1788.9     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 416.89     |
| AverageEpisodeLength | 914.43     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.24     |
| TotalNEpisodes       | 25161      |
| TotalNSamples        | 8.1139e+06 |
| ExplainedVariance    | 0.1219     |
-------------------------------------
[2018-01-21 16:48:16.296397 UTC] Saving snapshot
[2018-01-21 16:48:16.296587 UTC] Starting iteration 1622
[2018-01-21 16:48:16.296695 UTC] Start collecting samples
[2018-01-21 16:48:24.166686 UTC] Computing input variables for policy optimization
[2018-01-21 16:48:24.371091 UTC] Performing policy update
[2018-01-21 16:48:24.371828 UTC] Computing gradient in Euclidean space
[2018-01-21 16:48:24.496851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:48:26.081817 UTC] Performing line search
[2018-01-21 16:48:26.285562 UTC] Updating baseline
[2018-01-21 16:48:28.910278 UTC] Computing logging information
-------------------------------------
| Iteration            | 1622       |
| ExpectedImprovement  | 0.018052   |
| ActualImprovement    | 0.017373   |
| ImprovementRatio     | 0.96238    |
| MeanKL               | 0.007756   |
| Entropy              | -1.8388    |
| Perplexity           | 0.159      |
| AveragePolicyStd     | 0.18011    |
| AveragePolicyStd[0]  | 0.20413    |
| AveragePolicyStd[1]  | 0.18932    |
| AveragePolicyStd[2]  | 0.14646    |
| AveragePolicyStd[3]  | 0.17192    |
| AveragePolicyStd[4]  | 0.14959    |
| AveragePolicyStd[5]  | 0.21926    |
| AverageReturn        | 1791.1     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 417.72     |
| AverageEpisodeLength | 914.43     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.24     |
| TotalNEpisodes       | 25169      |
| TotalNSamples        | 8.1219e+06 |
| ExplainedVariance    | 0.001483   |
-------------------------------------
[2018-01-21 16:48:29.855369 UTC] Saving snapshot
[2018-01-21 16:48:29.855716 UTC] Starting iteration 1623
[2018-01-21 16:48:29.855986 UTC] Start collecting samples
[2018-01-21 16:48:37.103731 UTC] Computing input variables for policy optimization
[2018-01-21 16:48:37.293762 UTC] Performing policy update
[2018-01-21 16:48:37.294497 UTC] Computing gradient in Euclidean space
[2018-01-21 16:48:37.412737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:48:38.908298 UTC] Performing line search
[2018-01-21 16:48:39.120927 UTC] Updating baseline
[2018-01-21 16:48:41.636605 UTC] Computing logging information
--------------------------------------
| Iteration            | 1623        |
| ExpectedImprovement  | 0.015828    |
| ActualImprovement    | 0.014995    |
| ImprovementRatio     | 0.94736     |
| MeanKL               | 0.0084018   |
| Entropy              | -1.8416     |
| Perplexity           | 0.15856     |
| AveragePolicyStd     | 0.18002     |
| AveragePolicyStd[0]  | 0.20409     |
| AveragePolicyStd[1]  | 0.18927     |
| AveragePolicyStd[2]  | 0.14633     |
| AveragePolicyStd[3]  | 0.1721      |
| AveragePolicyStd[4]  | 0.14946     |
| AveragePolicyStd[5]  | 0.21889     |
| AverageReturn        | 1799.8      |
| MinReturn            | 336.53      |
| MaxReturn            | 2059.5      |
| StdReturn            | 411.94      |
| AverageEpisodeLength | 918.74      |
| MinEpisodeLength     | 203         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 198.39      |
| TotalNEpisodes       | 25171       |
| TotalNSamples        | 8.1239e+06  |
| ExplainedVariance    | -6.3561e-05 |
--------------------------------------
[2018-01-21 16:48:42.643398 UTC] Saving snapshot
[2018-01-21 16:48:42.643793 UTC] Starting iteration 1624
[2018-01-21 16:48:42.643997 UTC] Start collecting samples
[2018-01-21 16:48:50.067131 UTC] Computing input variables for policy optimization
[2018-01-21 16:48:50.233962 UTC] Performing policy update
[2018-01-21 16:48:50.234622 UTC] Computing gradient in Euclidean space
[2018-01-21 16:48:50.357622 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:48:51.964433 UTC] Performing line search
[2018-01-21 16:48:52.165857 UTC] Updating baseline
[2018-01-21 16:48:54.085477 UTC] Computing logging information
-------------------------------------
| Iteration            | 1624       |
| ExpectedImprovement  | 0.017892   |
| ActualImprovement    | 0.016778   |
| ImprovementRatio     | 0.9377     |
| MeanKL               | 0.0084657  |
| Entropy              | -1.8404    |
| Perplexity           | 0.15875    |
| AveragePolicyStd     | 0.18006    |
| AveragePolicyStd[0]  | 0.20389    |
| AveragePolicyStd[1]  | 0.18967    |
| AveragePolicyStd[2]  | 0.14635    |
| AveragePolicyStd[3]  | 0.17216    |
| AveragePolicyStd[4]  | 0.14935    |
| AveragePolicyStd[5]  | 0.21897    |
| AverageReturn        | 1806.3     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 397.96     |
| AverageEpisodeLength | 921.65     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 191.42     |
| TotalNEpisodes       | 25177      |
| TotalNSamples        | 8.1295e+06 |
| ExplainedVariance    | 0.09727    |
-------------------------------------
[2018-01-21 16:48:55.097353 UTC] Saving snapshot
[2018-01-21 16:48:55.097708 UTC] Starting iteration 1625
[2018-01-21 16:48:55.097975 UTC] Start collecting samples
[2018-01-21 16:49:02.574270 UTC] Computing input variables for policy optimization
[2018-01-21 16:49:02.808838 UTC] Performing policy update
[2018-01-21 16:49:02.809614 UTC] Computing gradient in Euclidean space
[2018-01-21 16:49:02.940929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:49:04.463823 UTC] Performing line search
[2018-01-21 16:49:04.665941 UTC] Updating baseline
[2018-01-21 16:49:07.002964 UTC] Computing logging information
-------------------------------------
| Iteration            | 1625       |
| ExpectedImprovement  | 0.018415   |
| ActualImprovement    | 0.017942   |
| ImprovementRatio     | 0.97432    |
| MeanKL               | 0.0081171  |
| Entropy              | -1.8395    |
| Perplexity           | 0.1589     |
| AveragePolicyStd     | 0.18008    |
| AveragePolicyStd[0]  | 0.20387    |
| AveragePolicyStd[1]  | 0.18975    |
| AveragePolicyStd[2]  | 0.1462     |
| AveragePolicyStd[3]  | 0.17233    |
| AveragePolicyStd[4]  | 0.14959    |
| AveragePolicyStd[5]  | 0.21874    |
| AverageReturn        | 1824.6     |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 370.45     |
| AverageEpisodeLength | 931.53     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.4      |
| TotalNEpisodes       | 25185      |
| TotalNSamples        | 8.1375e+06 |
| ExplainedVariance    | 0.00089427 |
-------------------------------------
[2018-01-21 16:49:07.891680 UTC] Saving snapshot
[2018-01-21 16:49:07.891972 UTC] Starting iteration 1626
[2018-01-21 16:49:07.892170 UTC] Start collecting samples
[2018-01-21 16:49:15.109412 UTC] Computing input variables for policy optimization
[2018-01-21 16:49:15.287757 UTC] Performing policy update
[2018-01-21 16:49:15.288490 UTC] Computing gradient in Euclidean space
[2018-01-21 16:49:15.406202 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:49:16.869698 UTC] Performing line search
[2018-01-21 16:49:17.071619 UTC] Updating baseline
[2018-01-21 16:49:20.646104 UTC] Computing logging information
-------------------------------------
| Iteration            | 1626       |
| ExpectedImprovement  | 0.017618   |
| ActualImprovement    | 0.016311   |
| ImprovementRatio     | 0.92586    |
| MeanKL               | 0.007425   |
| Entropy              | -1.8343    |
| Perplexity           | 0.15973    |
| AveragePolicyStd     | 0.18024    |
| AveragePolicyStd[0]  | 0.20403    |
| AveragePolicyStd[1]  | 0.18995    |
| AveragePolicyStd[2]  | 0.1465     |
| AveragePolicyStd[3]  | 0.17251    |
| AveragePolicyStd[4]  | 0.14951    |
| AveragePolicyStd[5]  | 0.21893    |
| AverageReturn        | 1825       |
| MinReturn            | 336.53     |
| MaxReturn            | 2059.5     |
| StdReturn            | 370.59     |
| AverageEpisodeLength | 931.53     |
| MinEpisodeLength     | 203        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.4      |
| TotalNEpisodes       | 25187      |
| TotalNSamples        | 8.1395e+06 |
| ExplainedVariance    | 0.0031456  |
-------------------------------------
[2018-01-21 16:49:21.599980 UTC] Saving snapshot
[2018-01-21 16:49:21.600544 UTC] Starting iteration 1627
[2018-01-21 16:49:21.600837 UTC] Start collecting samples
[2018-01-21 16:49:30.202984 UTC] Computing input variables for policy optimization
[2018-01-21 16:49:30.406708 UTC] Performing policy update
[2018-01-21 16:49:30.407763 UTC] Computing gradient in Euclidean space
[2018-01-21 16:49:30.567353 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:49:32.423674 UTC] Performing line search
[2018-01-21 16:49:32.663024 UTC] Updating baseline
[2018-01-21 16:49:35.809607 UTC] Computing logging information
-------------------------------------
| Iteration            | 1627       |
| ExpectedImprovement  | 0.017964   |
| ActualImprovement    | 0.016923   |
| ImprovementRatio     | 0.94209    |
| MeanKL               | 0.0079542  |
| Entropy              | -1.8339    |
| Perplexity           | 0.15979    |
| AveragePolicyStd     | 0.18022    |
| AveragePolicyStd[0]  | 0.20419    |
| AveragePolicyStd[1]  | 0.18945    |
| AveragePolicyStd[2]  | 0.14682    |
| AveragePolicyStd[3]  | 0.17266    |
| AveragePolicyStd[4]  | 0.14968    |
| AveragePolicyStd[5]  | 0.21851    |
| AverageReturn        | 1862.1     |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 311.29     |
| AverageEpisodeLength | 949.63     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.03     |
| TotalNEpisodes       | 25192      |
| TotalNSamples        | 8.1445e+06 |
| ExplainedVariance    | 0.004975   |
-------------------------------------
[2018-01-21 16:49:36.921020 UTC] Saving snapshot
[2018-01-21 16:49:36.921282 UTC] Starting iteration 1628
[2018-01-21 16:49:36.921445 UTC] Start collecting samples
[2018-01-21 16:49:42.592466 UTC] Computing input variables for policy optimization
[2018-01-21 16:49:42.729232 UTC] Performing policy update
[2018-01-21 16:49:42.730317 UTC] Computing gradient in Euclidean space
[2018-01-21 16:49:42.855578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:49:44.373720 UTC] Performing line search
[2018-01-21 16:49:44.587946 UTC] Updating baseline
[2018-01-21 16:49:46.869211 UTC] Computing logging information
-------------------------------------
| Iteration            | 1628       |
| ExpectedImprovement  | 0.017375   |
| ActualImprovement    | 0.016906   |
| ImprovementRatio     | 0.97303    |
| MeanKL               | 0.0082492  |
| Entropy              | -1.8294    |
| Perplexity           | 0.1605     |
| AveragePolicyStd     | 0.18036    |
| AveragePolicyStd[0]  | 0.20484    |
| AveragePolicyStd[1]  | 0.18991    |
| AveragePolicyStd[2]  | 0.14697    |
| AveragePolicyStd[3]  | 0.17197    |
| AveragePolicyStd[4]  | 0.14994    |
| AveragePolicyStd[5]  | 0.21852    |
| AverageReturn        | 1869       |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 299.06     |
| AverageEpisodeLength | 954.29     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.13     |
| TotalNEpisodes       | 25201      |
| TotalNSamples        | 8.1535e+06 |
| ExplainedVariance    | -0.0032041 |
-------------------------------------
[2018-01-21 16:49:47.740264 UTC] Saving snapshot
[2018-01-21 16:49:47.740537 UTC] Starting iteration 1629
[2018-01-21 16:49:47.740735 UTC] Start collecting samples
[2018-01-21 16:49:53.072278 UTC] Computing input variables for policy optimization
[2018-01-21 16:49:53.249486 UTC] Performing policy update
[2018-01-21 16:49:53.250402 UTC] Computing gradient in Euclidean space
[2018-01-21 16:49:53.380145 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:49:54.767473 UTC] Performing line search
[2018-01-21 16:49:54.960942 UTC] Updating baseline
[2018-01-21 16:49:57.872726 UTC] Computing logging information
-------------------------------------
| Iteration            | 1629       |
| ExpectedImprovement  | 0.017979   |
| ActualImprovement    | 0.016282   |
| ImprovementRatio     | 0.90559    |
| MeanKL               | 0.0083691  |
| Entropy              | -1.8251    |
| Perplexity           | 0.1612     |
| AveragePolicyStd     | 0.18048    |
| AveragePolicyStd[0]  | 0.20497    |
| AveragePolicyStd[1]  | 0.18971    |
| AveragePolicyStd[2]  | 0.14726    |
| AveragePolicyStd[3]  | 0.17241    |
| AveragePolicyStd[4]  | 0.14984    |
| AveragePolicyStd[5]  | 0.21873    |
| AverageReturn        | 1869.3     |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 299.2      |
| AverageEpisodeLength | 954.62     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.2      |
| TotalNEpisodes       | 25203      |
| TotalNSamples        | 8.1555e+06 |
| ExplainedVariance    | 0.0042582  |
-------------------------------------
[2018-01-21 16:49:58.732745 UTC] Saving snapshot
[2018-01-21 16:49:58.733006 UTC] Starting iteration 1630
[2018-01-21 16:49:58.733219 UTC] Start collecting samples
[2018-01-21 16:50:03.947370 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:04.089375 UTC] Performing policy update
[2018-01-21 16:50:04.090835 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:04.233583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:05.712830 UTC] Performing line search
[2018-01-21 16:50:05.940127 UTC] Updating baseline
[2018-01-21 16:50:08.650780 UTC] Computing logging information
-------------------------------------
| Iteration            | 1630       |
| ExpectedImprovement  | 0.017753   |
| ActualImprovement    | 0.016482   |
| ImprovementRatio     | 0.92837    |
| MeanKL               | 0.0082735  |
| Entropy              | -1.8282    |
| Perplexity           | 0.1607     |
| AveragePolicyStd     | 0.18035    |
| AveragePolicyStd[0]  | 0.20434    |
| AveragePolicyStd[1]  | 0.19008    |
| AveragePolicyStd[2]  | 0.14759    |
| AveragePolicyStd[3]  | 0.17206    |
| AveragePolicyStd[4]  | 0.14994    |
| AveragePolicyStd[5]  | 0.2181     |
| AverageReturn        | 1868.7     |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 299.04     |
| AverageEpisodeLength | 954.62     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.2      |
| TotalNEpisodes       | 25206      |
| TotalNSamples        | 8.1585e+06 |
| ExplainedVariance    | 0.0025634  |
-------------------------------------
[2018-01-21 16:50:09.599078 UTC] Saving snapshot
[2018-01-21 16:50:09.608601 UTC] Starting iteration 1631
[2018-01-21 16:50:09.608838 UTC] Start collecting samples
[2018-01-21 16:50:14.783452 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:14.921993 UTC] Performing policy update
[2018-01-21 16:50:14.922728 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:15.040286 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:16.489805 UTC] Performing line search
[2018-01-21 16:50:16.678331 UTC] Updating baseline
[2018-01-21 16:50:18.522020 UTC] Computing logging information
-------------------------------------
| Iteration            | 1631       |
| ExpectedImprovement  | 0.019107   |
| ActualImprovement    | 0.01782    |
| ImprovementRatio     | 0.93265    |
| MeanKL               | 0.0085158  |
| Entropy              | -1.8346    |
| Perplexity           | 0.15967    |
| AveragePolicyStd     | 0.18016    |
| AveragePolicyStd[0]  | 0.20452    |
| AveragePolicyStd[1]  | 0.18953    |
| AveragePolicyStd[2]  | 0.14728    |
| AveragePolicyStd[3]  | 0.17143    |
| AveragePolicyStd[4]  | 0.15024    |
| AveragePolicyStd[5]  | 0.21797    |
| AverageReturn        | 1860.9     |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 302.83     |
| AverageEpisodeLength | 951.63     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.67     |
| TotalNEpisodes       | 25213      |
| TotalNSamples        | 8.1649e+06 |
| ExplainedVariance    | 0.15486    |
-------------------------------------
[2018-01-21 16:50:19.384285 UTC] Saving snapshot
[2018-01-21 16:50:19.384542 UTC] Starting iteration 1632
[2018-01-21 16:50:19.384688 UTC] Start collecting samples
[2018-01-21 16:50:24.685825 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:24.815940 UTC] Performing policy update
[2018-01-21 16:50:24.816537 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:24.939282 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:26.400246 UTC] Performing line search
[2018-01-21 16:50:26.597329 UTC] Updating baseline
[2018-01-21 16:50:28.913931 UTC] Computing logging information
-------------------------------------
| Iteration            | 1632       |
| ExpectedImprovement  | 0.018747   |
| ActualImprovement    | 0.017818   |
| ImprovementRatio     | 0.95045    |
| MeanKL               | 0.0070327  |
| Entropy              | -1.8372    |
| Perplexity           | 0.15926    |
| AveragePolicyStd     | 0.18006    |
| AveragePolicyStd[0]  | 0.20395    |
| AveragePolicyStd[1]  | 0.18966    |
| AveragePolicyStd[2]  | 0.1476     |
| AveragePolicyStd[3]  | 0.17137    |
| AveragePolicyStd[4]  | 0.15009    |
| AveragePolicyStd[5]  | 0.21767    |
| AverageReturn        | 1869.1     |
| MinReturn            | 364.3      |
| MaxReturn            | 2059.5     |
| StdReturn            | 298.24     |
| AverageEpisodeLength | 955.05     |
| MinEpisodeLength     | 214        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 143.57     |
| TotalNEpisodes       | 25219      |
| TotalNSamples        | 8.1709e+06 |
| ExplainedVariance    | 0.0081516  |
-------------------------------------
[2018-01-21 16:50:29.769248 UTC] Saving snapshot
[2018-01-21 16:50:29.769509 UTC] Starting iteration 1633
[2018-01-21 16:50:29.769687 UTC] Start collecting samples
[2018-01-21 16:50:35.189344 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:35.319928 UTC] Performing policy update
[2018-01-21 16:50:35.321087 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:35.456041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:37.059686 UTC] Performing line search
[2018-01-21 16:50:37.250439 UTC] Updating baseline
[2018-01-21 16:50:39.321787 UTC] Computing logging information
-------------------------------------
| Iteration            | 1633       |
| ExpectedImprovement  | 0.018094   |
| ActualImprovement    | 0.017175   |
| ImprovementRatio     | 0.94921    |
| MeanKL               | 0.007627   |
| Entropy              | -1.8405    |
| Perplexity           | 0.15874    |
| AveragePolicyStd     | 0.17998    |
| AveragePolicyStd[0]  | 0.20413    |
| AveragePolicyStd[1]  | 0.18997    |
| AveragePolicyStd[2]  | 0.14695    |
| AveragePolicyStd[3]  | 0.17152    |
| AveragePolicyStd[4]  | 0.15002    |
| AveragePolicyStd[5]  | 0.21727    |
| AverageReturn        | 1874.4     |
| MinReturn            | 628.97     |
| MaxReturn            | 2059.5     |
| StdReturn            | 272.85     |
| AverageEpisodeLength | 957.74     |
| MinEpisodeLength     | 366        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.85     |
| TotalNEpisodes       | 25223      |
| TotalNSamples        | 8.1742e+06 |
| ExplainedVariance    | 0.28292    |
-------------------------------------
[2018-01-21 16:50:40.144755 UTC] Saving snapshot
[2018-01-21 16:50:40.145014 UTC] Starting iteration 1634
[2018-01-21 16:50:40.145179 UTC] Start collecting samples
[2018-01-21 16:50:45.443042 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:45.572667 UTC] Performing policy update
[2018-01-21 16:50:45.573709 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:45.696732 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:47.104090 UTC] Performing line search
[2018-01-21 16:50:47.300116 UTC] Updating baseline
[2018-01-21 16:50:49.279754 UTC] Computing logging information
-------------------------------------
| Iteration            | 1634       |
| ExpectedImprovement  | 0.017729   |
| ActualImprovement    | 0.016993   |
| ImprovementRatio     | 0.9585     |
| MeanKL               | 0.0079291  |
| Entropy              | -1.8471    |
| Perplexity           | 0.15769    |
| AveragePolicyStd     | 0.17978    |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.19002    |
| AveragePolicyStd[2]  | 0.14693    |
| AveragePolicyStd[3]  | 0.17099    |
| AveragePolicyStd[4]  | 0.14989    |
| AveragePolicyStd[5]  | 0.21684    |
| AverageReturn        | 1877.7     |
| MinReturn            | 628.97     |
| MaxReturn            | 2059.5     |
| StdReturn            | 269.65     |
| AverageEpisodeLength | 960.06     |
| MinEpisodeLength     | 366        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.5      |
| TotalNEpisodes       | 25229      |
| TotalNSamples        | 8.1802e+06 |
| ExplainedVariance    | -0.031369  |
-------------------------------------
[2018-01-21 16:50:50.166297 UTC] Saving snapshot
[2018-01-21 16:50:50.166543 UTC] Starting iteration 1635
[2018-01-21 16:50:50.166703 UTC] Start collecting samples
[2018-01-21 16:50:55.469713 UTC] Computing input variables for policy optimization
[2018-01-21 16:50:55.604335 UTC] Performing policy update
[2018-01-21 16:50:55.605446 UTC] Computing gradient in Euclidean space
[2018-01-21 16:50:55.737117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:50:57.236285 UTC] Performing line search
[2018-01-21 16:50:57.449198 UTC] Updating baseline
[2018-01-21 16:50:59.834165 UTC] Computing logging information
-------------------------------------
| Iteration            | 1635       |
| ExpectedImprovement  | 0.016866   |
| ActualImprovement    | 0.01596    |
| ImprovementRatio     | 0.94624    |
| MeanKL               | 0.0077992  |
| Entropy              | -1.8418    |
| Perplexity           | 0.15853    |
| AveragePolicyStd     | 0.17993    |
| AveragePolicyStd[0]  | 0.20408    |
| AveragePolicyStd[1]  | 0.19019    |
| AveragePolicyStd[2]  | 0.1469     |
| AveragePolicyStd[3]  | 0.17122    |
| AveragePolicyStd[4]  | 0.15023    |
| AveragePolicyStd[5]  | 0.21693    |
| AverageReturn        | 1891.6     |
| MinReturn            | 628.97     |
| MaxReturn            | 2016.3     |
| StdReturn            | 236.74     |
| AverageEpisodeLength | 968.29     |
| MinEpisodeLength     | 366        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 112.96     |
| TotalNEpisodes       | 25235      |
| TotalNSamples        | 8.1862e+06 |
| ExplainedVariance    | 0.0029273  |
-------------------------------------
[2018-01-21 16:51:00.688518 UTC] Saving snapshot
[2018-01-21 16:51:00.688803 UTC] Starting iteration 1636
[2018-01-21 16:51:00.688987 UTC] Start collecting samples
[2018-01-21 16:51:05.608035 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:05.746934 UTC] Performing policy update
[2018-01-21 16:51:05.747584 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:05.866655 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:07.279257 UTC] Performing line search
[2018-01-21 16:51:07.480317 UTC] Updating baseline
[2018-01-21 16:51:09.372772 UTC] Computing logging information
-------------------------------------
| Iteration            | 1636       |
| ExpectedImprovement  | 0.021609   |
| ActualImprovement    | 0.019634   |
| ImprovementRatio     | 0.90861    |
| MeanKL               | 0.0074358  |
| Entropy              | -1.8436    |
| Perplexity           | 0.15825    |
| AveragePolicyStd     | 0.17986    |
| AveragePolicyStd[0]  | 0.20383    |
| AveragePolicyStd[1]  | 0.19072    |
| AveragePolicyStd[2]  | 0.14707    |
| AveragePolicyStd[3]  | 0.17113    |
| AveragePolicyStd[4]  | 0.15007    |
| AveragePolicyStd[5]  | 0.21632    |
| AverageReturn        | 1878.5     |
| MinReturn            | 628.97     |
| MaxReturn            | 2016.3     |
| StdReturn            | 265.76     |
| AverageEpisodeLength | 962.15     |
| MinEpisodeLength     | 366        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.9      |
| TotalNEpisodes       | 25239      |
| TotalNSamples        | 8.1896e+06 |
| ExplainedVariance    | 0.13295    |
-------------------------------------
[2018-01-21 16:51:10.260667 UTC] Saving snapshot
[2018-01-21 16:51:10.260917 UTC] Starting iteration 1637
[2018-01-21 16:51:10.261077 UTC] Start collecting samples
[2018-01-21 16:51:15.350708 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:15.497511 UTC] Performing policy update
[2018-01-21 16:51:15.498243 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:15.627927 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:17.080690 UTC] Performing line search
[2018-01-21 16:51:17.344854 UTC] Updating baseline
[2018-01-21 16:51:19.400113 UTC] Computing logging information
-------------------------------------
| Iteration            | 1637       |
| ExpectedImprovement  | 0.018871   |
| ActualImprovement    | 0.017436   |
| ImprovementRatio     | 0.92397    |
| MeanKL               | 0.0080011  |
| Entropy              | -1.8471    |
| Perplexity           | 0.15769    |
| AveragePolicyStd     | 0.17977    |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.19087    |
| AveragePolicyStd[2]  | 0.14703    |
| AveragePolicyStd[3]  | 0.17066    |
| AveragePolicyStd[4]  | 0.14986    |
| AveragePolicyStd[5]  | 0.21652    |
| AverageReturn        | 1889       |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 233.73     |
| AverageEpisodeLength | 968.49     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.91     |
| TotalNEpisodes       | 25246      |
| TotalNSamples        | 8.1966e+06 |
| ExplainedVariance    | -0.0015671 |
-------------------------------------
[2018-01-21 16:51:20.212135 UTC] Saving snapshot
[2018-01-21 16:51:20.212360 UTC] Starting iteration 1638
[2018-01-21 16:51:20.212532 UTC] Start collecting samples
[2018-01-21 16:51:25.696069 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:25.838228 UTC] Performing policy update
[2018-01-21 16:51:25.839357 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:25.964098 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:27.552695 UTC] Performing line search
[2018-01-21 16:51:27.759195 UTC] Updating baseline
[2018-01-21 16:51:30.764308 UTC] Computing logging information
-------------------------------------
| Iteration            | 1638       |
| ExpectedImprovement  | 0.017592   |
| ActualImprovement    | 0.016626   |
| ImprovementRatio     | 0.94507    |
| MeanKL               | 0.0080949  |
| Entropy              | -1.85      |
| Perplexity           | 0.15724    |
| AveragePolicyStd     | 0.17967    |
| AveragePolicyStd[0]  | 0.20376    |
| AveragePolicyStd[1]  | 0.19065    |
| AveragePolicyStd[2]  | 0.14732    |
| AveragePolicyStd[3]  | 0.17041    |
| AveragePolicyStd[4]  | 0.14975    |
| AveragePolicyStd[5]  | 0.21612    |
| AverageReturn        | 1888       |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 233.36     |
| AverageEpisodeLength | 968.49     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.91     |
| TotalNEpisodes       | 25250      |
| TotalNSamples        | 8.2006e+06 |
| ExplainedVariance    | 0.0081585  |
-------------------------------------
[2018-01-21 16:51:31.611463 UTC] Saving snapshot
[2018-01-21 16:51:31.611715 UTC] Starting iteration 1639
[2018-01-21 16:51:31.611899 UTC] Start collecting samples
[2018-01-21 16:51:36.944328 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:37.084350 UTC] Performing policy update
[2018-01-21 16:51:37.085390 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:37.224354 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:38.698894 UTC] Performing line search
[2018-01-21 16:51:38.906401 UTC] Updating baseline
[2018-01-21 16:51:41.912239 UTC] Computing logging information
-------------------------------------
| Iteration            | 1639       |
| ExpectedImprovement  | 0.020426   |
| ActualImprovement    | 0.018302   |
| ImprovementRatio     | 0.89601    |
| MeanKL               | 0.0073785  |
| Entropy              | -1.8486    |
| Perplexity           | 0.15746    |
| AveragePolicyStd     | 0.17972    |
| AveragePolicyStd[0]  | 0.20382    |
| AveragePolicyStd[1]  | 0.19114    |
| AveragePolicyStd[2]  | 0.14724    |
| AveragePolicyStd[3]  | 0.17042    |
| AveragePolicyStd[4]  | 0.1496     |
| AveragePolicyStd[5]  | 0.21612    |
| AverageReturn        | 1894.3     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 214.63     |
| AverageEpisodeLength | 971.76     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 102.21     |
| TotalNEpisodes       | 25253      |
| TotalNSamples        | 8.2034e+06 |
| ExplainedVariance    | 0.15961    |
-------------------------------------
[2018-01-21 16:51:42.835980 UTC] Saving snapshot
[2018-01-21 16:51:42.836303 UTC] Starting iteration 1640
[2018-01-21 16:51:42.836524 UTC] Start collecting samples
[2018-01-21 16:51:47.525353 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:47.742088 UTC] Performing policy update
[2018-01-21 16:51:47.743065 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:47.877333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:49.311226 UTC] Performing line search
[2018-01-21 16:51:49.501304 UTC] Updating baseline
[2018-01-21 16:51:51.326309 UTC] Computing logging information
-------------------------------------
| Iteration            | 1640       |
| ExpectedImprovement  | 0.018121   |
| ActualImprovement    | 0.017346   |
| ImprovementRatio     | 0.95723    |
| MeanKL               | 0.008437   |
| Entropy              | -1.8481    |
| Perplexity           | 0.15753    |
| AveragePolicyStd     | 0.17974    |
| AveragePolicyStd[0]  | 0.20398    |
| AveragePolicyStd[1]  | 0.19143    |
| AveragePolicyStd[2]  | 0.14717    |
| AveragePolicyStd[3]  | 0.17073    |
| AveragePolicyStd[4]  | 0.14935    |
| AveragePolicyStd[5]  | 0.21579    |
| AverageReturn        | 1900.6     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 201.7      |
| AverageEpisodeLength | 975.47     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.261     |
| TotalNEpisodes       | 25261      |
| TotalNSamples        | 8.2114e+06 |
| ExplainedVariance    | 0.0048836  |
-------------------------------------
[2018-01-21 16:51:52.152733 UTC] Saving snapshot
[2018-01-21 16:51:52.159332 UTC] Starting iteration 1641
[2018-01-21 16:51:52.159564 UTC] Start collecting samples
[2018-01-21 16:51:57.044836 UTC] Computing input variables for policy optimization
[2018-01-21 16:51:57.171869 UTC] Performing policy update
[2018-01-21 16:51:57.172843 UTC] Computing gradient in Euclidean space
[2018-01-21 16:51:57.292700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:51:58.862699 UTC] Performing line search
[2018-01-21 16:51:59.058232 UTC] Updating baseline
[2018-01-21 16:52:00.779098 UTC] Computing logging information
-------------------------------------
| Iteration            | 1641       |
| ExpectedImprovement  | 0.018567   |
| ActualImprovement    | 0.017103   |
| ImprovementRatio     | 0.92116    |
| MeanKL               | 0.0083504  |
| Entropy              | -1.8446    |
| Perplexity           | 0.15808    |
| AveragePolicyStd     | 0.17984    |
| AveragePolicyStd[0]  | 0.20371    |
| AveragePolicyStd[1]  | 0.1918     |
| AveragePolicyStd[2]  | 0.14715    |
| AveragePolicyStd[3]  | 0.17076    |
| AveragePolicyStd[4]  | 0.14969    |
| AveragePolicyStd[5]  | 0.2159     |
| AverageReturn        | 1899.5     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 201.63     |
| AverageEpisodeLength | 975.47     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.261     |
| TotalNEpisodes       | 25266      |
| TotalNSamples        | 8.2164e+06 |
| ExplainedVariance    | -0.0018207 |
-------------------------------------
[2018-01-21 16:52:01.698544 UTC] Saving snapshot
[2018-01-21 16:52:01.698780 UTC] Starting iteration 1642
[2018-01-21 16:52:01.698930 UTC] Start collecting samples
[2018-01-21 16:52:06.387144 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:06.545519 UTC] Performing policy update
[2018-01-21 16:52:06.546678 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:06.675748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:08.094499 UTC] Performing line search
[2018-01-21 16:52:08.289432 UTC] Updating baseline
[2018-01-21 16:52:10.815228 UTC] Computing logging information
-------------------------------------
| Iteration            | 1642       |
| ExpectedImprovement  | 0.018135   |
| ActualImprovement    | 0.017134   |
| ImprovementRatio     | 0.94483    |
| MeanKL               | 0.0089018  |
| Entropy              | -1.8443    |
| Perplexity           | 0.15814    |
| AveragePolicyStd     | 0.17985    |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.19168    |
| AveragePolicyStd[2]  | 0.14703    |
| AveragePolicyStd[3]  | 0.17099    |
| AveragePolicyStd[4]  | 0.14966    |
| AveragePolicyStd[5]  | 0.21609    |
| AverageReturn        | 1895.9     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 202.51     |
| AverageEpisodeLength | 974.01     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 96.982     |
| TotalNEpisodes       | 25270      |
| TotalNSamples        | 8.2203e+06 |
| ExplainedVariance    | 0.10986    |
-------------------------------------
[2018-01-21 16:52:11.682545 UTC] Saving snapshot
[2018-01-21 16:52:11.682766 UTC] Starting iteration 1643
[2018-01-21 16:52:11.682947 UTC] Start collecting samples
[2018-01-21 16:52:16.603857 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:16.729364 UTC] Performing policy update
[2018-01-21 16:52:16.729997 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:16.859508 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:18.339182 UTC] Performing line search
[2018-01-21 16:52:18.546056 UTC] Updating baseline
[2018-01-21 16:52:20.714842 UTC] Computing logging information
-------------------------------------
| Iteration            | 1643       |
| ExpectedImprovement  | 0.018544   |
| ActualImprovement    | 0.017294   |
| ImprovementRatio     | 0.93256    |
| MeanKL               | 0.007575   |
| Entropy              | -1.8456    |
| Perplexity           | 0.15792    |
| AveragePolicyStd     | 0.17981    |
| AveragePolicyStd[0]  | 0.2035     |
| AveragePolicyStd[1]  | 0.19165    |
| AveragePolicyStd[2]  | 0.14698    |
| AveragePolicyStd[3]  | 0.17084    |
| AveragePolicyStd[4]  | 0.14975    |
| AveragePolicyStd[5]  | 0.21613    |
| AverageReturn        | 1895.9     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 200.72     |
| AverageEpisodeLength | 974.38     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 95.611     |
| TotalNEpisodes       | 25276      |
| TotalNSamples        | 8.2259e+06 |
| ExplainedVariance    | 0.097757   |
-------------------------------------
[2018-01-21 16:52:21.558610 UTC] Saving snapshot
[2018-01-21 16:52:21.558869 UTC] Starting iteration 1644
[2018-01-21 16:52:21.559041 UTC] Start collecting samples
[2018-01-21 16:52:26.902895 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:27.032287 UTC] Performing policy update
[2018-01-21 16:52:27.033404 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:27.156960 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:28.600747 UTC] Performing line search
[2018-01-21 16:52:28.799576 UTC] Updating baseline
[2018-01-21 16:52:31.462275 UTC] Computing logging information
-------------------------------------
| Iteration            | 1644       |
| ExpectedImprovement  | 0.017552   |
| ActualImprovement    | 0.016827   |
| ImprovementRatio     | 0.95872    |
| MeanKL               | 0.0075977  |
| Entropy              | -1.8528    |
| Perplexity           | 0.15679    |
| AveragePolicyStd     | 0.17962    |
| AveragePolicyStd[0]  | 0.20299    |
| AveragePolicyStd[1]  | 0.19153    |
| AveragePolicyStd[2]  | 0.14678    |
| AveragePolicyStd[3]  | 0.16996    |
| AveragePolicyStd[4]  | 0.14967    |
| AveragePolicyStd[5]  | 0.21678    |
| AverageReturn        | 1896.3     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 200.87     |
| AverageEpisodeLength | 974.38     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 95.611     |
| TotalNEpisodes       | 25282      |
| TotalNSamples        | 8.2319e+06 |
| ExplainedVariance    | 0.0035909  |
-------------------------------------
[2018-01-21 16:52:32.416926 UTC] Saving snapshot
[2018-01-21 16:52:32.417240 UTC] Starting iteration 1645
[2018-01-21 16:52:32.417402 UTC] Start collecting samples
[2018-01-21 16:52:37.605274 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:37.772192 UTC] Performing policy update
[2018-01-21 16:52:37.772782 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:37.899366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:39.303422 UTC] Performing line search
[2018-01-21 16:52:39.508425 UTC] Updating baseline
[2018-01-21 16:52:41.639349 UTC] Computing logging information
-------------------------------------
| Iteration            | 1645       |
| ExpectedImprovement  | 0.017778   |
| ActualImprovement    | 0.016625   |
| ImprovementRatio     | 0.93517    |
| MeanKL               | 0.0075339  |
| Entropy              | -1.8517    |
| Perplexity           | 0.15697    |
| AveragePolicyStd     | 0.17967    |
| AveragePolicyStd[0]  | 0.2028     |
| AveragePolicyStd[1]  | 0.19186    |
| AveragePolicyStd[2]  | 0.14681    |
| AveragePolicyStd[3]  | 0.1698     |
| AveragePolicyStd[4]  | 0.14956    |
| AveragePolicyStd[5]  | 0.21719    |
| AverageReturn        | 1885.4     |
| MinReturn            | 671.88     |
| MaxReturn            | 2016.3     |
| StdReturn            | 227.62     |
| AverageEpisodeLength | 968.94     |
| MinEpisodeLength     | 386        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.59     |
| TotalNEpisodes       | 25286      |
| TotalNSamples        | 8.2354e+06 |
| ExplainedVariance    | 0.15857    |
-------------------------------------
[2018-01-21 16:52:42.508976 UTC] Saving snapshot
[2018-01-21 16:52:42.509222 UTC] Starting iteration 1646
[2018-01-21 16:52:42.509383 UTC] Start collecting samples
[2018-01-21 16:52:47.999482 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:48.124976 UTC] Performing policy update
[2018-01-21 16:52:48.125572 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:48.240769 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:49.765143 UTC] Performing line search
[2018-01-21 16:52:49.975953 UTC] Updating baseline
[2018-01-21 16:52:52.098225 UTC] Computing logging information
-------------------------------------
| Iteration            | 1646       |
| ExpectedImprovement  | 0.016704   |
| ActualImprovement    | 0.015642   |
| ImprovementRatio     | 0.93643    |
| MeanKL               | 0.008042   |
| Entropy              | -1.8572    |
| Perplexity           | 0.1561     |
| AveragePolicyStd     | 0.17955    |
| AveragePolicyStd[0]  | 0.20312    |
| AveragePolicyStd[1]  | 0.19093    |
| AveragePolicyStd[2]  | 0.14626    |
| AveragePolicyStd[3]  | 0.16957    |
| AveragePolicyStd[4]  | 0.14946    |
| AveragePolicyStd[5]  | 0.21795    |
| AverageReturn        | 1870.8     |
| MinReturn            | 481.36     |
| MaxReturn            | 2016.3     |
| StdReturn            | 267        |
| AverageEpisodeLength | 961.86     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.73     |
| TotalNEpisodes       | 25291      |
| TotalNSamples        | 8.2396e+06 |
| ExplainedVariance    | 0.12888    |
-------------------------------------
[2018-01-21 16:52:52.991645 UTC] Saving snapshot
[2018-01-21 16:52:52.991909 UTC] Starting iteration 1647
[2018-01-21 16:52:52.992156 UTC] Start collecting samples
[2018-01-21 16:52:58.151732 UTC] Computing input variables for policy optimization
[2018-01-21 16:52:58.283541 UTC] Performing policy update
[2018-01-21 16:52:58.284435 UTC] Computing gradient in Euclidean space
[2018-01-21 16:52:58.406516 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:52:59.995572 UTC] Performing line search
[2018-01-21 16:53:00.228064 UTC] Updating baseline
[2018-01-21 16:53:02.289517 UTC] Computing logging information
-------------------------------------
| Iteration            | 1647       |
| ExpectedImprovement  | 0.017298   |
| ActualImprovement    | 0.016549   |
| ImprovementRatio     | 0.95673    |
| MeanKL               | 0.0082737  |
| Entropy              | -1.8548    |
| Perplexity           | 0.15648    |
| AveragePolicyStd     | 0.17964    |
| AveragePolicyStd[0]  | 0.20307    |
| AveragePolicyStd[1]  | 0.19116    |
| AveragePolicyStd[2]  | 0.14618    |
| AveragePolicyStd[3]  | 0.16939    |
| AveragePolicyStd[4]  | 0.1495     |
| AveragePolicyStd[5]  | 0.21856    |
| AverageReturn        | 1860.3     |
| MinReturn            | 481.36     |
| MaxReturn            | 2016.3     |
| StdReturn            | 283.15     |
| AverageEpisodeLength | 956.77     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.98     |
| TotalNEpisodes       | 25297      |
| TotalNSamples        | 8.2451e+06 |
| ExplainedVariance    | 0.11121    |
-------------------------------------
[2018-01-21 16:53:03.162218 UTC] Saving snapshot
[2018-01-21 16:53:03.162787 UTC] Starting iteration 1648
[2018-01-21 16:53:03.162935 UTC] Start collecting samples
[2018-01-21 16:53:08.330794 UTC] Computing input variables for policy optimization
[2018-01-21 16:53:08.501279 UTC] Performing policy update
[2018-01-21 16:53:08.501934 UTC] Computing gradient in Euclidean space
[2018-01-21 16:53:08.623647 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:53:10.153883 UTC] Performing line search
[2018-01-21 16:53:10.346998 UTC] Updating baseline
[2018-01-21 16:53:13.320875 UTC] Computing logging information
-------------------------------------
| Iteration            | 1648       |
| ExpectedImprovement  | 0.016485   |
| ActualImprovement    | 0.015773   |
| ImprovementRatio     | 0.95681    |
| MeanKL               | 0.0085722  |
| Entropy              | -1.8652    |
| Perplexity           | 0.15486    |
| AveragePolicyStd     | 0.17932    |
| AveragePolicyStd[0]  | 0.20291    |
| AveragePolicyStd[1]  | 0.19066    |
| AveragePolicyStd[2]  | 0.14628    |
| AveragePolicyStd[3]  | 0.16877    |
| AveragePolicyStd[4]  | 0.14921    |
| AveragePolicyStd[5]  | 0.2181     |
| AverageReturn        | 1861.6     |
| MinReturn            | 481.36     |
| MaxReturn            | 2016.3     |
| StdReturn            | 283.59     |
| AverageEpisodeLength | 956.77     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.98     |
| TotalNEpisodes       | 25302      |
| TotalNSamples        | 8.2501e+06 |
| ExplainedVariance    | 0.019964   |
-------------------------------------
[2018-01-21 16:53:14.202575 UTC] Saving snapshot
[2018-01-21 16:53:14.202888 UTC] Starting iteration 1649
[2018-01-21 16:53:14.203097 UTC] Start collecting samples
[2018-01-21 16:53:19.770828 UTC] Computing input variables for policy optimization
[2018-01-21 16:53:19.915989 UTC] Performing policy update
[2018-01-21 16:53:19.917068 UTC] Computing gradient in Euclidean space
[2018-01-21 16:53:20.043507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:53:21.653035 UTC] Performing line search
[2018-01-21 16:53:21.842585 UTC] Updating baseline
[2018-01-21 16:53:25.175519 UTC] Computing logging information
-------------------------------------
| Iteration            | 1649       |
| ExpectedImprovement  | 0.0171     |
| ActualImprovement    | 0.015775   |
| ImprovementRatio     | 0.92254    |
| MeanKL               | 0.0087711  |
| Entropy              | -1.8586    |
| Perplexity           | 0.15589    |
| AveragePolicyStd     | 0.17956    |
| AveragePolicyStd[0]  | 0.20312    |
| AveragePolicyStd[1]  | 0.19108    |
| AveragePolicyStd[2]  | 0.14626    |
| AveragePolicyStd[3]  | 0.16898    |
| AveragePolicyStd[4]  | 0.14899    |
| AveragePolicyStd[5]  | 0.21894    |
| AverageReturn        | 1861.2     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 283.41     |
| AverageEpisodeLength | 956.77     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.98     |
| TotalNEpisodes       | 25305      |
| TotalNSamples        | 8.2531e+06 |
| ExplainedVariance    | 0.0056753  |
-------------------------------------
[2018-01-21 16:53:26.130024 UTC] Saving snapshot
[2018-01-21 16:53:26.130384 UTC] Starting iteration 1650
[2018-01-21 16:53:26.130687 UTC] Start collecting samples
[2018-01-21 16:53:32.604814 UTC] Computing input variables for policy optimization
[2018-01-21 16:53:32.754703 UTC] Performing policy update
[2018-01-21 16:53:32.755771 UTC] Computing gradient in Euclidean space
[2018-01-21 16:53:32.915730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:53:34.553102 UTC] Performing line search
[2018-01-21 16:53:34.769710 UTC] Updating baseline
[2018-01-21 16:53:36.643016 UTC] Computing logging information
-------------------------------------
| Iteration            | 1650       |
| ExpectedImprovement  | 0.019001   |
| ActualImprovement    | 0.01825    |
| ImprovementRatio     | 0.96049    |
| MeanKL               | 0.0084254  |
| Entropy              | -1.8597    |
| Perplexity           | 0.15572    |
| AveragePolicyStd     | 0.17952    |
| AveragePolicyStd[0]  | 0.20347    |
| AveragePolicyStd[1]  | 0.19124    |
| AveragePolicyStd[2]  | 0.14618    |
| AveragePolicyStd[3]  | 0.16893    |
| AveragePolicyStd[4]  | 0.14896    |
| AveragePolicyStd[5]  | 0.21837    |
| AverageReturn        | 1872.3     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 275.82     |
| AverageEpisodeLength | 962.1      |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.84     |
| TotalNEpisodes       | 25313      |
| TotalNSamples        | 8.2611e+06 |
| ExplainedVariance    | 0.038196   |
-------------------------------------
[2018-01-21 16:53:37.570576 UTC] Saving snapshot
[2018-01-21 16:53:37.584178 UTC] Starting iteration 1651
[2018-01-21 16:53:37.584453 UTC] Start collecting samples
[2018-01-21 16:53:43.976541 UTC] Computing input variables for policy optimization
[2018-01-21 16:53:44.111787 UTC] Performing policy update
[2018-01-21 16:53:44.112398 UTC] Computing gradient in Euclidean space
[2018-01-21 16:53:44.247831 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:53:45.812321 UTC] Performing line search
[2018-01-21 16:53:46.015631 UTC] Updating baseline
[2018-01-21 16:53:48.161903 UTC] Computing logging information
-------------------------------------
| Iteration            | 1651       |
| ExpectedImprovement  | 0.018524   |
| ActualImprovement    | 0.01668    |
| ImprovementRatio     | 0.90043    |
| MeanKL               | 0.0078586  |
| Entropy              | -1.8514    |
| Perplexity           | 0.15702    |
| AveragePolicyStd     | 0.17976    |
| AveragePolicyStd[0]  | 0.2039     |
| AveragePolicyStd[1]  | 0.19103    |
| AveragePolicyStd[2]  | 0.14627    |
| AveragePolicyStd[3]  | 0.16919    |
| AveragePolicyStd[4]  | 0.14951    |
| AveragePolicyStd[5]  | 0.21869    |
| AverageReturn        | 1865.8     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 280.51     |
| AverageEpisodeLength | 959.15     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.23     |
| TotalNEpisodes       | 25318      |
| TotalNSamples        | 8.2658e+06 |
| ExplainedVariance    | 0.024514   |
-------------------------------------
[2018-01-21 16:53:49.071511 UTC] Saving snapshot
[2018-01-21 16:53:49.071755 UTC] Starting iteration 1652
[2018-01-21 16:53:49.071926 UTC] Start collecting samples
[2018-01-21 16:53:54.866218 UTC] Computing input variables for policy optimization
[2018-01-21 16:53:55.017239 UTC] Performing policy update
[2018-01-21 16:53:55.018007 UTC] Computing gradient in Euclidean space
[2018-01-21 16:53:55.165156 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:53:56.680909 UTC] Performing line search
[2018-01-21 16:53:56.955519 UTC] Updating baseline
[2018-01-21 16:54:00.376057 UTC] Computing logging information
-------------------------------------
| Iteration            | 1652       |
| ExpectedImprovement  | 0.017493   |
| ActualImprovement    | 0.016567   |
| ImprovementRatio     | 0.94706    |
| MeanKL               | 0.0081122  |
| Entropy              | -1.8505    |
| Perplexity           | 0.15716    |
| AveragePolicyStd     | 0.17981    |
| AveragePolicyStd[0]  | 0.20422    |
| AveragePolicyStd[1]  | 0.19075    |
| AveragePolicyStd[2]  | 0.14601    |
| AveragePolicyStd[3]  | 0.16939    |
| AveragePolicyStd[4]  | 0.14957    |
| AveragePolicyStd[5]  | 0.21889    |
| AverageReturn        | 1865.5     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 280.4      |
| AverageEpisodeLength | 959.15     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.23     |
| TotalNEpisodes       | 25320      |
| TotalNSamples        | 8.2678e+06 |
| ExplainedVariance    | 0.0077299  |
-------------------------------------
[2018-01-21 16:54:01.488939 UTC] Saving snapshot
[2018-01-21 16:54:01.489300 UTC] Starting iteration 1653
[2018-01-21 16:54:01.489548 UTC] Start collecting samples
[2018-01-21 16:54:07.502893 UTC] Computing input variables for policy optimization
[2018-01-21 16:54:07.638467 UTC] Performing policy update
[2018-01-21 16:54:07.639451 UTC] Computing gradient in Euclidean space
[2018-01-21 16:54:07.812415 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:54:09.403974 UTC] Performing line search
[2018-01-21 16:54:09.626668 UTC] Updating baseline
[2018-01-21 16:54:11.851309 UTC] Computing logging information
-------------------------------------
| Iteration            | 1653       |
| ExpectedImprovement  | 0.017774   |
| ActualImprovement    | 0.016955   |
| ImprovementRatio     | 0.95392    |
| MeanKL               | 0.0078312  |
| Entropy              | -1.8514    |
| Perplexity           | 0.15702    |
| AveragePolicyStd     | 0.17974    |
| AveragePolicyStd[0]  | 0.20428    |
| AveragePolicyStd[1]  | 0.1905     |
| AveragePolicyStd[2]  | 0.14644    |
| AveragePolicyStd[3]  | 0.16956    |
| AveragePolicyStd[4]  | 0.14945    |
| AveragePolicyStd[5]  | 0.21824    |
| AverageReturn        | 1880.3     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 263.04     |
| AverageEpisodeLength | 966.36     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.29     |
| TotalNEpisodes       | 25329      |
| TotalNSamples        | 8.2768e+06 |
| ExplainedVariance    | -0.0051412 |
-------------------------------------
[2018-01-21 16:54:12.758697 UTC] Saving snapshot
[2018-01-21 16:54:12.759029 UTC] Starting iteration 1654
[2018-01-21 16:54:12.759265 UTC] Start collecting samples
[2018-01-21 16:54:18.510280 UTC] Computing input variables for policy optimization
[2018-01-21 16:54:18.685858 UTC] Performing policy update
[2018-01-21 16:54:18.686810 UTC] Computing gradient in Euclidean space
[2018-01-21 16:54:18.806100 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:54:20.818400 UTC] Performing line search
[2018-01-21 16:54:21.126118 UTC] Updating baseline
[2018-01-21 16:54:24.142221 UTC] Computing logging information
-------------------------------------
| Iteration            | 1654       |
| ExpectedImprovement  | 0.019352   |
| ActualImprovement    | 0.017869   |
| ImprovementRatio     | 0.92337    |
| MeanKL               | 0.0075219  |
| Entropy              | -1.8505    |
| Perplexity           | 0.15715    |
| AveragePolicyStd     | 0.17976    |
| AveragePolicyStd[0]  | 0.20412    |
| AveragePolicyStd[1]  | 0.19059    |
| AveragePolicyStd[2]  | 0.14681    |
| AveragePolicyStd[3]  | 0.16969    |
| AveragePolicyStd[4]  | 0.14917    |
| AveragePolicyStd[5]  | 0.21818    |
| AverageReturn        | 1882.1     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 263.4      |
| AverageEpisodeLength | 966.36     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.29     |
| TotalNEpisodes       | 25334      |
| TotalNSamples        | 8.2818e+06 |
| ExplainedVariance    | 0.035586   |
-------------------------------------
[2018-01-21 16:54:25.112043 UTC] Saving snapshot
[2018-01-21 16:54:25.112318 UTC] Starting iteration 1655
[2018-01-21 16:54:25.112508 UTC] Start collecting samples
[2018-01-21 16:54:32.683479 UTC] Computing input variables for policy optimization
[2018-01-21 16:54:32.868027 UTC] Performing policy update
[2018-01-21 16:54:32.868732 UTC] Computing gradient in Euclidean space
[2018-01-21 16:54:32.999750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:54:34.830827 UTC] Performing line search
[2018-01-21 16:54:35.157463 UTC] Updating baseline
[2018-01-21 16:54:37.722659 UTC] Computing logging information
-------------------------------------
| Iteration            | 1655       |
| ExpectedImprovement  | 0.022204   |
| ActualImprovement    | 0.019616   |
| ImprovementRatio     | 0.88341    |
| MeanKL               | 0.0086176  |
| Entropy              | -1.8507    |
| Perplexity           | 0.15713    |
| AveragePolicyStd     | 0.17977    |
| AveragePolicyStd[0]  | 0.20407    |
| AveragePolicyStd[1]  | 0.19068    |
| AveragePolicyStd[2]  | 0.14684    |
| AveragePolicyStd[3]  | 0.16973    |
| AveragePolicyStd[4]  | 0.14895    |
| AveragePolicyStd[5]  | 0.21832    |
| AverageReturn        | 1880.1     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 252.71     |
| AverageEpisodeLength | 965.46     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.88     |
| TotalNEpisodes       | 25337      |
| TotalNSamples        | 8.2841e+06 |
| ExplainedVariance    | 0.42663    |
-------------------------------------
[2018-01-21 16:54:38.633439 UTC] Saving snapshot
[2018-01-21 16:54:38.633701 UTC] Starting iteration 1656
[2018-01-21 16:54:38.633884 UTC] Start collecting samples
[2018-01-21 16:54:44.634231 UTC] Computing input variables for policy optimization
[2018-01-21 16:54:44.764206 UTC] Performing policy update
[2018-01-21 16:54:44.764810 UTC] Computing gradient in Euclidean space
[2018-01-21 16:54:44.884617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:54:46.330919 UTC] Performing line search
[2018-01-21 16:54:46.527997 UTC] Updating baseline
[2018-01-21 16:54:48.508734 UTC] Computing logging information
--------------------------------------
| Iteration            | 1656        |
| ExpectedImprovement  | 0.018637    |
| ActualImprovement    | 0.01746     |
| ImprovementRatio     | 0.93685     |
| MeanKL               | 0.0092441   |
| Entropy              | -1.8558     |
| Perplexity           | 0.15632     |
| AveragePolicyStd     | 0.17961     |
| AveragePolicyStd[0]  | 0.20407     |
| AveragePolicyStd[1]  | 0.19029     |
| AveragePolicyStd[2]  | 0.14656     |
| AveragePolicyStd[3]  | 0.16972     |
| AveragePolicyStd[4]  | 0.14896     |
| AveragePolicyStd[5]  | 0.21806     |
| AverageReturn        | 1882.4      |
| MinReturn            | 481.36      |
| MaxReturn            | 2010.2      |
| StdReturn            | 253.24      |
| AverageEpisodeLength | 965.46      |
| MinEpisodeLength     | 292         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 120.88      |
| TotalNEpisodes       | 25343       |
| TotalNSamples        | 8.2901e+06  |
| ExplainedVariance    | -0.00015331 |
--------------------------------------
[2018-01-21 16:54:49.373812 UTC] Saving snapshot
[2018-01-21 16:54:49.374143 UTC] Starting iteration 1657
[2018-01-21 16:54:49.374306 UTC] Start collecting samples
[2018-01-21 16:54:54.892734 UTC] Computing input variables for policy optimization
[2018-01-21 16:54:55.060825 UTC] Performing policy update
[2018-01-21 16:54:55.061830 UTC] Computing gradient in Euclidean space
[2018-01-21 16:54:55.181525 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:54:56.720452 UTC] Performing line search
[2018-01-21 16:54:56.916985 UTC] Updating baseline
[2018-01-21 16:54:59.625472 UTC] Computing logging information
-------------------------------------
| Iteration            | 1657       |
| ExpectedImprovement  | 0.016311   |
| ActualImprovement    | 0.015841   |
| ImprovementRatio     | 0.9712     |
| MeanKL               | 0.0084808  |
| Entropy              | -1.8567    |
| Perplexity           | 0.15619    |
| AveragePolicyStd     | 0.17957    |
| AveragePolicyStd[0]  | 0.20339    |
| AveragePolicyStd[1]  | 0.19029    |
| AveragePolicyStd[2]  | 0.1468     |
| AveragePolicyStd[3]  | 0.16982    |
| AveragePolicyStd[4]  | 0.14895    |
| AveragePolicyStd[5]  | 0.21814    |
| AverageReturn        | 1884.5     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 253.77     |
| AverageEpisodeLength | 965.46     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.88     |
| TotalNEpisodes       | 25350      |
| TotalNSamples        | 8.2971e+06 |
| ExplainedVariance    | -0.0086344 |
-------------------------------------
[2018-01-21 16:55:00.554571 UTC] Saving snapshot
[2018-01-21 16:55:00.554866 UTC] Starting iteration 1658
[2018-01-21 16:55:00.555056 UTC] Start collecting samples
[2018-01-21 16:55:06.079298 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:06.218136 UTC] Performing policy update
[2018-01-21 16:55:06.218758 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:06.344304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:55:07.790234 UTC] Performing line search
[2018-01-21 16:55:08.002254 UTC] Updating baseline
[2018-01-21 16:55:10.141195 UTC] Computing logging information
-------------------------------------
| Iteration            | 1658       |
| ExpectedImprovement  | 0.018826   |
| ActualImprovement    | 0.017144   |
| ImprovementRatio     | 0.91066    |
| MeanKL               | 0.0075181  |
| Entropy              | -1.8569    |
| Perplexity           | 0.15616    |
| AveragePolicyStd     | 0.17954    |
| AveragePolicyStd[0]  | 0.20284    |
| AveragePolicyStd[1]  | 0.19107    |
| AveragePolicyStd[2]  | 0.14679    |
| AveragePolicyStd[3]  | 0.17029    |
| AveragePolicyStd[4]  | 0.14877    |
| AveragePolicyStd[5]  | 0.21746    |
| AverageReturn        | 1879       |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 266.8      |
| AverageEpisodeLength | 962.85     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 126.75     |
| TotalNEpisodes       | 25354      |
| TotalNSamples        | 8.3007e+06 |
| ExplainedVariance    | 0.14629    |
-------------------------------------
[2018-01-21 16:55:11.028411 UTC] Saving snapshot
[2018-01-21 16:55:11.028623 UTC] Starting iteration 1659
[2018-01-21 16:55:11.028799 UTC] Start collecting samples
[2018-01-21 16:55:16.727603 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:16.870468 UTC] Performing policy update
[2018-01-21 16:55:16.871117 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:17.028942 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:55:18.572362 UTC] Performing line search
[2018-01-21 16:55:18.813071 UTC] Updating baseline
[2018-01-21 16:55:20.646541 UTC] Computing logging information
-------------------------------------
| Iteration            | 1659       |
| ExpectedImprovement  | 0.01854    |
| ActualImprovement    | 0.017423   |
| ImprovementRatio     | 0.93976    |
| MeanKL               | 0.0082104  |
| Entropy              | -1.856     |
| Perplexity           | 0.15629    |
| AveragePolicyStd     | 0.17958    |
| AveragePolicyStd[0]  | 0.20292    |
| AveragePolicyStd[1]  | 0.19158    |
| AveragePolicyStd[2]  | 0.14654    |
| AveragePolicyStd[3]  | 0.1704     |
| AveragePolicyStd[4]  | 0.14866    |
| AveragePolicyStd[5]  | 0.21737    |
| AverageReturn        | 1869       |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 278.71     |
| AverageEpisodeLength | 957.55     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.38     |
| TotalNEpisodes       | 25358      |
| TotalNSamples        | 8.3042e+06 |
| ExplainedVariance    | 0.29151    |
-------------------------------------
[2018-01-21 16:55:21.535251 UTC] Saving snapshot
[2018-01-21 16:55:21.535740 UTC] Starting iteration 1660
[2018-01-21 16:55:21.535933 UTC] Start collecting samples
[2018-01-21 16:55:27.163510 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:27.313769 UTC] Performing policy update
[2018-01-21 16:55:27.314409 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:27.451423 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:55:28.901485 UTC] Performing line search
[2018-01-21 16:55:29.103468 UTC] Updating baseline
[2018-01-21 16:55:30.850335 UTC] Computing logging information
-------------------------------------
| Iteration            | 1660       |
| ExpectedImprovement  | 0.018792   |
| ActualImprovement    | 0.018034   |
| ImprovementRatio     | 0.95962    |
| MeanKL               | 0.0087325  |
| Entropy              | -1.8565    |
| Perplexity           | 0.15622    |
| AveragePolicyStd     | 0.17959    |
| AveragePolicyStd[0]  | 0.20306    |
| AveragePolicyStd[1]  | 0.1915     |
| AveragePolicyStd[2]  | 0.14644    |
| AveragePolicyStd[3]  | 0.17048    |
| AveragePolicyStd[4]  | 0.14837    |
| AveragePolicyStd[5]  | 0.2177     |
| AverageReturn        | 1871.1     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 279.21     |
| AverageEpisodeLength | 957.55     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.38     |
| TotalNEpisodes       | 25365      |
| TotalNSamples        | 8.3112e+06 |
| ExplainedVariance    | -0.066562  |
-------------------------------------
[2018-01-21 16:55:31.715263 UTC] Saving snapshot
[2018-01-21 16:55:31.726066 UTC] Starting iteration 1661
[2018-01-21 16:55:31.726342 UTC] Start collecting samples
[2018-01-21 16:55:37.152950 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:37.280321 UTC] Performing policy update
[2018-01-21 16:55:37.280993 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:37.406082 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:55:38.876982 UTC] Performing line search
[2018-01-21 16:55:39.083515 UTC] Updating baseline
[2018-01-21 16:55:42.034495 UTC] Computing logging information
--------------------------------------
| Iteration            | 1661        |
| ExpectedImprovement  | 0.017376    |
| ActualImprovement    | 0.016311    |
| ImprovementRatio     | 0.93868     |
| MeanKL               | 0.008393    |
| Entropy              | -1.8574     |
| Perplexity           | 0.15608     |
| AveragePolicyStd     | 0.17956     |
| AveragePolicyStd[0]  | 0.20313     |
| AveragePolicyStd[1]  | 0.19105     |
| AveragePolicyStd[2]  | 0.14661     |
| AveragePolicyStd[3]  | 0.17016     |
| AveragePolicyStd[4]  | 0.14844     |
| AveragePolicyStd[5]  | 0.21799     |
| AverageReturn        | 1874        |
| MinReturn            | 481.36      |
| MaxReturn            | 2010.2      |
| StdReturn            | 278.76      |
| AverageEpisodeLength | 959.01      |
| MinEpisodeLength     | 292         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 132.03      |
| TotalNEpisodes       | 25370       |
| TotalNSamples        | 8.3162e+06  |
| ExplainedVariance    | -2.9044e-05 |
--------------------------------------
[2018-01-21 16:55:42.936597 UTC] Saving snapshot
[2018-01-21 16:55:42.936789 UTC] Starting iteration 1662
[2018-01-21 16:55:42.936892 UTC] Start collecting samples
[2018-01-21 16:55:48.250125 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:48.386720 UTC] Performing policy update
[2018-01-21 16:55:48.387336 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:48.504952 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:55:50.062085 UTC] Performing line search
[2018-01-21 16:55:50.295792 UTC] Updating baseline
[2018-01-21 16:55:52.761708 UTC] Computing logging information
-------------------------------------
| Iteration            | 1662       |
| ExpectedImprovement  | 0.018229   |
| ActualImprovement    | 0.015972   |
| ImprovementRatio     | 0.87621    |
| MeanKL               | 0.0085917  |
| Entropy              | -1.8591    |
| Perplexity           | 0.15581    |
| AveragePolicyStd     | 0.17953    |
| AveragePolicyStd[0]  | 0.20338    |
| AveragePolicyStd[1]  | 0.19077    |
| AveragePolicyStd[2]  | 0.14628    |
| AveragePolicyStd[3]  | 0.17009    |
| AveragePolicyStd[4]  | 0.14843    |
| AveragePolicyStd[5]  | 0.21825    |
| AverageReturn        | 1873       |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 278.57     |
| AverageEpisodeLength | 959.01     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.03     |
| TotalNEpisodes       | 25372      |
| TotalNSamples        | 8.3182e+06 |
| ExplainedVariance    | 0.0052798  |
-------------------------------------
[2018-01-21 16:55:53.693935 UTC] Saving snapshot
[2018-01-21 16:55:53.694247 UTC] Starting iteration 1663
[2018-01-21 16:55:53.694450 UTC] Start collecting samples
[2018-01-21 16:55:59.069854 UTC] Computing input variables for policy optimization
[2018-01-21 16:55:59.264750 UTC] Performing policy update
[2018-01-21 16:55:59.265358 UTC] Computing gradient in Euclidean space
[2018-01-21 16:55:59.393179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:56:01.002124 UTC] Performing line search
[2018-01-21 16:56:01.256090 UTC] Updating baseline
[2018-01-21 16:56:04.180214 UTC] Computing logging information
-------------------------------------
| Iteration            | 1663       |
| ExpectedImprovement  | 0.01883    |
| ActualImprovement    | 0.017428   |
| ImprovementRatio     | 0.92555    |
| MeanKL               | 0.0089824  |
| Entropy              | -1.8612    |
| Perplexity           | 0.15549    |
| AveragePolicyStd     | 0.17949    |
| AveragePolicyStd[0]  | 0.20313    |
| AveragePolicyStd[1]  | 0.19023    |
| AveragePolicyStd[2]  | 0.14632    |
| AveragePolicyStd[3]  | 0.17018    |
| AveragePolicyStd[4]  | 0.14822    |
| AveragePolicyStd[5]  | 0.21883    |
| AverageReturn        | 1881.1     |
| MinReturn            | 481.36     |
| MaxReturn            | 2010.2     |
| StdReturn            | 269.83     |
| AverageEpisodeLength | 962.65     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.03     |
| TotalNEpisodes       | 25379      |
| TotalNSamples        | 8.3252e+06 |
| ExplainedVariance    | 0.0048692  |
-------------------------------------
[2018-01-21 16:56:05.099168 UTC] Saving snapshot
[2018-01-21 16:56:05.099480 UTC] Starting iteration 1664
[2018-01-21 16:56:05.099706 UTC] Start collecting samples
[2018-01-21 16:56:13.632146 UTC] Computing input variables for policy optimization
[2018-01-21 16:56:13.848686 UTC] Performing policy update
[2018-01-21 16:56:13.853612 UTC] Computing gradient in Euclidean space
[2018-01-21 16:56:14.017000 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:56:15.786206 UTC] Performing line search
[2018-01-21 16:56:16.077321 UTC] Updating baseline
[2018-01-21 16:56:19.875940 UTC] Computing logging information
-------------------------------------
| Iteration            | 1664       |
| ExpectedImprovement  | 0.018115   |
| ActualImprovement    | 0.017619   |
| ImprovementRatio     | 0.97261    |
| MeanKL               | 0.0089728  |
| Entropy              | -1.8605    |
| Perplexity           | 0.1556     |
| AveragePolicyStd     | 0.17955    |
| AveragePolicyStd[0]  | 0.20335    |
| AveragePolicyStd[1]  | 0.19041    |
| AveragePolicyStd[2]  | 0.14609    |
| AveragePolicyStd[3]  | 0.16975    |
| AveragePolicyStd[4]  | 0.14812    |
| AveragePolicyStd[5]  | 0.21958    |
| AverageReturn        | 1877.5     |
| MinReturn            | 467.89     |
| MaxReturn            | 2011       |
| StdReturn            | 285.59     |
| AverageEpisodeLength | 961.64     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.35     |
| TotalNEpisodes       | 25386      |
| TotalNSamples        | 8.3315e+06 |
| ExplainedVariance    | 0.064877   |
-------------------------------------
[2018-01-21 16:56:20.921386 UTC] Saving snapshot
[2018-01-21 16:56:20.921737 UTC] Starting iteration 1665
[2018-01-21 16:56:20.921954 UTC] Start collecting samples
[2018-01-21 16:56:28.468810 UTC] Computing input variables for policy optimization
[2018-01-21 16:56:28.632170 UTC] Performing policy update
[2018-01-21 16:56:28.632868 UTC] Computing gradient in Euclidean space
[2018-01-21 16:56:28.754263 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:56:30.537618 UTC] Performing line search
[2018-01-21 16:56:30.773255 UTC] Updating baseline
[2018-01-21 16:56:32.894324 UTC] Computing logging information
-------------------------------------
| Iteration            | 1665       |
| ExpectedImprovement  | 0.016136   |
| ActualImprovement    | 0.015149   |
| ImprovementRatio     | 0.93883    |
| MeanKL               | 0.0091154  |
| Entropy              | -1.8536    |
| Perplexity           | 0.15668    |
| AveragePolicyStd     | 0.17978    |
| AveragePolicyStd[0]  | 0.20325    |
| AveragePolicyStd[1]  | 0.19076    |
| AveragePolicyStd[2]  | 0.14585    |
| AveragePolicyStd[3]  | 0.17004    |
| AveragePolicyStd[4]  | 0.14846    |
| AveragePolicyStd[5]  | 0.2203     |
| AverageReturn        | 1878.4     |
| MinReturn            | 467.89     |
| MaxReturn            | 2011       |
| StdReturn            | 285.86     |
| AverageEpisodeLength | 961.64     |
| MinEpisodeLength     | 292        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.35     |
| TotalNEpisodes       | 25389      |
| TotalNSamples        | 8.3345e+06 |
| ExplainedVariance    | 0.0030803  |
-------------------------------------
[2018-01-21 16:56:33.821961 UTC] Saving snapshot
[2018-01-21 16:56:33.822253 UTC] Starting iteration 1666
[2018-01-21 16:56:33.822448 UTC] Start collecting samples
[2018-01-21 16:56:39.745707 UTC] Computing input variables for policy optimization
[2018-01-21 16:56:39.895968 UTC] Performing policy update
[2018-01-21 16:56:39.896719 UTC] Computing gradient in Euclidean space
[2018-01-21 16:56:40.025501 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:56:41.534673 UTC] Performing line search
[2018-01-21 16:56:41.756862 UTC] Updating baseline
[2018-01-21 16:56:44.149008 UTC] Computing logging information
-------------------------------------
| Iteration            | 1666       |
| ExpectedImprovement  | 0.018473   |
| ActualImprovement    | 0.017519   |
| ImprovementRatio     | 0.94838    |
| MeanKL               | 0.008524   |
| Entropy              | -1.8459    |
| Perplexity           | 0.15788    |
| AveragePolicyStd     | 0.18003    |
| AveragePolicyStd[0]  | 0.20354    |
| AveragePolicyStd[1]  | 0.19081    |
| AveragePolicyStd[2]  | 0.14578    |
| AveragePolicyStd[3]  | 0.17017    |
| AveragePolicyStd[4]  | 0.14877    |
| AveragePolicyStd[5]  | 0.22108    |
| AverageReturn        | 1893.6     |
| MinReturn            | 467.89     |
| MaxReturn            | 2011       |
| StdReturn            | 249.23     |
| AverageEpisodeLength | 968.52     |
| MinEpisodeLength     | 355        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.97     |
| TotalNEpisodes       | 25395      |
| TotalNSamples        | 8.3405e+06 |
| ExplainedVariance    | 0.097347   |
-------------------------------------
[2018-01-21 16:56:45.150009 UTC] Saving snapshot
[2018-01-21 16:56:45.150298 UTC] Starting iteration 1667
[2018-01-21 16:56:45.150568 UTC] Start collecting samples
[2018-01-21 16:56:50.774755 UTC] Computing input variables for policy optimization
[2018-01-21 16:56:50.916464 UTC] Performing policy update
[2018-01-21 16:56:50.917159 UTC] Computing gradient in Euclidean space
[2018-01-21 16:56:51.030748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:56:52.487086 UTC] Performing line search
[2018-01-21 16:56:52.687666 UTC] Updating baseline
[2018-01-21 16:56:54.539374 UTC] Computing logging information
-------------------------------------
| Iteration            | 1667       |
| ExpectedImprovement  | 0.017151   |
| ActualImprovement    | 0.017127   |
| ImprovementRatio     | 0.99857    |
| MeanKL               | 0.0077313  |
| Entropy              | -1.839     |
| Perplexity           | 0.15897    |
| AveragePolicyStd     | 0.18026    |
| AveragePolicyStd[0]  | 0.20367    |
| AveragePolicyStd[1]  | 0.19097    |
| AveragePolicyStd[2]  | 0.14543    |
| AveragePolicyStd[3]  | 0.1703     |
| AveragePolicyStd[4]  | 0.14922    |
| AveragePolicyStd[5]  | 0.22195    |
| AverageReturn        | 1886.6     |
| MinReturn            | 467.89     |
| MaxReturn            | 2011       |
| StdReturn            | 266.67     |
| AverageEpisodeLength | 966.58     |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 123.34     |
| TotalNEpisodes       | 25402      |
| TotalNSamples        | 8.3468e+06 |
| ExplainedVariance    | 0.066643   |
-------------------------------------
[2018-01-21 16:56:55.469343 UTC] Saving snapshot
[2018-01-21 16:56:55.469599 UTC] Starting iteration 1668
[2018-01-21 16:56:55.469784 UTC] Start collecting samples
[2018-01-21 16:57:00.960612 UTC] Computing input variables for policy optimization
[2018-01-21 16:57:01.109937 UTC] Performing policy update
[2018-01-21 16:57:01.110597 UTC] Computing gradient in Euclidean space
[2018-01-21 16:57:01.228821 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:57:02.673282 UTC] Performing line search
[2018-01-21 16:57:02.896431 UTC] Updating baseline
[2018-01-21 16:57:05.264500 UTC] Computing logging information
-------------------------------------
| Iteration            | 1668       |
| ExpectedImprovement  | 0.018914   |
| ActualImprovement    | 0.017494   |
| ImprovementRatio     | 0.92491    |
| MeanKL               | 0.0079098  |
| Entropy              | -1.8382    |
| Perplexity           | 0.1591     |
| AveragePolicyStd     | 0.18028    |
| AveragePolicyStd[0]  | 0.20331    |
| AveragePolicyStd[1]  | 0.19094    |
| AveragePolicyStd[2]  | 0.14545    |
| AveragePolicyStd[3]  | 0.17046    |
| AveragePolicyStd[4]  | 0.14935    |
| AveragePolicyStd[5]  | 0.22213    |
| AverageReturn        | 1847.8     |
| MinReturn            | 467.89     |
| MaxReturn            | 2011       |
| StdReturn            | 336.65     |
| AverageEpisodeLength | 947.9      |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.13     |
| TotalNEpisodes       | 25408      |
| TotalNSamples        | 8.3509e+06 |
| ExplainedVariance    | 0.30011    |
-------------------------------------
[2018-01-21 16:57:06.257284 UTC] Saving snapshot
[2018-01-21 16:57:06.257551 UTC] Starting iteration 1669
[2018-01-21 16:57:06.257734 UTC] Start collecting samples
[2018-01-21 16:57:11.409635 UTC] Computing input variables for policy optimization
[2018-01-21 16:57:11.538570 UTC] Performing policy update
[2018-01-21 16:57:11.539177 UTC] Computing gradient in Euclidean space
[2018-01-21 16:57:11.659963 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:57:13.104169 UTC] Performing line search
[2018-01-21 16:57:13.300691 UTC] Updating baseline
[2018-01-21 16:57:16.094764 UTC] Computing logging information
-------------------------------------
| Iteration            | 1669       |
| ExpectedImprovement  | 0.018536   |
| ActualImprovement    | 0.017471   |
| ImprovementRatio     | 0.94255    |
| MeanKL               | 0.0076375  |
| Entropy              | -1.8428    |
| Perplexity           | 0.15837    |
| AveragePolicyStd     | 0.18013    |
| AveragePolicyStd[0]  | 0.20265    |
| AveragePolicyStd[1]  | 0.19098    |
| AveragePolicyStd[2]  | 0.14539    |
| AveragePolicyStd[3]  | 0.17021    |
| AveragePolicyStd[4]  | 0.1494     |
| AveragePolicyStd[5]  | 0.22217    |
| AverageReturn        | 1847.3     |
| MinReturn            | 467.89     |
| MaxReturn            | 2019.1     |
| StdReturn            | 340.44     |
| AverageEpisodeLength | 946.82     |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.13     |
| TotalNEpisodes       | 25414      |
| TotalNSamples        | 8.3565e+06 |
| ExplainedVariance    | 0.094843   |
-------------------------------------
[2018-01-21 16:57:17.022246 UTC] Saving snapshot
[2018-01-21 16:57:17.022591 UTC] Starting iteration 1670
[2018-01-21 16:57:17.022835 UTC] Start collecting samples
[2018-01-21 16:57:22.697568 UTC] Computing input variables for policy optimization
[2018-01-21 16:57:22.838256 UTC] Performing policy update
[2018-01-21 16:57:22.838985 UTC] Computing gradient in Euclidean space
[2018-01-21 16:57:22.967598 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:57:24.590545 UTC] Performing line search
[2018-01-21 16:57:24.799089 UTC] Updating baseline
[2018-01-21 16:57:27.173706 UTC] Computing logging information
-------------------------------------
| Iteration            | 1670       |
| ExpectedImprovement  | 0.019479   |
| ActualImprovement    | 0.01742    |
| ImprovementRatio     | 0.89429    |
| MeanKL               | 0.0080355  |
| Entropy              | -1.8439    |
| Perplexity           | 0.1582     |
| AveragePolicyStd     | 0.18014    |
| AveragePolicyStd[0]  | 0.20237    |
| AveragePolicyStd[1]  | 0.19133    |
| AveragePolicyStd[2]  | 0.14522    |
| AveragePolicyStd[3]  | 0.17053    |
| AveragePolicyStd[4]  | 0.14875    |
| AveragePolicyStd[5]  | 0.22262    |
| AverageReturn        | 1845.1     |
| MinReturn            | 467.89     |
| MaxReturn            | 2019.1     |
| StdReturn            | 340.97     |
| AverageEpisodeLength | 945.38     |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.3      |
| TotalNEpisodes       | 25418      |
| TotalNSamples        | 8.3604e+06 |
| ExplainedVariance    | 0.16604    |
-------------------------------------
[2018-01-21 16:57:28.161142 UTC] Saving snapshot
[2018-01-21 16:57:28.171073 UTC] Starting iteration 1671
[2018-01-21 16:57:28.171277 UTC] Start collecting samples
[2018-01-21 16:57:34.646370 UTC] Computing input variables for policy optimization
[2018-01-21 16:57:34.840374 UTC] Performing policy update
[2018-01-21 16:57:34.841348 UTC] Computing gradient in Euclidean space
[2018-01-21 16:57:34.967958 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:57:36.426481 UTC] Performing line search
[2018-01-21 16:57:36.630639 UTC] Updating baseline
[2018-01-21 16:57:39.282659 UTC] Computing logging information
-------------------------------------
| Iteration            | 1671       |
| ExpectedImprovement  | 0.017859   |
| ActualImprovement    | 0.017073   |
| ImprovementRatio     | 0.95603    |
| MeanKL               | 0.0085483  |
| Entropy              | -1.8415    |
| Perplexity           | 0.15858    |
| AveragePolicyStd     | 0.18021    |
| AveragePolicyStd[0]  | 0.2028     |
| AveragePolicyStd[1]  | 0.19178    |
| AveragePolicyStd[2]  | 0.1451     |
| AveragePolicyStd[3]  | 0.1705     |
| AveragePolicyStd[4]  | 0.14883    |
| AveragePolicyStd[5]  | 0.22227    |
| AverageReturn        | 1846.5     |
| MinReturn            | 467.89     |
| MaxReturn            | 2019.1     |
| StdReturn            | 341.46     |
| AverageEpisodeLength | 945.38     |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.3      |
| TotalNEpisodes       | 25424      |
| TotalNSamples        | 8.3664e+06 |
| ExplainedVariance    | 0.00054464 |
-------------------------------------
[2018-01-21 16:57:40.259520 UTC] Saving snapshot
[2018-01-21 16:57:40.259824 UTC] Starting iteration 1672
[2018-01-21 16:57:40.260041 UTC] Start collecting samples
[2018-01-21 16:57:46.857596 UTC] Computing input variables for policy optimization
[2018-01-21 16:57:47.077962 UTC] Performing policy update
[2018-01-21 16:57:47.078729 UTC] Computing gradient in Euclidean space
[2018-01-21 16:57:47.245028 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:57:48.942419 UTC] Performing line search
[2018-01-21 16:57:49.139141 UTC] Updating baseline
[2018-01-21 16:57:52.841730 UTC] Computing logging information
-------------------------------------
| Iteration            | 1672       |
| ExpectedImprovement  | 0.01807    |
| ActualImprovement    | 0.017169   |
| ImprovementRatio     | 0.95015    |
| MeanKL               | 0.0079296  |
| Entropy              | -1.8422    |
| Perplexity           | 0.15847    |
| AveragePolicyStd     | 0.18019    |
| AveragePolicyStd[0]  | 0.20274    |
| AveragePolicyStd[1]  | 0.19205    |
| AveragePolicyStd[2]  | 0.14512    |
| AveragePolicyStd[3]  | 0.17038    |
| AveragePolicyStd[4]  | 0.14871    |
| AveragePolicyStd[5]  | 0.22217    |
| AverageReturn        | 1846.6     |
| MinReturn            | 467.89     |
| MaxReturn            | 2019.1     |
| StdReturn            | 341.54     |
| AverageEpisodeLength | 945.38     |
| MinEpisodeLength     | 297        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.3      |
| TotalNEpisodes       | 25429      |
| TotalNSamples        | 8.3714e+06 |
| ExplainedVariance    | -0.01553   |
-------------------------------------
[2018-01-21 16:57:53.793087 UTC] Saving snapshot
[2018-01-21 16:57:53.793319 UTC] Starting iteration 1673
[2018-01-21 16:57:53.793460 UTC] Start collecting samples
[2018-01-21 16:58:00.287172 UTC] Computing input variables for policy optimization
[2018-01-21 16:58:00.438137 UTC] Performing policy update
[2018-01-21 16:58:00.438961 UTC] Computing gradient in Euclidean space
[2018-01-21 16:58:00.626217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:58:02.351429 UTC] Performing line search
[2018-01-21 16:58:02.597944 UTC] Updating baseline
[2018-01-21 16:58:05.220094 UTC] Computing logging information
-------------------------------------
| Iteration            | 1673       |
| ExpectedImprovement  | 0.01547    |
| ActualImprovement    | 0.014626   |
| ImprovementRatio     | 0.94549    |
| MeanKL               | 0.0076664  |
| Entropy              | -1.8522    |
| Perplexity           | 0.15689    |
| AveragePolicyStd     | 0.17991    |
| AveragePolicyStd[0]  | 0.20209    |
| AveragePolicyStd[1]  | 0.19197    |
| AveragePolicyStd[2]  | 0.14485    |
| AveragePolicyStd[3]  | 0.1702     |
| AveragePolicyStd[4]  | 0.14825    |
| AveragePolicyStd[5]  | 0.22209    |
| AverageReturn        | 1831.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2021.7     |
| StdReturn            | 374.98     |
| AverageEpisodeLength | 937.32     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.67     |
| TotalNEpisodes       | 25434      |
| TotalNSamples        | 8.3756e+06 |
| ExplainedVariance    | 0.1304     |
-------------------------------------
[2018-01-21 16:58:06.234146 UTC] Saving snapshot
[2018-01-21 16:58:06.234534 UTC] Starting iteration 1674
[2018-01-21 16:58:06.234805 UTC] Start collecting samples
[2018-01-21 16:58:12.772344 UTC] Computing input variables for policy optimization
[2018-01-21 16:58:12.956902 UTC] Performing policy update
[2018-01-21 16:58:12.958003 UTC] Computing gradient in Euclidean space
[2018-01-21 16:58:13.080417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:58:14.764921 UTC] Performing line search
[2018-01-21 16:58:15.005325 UTC] Updating baseline
[2018-01-21 16:58:17.222371 UTC] Computing logging information
-------------------------------------
| Iteration            | 1674       |
| ExpectedImprovement  | 0.019265   |
| ActualImprovement    | 0.018296   |
| ImprovementRatio     | 0.94973    |
| MeanKL               | 0.0084188  |
| Entropy              | -1.8444    |
| Perplexity           | 0.15813    |
| AveragePolicyStd     | 0.1802     |
| AveragePolicyStd[0]  | 0.20278    |
| AveragePolicyStd[1]  | 0.19234    |
| AveragePolicyStd[2]  | 0.14421    |
| AveragePolicyStd[3]  | 0.17032    |
| AveragePolicyStd[4]  | 0.14858    |
| AveragePolicyStd[5]  | 0.22299    |
| AverageReturn        | 1835.7     |
| MinReturn            | 297.96     |
| MaxReturn            | 2021.7     |
| StdReturn            | 373.77     |
| AverageEpisodeLength | 939.61     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.79     |
| TotalNEpisodes       | 25441      |
| TotalNSamples        | 8.3821e+06 |
| ExplainedVariance    | 0.1109     |
-------------------------------------
[2018-01-21 16:58:18.108807 UTC] Saving snapshot
[2018-01-21 16:58:18.109103 UTC] Starting iteration 1675
[2018-01-21 16:58:18.109269 UTC] Start collecting samples
[2018-01-21 16:58:24.801271 UTC] Computing input variables for policy optimization
[2018-01-21 16:58:25.010842 UTC] Performing policy update
[2018-01-21 16:58:25.011439 UTC] Computing gradient in Euclidean space
[2018-01-21 16:58:25.180441 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:58:26.878935 UTC] Performing line search
[2018-01-21 16:58:27.092836 UTC] Updating baseline
[2018-01-21 16:58:29.363707 UTC] Computing logging information
-------------------------------------
| Iteration            | 1675       |
| ExpectedImprovement  | 0.018752   |
| ActualImprovement    | 0.016935   |
| ImprovementRatio     | 0.90309    |
| MeanKL               | 0.0085196  |
| Entropy              | -1.851     |
| Perplexity           | 0.15708    |
| AveragePolicyStd     | 0.18       |
| AveragePolicyStd[0]  | 0.20225    |
| AveragePolicyStd[1]  | 0.19231    |
| AveragePolicyStd[2]  | 0.14398    |
| AveragePolicyStd[3]  | 0.17011    |
| AveragePolicyStd[4]  | 0.14852    |
| AveragePolicyStd[5]  | 0.22284    |
| AverageReturn        | 1830.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 378.27     |
| AverageEpisodeLength | 936.4      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.56     |
| TotalNEpisodes       | 25444      |
| TotalNSamples        | 8.3848e+06 |
| ExplainedVariance    | 0.31779    |
-------------------------------------
[2018-01-21 16:58:30.431654 UTC] Saving snapshot
[2018-01-21 16:58:30.432016 UTC] Starting iteration 1676
[2018-01-21 16:58:30.432255 UTC] Start collecting samples
[2018-01-21 16:58:36.970839 UTC] Computing input variables for policy optimization
[2018-01-21 16:58:37.097577 UTC] Performing policy update
[2018-01-21 16:58:37.098654 UTC] Computing gradient in Euclidean space
[2018-01-21 16:58:37.231359 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:58:38.847643 UTC] Performing line search
[2018-01-21 16:58:39.043393 UTC] Updating baseline
[2018-01-21 16:58:41.913897 UTC] Computing logging information
-------------------------------------
| Iteration            | 1676       |
| ExpectedImprovement  | 0.017818   |
| ActualImprovement    | 0.016992   |
| ImprovementRatio     | 0.95364    |
| MeanKL               | 0.0086947  |
| Entropy              | -1.8538    |
| Perplexity           | 0.15663    |
| AveragePolicyStd     | 0.17992    |
| AveragePolicyStd[0]  | 0.20204    |
| AveragePolicyStd[1]  | 0.19237    |
| AveragePolicyStd[2]  | 0.14387    |
| AveragePolicyStd[3]  | 0.16995    |
| AveragePolicyStd[4]  | 0.14852    |
| AveragePolicyStd[5]  | 0.22274    |
| AverageReturn        | 1830       |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 378.22     |
| AverageEpisodeLength | 936.4      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.56     |
| TotalNEpisodes       | 25449      |
| TotalNSamples        | 8.3898e+06 |
| ExplainedVariance    | -0.021841  |
-------------------------------------
[2018-01-21 16:58:42.778708 UTC] Saving snapshot
[2018-01-21 16:58:42.779000 UTC] Starting iteration 1677
[2018-01-21 16:58:42.779211 UTC] Start collecting samples
[2018-01-21 16:58:47.921691 UTC] Computing input variables for policy optimization
[2018-01-21 16:58:48.077971 UTC] Performing policy update
[2018-01-21 16:58:48.078697 UTC] Computing gradient in Euclidean space
[2018-01-21 16:58:48.201182 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:58:49.727481 UTC] Performing line search
[2018-01-21 16:58:49.925099 UTC] Updating baseline
[2018-01-21 16:58:52.345622 UTC] Computing logging information
-------------------------------------
| Iteration            | 1677       |
| ExpectedImprovement  | 0.017086   |
| ActualImprovement    | 0.016184   |
| ImprovementRatio     | 0.9472     |
| MeanKL               | 0.0084129  |
| Entropy              | -1.8503    |
| Perplexity           | 0.15719    |
| AveragePolicyStd     | 0.18002    |
| AveragePolicyStd[0]  | 0.20211    |
| AveragePolicyStd[1]  | 0.19244    |
| AveragePolicyStd[2]  | 0.14395    |
| AveragePolicyStd[3]  | 0.1701     |
| AveragePolicyStd[4]  | 0.14859    |
| AveragePolicyStd[5]  | 0.22294    |
| AverageReturn        | 1839.2     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 368.94     |
| AverageEpisodeLength | 940.85     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.52     |
| TotalNEpisodes       | 25454      |
| TotalNSamples        | 8.3948e+06 |
| ExplainedVariance    | 0.0026518  |
-------------------------------------
[2018-01-21 16:58:53.361083 UTC] Saving snapshot
[2018-01-21 16:58:53.361348 UTC] Starting iteration 1678
[2018-01-21 16:58:53.361532 UTC] Start collecting samples
[2018-01-21 16:59:01.796169 UTC] Computing input variables for policy optimization
[2018-01-21 16:59:01.975779 UTC] Performing policy update
[2018-01-21 16:59:01.982631 UTC] Computing gradient in Euclidean space
[2018-01-21 16:59:02.214891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:59:04.119397 UTC] Performing line search
[2018-01-21 16:59:04.407469 UTC] Updating baseline
[2018-01-21 16:59:07.663220 UTC] Computing logging information
-------------------------------------
| Iteration            | 1678       |
| ExpectedImprovement  | 0.01779    |
| ActualImprovement    | 0.016512   |
| ImprovementRatio     | 0.92817    |
| MeanKL               | 0.0074569  |
| Entropy              | -1.8472    |
| Perplexity           | 0.15768    |
| AveragePolicyStd     | 0.18012    |
| AveragePolicyStd[0]  | 0.2021     |
| AveragePolicyStd[1]  | 0.19284    |
| AveragePolicyStd[2]  | 0.14376    |
| AveragePolicyStd[3]  | 0.17043    |
| AveragePolicyStd[4]  | 0.14867    |
| AveragePolicyStd[5]  | 0.22294    |
| AverageReturn        | 1849.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 360.86     |
| AverageEpisodeLength | 946.15     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.84     |
| TotalNEpisodes       | 25460      |
| TotalNSamples        | 8.4008e+06 |
| ExplainedVariance    | 0.0034076  |
-------------------------------------
[2018-01-21 16:59:08.706801 UTC] Saving snapshot
[2018-01-21 16:59:08.707201 UTC] Starting iteration 1679
[2018-01-21 16:59:08.707456 UTC] Start collecting samples
[2018-01-21 16:59:15.735488 UTC] Computing input variables for policy optimization
[2018-01-21 16:59:15.917185 UTC] Performing policy update
[2018-01-21 16:59:15.918493 UTC] Computing gradient in Euclidean space
[2018-01-21 16:59:16.056998 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:59:17.998716 UTC] Performing line search
[2018-01-21 16:59:18.234298 UTC] Updating baseline
[2018-01-21 16:59:20.699814 UTC] Computing logging information
-------------------------------------
| Iteration            | 1679       |
| ExpectedImprovement  | 0.015954   |
| ActualImprovement    | 0.015107   |
| ImprovementRatio     | 0.94691    |
| MeanKL               | 0.0086671  |
| Entropy              | -1.8565    |
| Perplexity           | 0.15622    |
| AveragePolicyStd     | 0.17983    |
| AveragePolicyStd[0]  | 0.20149    |
| AveragePolicyStd[1]  | 0.19278    |
| AveragePolicyStd[2]  | 0.14379    |
| AveragePolicyStd[3]  | 0.17022    |
| AveragePolicyStd[4]  | 0.1483     |
| AveragePolicyStd[5]  | 0.2224     |
| AverageReturn        | 1838       |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 374.42     |
| AverageEpisodeLength | 940.8      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.29     |
| TotalNEpisodes       | 25465      |
| TotalNSamples        | 8.4052e+06 |
| ExplainedVariance    | 0.081657   |
-------------------------------------
[2018-01-21 16:59:21.658407 UTC] Saving snapshot
[2018-01-21 16:59:21.658683 UTC] Starting iteration 1680
[2018-01-21 16:59:21.658845 UTC] Start collecting samples
[2018-01-21 16:59:26.704923 UTC] Computing input variables for policy optimization
[2018-01-21 16:59:26.840650 UTC] Performing policy update
[2018-01-21 16:59:26.841258 UTC] Computing gradient in Euclidean space
[2018-01-21 16:59:26.959090 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:59:28.410580 UTC] Performing line search
[2018-01-21 16:59:28.615903 UTC] Updating baseline
[2018-01-21 16:59:31.401783 UTC] Computing logging information
------------------------------------
| Iteration            | 1680      |
| ExpectedImprovement  | 0.018576  |
| ActualImprovement    | 0.017387  |
| ImprovementRatio     | 0.936     |
| MeanKL               | 0.0087243 |
| Entropy              | -1.8526   |
| Perplexity           | 0.15683   |
| AveragePolicyStd     | 0.17993   |
| AveragePolicyStd[0]  | 0.2015    |
| AveragePolicyStd[1]  | 0.19289   |
| AveragePolicyStd[2]  | 0.14411   |
| AveragePolicyStd[3]  | 0.16994   |
| AveragePolicyStd[4]  | 0.14865   |
| AveragePolicyStd[5]  | 0.22247   |
| AverageReturn        | 1833.8    |
| MinReturn            | 297.96    |
| MaxReturn            | 2025.9    |
| StdReturn            | 375.58    |
| AverageEpisodeLength | 938.55    |
| MinEpisodeLength     | 194       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 178.94    |
| TotalNEpisodes       | 25470     |
| TotalNSamples        | 8.41e+06  |
| ExplainedVariance    | 0.076136  |
------------------------------------
[2018-01-21 16:59:32.328665 UTC] Saving snapshot
[2018-01-21 16:59:32.339725 UTC] Starting iteration 1681
[2018-01-21 16:59:32.339993 UTC] Start collecting samples
[2018-01-21 16:59:38.541993 UTC] Computing input variables for policy optimization
[2018-01-21 16:59:38.710396 UTC] Performing policy update
[2018-01-21 16:59:38.712229 UTC] Computing gradient in Euclidean space
[2018-01-21 16:59:38.844404 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:59:40.479571 UTC] Performing line search
[2018-01-21 16:59:40.670309 UTC] Updating baseline
[2018-01-21 16:59:43.231865 UTC] Computing logging information
-------------------------------------
| Iteration            | 1681       |
| ExpectedImprovement  | 0.019052   |
| ActualImprovement    | 0.017773   |
| ImprovementRatio     | 0.93287    |
| MeanKL               | 0.0077797  |
| Entropy              | -1.8507    |
| Perplexity           | 0.15712    |
| AveragePolicyStd     | 0.17996    |
| AveragePolicyStd[0]  | 0.20134    |
| AveragePolicyStd[1]  | 0.19292    |
| AveragePolicyStd[2]  | 0.14438    |
| AveragePolicyStd[3]  | 0.17004    |
| AveragePolicyStd[4]  | 0.14874    |
| AveragePolicyStd[5]  | 0.22234    |
| AverageReturn        | 1821.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 399.52     |
| AverageEpisodeLength | 931.3      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.61     |
| TotalNEpisodes       | 25477      |
| TotalNSamples        | 8.4163e+06 |
| ExplainedVariance    | 0.078864   |
-------------------------------------
[2018-01-21 16:59:44.261106 UTC] Saving snapshot
[2018-01-21 16:59:44.261397 UTC] Starting iteration 1682
[2018-01-21 16:59:44.261610 UTC] Start collecting samples
[2018-01-21 16:59:50.313419 UTC] Computing input variables for policy optimization
[2018-01-21 16:59:50.475559 UTC] Performing policy update
[2018-01-21 16:59:50.476390 UTC] Computing gradient in Euclidean space
[2018-01-21 16:59:50.607289 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 16:59:52.149846 UTC] Performing line search
[2018-01-21 16:59:52.364680 UTC] Updating baseline
[2018-01-21 16:59:54.744099 UTC] Computing logging information
-------------------------------------
| Iteration            | 1682       |
| ExpectedImprovement  | 0.018179   |
| ActualImprovement    | 0.017352   |
| ImprovementRatio     | 0.95451    |
| MeanKL               | 0.0084758  |
| Entropy              | -1.8566    |
| Perplexity           | 0.1562     |
| AveragePolicyStd     | 0.1798     |
| AveragePolicyStd[0]  | 0.20088    |
| AveragePolicyStd[1]  | 0.19278    |
| AveragePolicyStd[2]  | 0.14403    |
| AveragePolicyStd[3]  | 0.17005    |
| AveragePolicyStd[4]  | 0.14857    |
| AveragePolicyStd[5]  | 0.22249    |
| AverageReturn        | 1807       |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 420.28     |
| AverageEpisodeLength | 924.21     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.84     |
| TotalNEpisodes       | 25482      |
| TotalNSamples        | 8.4206e+06 |
| ExplainedVariance    | 0.12261    |
-------------------------------------
[2018-01-21 16:59:55.749478 UTC] Saving snapshot
[2018-01-21 16:59:55.749737 UTC] Starting iteration 1683
[2018-01-21 16:59:55.749912 UTC] Start collecting samples
[2018-01-21 17:00:01.413144 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:01.575472 UTC] Performing policy update
[2018-01-21 17:00:01.576188 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:01.697890 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:03.213284 UTC] Performing line search
[2018-01-21 17:00:03.560043 UTC] Updating baseline
[2018-01-21 17:00:05.960379 UTC] Computing logging information
-------------------------------------
| Iteration            | 1683       |
| ExpectedImprovement  | 0.017728   |
| ActualImprovement    | 0.017077   |
| ImprovementRatio     | 0.96329    |
| MeanKL               | 0.0088803  |
| Entropy              | -1.8552    |
| Perplexity           | 0.15642    |
| AveragePolicyStd     | 0.17985    |
| AveragePolicyStd[0]  | 0.20112    |
| AveragePolicyStd[1]  | 0.19252    |
| AveragePolicyStd[2]  | 0.14438    |
| AveragePolicyStd[3]  | 0.1698     |
| AveragePolicyStd[4]  | 0.14837    |
| AveragePolicyStd[5]  | 0.22292    |
| AverageReturn        | 1796.9     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 426.81     |
| AverageEpisodeLength | 919.1      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.72     |
| TotalNEpisodes       | 25490      |
| TotalNSamples        | 8.4274e+06 |
| ExplainedVariance    | 0.16112    |
-------------------------------------
[2018-01-21 17:00:07.045902 UTC] Saving snapshot
[2018-01-21 17:00:07.046202 UTC] Starting iteration 1684
[2018-01-21 17:00:07.046407 UTC] Start collecting samples
[2018-01-21 17:00:13.154215 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:13.296946 UTC] Performing policy update
[2018-01-21 17:00:13.297865 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:13.452895 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:14.914378 UTC] Performing line search
[2018-01-21 17:00:15.107739 UTC] Updating baseline
[2018-01-21 17:00:18.129573 UTC] Computing logging information
-------------------------------------
| Iteration            | 1684       |
| ExpectedImprovement  | 0.018484   |
| ActualImprovement    | 0.017304   |
| ImprovementRatio     | 0.93617    |
| MeanKL               | 0.0077559  |
| Entropy              | -1.8501    |
| Perplexity           | 0.15721    |
| AveragePolicyStd     | 0.18       |
| AveragePolicyStd[0]  | 0.20154    |
| AveragePolicyStd[1]  | 0.19213    |
| AveragePolicyStd[2]  | 0.14457    |
| AveragePolicyStd[3]  | 0.16975    |
| AveragePolicyStd[4]  | 0.14874    |
| AveragePolicyStd[5]  | 0.22324    |
| AverageReturn        | 1797.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 426.96     |
| AverageEpisodeLength | 919.1      |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.72     |
| TotalNEpisodes       | 25492      |
| TotalNSamples        | 8.4294e+06 |
| ExplainedVariance    | -0.026774  |
-------------------------------------
[2018-01-21 17:00:19.097015 UTC] Saving snapshot
[2018-01-21 17:00:19.097280 UTC] Starting iteration 1685
[2018-01-21 17:00:19.097469 UTC] Start collecting samples
[2018-01-21 17:00:24.024780 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:24.183214 UTC] Performing policy update
[2018-01-21 17:00:24.183842 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:24.302395 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:25.993628 UTC] Performing line search
[2018-01-21 17:00:26.228024 UTC] Updating baseline
[2018-01-21 17:00:28.394731 UTC] Computing logging information
-------------------------------------
| Iteration            | 1685       |
| ExpectedImprovement  | 0.018023   |
| ActualImprovement    | 0.017242   |
| ImprovementRatio     | 0.95668    |
| MeanKL               | 0.0084558  |
| Entropy              | -1.8527    |
| Perplexity           | 0.15681    |
| AveragePolicyStd     | 0.17993    |
| AveragePolicyStd[0]  | 0.20155    |
| AveragePolicyStd[1]  | 0.19219    |
| AveragePolicyStd[2]  | 0.14462    |
| AveragePolicyStd[3]  | 0.16927    |
| AveragePolicyStd[4]  | 0.14868    |
| AveragePolicyStd[5]  | 0.22326    |
| AverageReturn        | 1809.5     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 409.33     |
| AverageEpisodeLength | 924.12     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.64     |
| TotalNEpisodes       | 25498      |
| TotalNSamples        | 8.4352e+06 |
| ExplainedVariance    | 0.10687    |
-------------------------------------
[2018-01-21 17:00:29.313151 UTC] Saving snapshot
[2018-01-21 17:00:29.313444 UTC] Starting iteration 1686
[2018-01-21 17:00:29.313633 UTC] Start collecting samples
[2018-01-21 17:00:34.223641 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:34.356851 UTC] Performing policy update
[2018-01-21 17:00:34.357852 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:34.493804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:35.961815 UTC] Performing line search
[2018-01-21 17:00:36.165558 UTC] Updating baseline
[2018-01-21 17:00:38.286133 UTC] Computing logging information
-------------------------------------
| Iteration            | 1686       |
| ExpectedImprovement  | 0.017983   |
| ActualImprovement    | 0.017364   |
| ImprovementRatio     | 0.96558    |
| MeanKL               | 0.0085045  |
| Entropy              | -1.8523    |
| Perplexity           | 0.15687    |
| AveragePolicyStd     | 0.17994    |
| AveragePolicyStd[0]  | 0.20166    |
| AveragePolicyStd[1]  | 0.19195    |
| AveragePolicyStd[2]  | 0.14484    |
| AveragePolicyStd[3]  | 0.16934    |
| AveragePolicyStd[4]  | 0.14846    |
| AveragePolicyStd[5]  | 0.22338    |
| AverageReturn        | 1824.3     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 396.71     |
| AverageEpisodeLength | 930        |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.95     |
| TotalNEpisodes       | 25504      |
| TotalNSamples        | 8.4412e+06 |
| ExplainedVariance    | -0.011989  |
-------------------------------------
[2018-01-21 17:00:39.152716 UTC] Saving snapshot
[2018-01-21 17:00:39.153017 UTC] Starting iteration 1687
[2018-01-21 17:00:39.153239 UTC] Start collecting samples
[2018-01-21 17:00:44.076853 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:44.210409 UTC] Performing policy update
[2018-01-21 17:00:44.211098 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:44.342445 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:45.762217 UTC] Performing line search
[2018-01-21 17:00:45.950909 UTC] Updating baseline
[2018-01-21 17:00:48.493337 UTC] Computing logging information
-------------------------------------
| Iteration            | 1687       |
| ExpectedImprovement  | 0.018641   |
| ActualImprovement    | 0.017279   |
| ImprovementRatio     | 0.92696    |
| MeanKL               | 0.008177   |
| Entropy              | -1.8509    |
| Perplexity           | 0.15709    |
| AveragePolicyStd     | 0.17999    |
| AveragePolicyStd[0]  | 0.20198    |
| AveragePolicyStd[1]  | 0.19214    |
| AveragePolicyStd[2]  | 0.14487    |
| AveragePolicyStd[3]  | 0.16881    |
| AveragePolicyStd[4]  | 0.14862    |
| AveragePolicyStd[5]  | 0.22353    |
| AverageReturn        | 1849.5     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 358.76     |
| AverageEpisodeLength | 942.42     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.66     |
| TotalNEpisodes       | 25508      |
| TotalNSamples        | 8.4452e+06 |
| ExplainedVariance    | 0.20072    |
-------------------------------------
[2018-01-21 17:00:49.460868 UTC] Saving snapshot
[2018-01-21 17:00:49.461151 UTC] Starting iteration 1688
[2018-01-21 17:00:49.461325 UTC] Start collecting samples
[2018-01-21 17:00:54.502546 UTC] Computing input variables for policy optimization
[2018-01-21 17:00:54.635883 UTC] Performing policy update
[2018-01-21 17:00:54.636553 UTC] Computing gradient in Euclidean space
[2018-01-21 17:00:54.778322 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:00:56.259445 UTC] Performing line search
[2018-01-21 17:00:56.472545 UTC] Updating baseline
[2018-01-21 17:00:58.393361 UTC] Computing logging information
-------------------------------------
| Iteration            | 1688       |
| ExpectedImprovement  | 0.01961    |
| ActualImprovement    | 0.018354   |
| ImprovementRatio     | 0.93595    |
| MeanKL               | 0.0083785  |
| Entropy              | -1.8563    |
| Perplexity           | 0.15625    |
| AveragePolicyStd     | 0.17983    |
| AveragePolicyStd[0]  | 0.20142    |
| AveragePolicyStd[1]  | 0.19208    |
| AveragePolicyStd[2]  | 0.14485    |
| AveragePolicyStd[3]  | 0.16853    |
| AveragePolicyStd[4]  | 0.14852    |
| AveragePolicyStd[5]  | 0.22359    |
| AverageReturn        | 1858.1     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 352.23     |
| AverageEpisodeLength | 946.45     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.22     |
| TotalNEpisodes       | 25514      |
| TotalNSamples        | 8.4512e+06 |
| ExplainedVariance    | -0.006144  |
-------------------------------------
[2018-01-21 17:00:59.293276 UTC] Saving snapshot
[2018-01-21 17:00:59.293528 UTC] Starting iteration 1689
[2018-01-21 17:00:59.293708 UTC] Start collecting samples
[2018-01-21 17:01:04.209476 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:04.379759 UTC] Performing policy update
[2018-01-21 17:01:04.380634 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:04.506583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:05.984403 UTC] Performing line search
[2018-01-21 17:01:06.177988 UTC] Updating baseline
[2018-01-21 17:01:08.643830 UTC] Computing logging information
-------------------------------------
| Iteration            | 1689       |
| ExpectedImprovement  | 0.016524   |
| ActualImprovement    | 0.015876   |
| ImprovementRatio     | 0.9608     |
| MeanKL               | 0.0087587  |
| Entropy              | -1.8548    |
| Perplexity           | 0.15649    |
| AveragePolicyStd     | 0.17985    |
| AveragePolicyStd[0]  | 0.20143    |
| AveragePolicyStd[1]  | 0.19205    |
| AveragePolicyStd[2]  | 0.14495    |
| AveragePolicyStd[3]  | 0.16857    |
| AveragePolicyStd[4]  | 0.14878    |
| AveragePolicyStd[5]  | 0.22335    |
| AverageReturn        | 1861.1     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 351.83     |
| AverageEpisodeLength | 947.85     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.05     |
| TotalNEpisodes       | 25520      |
| TotalNSamples        | 8.4572e+06 |
| ExplainedVariance    | 0.10456    |
-------------------------------------
[2018-01-21 17:01:09.579572 UTC] Saving snapshot
[2018-01-21 17:01:09.579823 UTC] Starting iteration 1690
[2018-01-21 17:01:09.579974 UTC] Start collecting samples
[2018-01-21 17:01:14.741024 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:14.921152 UTC] Performing policy update
[2018-01-21 17:01:14.925761 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:15.061077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:16.462934 UTC] Performing line search
[2018-01-21 17:01:16.653550 UTC] Updating baseline
[2018-01-21 17:01:18.975055 UTC] Computing logging information
-------------------------------------
| Iteration            | 1690       |
| ExpectedImprovement  | 0.018707   |
| ActualImprovement    | 0.017566   |
| ImprovementRatio     | 0.93902    |
| MeanKL               | 0.0084748  |
| Entropy              | -1.8555    |
| Perplexity           | 0.15638    |
| AveragePolicyStd     | 0.17985    |
| AveragePolicyStd[0]  | 0.20145    |
| AveragePolicyStd[1]  | 0.1924     |
| AveragePolicyStd[2]  | 0.14458    |
| AveragePolicyStd[3]  | 0.16867    |
| AveragePolicyStd[4]  | 0.14868    |
| AveragePolicyStd[5]  | 0.22335    |
| AverageReturn        | 1861.4     |
| MinReturn            | 297.96     |
| MaxReturn            | 2025.9     |
| StdReturn            | 351.97     |
| AverageEpisodeLength | 947.85     |
| MinEpisodeLength     | 194        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.05     |
| TotalNEpisodes       | 25524      |
| TotalNSamples        | 8.4612e+06 |
| ExplainedVariance    | -0.030707  |
-------------------------------------
[2018-01-21 17:01:19.810013 UTC] Saving snapshot
[2018-01-21 17:01:19.816978 UTC] Starting iteration 1691
[2018-01-21 17:01:19.817235 UTC] Start collecting samples
[2018-01-21 17:01:25.050095 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:25.190849 UTC] Performing policy update
[2018-01-21 17:01:25.191537 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:25.306479 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:26.759422 UTC] Performing line search
[2018-01-21 17:01:26.958074 UTC] Updating baseline
[2018-01-21 17:01:29.828415 UTC] Computing logging information
-------------------------------------
| Iteration            | 1691       |
| ExpectedImprovement  | 0.0183     |
| ActualImprovement    | 0.017194   |
| ImprovementRatio     | 0.93961    |
| MeanKL               | 0.0081803  |
| Entropy              | -1.8537    |
| Perplexity           | 0.15665    |
| AveragePolicyStd     | 0.17991    |
| AveragePolicyStd[0]  | 0.20168    |
| AveragePolicyStd[1]  | 0.19239    |
| AveragePolicyStd[2]  | 0.14458    |
| AveragePolicyStd[3]  | 0.16861    |
| AveragePolicyStd[4]  | 0.14883    |
| AveragePolicyStd[5]  | 0.22335    |
| AverageReturn        | 1844       |
| MinReturn            | 147.59     |
| MaxReturn            | 2025.9     |
| StdReturn            | 391.12     |
| AverageEpisodeLength | 939.03     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.04     |
| TotalNEpisodes       | 25529      |
| TotalNSamples        | 8.4653e+06 |
| ExplainedVariance    | 0.10524    |
-------------------------------------
[2018-01-21 17:01:30.754191 UTC] Saving snapshot
[2018-01-21 17:01:30.754456 UTC] Starting iteration 1692
[2018-01-21 17:01:30.754630 UTC] Start collecting samples
[2018-01-21 17:01:36.124596 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:36.278584 UTC] Performing policy update
[2018-01-21 17:01:36.279195 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:36.401754 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:37.969032 UTC] Performing line search
[2018-01-21 17:01:38.169863 UTC] Updating baseline
[2018-01-21 17:01:40.497326 UTC] Computing logging information
-------------------------------------
| Iteration            | 1692       |
| ExpectedImprovement  | 0.018527   |
| ActualImprovement    | 0.017421   |
| ImprovementRatio     | 0.94029    |
| MeanKL               | 0.008429   |
| Entropy              | -1.8628    |
| Perplexity           | 0.15523    |
| AveragePolicyStd     | 0.17963    |
| AveragePolicyStd[0]  | 0.20121    |
| AveragePolicyStd[1]  | 0.19158    |
| AveragePolicyStd[2]  | 0.14428    |
| AveragePolicyStd[3]  | 0.16884    |
| AveragePolicyStd[4]  | 0.1486     |
| AveragePolicyStd[5]  | 0.22329    |
| AverageReturn        | 1861.3     |
| MinReturn            | 147.59     |
| MaxReturn            | 2025.9     |
| StdReturn            | 359.31     |
| AverageEpisodeLength | 947.09     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.57     |
| TotalNEpisodes       | 25534      |
| TotalNSamples        | 8.4703e+06 |
| ExplainedVariance    | 0.019839   |
-------------------------------------
[2018-01-21 17:01:41.384633 UTC] Saving snapshot
[2018-01-21 17:01:41.384882 UTC] Starting iteration 1693
[2018-01-21 17:01:41.385043 UTC] Start collecting samples
[2018-01-21 17:01:47.406705 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:47.551886 UTC] Performing policy update
[2018-01-21 17:01:47.552844 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:47.675439 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:49.314260 UTC] Performing line search
[2018-01-21 17:01:49.616335 UTC] Updating baseline
[2018-01-21 17:01:51.718936 UTC] Computing logging information
-------------------------------------
| Iteration            | 1693       |
| ExpectedImprovement  | 0.017016   |
| ActualImprovement    | 0.016376   |
| ImprovementRatio     | 0.96238    |
| MeanKL               | 0.0078795  |
| Entropy              | -1.8645    |
| Perplexity           | 0.15497    |
| AveragePolicyStd     | 0.17958    |
| AveragePolicyStd[0]  | 0.20156    |
| AveragePolicyStd[1]  | 0.19118    |
| AveragePolicyStd[2]  | 0.14406    |
| AveragePolicyStd[3]  | 0.1688     |
| AveragePolicyStd[4]  | 0.14874    |
| AveragePolicyStd[5]  | 0.22316    |
| AverageReturn        | 1864.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2025.9     |
| StdReturn            | 357.35     |
| AverageEpisodeLength | 947.72     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.14     |
| TotalNEpisodes       | 25540      |
| TotalNSamples        | 8.4759e+06 |
| ExplainedVariance    | 0.17892    |
-------------------------------------
[2018-01-21 17:01:52.662602 UTC] Saving snapshot
[2018-01-21 17:01:52.662917 UTC] Starting iteration 1694
[2018-01-21 17:01:52.663153 UTC] Start collecting samples
[2018-01-21 17:01:58.263428 UTC] Computing input variables for policy optimization
[2018-01-21 17:01:58.409993 UTC] Performing policy update
[2018-01-21 17:01:58.411182 UTC] Computing gradient in Euclidean space
[2018-01-21 17:01:58.534830 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:01:59.948638 UTC] Performing line search
[2018-01-21 17:02:00.193063 UTC] Updating baseline
[2018-01-21 17:02:02.302068 UTC] Computing logging information
-------------------------------------
| Iteration            | 1694       |
| ExpectedImprovement  | 0.017212   |
| ActualImprovement    | 0.016462   |
| ImprovementRatio     | 0.95643    |
| MeanKL               | 0.008222   |
| Entropy              | -1.8683    |
| Perplexity           | 0.15439    |
| AveragePolicyStd     | 0.1795     |
| AveragePolicyStd[0]  | 0.20129    |
| AveragePolicyStd[1]  | 0.19149    |
| AveragePolicyStd[2]  | 0.14373    |
| AveragePolicyStd[3]  | 0.16888    |
| AveragePolicyStd[4]  | 0.14838    |
| AveragePolicyStd[5]  | 0.22323    |
| AverageReturn        | 1865       |
| MinReturn            | 147.59     |
| MaxReturn            | 2025.6     |
| StdReturn            | 354.37     |
| AverageEpisodeLength | 948.34     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.27     |
| TotalNEpisodes       | 25545      |
| TotalNSamples        | 8.4806e+06 |
| ExplainedVariance    | 0.13816    |
-------------------------------------
[2018-01-21 17:02:03.247660 UTC] Saving snapshot
[2018-01-21 17:02:03.247858 UTC] Starting iteration 1695
[2018-01-21 17:02:03.247972 UTC] Start collecting samples
[2018-01-21 17:02:09.611606 UTC] Computing input variables for policy optimization
[2018-01-21 17:02:09.800566 UTC] Performing policy update
[2018-01-21 17:02:09.802972 UTC] Computing gradient in Euclidean space
[2018-01-21 17:02:09.961304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:02:11.816621 UTC] Performing line search
[2018-01-21 17:02:12.034078 UTC] Updating baseline
[2018-01-21 17:02:14.308166 UTC] Computing logging information
-------------------------------------
| Iteration            | 1695       |
| ExpectedImprovement  | 0.019033   |
| ActualImprovement    | 0.017954   |
| ImprovementRatio     | 0.94333    |
| MeanKL               | 0.0081623  |
| Entropy              | -1.8639    |
| Perplexity           | 0.15507    |
| AveragePolicyStd     | 0.17963    |
| AveragePolicyStd[0]  | 0.20166    |
| AveragePolicyStd[1]  | 0.1912     |
| AveragePolicyStd[2]  | 0.14391    |
| AveragePolicyStd[3]  | 0.1688     |
| AveragePolicyStd[4]  | 0.14859    |
| AveragePolicyStd[5]  | 0.22366    |
| AverageReturn        | 1860.7     |
| MinReturn            | 147.59     |
| MaxReturn            | 2025.6     |
| StdReturn            | 357.48     |
| AverageEpisodeLength | 945.69     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.5      |
| TotalNEpisodes       | 25550      |
| TotalNSamples        | 8.4854e+06 |
| ExplainedVariance    | 0.086733   |
-------------------------------------
[2018-01-21 17:02:15.229079 UTC] Saving snapshot
[2018-01-21 17:02:15.229383 UTC] Starting iteration 1696
[2018-01-21 17:02:15.229592 UTC] Start collecting samples
[2018-01-21 17:02:20.683296 UTC] Computing input variables for policy optimization
[2018-01-21 17:02:20.838376 UTC] Performing policy update
[2018-01-21 17:02:20.839134 UTC] Computing gradient in Euclidean space
[2018-01-21 17:02:20.963208 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:02:22.595231 UTC] Performing line search
[2018-01-21 17:02:22.807571 UTC] Updating baseline
[2018-01-21 17:02:25.147162 UTC] Computing logging information
-------------------------------------
| Iteration            | 1696       |
| ExpectedImprovement  | 0.018647   |
| ActualImprovement    | 0.017391   |
| ImprovementRatio     | 0.93264    |
| MeanKL               | 0.0076828  |
| Entropy              | -1.8701    |
| Perplexity           | 0.15411    |
| AveragePolicyStd     | 0.17946    |
| AveragePolicyStd[0]  | 0.20156    |
| AveragePolicyStd[1]  | 0.19136    |
| AveragePolicyStd[2]  | 0.1437     |
| AveragePolicyStd[3]  | 0.16887    |
| AveragePolicyStd[4]  | 0.14805    |
| AveragePolicyStd[5]  | 0.22322    |
| AverageReturn        | 1864.9     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 358.88     |
| AverageEpisodeLength | 945.69     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.5      |
| TotalNEpisodes       | 25556      |
| TotalNSamples        | 8.4914e+06 |
| ExplainedVariance    | 0.066512   |
-------------------------------------
[2018-01-21 17:02:26.185915 UTC] Saving snapshot
[2018-01-21 17:02:26.186149 UTC] Starting iteration 1697
[2018-01-21 17:02:26.186298 UTC] Start collecting samples
[2018-01-21 17:02:31.495963 UTC] Computing input variables for policy optimization
[2018-01-21 17:02:31.674712 UTC] Performing policy update
[2018-01-21 17:02:31.675413 UTC] Computing gradient in Euclidean space
[2018-01-21 17:02:31.798117 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:02:33.388982 UTC] Performing line search
[2018-01-21 17:02:33.580926 UTC] Updating baseline
[2018-01-21 17:02:36.094221 UTC] Computing logging information
-------------------------------------
| Iteration            | 1697       |
| ExpectedImprovement  | 0.017541   |
| ActualImprovement    | 0.016972   |
| ImprovementRatio     | 0.96759    |
| MeanKL               | 0.0090837  |
| Entropy              | -1.8734    |
| Perplexity           | 0.15361    |
| AveragePolicyStd     | 0.17938    |
| AveragePolicyStd[0]  | 0.20171    |
| AveragePolicyStd[1]  | 0.19128    |
| AveragePolicyStd[2]  | 0.14312    |
| AveragePolicyStd[3]  | 0.16915    |
| AveragePolicyStd[4]  | 0.14798    |
| AveragePolicyStd[5]  | 0.22308    |
| AverageReturn        | 1855.5     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 374.97     |
| AverageEpisodeLength | 939.89     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.2      |
| TotalNEpisodes       | 25561      |
| TotalNSamples        | 8.4958e+06 |
| ExplainedVariance    | 0.1256     |
-------------------------------------
[2018-01-21 17:02:36.934552 UTC] Saving snapshot
[2018-01-21 17:02:36.934805 UTC] Starting iteration 1698
[2018-01-21 17:02:36.934989 UTC] Start collecting samples
[2018-01-21 17:02:41.885569 UTC] Computing input variables for policy optimization
[2018-01-21 17:02:42.036457 UTC] Performing policy update
[2018-01-21 17:02:42.037122 UTC] Computing gradient in Euclidean space
[2018-01-21 17:02:42.158857 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:02:43.698949 UTC] Performing line search
[2018-01-21 17:02:43.890530 UTC] Updating baseline
[2018-01-21 17:02:45.891237 UTC] Computing logging information
-------------------------------------
| Iteration            | 1698       |
| ExpectedImprovement  | 0.018143   |
| ActualImprovement    | 0.017378   |
| ImprovementRatio     | 0.95782    |
| MeanKL               | 0.0082904  |
| Entropy              | -1.8705    |
| Perplexity           | 0.15405    |
| AveragePolicyStd     | 0.17947    |
| AveragePolicyStd[0]  | 0.20189    |
| AveragePolicyStd[1]  | 0.19089    |
| AveragePolicyStd[2]  | 0.14329    |
| AveragePolicyStd[3]  | 0.16967    |
| AveragePolicyStd[4]  | 0.14777    |
| AveragePolicyStd[5]  | 0.22332    |
| AverageReturn        | 1835.1     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 413.85     |
| AverageEpisodeLength | 928.39     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.25     |
| TotalNEpisodes       | 25569      |
| TotalNSamples        | 8.5021e+06 |
| ExplainedVariance    | 0.2143     |
-------------------------------------
[2018-01-21 17:02:46.811179 UTC] Saving snapshot
[2018-01-21 17:02:46.811412 UTC] Starting iteration 1699
[2018-01-21 17:02:46.811591 UTC] Start collecting samples
[2018-01-21 17:02:51.912605 UTC] Computing input variables for policy optimization
[2018-01-21 17:02:52.054086 UTC] Performing policy update
[2018-01-21 17:02:52.055214 UTC] Computing gradient in Euclidean space
[2018-01-21 17:02:52.172621 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:02:53.742241 UTC] Performing line search
[2018-01-21 17:02:53.948244 UTC] Updating baseline
[2018-01-21 17:02:55.973932 UTC] Computing logging information
-------------------------------------
| Iteration            | 1699       |
| ExpectedImprovement  | 0.018251   |
| ActualImprovement    | 0.016495   |
| ImprovementRatio     | 0.90382    |
| MeanKL               | 0.0080771  |
| Entropy              | -1.8661    |
| Perplexity           | 0.15472    |
| AveragePolicyStd     | 0.17962    |
| AveragePolicyStd[0]  | 0.20203    |
| AveragePolicyStd[1]  | 0.19103    |
| AveragePolicyStd[2]  | 0.14326    |
| AveragePolicyStd[3]  | 0.1698     |
| AveragePolicyStd[4]  | 0.14781    |
| AveragePolicyStd[5]  | 0.22381    |
| AverageReturn        | 1840.3     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 413.14     |
| AverageEpisodeLength | 930.64     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.77     |
| TotalNEpisodes       | 25574      |
| TotalNSamples        | 8.5071e+06 |
| ExplainedVariance    | -0.0032621 |
-------------------------------------
[2018-01-21 17:02:56.883357 UTC] Saving snapshot
[2018-01-21 17:02:56.883729 UTC] Starting iteration 1700
[2018-01-21 17:02:56.884005 UTC] Start collecting samples
[2018-01-21 17:03:01.888896 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:02.074736 UTC] Performing policy update
[2018-01-21 17:03:02.076713 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:02.248611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:03.800491 UTC] Performing line search
[2018-01-21 17:03:04.007891 UTC] Updating baseline
[2018-01-21 17:03:07.187285 UTC] Computing logging information
-------------------------------------
| Iteration            | 1700       |
| ExpectedImprovement  | 0.019334   |
| ActualImprovement    | 0.017783   |
| ImprovementRatio     | 0.91976    |
| MeanKL               | 0.0073805  |
| Entropy              | -1.8682    |
| Perplexity           | 0.1544     |
| AveragePolicyStd     | 0.17955    |
| AveragePolicyStd[0]  | 0.20146    |
| AveragePolicyStd[1]  | 0.19107    |
| AveragePolicyStd[2]  | 0.1432     |
| AveragePolicyStd[3]  | 0.16984    |
| AveragePolicyStd[4]  | 0.14785    |
| AveragePolicyStd[5]  | 0.22389    |
| AverageReturn        | 1855.9     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 390.35     |
| AverageEpisodeLength | 937.89     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.58     |
| TotalNEpisodes       | 25577      |
| TotalNSamples        | 8.5101e+06 |
| ExplainedVariance    | 0.00062072 |
-------------------------------------
[2018-01-21 17:03:08.044020 UTC] Saving snapshot
[2018-01-21 17:03:08.053762 UTC] Starting iteration 1701
[2018-01-21 17:03:08.054007 UTC] Start collecting samples
[2018-01-21 17:03:12.839524 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:13.024807 UTC] Performing policy update
[2018-01-21 17:03:13.025478 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:13.143919 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:14.591039 UTC] Performing line search
[2018-01-21 17:03:14.785548 UTC] Updating baseline
[2018-01-21 17:03:17.875399 UTC] Computing logging information
-------------------------------------
| Iteration            | 1701       |
| ExpectedImprovement  | 0.0173     |
| ActualImprovement    | 0.015958   |
| ImprovementRatio     | 0.92241    |
| MeanKL               | 0.0084458  |
| Entropy              | -1.8675    |
| Perplexity           | 0.15451    |
| AveragePolicyStd     | 0.17958    |
| AveragePolicyStd[0]  | 0.20171    |
| AveragePolicyStd[1]  | 0.19106    |
| AveragePolicyStd[2]  | 0.14313    |
| AveragePolicyStd[3]  | 0.16977    |
| AveragePolicyStd[4]  | 0.14788    |
| AveragePolicyStd[5]  | 0.22394    |
| AverageReturn        | 1872.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 366.77     |
| AverageEpisodeLength | 944.98     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.97     |
| TotalNEpisodes       | 25582      |
| TotalNSamples        | 8.5151e+06 |
| ExplainedVariance    | 0.0037931  |
-------------------------------------
[2018-01-21 17:03:18.700898 UTC] Saving snapshot
[2018-01-21 17:03:18.701204 UTC] Starting iteration 1702
[2018-01-21 17:03:18.701411 UTC] Start collecting samples
[2018-01-21 17:03:23.702084 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:23.834463 UTC] Performing policy update
[2018-01-21 17:03:23.835084 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:23.956536 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:25.395821 UTC] Performing line search
[2018-01-21 17:03:25.611574 UTC] Updating baseline
[2018-01-21 17:03:27.721959 UTC] Computing logging information
-------------------------------------
| Iteration            | 1702       |
| ExpectedImprovement  | 0.017981   |
| ActualImprovement    | 0.017164   |
| ImprovementRatio     | 0.95457    |
| MeanKL               | 0.0081418  |
| Entropy              | -1.874     |
| Perplexity           | 0.15351    |
| AveragePolicyStd     | 0.17941    |
| AveragePolicyStd[0]  | 0.20163    |
| AveragePolicyStd[1]  | 0.1906     |
| AveragePolicyStd[2]  | 0.1428     |
| AveragePolicyStd[3]  | 0.16942    |
| AveragePolicyStd[4]  | 0.14784    |
| AveragePolicyStd[5]  | 0.22415    |
| AverageReturn        | 1900.8     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 328.44     |
| AverageEpisodeLength | 956.54     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.18     |
| TotalNEpisodes       | 25590      |
| TotalNSamples        | 8.5231e+06 |
| ExplainedVariance    | 0.0043107  |
-------------------------------------
[2018-01-21 17:03:28.686323 UTC] Saving snapshot
[2018-01-21 17:03:28.686599 UTC] Starting iteration 1703
[2018-01-21 17:03:28.686753 UTC] Start collecting samples
[2018-01-21 17:03:33.705886 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:33.840619 UTC] Performing policy update
[2018-01-21 17:03:33.841306 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:33.970574 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:35.605159 UTC] Performing line search
[2018-01-21 17:03:35.803746 UTC] Updating baseline
[2018-01-21 17:03:38.192769 UTC] Computing logging information
------------------------------------
| Iteration            | 1703      |
| ExpectedImprovement  | 0.019271  |
| ActualImprovement    | 0.018741  |
| ImprovementRatio     | 0.97253   |
| MeanKL               | 0.0086237 |
| Entropy              | -1.875    |
| Perplexity           | 0.15336   |
| AveragePolicyStd     | 0.17938   |
| AveragePolicyStd[0]  | 0.20169   |
| AveragePolicyStd[1]  | 0.19015   |
| AveragePolicyStd[2]  | 0.14283   |
| AveragePolicyStd[3]  | 0.16949   |
| AveragePolicyStd[4]  | 0.14781   |
| AveragePolicyStd[5]  | 0.2243    |
| AverageReturn        | 1877.7    |
| MinReturn            | 147.59    |
| MaxReturn            | 2074.3    |
| StdReturn            | 359.09    |
| AverageEpisodeLength | 945.58    |
| MinEpisodeLength     | 118       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 171.4     |
| TotalNEpisodes       | 25595     |
| TotalNSamples        | 8.527e+06 |
| ExplainedVariance    | 0.29535   |
------------------------------------
[2018-01-21 17:03:39.109397 UTC] Saving snapshot
[2018-01-21 17:03:39.109627 UTC] Starting iteration 1704
[2018-01-21 17:03:39.109778 UTC] Start collecting samples
[2018-01-21 17:03:44.050371 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:44.185051 UTC] Performing policy update
[2018-01-21 17:03:44.185711 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:44.306324 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:45.744721 UTC] Performing line search
[2018-01-21 17:03:45.943610 UTC] Updating baseline
[2018-01-21 17:03:47.999035 UTC] Computing logging information
-------------------------------------
| Iteration            | 1704       |
| ExpectedImprovement  | 0.01776    |
| ActualImprovement    | 0.017191   |
| ImprovementRatio     | 0.96795    |
| MeanKL               | 0.0077445  |
| Entropy              | -1.8729    |
| Perplexity           | 0.15368    |
| AveragePolicyStd     | 0.17943    |
| AveragePolicyStd[0]  | 0.20172    |
| AveragePolicyStd[1]  | 0.19035    |
| AveragePolicyStd[2]  | 0.14296    |
| AveragePolicyStd[3]  | 0.16937    |
| AveragePolicyStd[4]  | 0.14794    |
| AveragePolicyStd[5]  | 0.22426    |
| AverageReturn        | 1864.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 376.39     |
| AverageEpisodeLength | 938.69     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.77     |
| TotalNEpisodes       | 25600      |
| TotalNSamples        | 8.5311e+06 |
| ExplainedVariance    | 0.22061    |
-------------------------------------
[2018-01-21 17:03:48.882090 UTC] Saving snapshot
[2018-01-21 17:03:48.882373 UTC] Starting iteration 1705
[2018-01-21 17:03:48.882571 UTC] Start collecting samples
[2018-01-21 17:03:53.858715 UTC] Computing input variables for policy optimization
[2018-01-21 17:03:54.010299 UTC] Performing policy update
[2018-01-21 17:03:54.011926 UTC] Computing gradient in Euclidean space
[2018-01-21 17:03:54.142759 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:03:55.664168 UTC] Performing line search
[2018-01-21 17:03:55.900137 UTC] Updating baseline
[2018-01-21 17:03:59.115172 UTC] Computing logging information
-------------------------------------
| Iteration            | 1705       |
| ExpectedImprovement  | 0.017746   |
| ActualImprovement    | 0.016512   |
| ImprovementRatio     | 0.93043    |
| MeanKL               | 0.0078841  |
| Entropy              | -1.8707    |
| Perplexity           | 0.15401    |
| AveragePolicyStd     | 0.17951    |
| AveragePolicyStd[0]  | 0.20188    |
| AveragePolicyStd[1]  | 0.19037    |
| AveragePolicyStd[2]  | 0.14278    |
| AveragePolicyStd[3]  | 0.16981    |
| AveragePolicyStd[4]  | 0.14791    |
| AveragePolicyStd[5]  | 0.22428    |
| AverageReturn        | 1862.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 377.22     |
| AverageEpisodeLength | 937.11     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.92     |
| TotalNEpisodes       | 25605      |
| TotalNSamples        | 8.5359e+06 |
| ExplainedVariance    | 0.079962   |
-------------------------------------
[2018-01-21 17:04:00.082747 UTC] Saving snapshot
[2018-01-21 17:04:00.083027 UTC] Starting iteration 1706
[2018-01-21 17:04:00.083205 UTC] Start collecting samples
[2018-01-21 17:04:05.316217 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:05.440735 UTC] Performing policy update
[2018-01-21 17:04:05.441382 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:05.568823 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:07.068510 UTC] Performing line search
[2018-01-21 17:04:07.265056 UTC] Updating baseline
[2018-01-21 17:04:09.462488 UTC] Computing logging information
-------------------------------------
| Iteration            | 1706       |
| ExpectedImprovement  | 0.017984   |
| ActualImprovement    | 0.016602   |
| ImprovementRatio     | 0.92316    |
| MeanKL               | 0.00752    |
| Entropy              | -1.8729    |
| Perplexity           | 0.15368    |
| AveragePolicyStd     | 0.17945    |
| AveragePolicyStd[0]  | 0.20195    |
| AveragePolicyStd[1]  | 0.19038    |
| AveragePolicyStd[2]  | 0.14287    |
| AveragePolicyStd[3]  | 0.16951    |
| AveragePolicyStd[4]  | 0.14768    |
| AveragePolicyStd[5]  | 0.22431    |
| AverageReturn        | 1865.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 377.83     |
| AverageEpisodeLength | 937.49     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.02     |
| TotalNEpisodes       | 25609      |
| TotalNSamples        | 8.5399e+06 |
| ExplainedVariance    | 0.0053966  |
-------------------------------------
[2018-01-21 17:04:10.455741 UTC] Saving snapshot
[2018-01-21 17:04:10.455996 UTC] Starting iteration 1707
[2018-01-21 17:04:10.456163 UTC] Start collecting samples
[2018-01-21 17:04:15.371684 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:15.507128 UTC] Performing policy update
[2018-01-21 17:04:15.507815 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:15.647088 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:17.167529 UTC] Performing line search
[2018-01-21 17:04:17.359459 UTC] Updating baseline
[2018-01-21 17:04:19.526100 UTC] Computing logging information
-------------------------------------
| Iteration            | 1707       |
| ExpectedImprovement  | 0.018564   |
| ActualImprovement    | 0.016999   |
| ImprovementRatio     | 0.91567    |
| MeanKL               | 0.0075213  |
| Entropy              | -1.8709    |
| Perplexity           | 0.15399    |
| AveragePolicyStd     | 0.17951    |
| AveragePolicyStd[0]  | 0.20154    |
| AveragePolicyStd[1]  | 0.19055    |
| AveragePolicyStd[2]  | 0.14278    |
| AveragePolicyStd[3]  | 0.16975    |
| AveragePolicyStd[4]  | 0.1479     |
| AveragePolicyStd[5]  | 0.2245     |
| AverageReturn        | 1865.7     |
| MinReturn            | 147.59     |
| MaxReturn            | 2074.3     |
| StdReturn            | 377.93     |
| AverageEpisodeLength | 937.49     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.02     |
| TotalNEpisodes       | 25614      |
| TotalNSamples        | 8.5449e+06 |
| ExplainedVariance    | 0.0048959  |
-------------------------------------
[2018-01-21 17:04:20.432315 UTC] Saving snapshot
[2018-01-21 17:04:20.432553 UTC] Starting iteration 1708
[2018-01-21 17:04:20.432702 UTC] Start collecting samples
[2018-01-21 17:04:25.617841 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:25.785679 UTC] Performing policy update
[2018-01-21 17:04:25.786383 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:25.909810 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:27.359918 UTC] Performing line search
[2018-01-21 17:04:27.570172 UTC] Updating baseline
[2018-01-21 17:04:29.948607 UTC] Computing logging information
-------------------------------------
| Iteration            | 1708       |
| ExpectedImprovement  | 0.020371   |
| ActualImprovement    | 0.018559   |
| ImprovementRatio     | 0.91104    |
| MeanKL               | 0.0081652  |
| Entropy              | -1.8783    |
| Perplexity           | 0.15285    |
| AveragePolicyStd     | 0.17928    |
| AveragePolicyStd[0]  | 0.20181    |
| AveragePolicyStd[1]  | 0.18946    |
| AveragePolicyStd[2]  | 0.14252    |
| AveragePolicyStd[3]  | 0.16966    |
| AveragePolicyStd[4]  | 0.14798    |
| AveragePolicyStd[5]  | 0.22424    |
| AverageReturn        | 1868.6     |
| MinReturn            | 147.59     |
| MaxReturn            | 2088.8     |
| StdReturn            | 378.96     |
| AverageEpisodeLength | 937.53     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.03     |
| TotalNEpisodes       | 25620      |
| TotalNSamples        | 8.5509e+06 |
| ExplainedVariance    | 0.0016313  |
-------------------------------------
[2018-01-21 17:04:30.828394 UTC] Saving snapshot
[2018-01-21 17:04:30.828648 UTC] Starting iteration 1709
[2018-01-21 17:04:30.828823 UTC] Start collecting samples
[2018-01-21 17:04:35.922918 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:36.084246 UTC] Performing policy update
[2018-01-21 17:04:36.084940 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:36.214681 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:37.691453 UTC] Performing line search
[2018-01-21 17:04:37.902010 UTC] Updating baseline
[2018-01-21 17:04:40.271532 UTC] Computing logging information
-------------------------------------
| Iteration            | 1709       |
| ExpectedImprovement  | 0.016175   |
| ActualImprovement    | 0.01463    |
| ImprovementRatio     | 0.90447    |
| MeanKL               | 0.0082801  |
| Entropy              | -1.88      |
| Perplexity           | 0.15259    |
| AveragePolicyStd     | 0.17925    |
| AveragePolicyStd[0]  | 0.20236    |
| AveragePolicyStd[1]  | 0.1892     |
| AveragePolicyStd[2]  | 0.14274    |
| AveragePolicyStd[3]  | 0.16931    |
| AveragePolicyStd[4]  | 0.14752    |
| AveragePolicyStd[5]  | 0.22439    |
| AverageReturn        | 1857.1     |
| MinReturn            | 147.59     |
| MaxReturn            | 2088.8     |
| StdReturn            | 391.21     |
| AverageEpisodeLength | 932.25     |
| MinEpisodeLength     | 118        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.77     |
| TotalNEpisodes       | 25624      |
| TotalNSamples        | 8.5544e+06 |
| ExplainedVariance    | 0.12915    |
-------------------------------------
[2018-01-21 17:04:41.151192 UTC] Saving snapshot
[2018-01-21 17:04:41.151481 UTC] Starting iteration 1710
[2018-01-21 17:04:41.151663 UTC] Start collecting samples
[2018-01-21 17:04:46.690594 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:46.822361 UTC] Performing policy update
[2018-01-21 17:04:46.823065 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:46.961633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:48.398956 UTC] Performing line search
[2018-01-21 17:04:48.625423 UTC] Updating baseline
[2018-01-21 17:04:50.614998 UTC] Computing logging information
-------------------------------------
| Iteration            | 1710       |
| ExpectedImprovement  | 0.017734   |
| ActualImprovement    | 0.017275   |
| ImprovementRatio     | 0.97414    |
| MeanKL               | 0.0075408  |
| Entropy              | -1.8843    |
| Perplexity           | 0.15194    |
| AveragePolicyStd     | 0.1791     |
| AveragePolicyStd[0]  | 0.20239    |
| AveragePolicyStd[1]  | 0.1886     |
| AveragePolicyStd[2]  | 0.14287    |
| AveragePolicyStd[3]  | 0.16946    |
| AveragePolicyStd[4]  | 0.14742    |
| AveragePolicyStd[5]  | 0.22386    |
| AverageReturn        | 1873.6     |
| MinReturn            | 332.37     |
| MaxReturn            | 2088.8     |
| StdReturn            | 353.24     |
| AverageEpisodeLength | 939.25     |
| MinEpisodeLength     | 198        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.22     |
| TotalNEpisodes       | 25632      |
| TotalNSamples        | 8.5622e+06 |
| ExplainedVariance    | 0.055425   |
-------------------------------------
[2018-01-21 17:04:51.513113 UTC] Saving snapshot
[2018-01-21 17:04:51.524314 UTC] Starting iteration 1711
[2018-01-21 17:04:51.524573 UTC] Start collecting samples
[2018-01-21 17:04:56.612720 UTC] Computing input variables for policy optimization
[2018-01-21 17:04:56.755226 UTC] Performing policy update
[2018-01-21 17:04:56.755834 UTC] Computing gradient in Euclidean space
[2018-01-21 17:04:56.880758 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:04:58.404225 UTC] Performing line search
[2018-01-21 17:04:58.641226 UTC] Updating baseline
[2018-01-21 17:05:01.102771 UTC] Computing logging information
-------------------------------------
| Iteration            | 1711       |
| ExpectedImprovement  | 0.019377   |
| ActualImprovement    | 0.01781    |
| ImprovementRatio     | 0.91912    |
| MeanKL               | 0.0086772  |
| Entropy              | -1.8819    |
| Perplexity           | 0.1523     |
| AveragePolicyStd     | 0.17917    |
| AveragePolicyStd[0]  | 0.2025     |
| AveragePolicyStd[1]  | 0.18846    |
| AveragePolicyStd[2]  | 0.14289    |
| AveragePolicyStd[3]  | 0.16948    |
| AveragePolicyStd[4]  | 0.1476     |
| AveragePolicyStd[5]  | 0.2241     |
| AverageReturn        | 1873       |
| MinReturn            | 332.37     |
| MaxReturn            | 2088.8     |
| StdReturn            | 353.11     |
| AverageEpisodeLength | 939.25     |
| MinEpisodeLength     | 198        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.22     |
| TotalNEpisodes       | 25636      |
| TotalNSamples        | 8.5662e+06 |
| ExplainedVariance    | -0.0068453 |
-------------------------------------
[2018-01-21 17:05:02.044209 UTC] Saving snapshot
[2018-01-21 17:05:02.044438 UTC] Starting iteration 1712
[2018-01-21 17:05:02.044583 UTC] Start collecting samples
[2018-01-21 17:05:07.072903 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:07.230248 UTC] Performing policy update
[2018-01-21 17:05:07.230939 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:07.359093 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:08.809741 UTC] Performing line search
[2018-01-21 17:05:09.011964 UTC] Updating baseline
[2018-01-21 17:05:10.935661 UTC] Computing logging information
------------------------------------
| Iteration            | 1712      |
| ExpectedImprovement  | 0.019735  |
| ActualImprovement    | 0.018584  |
| ImprovementRatio     | 0.94169   |
| MeanKL               | 0.0079253 |
| Entropy              | -1.8839   |
| Perplexity           | 0.152     |
| AveragePolicyStd     | 0.17911   |
| AveragePolicyStd[0]  | 0.20195   |
| AveragePolicyStd[1]  | 0.18856   |
| AveragePolicyStd[2]  | 0.14327   |
| AveragePolicyStd[3]  | 0.16945   |
| AveragePolicyStd[4]  | 0.14716   |
| AveragePolicyStd[5]  | 0.22425   |
| AverageReturn        | 1861.7    |
| MinReturn            | 332.37    |
| MaxReturn            | 2088.8    |
| StdReturn            | 367.56    |
| AverageEpisodeLength | 934.03    |
| MinEpisodeLength     | 198       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 174.2     |
| TotalNEpisodes       | 25642     |
| TotalNSamples        | 8.571e+06 |
| ExplainedVariance    | 0.20025   |
------------------------------------
[2018-01-21 17:05:11.846334 UTC] Saving snapshot
[2018-01-21 17:05:11.846611 UTC] Starting iteration 1713
[2018-01-21 17:05:11.846787 UTC] Start collecting samples
[2018-01-21 17:05:17.082116 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:17.206104 UTC] Performing policy update
[2018-01-21 17:05:17.206767 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:17.331507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:18.804662 UTC] Performing line search
[2018-01-21 17:05:19.009832 UTC] Updating baseline
[2018-01-21 17:05:20.829173 UTC] Computing logging information
------------------------------------
| Iteration            | 1713      |
| ExpectedImprovement  | 0.016986  |
| ActualImprovement    | 0.015854  |
| ImprovementRatio     | 0.93332   |
| MeanKL               | 0.0082851 |
| Entropy              | -1.8837   |
| Perplexity           | 0.15203   |
| AveragePolicyStd     | 0.17914   |
| AveragePolicyStd[0]  | 0.20213   |
| AveragePolicyStd[1]  | 0.18826   |
| AveragePolicyStd[2]  | 0.14334   |
| AveragePolicyStd[3]  | 0.16928   |
| AveragePolicyStd[4]  | 0.147     |
| AveragePolicyStd[5]  | 0.22481   |
| AverageReturn        | 1864      |
| MinReturn            | 332.37    |
| MaxReturn            | 2088.8    |
| StdReturn            | 368.32    |
| AverageEpisodeLength | 934.03    |
| MinEpisodeLength     | 198       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 174.2     |
| TotalNEpisodes       | 25647     |
| TotalNSamples        | 8.576e+06 |
| ExplainedVariance    | -0.017378 |
------------------------------------
[2018-01-21 17:05:21.717021 UTC] Saving snapshot
[2018-01-21 17:05:21.717286 UTC] Starting iteration 1714
[2018-01-21 17:05:21.717474 UTC] Start collecting samples
[2018-01-21 17:05:26.424042 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:26.559595 UTC] Performing policy update
[2018-01-21 17:05:26.560382 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:26.701227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:28.133934 UTC] Performing line search
[2018-01-21 17:05:28.332521 UTC] Updating baseline
[2018-01-21 17:05:30.603097 UTC] Computing logging information
-------------------------------------
| Iteration            | 1714       |
| ExpectedImprovement  | 0.020315   |
| ActualImprovement    | 0.018395   |
| ImprovementRatio     | 0.90547    |
| MeanKL               | 0.0076035  |
| Entropy              | -1.8772    |
| Perplexity           | 0.15301    |
| AveragePolicyStd     | 0.17932    |
| AveragePolicyStd[0]  | 0.20239    |
| AveragePolicyStd[1]  | 0.18855    |
| AveragePolicyStd[2]  | 0.14347    |
| AveragePolicyStd[3]  | 0.16974    |
| AveragePolicyStd[4]  | 0.14703    |
| AveragePolicyStd[5]  | 0.22477    |
| AverageReturn        | 1868.6     |
| MinReturn            | 332.37     |
| MaxReturn            | 2088.8     |
| StdReturn            | 365.5      |
| AverageEpisodeLength | 935.98     |
| MinEpisodeLength     | 198        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.05     |
| TotalNEpisodes       | 25651      |
| TotalNSamples        | 8.5799e+06 |
| ExplainedVariance    | 0.1442     |
-------------------------------------
[2018-01-21 17:05:31.440056 UTC] Saving snapshot
[2018-01-21 17:05:31.440411 UTC] Starting iteration 1715
[2018-01-21 17:05:31.440653 UTC] Start collecting samples
[2018-01-21 17:05:36.743280 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:36.884966 UTC] Performing policy update
[2018-01-21 17:05:36.885693 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:37.008697 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:38.464273 UTC] Performing line search
[2018-01-21 17:05:38.659189 UTC] Updating baseline
[2018-01-21 17:05:40.667069 UTC] Computing logging information
-------------------------------------
| Iteration            | 1715       |
| ExpectedImprovement  | 0.019634   |
| ActualImprovement    | 0.018555   |
| ImprovementRatio     | 0.94504    |
| MeanKL               | 0.0080063  |
| Entropy              | -1.8789    |
| Perplexity           | 0.15275    |
| AveragePolicyStd     | 0.17929    |
| AveragePolicyStd[0]  | 0.20236    |
| AveragePolicyStd[1]  | 0.18873    |
| AveragePolicyStd[2]  | 0.14329    |
| AveragePolicyStd[3]  | 0.16964    |
| AveragePolicyStd[4]  | 0.14687    |
| AveragePolicyStd[5]  | 0.22485    |
| AverageReturn        | 1869.8     |
| MinReturn            | 332.37     |
| MaxReturn            | 2088.8     |
| StdReturn            | 363.65     |
| AverageEpisodeLength | 936.5      |
| MinEpisodeLength     | 198        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.57     |
| TotalNEpisodes       | 25659      |
| TotalNSamples        | 8.5874e+06 |
| ExplainedVariance    | 0.10325    |
-------------------------------------
[2018-01-21 17:05:41.554953 UTC] Saving snapshot
[2018-01-21 17:05:41.555217 UTC] Starting iteration 1716
[2018-01-21 17:05:41.555388 UTC] Start collecting samples
[2018-01-21 17:05:46.627411 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:46.798054 UTC] Performing policy update
[2018-01-21 17:05:46.798732 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:46.917481 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:48.465058 UTC] Performing line search
[2018-01-21 17:05:48.667761 UTC] Updating baseline
[2018-01-21 17:05:50.479420 UTC] Computing logging information
------------------------------------
| Iteration            | 1716      |
| ExpectedImprovement  | 0.018259  |
| ActualImprovement    | 0.017324  |
| ImprovementRatio     | 0.94876   |
| MeanKL               | 0.0084646 |
| Entropy              | -1.8887   |
| Perplexity           | 0.15126   |
| AveragePolicyStd     | 0.179     |
| AveragePolicyStd[0]  | 0.20213   |
| AveragePolicyStd[1]  | 0.18804   |
| AveragePolicyStd[2]  | 0.14312   |
| AveragePolicyStd[3]  | 0.16934   |
| AveragePolicyStd[4]  | 0.14671   |
| AveragePolicyStd[5]  | 0.22466   |
| AverageReturn        | 1859.8    |
| MinReturn            | 437.45    |
| MaxReturn            | 2088.8    |
| StdReturn            | 378.33    |
| AverageEpisodeLength | 932.33    |
| MinEpisodeLength     | 255       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 178.79    |
| TotalNEpisodes       | 25666     |
| TotalNSamples        | 8.593e+06 |
| ExplainedVariance    | 0.2937    |
------------------------------------
[2018-01-21 17:05:51.330997 UTC] Saving snapshot
[2018-01-21 17:05:51.331176 UTC] Starting iteration 1717
[2018-01-21 17:05:51.331314 UTC] Start collecting samples
[2018-01-21 17:05:56.597686 UTC] Computing input variables for policy optimization
[2018-01-21 17:05:56.720247 UTC] Performing policy update
[2018-01-21 17:05:56.721366 UTC] Computing gradient in Euclidean space
[2018-01-21 17:05:56.845295 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:05:58.398715 UTC] Performing line search
[2018-01-21 17:05:58.598961 UTC] Updating baseline
[2018-01-21 17:06:00.863172 UTC] Computing logging information
-------------------------------------
| Iteration            | 1717       |
| ExpectedImprovement  | 0.017457   |
| ActualImprovement    | 0.017245   |
| ImprovementRatio     | 0.98782    |
| MeanKL               | 0.0082974  |
| Entropy              | -1.8849    |
| Perplexity           | 0.15185    |
| AveragePolicyStd     | 0.17909    |
| AveragePolicyStd[0]  | 0.20211    |
| AveragePolicyStd[1]  | 0.18856    |
| AveragePolicyStd[2]  | 0.14359    |
| AveragePolicyStd[3]  | 0.16949    |
| AveragePolicyStd[4]  | 0.14655    |
| AveragePolicyStd[5]  | 0.22424    |
| AverageReturn        | 1855.2     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 379.95     |
| AverageEpisodeLength | 929.74     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.81     |
| TotalNEpisodes       | 25670      |
| TotalNSamples        | 8.5961e+06 |
| ExplainedVariance    | 0.24642    |
-------------------------------------
[2018-01-21 17:06:01.780163 UTC] Saving snapshot
[2018-01-21 17:06:01.780502 UTC] Starting iteration 1718
[2018-01-21 17:06:01.780696 UTC] Start collecting samples
[2018-01-21 17:06:06.871383 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:07.009578 UTC] Performing policy update
[2018-01-21 17:06:07.010737 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:07.132924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:08.589801 UTC] Performing line search
[2018-01-21 17:06:08.857179 UTC] Updating baseline
[2018-01-21 17:06:10.704964 UTC] Computing logging information
-------------------------------------
| Iteration            | 1718       |
| ExpectedImprovement  | 0.016275   |
| ActualImprovement    | 0.016042   |
| ImprovementRatio     | 0.98566    |
| MeanKL               | 0.0075924  |
| Entropy              | -1.8846    |
| Perplexity           | 0.15188    |
| AveragePolicyStd     | 0.17908    |
| AveragePolicyStd[0]  | 0.20201    |
| AveragePolicyStd[1]  | 0.18877    |
| AveragePolicyStd[2]  | 0.14363    |
| AveragePolicyStd[3]  | 0.16963    |
| AveragePolicyStd[4]  | 0.14658    |
| AveragePolicyStd[5]  | 0.22384    |
| AverageReturn        | 1830.5     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 396.76     |
| AverageEpisodeLength | 918.59     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.38     |
| TotalNEpisodes       | 25677      |
| TotalNSamples        | 8.6019e+06 |
| ExplainedVariance    | 0.38603    |
-------------------------------------
[2018-01-21 17:06:11.677751 UTC] Saving snapshot
[2018-01-21 17:06:11.677991 UTC] Starting iteration 1719
[2018-01-21 17:06:11.678168 UTC] Start collecting samples
[2018-01-21 17:06:16.637295 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:16.756400 UTC] Performing policy update
[2018-01-21 17:06:16.757041 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:16.878284 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:18.417418 UTC] Performing line search
[2018-01-21 17:06:18.624144 UTC] Updating baseline
[2018-01-21 17:06:21.803323 UTC] Computing logging information
-------------------------------------
| Iteration            | 1719       |
| ExpectedImprovement  | 0.018145   |
| ActualImprovement    | 0.017647   |
| ImprovementRatio     | 0.97258    |
| MeanKL               | 0.0077849  |
| Entropy              | -1.8888    |
| Perplexity           | 0.15125    |
| AveragePolicyStd     | 0.17894    |
| AveragePolicyStd[0]  | 0.20121    |
| AveragePolicyStd[1]  | 0.18912    |
| AveragePolicyStd[2]  | 0.14347    |
| AveragePolicyStd[3]  | 0.1687     |
| AveragePolicyStd[4]  | 0.14717    |
| AveragePolicyStd[5]  | 0.22397    |
| AverageReturn        | 1827.6     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 396.17     |
| AverageEpisodeLength | 917.4      |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.23     |
| TotalNEpisodes       | 25682      |
| TotalNSamples        | 8.6068e+06 |
| ExplainedVariance    | 0.097939   |
-------------------------------------
[2018-01-21 17:06:22.663207 UTC] Saving snapshot
[2018-01-21 17:06:22.663501 UTC] Starting iteration 1720
[2018-01-21 17:06:22.663699 UTC] Start collecting samples
[2018-01-21 17:06:27.617140 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:27.757728 UTC] Performing policy update
[2018-01-21 17:06:27.758694 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:27.888038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:29.403365 UTC] Performing line search
[2018-01-21 17:06:29.593678 UTC] Updating baseline
[2018-01-21 17:06:31.507366 UTC] Computing logging information
-------------------------------------
| Iteration            | 1720       |
| ExpectedImprovement  | 0.018795   |
| ActualImprovement    | 0.017119   |
| ImprovementRatio     | 0.91084    |
| MeanKL               | 0.008332   |
| Entropy              | -1.8925    |
| Perplexity           | 0.15069    |
| AveragePolicyStd     | 0.17884    |
| AveragePolicyStd[0]  | 0.20118    |
| AveragePolicyStd[1]  | 0.1897     |
| AveragePolicyStd[2]  | 0.14319    |
| AveragePolicyStd[3]  | 0.16871    |
| AveragePolicyStd[4]  | 0.14686    |
| AveragePolicyStd[5]  | 0.22338    |
| AverageReturn        | 1806.9     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 409.27     |
| AverageEpisodeLength | 907.66     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.34     |
| TotalNEpisodes       | 25686      |
| TotalNSamples        | 8.6099e+06 |
| ExplainedVariance    | 0.46732    |
-------------------------------------
[2018-01-21 17:06:32.365916 UTC] Saving snapshot
[2018-01-21 17:06:32.376628 UTC] Starting iteration 1721
[2018-01-21 17:06:32.376895 UTC] Start collecting samples
[2018-01-21 17:06:37.518136 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:37.670423 UTC] Performing policy update
[2018-01-21 17:06:37.671126 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:37.834947 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:39.378515 UTC] Performing line search
[2018-01-21 17:06:39.580286 UTC] Updating baseline
[2018-01-21 17:06:41.715363 UTC] Computing logging information
-------------------------------------
| Iteration            | 1721       |
| ExpectedImprovement  | 0.017846   |
| ActualImprovement    | 0.017135   |
| ImprovementRatio     | 0.96013    |
| MeanKL               | 0.0083634  |
| Entropy              | -1.8982    |
| Perplexity           | 0.14984    |
| AveragePolicyStd     | 0.17863    |
| AveragePolicyStd[0]  | 0.20097    |
| AveragePolicyStd[1]  | 0.18971    |
| AveragePolicyStd[2]  | 0.14327    |
| AveragePolicyStd[3]  | 0.16835    |
| AveragePolicyStd[4]  | 0.14696    |
| AveragePolicyStd[5]  | 0.22253    |
| AverageReturn        | 1825.5     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 385.22     |
| AverageEpisodeLength | 918.25     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.09     |
| TotalNEpisodes       | 25693      |
| TotalNSamples        | 8.6169e+06 |
| ExplainedVariance    | 0.0010397  |
-------------------------------------
[2018-01-21 17:06:42.620669 UTC] Saving snapshot
[2018-01-21 17:06:42.620928 UTC] Starting iteration 1722
[2018-01-21 17:06:42.621116 UTC] Start collecting samples
[2018-01-21 17:06:47.866927 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:48.010385 UTC] Performing policy update
[2018-01-21 17:06:48.011629 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:48.136344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:49.600289 UTC] Performing line search
[2018-01-21 17:06:49.807010 UTC] Updating baseline
[2018-01-21 17:06:52.107508 UTC] Computing logging information
-------------------------------------
| Iteration            | 1722       |
| ExpectedImprovement  | 0.018813   |
| ActualImprovement    | 0.017557   |
| ImprovementRatio     | 0.93324    |
| MeanKL               | 0.0080832  |
| Entropy              | -1.904     |
| Perplexity           | 0.14897    |
| AveragePolicyStd     | 0.17847    |
| AveragePolicyStd[0]  | 0.20093    |
| AveragePolicyStd[1]  | 0.18979    |
| AveragePolicyStd[2]  | 0.14309    |
| AveragePolicyStd[3]  | 0.1679     |
| AveragePolicyStd[4]  | 0.14677    |
| AveragePolicyStd[5]  | 0.22235    |
| AverageReturn        | 1831       |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 391.79     |
| AverageEpisodeLength | 920.64     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 184.26     |
| TotalNEpisodes       | 25697      |
| TotalNSamples        | 8.6201e+06 |
| ExplainedVariance    | 0.13614    |
-------------------------------------
[2018-01-21 17:06:52.999255 UTC] Saving snapshot
[2018-01-21 17:06:52.999506 UTC] Starting iteration 1723
[2018-01-21 17:06:52.999678 UTC] Start collecting samples
[2018-01-21 17:06:57.994193 UTC] Computing input variables for policy optimization
[2018-01-21 17:06:58.153324 UTC] Performing policy update
[2018-01-21 17:06:58.154554 UTC] Computing gradient in Euclidean space
[2018-01-21 17:06:58.276432 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:06:59.731118 UTC] Performing line search
[2018-01-21 17:06:59.954647 UTC] Updating baseline
[2018-01-21 17:07:02.598243 UTC] Computing logging information
-------------------------------------
| Iteration            | 1723       |
| ExpectedImprovement  | 0.019277   |
| ActualImprovement    | 0.018149   |
| ImprovementRatio     | 0.9415     |
| MeanKL               | 0.0081054  |
| Entropy              | -1.9062    |
| Perplexity           | 0.14864    |
| AveragePolicyStd     | 0.17839    |
| AveragePolicyStd[0]  | 0.20079    |
| AveragePolicyStd[1]  | 0.19018    |
| AveragePolicyStd[2]  | 0.14319    |
| AveragePolicyStd[3]  | 0.16777    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.22167    |
| AverageReturn        | 1824.7     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 400.28     |
| AverageEpisodeLength | 917.42     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.37     |
| TotalNEpisodes       | 25704      |
| TotalNSamples        | 8.6267e+06 |
| ExplainedVariance    | 0.080961   |
-------------------------------------
[2018-01-21 17:07:03.498279 UTC] Saving snapshot
[2018-01-21 17:07:03.498606 UTC] Starting iteration 1724
[2018-01-21 17:07:03.498803 UTC] Start collecting samples
[2018-01-21 17:07:08.605440 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:08.727479 UTC] Performing policy update
[2018-01-21 17:07:08.728186 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:08.848524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:07:10.272922 UTC] Performing line search
[2018-01-21 17:07:10.465320 UTC] Updating baseline
[2018-01-21 17:07:12.388579 UTC] Computing logging information
-------------------------------------
| Iteration            | 1724       |
| ExpectedImprovement  | 0.01834    |
| ActualImprovement    | 0.017173   |
| ImprovementRatio     | 0.93633    |
| MeanKL               | 0.0081682  |
| Entropy              | -1.9045    |
| Perplexity           | 0.1489     |
| AveragePolicyStd     | 0.17846    |
| AveragePolicyStd[0]  | 0.20101    |
| AveragePolicyStd[1]  | 0.19028    |
| AveragePolicyStd[2]  | 0.14295    |
| AveragePolicyStd[3]  | 0.16787    |
| AveragePolicyStd[4]  | 0.14677    |
| AveragePolicyStd[5]  | 0.22186    |
| AverageReturn        | 1818.3     |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 400.37     |
| AverageEpisodeLength | 915.35     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.59     |
| TotalNEpisodes       | 25708      |
| TotalNSamples        | 8.6305e+06 |
| ExplainedVariance    | 0.12513    |
-------------------------------------
[2018-01-21 17:07:13.264520 UTC] Saving snapshot
[2018-01-21 17:07:13.264767 UTC] Starting iteration 1725
[2018-01-21 17:07:13.264916 UTC] Start collecting samples
[2018-01-21 17:07:18.157836 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:18.285375 UTC] Performing policy update
[2018-01-21 17:07:18.286018 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:18.404555 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:07:19.931647 UTC] Performing line search
[2018-01-21 17:07:20.129322 UTC] Updating baseline
[2018-01-21 17:07:22.147738 UTC] Computing logging information
-------------------------------------
| Iteration            | 1725       |
| ExpectedImprovement  | 0.01773    |
| ActualImprovement    | 0.016793   |
| ImprovementRatio     | 0.94713    |
| MeanKL               | 0.0083935  |
| Entropy              | -1.9048    |
| Perplexity           | 0.14885    |
| AveragePolicyStd     | 0.17844    |
| AveragePolicyStd[0]  | 0.20059    |
| AveragePolicyStd[1]  | 0.18997    |
| AveragePolicyStd[2]  | 0.14299    |
| AveragePolicyStd[3]  | 0.1682     |
| AveragePolicyStd[4]  | 0.14676    |
| AveragePolicyStd[5]  | 0.22212    |
| AverageReturn        | 1819       |
| MinReturn            | 437.45     |
| MaxReturn            | 2088.8     |
| StdReturn            | 400.61     |
| AverageEpisodeLength | 915.35     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.59     |
| TotalNEpisodes       | 25712      |
| TotalNSamples        | 8.6345e+06 |
| ExplainedVariance    | 0.0042938  |
-------------------------------------
[2018-01-21 17:07:23.021469 UTC] Saving snapshot
[2018-01-21 17:07:23.021696 UTC] Starting iteration 1726
[2018-01-21 17:07:23.021876 UTC] Start collecting samples
[2018-01-21 17:07:28.186255 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:28.339578 UTC] Performing policy update
[2018-01-21 17:07:28.340342 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:28.462371 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:07:29.929371 UTC] Performing line search
[2018-01-21 17:07:30.126796 UTC] Updating baseline
[2018-01-21 17:07:32.262040 UTC] Computing logging information
-------------------------------------
| Iteration            | 1726       |
| ExpectedImprovement  | 0.018434   |
| ActualImprovement    | 0.01716    |
| ImprovementRatio     | 0.9309     |
| MeanKL               | 0.0078599  |
| Entropy              | -1.9039    |
| Perplexity           | 0.14898    |
| AveragePolicyStd     | 0.17845    |
| AveragePolicyStd[0]  | 0.2005     |
| AveragePolicyStd[1]  | 0.18986    |
| AveragePolicyStd[2]  | 0.1433     |
| AveragePolicyStd[3]  | 0.1678     |
| AveragePolicyStd[4]  | 0.14699    |
| AveragePolicyStd[5]  | 0.22224    |
| AverageReturn        | 1818.5     |
| MinReturn            | 437.45     |
| MaxReturn            | 2058.1     |
| StdReturn            | 400.38     |
| AverageEpisodeLength | 915.35     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.59     |
| TotalNEpisodes       | 25720      |
| TotalNSamples        | 8.6425e+06 |
| ExplainedVariance    | -0.0010638 |
-------------------------------------
[2018-01-21 17:07:33.242100 UTC] Saving snapshot
[2018-01-21 17:07:33.242413 UTC] Starting iteration 1727
[2018-01-21 17:07:33.242774 UTC] Start collecting samples
[2018-01-21 17:07:38.602374 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:38.748618 UTC] Performing policy update
[2018-01-21 17:07:38.749461 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:38.875548 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:07:40.433926 UTC] Performing line search
[2018-01-21 17:07:40.630634 UTC] Updating baseline
[2018-01-21 17:07:42.955420 UTC] Computing logging information
-------------------------------------
| Iteration            | 1727       |
| ExpectedImprovement  | 0.019783   |
| ActualImprovement    | 0.017289   |
| ImprovementRatio     | 0.87395    |
| MeanKL               | 0.0080574  |
| Entropy              | -1.9058    |
| Perplexity           | 0.1487     |
| AveragePolicyStd     | 0.17842    |
| AveragePolicyStd[0]  | 0.20038    |
| AveragePolicyStd[1]  | 0.19022    |
| AveragePolicyStd[2]  | 0.14284    |
| AveragePolicyStd[3]  | 0.16811    |
| AveragePolicyStd[4]  | 0.14677    |
| AveragePolicyStd[5]  | 0.22217    |
| AverageReturn        | 1830.8     |
| MinReturn            | 437.45     |
| MaxReturn            | 2058.1     |
| StdReturn            | 389.86     |
| AverageEpisodeLength | 920.63     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.43     |
| TotalNEpisodes       | 25724      |
| TotalNSamples        | 8.6465e+06 |
| ExplainedVariance    | 0.0032726  |
-------------------------------------
[2018-01-21 17:07:43.863510 UTC] Saving snapshot
[2018-01-21 17:07:43.863758 UTC] Starting iteration 1728
[2018-01-21 17:07:43.863938 UTC] Start collecting samples
[2018-01-21 17:07:48.879936 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:49.048435 UTC] Performing policy update
[2018-01-21 17:07:49.049232 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:49.174664 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:07:50.673715 UTC] Performing line search
[2018-01-21 17:07:50.877718 UTC] Updating baseline
[2018-01-21 17:07:53.342652 UTC] Computing logging information
-------------------------------------
| Iteration            | 1728       |
| ExpectedImprovement  | 0.022429   |
| ActualImprovement    | 0.020955   |
| ImprovementRatio     | 0.93428    |
| MeanKL               | 0.0076797  |
| Entropy              | -1.9123    |
| Perplexity           | 0.14774    |
| AveragePolicyStd     | 0.17822    |
| AveragePolicyStd[0]  | 0.20066    |
| AveragePolicyStd[1]  | 0.19011    |
| AveragePolicyStd[2]  | 0.14276    |
| AveragePolicyStd[3]  | 0.16785    |
| AveragePolicyStd[4]  | 0.14647    |
| AveragePolicyStd[5]  | 0.22148    |
| AverageReturn        | 1813       |
| MinReturn            | 437.45     |
| MaxReturn            | 2058.1     |
| StdReturn            | 400.15     |
| AverageEpisodeLength | 913.14     |
| MinEpisodeLength     | 255        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.79     |
| TotalNEpisodes       | 25730      |
| TotalNSamples        | 8.6517e+06 |
| ExplainedVariance    | 0.14599    |
-------------------------------------
[2018-01-21 17:07:54.281081 UTC] Saving snapshot
[2018-01-21 17:07:54.281364 UTC] Starting iteration 1729
[2018-01-21 17:07:54.281539 UTC] Start collecting samples
[2018-01-21 17:07:59.134849 UTC] Computing input variables for policy optimization
[2018-01-21 17:07:59.292720 UTC] Performing policy update
[2018-01-21 17:07:59.293314 UTC] Computing gradient in Euclidean space
[2018-01-21 17:07:59.409740 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:00.885327 UTC] Performing line search
[2018-01-21 17:08:01.111970 UTC] Updating baseline
[2018-01-21 17:08:02.890090 UTC] Computing logging information
-------------------------------------
| Iteration            | 1729       |
| ExpectedImprovement  | 0.018847   |
| ActualImprovement    | 0.017842   |
| ImprovementRatio     | 0.94667    |
| MeanKL               | 0.0083045  |
| Entropy              | -1.9126    |
| Perplexity           | 0.14769    |
| AveragePolicyStd     | 0.17821    |
| AveragePolicyStd[0]  | 0.20038    |
| AveragePolicyStd[1]  | 0.19018    |
| AveragePolicyStd[2]  | 0.143      |
| AveragePolicyStd[3]  | 0.16774    |
| AveragePolicyStd[4]  | 0.14632    |
| AveragePolicyStd[5]  | 0.22164    |
| AverageReturn        | 1791       |
| MinReturn            | 357.81     |
| MaxReturn            | 2058.1     |
| StdReturn            | 434.92     |
| AverageEpisodeLength | 901.86     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 205.22     |
| TotalNEpisodes       | 25736      |
| TotalNSamples        | 8.6564e+06 |
| ExplainedVariance    | 0.24729    |
-------------------------------------
[2018-01-21 17:08:03.858656 UTC] Saving snapshot
[2018-01-21 17:08:03.858889 UTC] Starting iteration 1730
[2018-01-21 17:08:03.859031 UTC] Start collecting samples
[2018-01-21 17:08:08.772419 UTC] Computing input variables for policy optimization
[2018-01-21 17:08:08.907300 UTC] Performing policy update
[2018-01-21 17:08:08.908643 UTC] Computing gradient in Euclidean space
[2018-01-21 17:08:09.039319 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:10.517590 UTC] Performing line search
[2018-01-21 17:08:10.731204 UTC] Updating baseline
[2018-01-21 17:08:13.141612 UTC] Computing logging information
-------------------------------------
| Iteration            | 1730       |
| ExpectedImprovement  | 0.018192   |
| ActualImprovement    | 0.016864   |
| ImprovementRatio     | 0.92701    |
| MeanKL               | 0.0083583  |
| Entropy              | -1.9097    |
| Perplexity           | 0.14812    |
| AveragePolicyStd     | 0.17827    |
| AveragePolicyStd[0]  | 0.20069    |
| AveragePolicyStd[1]  | 0.18997    |
| AveragePolicyStd[2]  | 0.14321    |
| AveragePolicyStd[3]  | 0.16803    |
| AveragePolicyStd[4]  | 0.14645    |
| AveragePolicyStd[5]  | 0.22129    |
| AverageReturn        | 1803.9     |
| MinReturn            | 357.81     |
| MaxReturn            | 2058.1     |
| StdReturn            | 429.99     |
| AverageEpisodeLength | 908.01     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 203        |
| TotalNEpisodes       | 25740      |
| TotalNSamples        | 8.6604e+06 |
| ExplainedVariance    | -0.013273  |
-------------------------------------
[2018-01-21 17:08:14.004273 UTC] Saving snapshot
[2018-01-21 17:08:14.011444 UTC] Starting iteration 1731
[2018-01-21 17:08:14.011632 UTC] Start collecting samples
[2018-01-21 17:08:18.974346 UTC] Computing input variables for policy optimization
[2018-01-21 17:08:19.126069 UTC] Performing policy update
[2018-01-21 17:08:19.129658 UTC] Computing gradient in Euclidean space
[2018-01-21 17:08:19.255504 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:20.721262 UTC] Performing line search
[2018-01-21 17:08:20.914133 UTC] Updating baseline
[2018-01-21 17:08:23.183452 UTC] Computing logging information
-------------------------------------
| Iteration            | 1731       |
| ExpectedImprovement  | 0.018183   |
| ActualImprovement    | 0.01716    |
| ImprovementRatio     | 0.94376    |
| MeanKL               | 0.0092262  |
| Entropy              | -1.9106    |
| Perplexity           | 0.14799    |
| AveragePolicyStd     | 0.17824    |
| AveragePolicyStd[0]  | 0.20041    |
| AveragePolicyStd[1]  | 0.19002    |
| AveragePolicyStd[2]  | 0.14311    |
| AveragePolicyStd[3]  | 0.16831    |
| AveragePolicyStd[4]  | 0.14643    |
| AveragePolicyStd[5]  | 0.22116    |
| AverageReturn        | 1800.8     |
| MinReturn            | 357.81     |
| MaxReturn            | 2058.1     |
| StdReturn            | 421.27     |
| AverageEpisodeLength | 907.57     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.36     |
| TotalNEpisodes       | 25747      |
| TotalNSamples        | 8.6668e+06 |
| ExplainedVariance    | 0.15055    |
-------------------------------------
[2018-01-21 17:08:24.128962 UTC] Saving snapshot
[2018-01-21 17:08:24.129322 UTC] Starting iteration 1732
[2018-01-21 17:08:24.129554 UTC] Start collecting samples
[2018-01-21 17:08:28.973547 UTC] Computing input variables for policy optimization
[2018-01-21 17:08:29.171143 UTC] Performing policy update
[2018-01-21 17:08:29.171900 UTC] Computing gradient in Euclidean space
[2018-01-21 17:08:29.302608 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:30.780014 UTC] Performing line search
[2018-01-21 17:08:30.988596 UTC] Updating baseline
[2018-01-21 17:08:33.190756 UTC] Computing logging information
-------------------------------------
| Iteration            | 1732       |
| ExpectedImprovement  | 0.018031   |
| ActualImprovement    | 0.016745   |
| ImprovementRatio     | 0.92866    |
| MeanKL               | 0.0092731  |
| Entropy              | -1.9109    |
| Perplexity           | 0.14795    |
| AveragePolicyStd     | 0.17822    |
| AveragePolicyStd[0]  | 0.20049    |
| AveragePolicyStd[1]  | 0.19007    |
| AveragePolicyStd[2]  | 0.14318    |
| AveragePolicyStd[3]  | 0.16817    |
| AveragePolicyStd[4]  | 0.14648    |
| AveragePolicyStd[5]  | 0.22094    |
| AverageReturn        | 1801.6     |
| MinReturn            | 357.81     |
| MaxReturn            | 2058.1     |
| StdReturn            | 421.44     |
| AverageEpisodeLength | 908.27     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.56     |
| TotalNEpisodes       | 25751      |
| TotalNSamples        | 8.6708e+06 |
| ExplainedVariance    | 0.0054335  |
-------------------------------------
[2018-01-21 17:08:34.192429 UTC] Saving snapshot
[2018-01-21 17:08:34.192704 UTC] Starting iteration 1733
[2018-01-21 17:08:34.192898 UTC] Start collecting samples
[2018-01-21 17:08:40.331428 UTC] Computing input variables for policy optimization
[2018-01-21 17:08:40.492996 UTC] Performing policy update
[2018-01-21 17:08:40.493723 UTC] Computing gradient in Euclidean space
[2018-01-21 17:08:40.674332 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:42.240004 UTC] Performing line search
[2018-01-21 17:08:42.432218 UTC] Updating baseline
[2018-01-21 17:08:45.760384 UTC] Computing logging information
-------------------------------------
| Iteration            | 1733       |
| ExpectedImprovement  | 0.015671   |
| ActualImprovement    | 0.015313   |
| ImprovementRatio     | 0.97714    |
| MeanKL               | 0.0080083  |
| Entropy              | -1.9099    |
| Perplexity           | 0.1481     |
| AveragePolicyStd     | 0.17829    |
| AveragePolicyStd[0]  | 0.20046    |
| AveragePolicyStd[1]  | 0.19079    |
| AveragePolicyStd[2]  | 0.14282    |
| AveragePolicyStd[3]  | 0.16798    |
| AveragePolicyStd[4]  | 0.14638    |
| AveragePolicyStd[5]  | 0.22131    |
| AverageReturn        | 1811.2     |
| MinReturn            | 357.81     |
| MaxReturn            | 2049.3     |
| StdReturn            | 410.38     |
| AverageEpisodeLength | 913.55     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.88     |
| TotalNEpisodes       | 25756      |
| TotalNSamples        | 8.6758e+06 |
| ExplainedVariance    | -0.009546  |
-------------------------------------
[2018-01-21 17:08:46.668185 UTC] Saving snapshot
[2018-01-21 17:08:46.668420 UTC] Starting iteration 1734
[2018-01-21 17:08:46.668567 UTC] Start collecting samples
[2018-01-21 17:08:51.570495 UTC] Computing input variables for policy optimization
[2018-01-21 17:08:51.704552 UTC] Performing policy update
[2018-01-21 17:08:51.705095 UTC] Computing gradient in Euclidean space
[2018-01-21 17:08:51.829138 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:08:53.229218 UTC] Performing line search
[2018-01-21 17:08:53.426750 UTC] Updating baseline
[2018-01-21 17:08:55.441224 UTC] Computing logging information
-------------------------------------
| Iteration            | 1734       |
| ExpectedImprovement  | 0.016808   |
| ActualImprovement    | 0.015575   |
| ImprovementRatio     | 0.92667    |
| MeanKL               | 0.0080333  |
| Entropy              | -1.909     |
| Perplexity           | 0.14823    |
| AveragePolicyStd     | 0.17832    |
| AveragePolicyStd[0]  | 0.20052    |
| AveragePolicyStd[1]  | 0.19067    |
| AveragePolicyStd[2]  | 0.14263    |
| AveragePolicyStd[3]  | 0.16835    |
| AveragePolicyStd[4]  | 0.14645    |
| AveragePolicyStd[5]  | 0.22129    |
| AverageReturn        | 1810.5     |
| MinReturn            | 357.81     |
| MaxReturn            | 2049.3     |
| StdReturn            | 410.05     |
| AverageEpisodeLength | 913.55     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.88     |
| TotalNEpisodes       | 25759      |
| TotalNSamples        | 8.6788e+06 |
| ExplainedVariance    | 0.0052732  |
-------------------------------------
[2018-01-21 17:08:56.308840 UTC] Saving snapshot
[2018-01-21 17:08:56.309121 UTC] Starting iteration 1735
[2018-01-21 17:08:56.309311 UTC] Start collecting samples
[2018-01-21 17:09:01.544429 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:01.702151 UTC] Performing policy update
[2018-01-21 17:09:01.702930 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:01.821097 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:03.241316 UTC] Performing line search
[2018-01-21 17:09:03.426006 UTC] Updating baseline
[2018-01-21 17:09:06.317677 UTC] Computing logging information
-------------------------------------
| Iteration            | 1735       |
| ExpectedImprovement  | 0.017588   |
| ActualImprovement    | 0.016664   |
| ImprovementRatio     | 0.94743    |
| MeanKL               | 0.008588   |
| Entropy              | -1.9026    |
| Perplexity           | 0.14918    |
| AveragePolicyStd     | 0.17853    |
| AveragePolicyStd[0]  | 0.20063    |
| AveragePolicyStd[1]  | 0.19065    |
| AveragePolicyStd[2]  | 0.14264    |
| AveragePolicyStd[3]  | 0.16872    |
| AveragePolicyStd[4]  | 0.14651    |
| AveragePolicyStd[5]  | 0.22202    |
| AverageReturn        | 1839       |
| MinReturn            | 357.81     |
| MaxReturn            | 2049.3     |
| StdReturn            | 367.56     |
| AverageEpisodeLength | 927.67     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.18     |
| TotalNEpisodes       | 25766      |
| TotalNSamples        | 8.6858e+06 |
| ExplainedVariance    | 0.0046554  |
-------------------------------------
[2018-01-21 17:09:07.324041 UTC] Saving snapshot
[2018-01-21 17:09:07.324328 UTC] Starting iteration 1736
[2018-01-21 17:09:07.324509 UTC] Start collecting samples
[2018-01-21 17:09:12.261846 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:12.392925 UTC] Performing policy update
[2018-01-21 17:09:12.395898 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:12.515555 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:14.021803 UTC] Performing line search
[2018-01-21 17:09:14.224843 UTC] Updating baseline
[2018-01-21 17:09:17.276757 UTC] Computing logging information
-------------------------------------
| Iteration            | 1736       |
| ExpectedImprovement  | 0.017487   |
| ActualImprovement    | 0.016604   |
| ImprovementRatio     | 0.94952    |
| MeanKL               | 0.0082417  |
| Entropy              | -1.9074    |
| Perplexity           | 0.14846    |
| AveragePolicyStd     | 0.17838    |
| AveragePolicyStd[0]  | 0.20037    |
| AveragePolicyStd[1]  | 0.19042    |
| AveragePolicyStd[2]  | 0.14244    |
| AveragePolicyStd[3]  | 0.16875    |
| AveragePolicyStd[4]  | 0.14644    |
| AveragePolicyStd[5]  | 0.22187    |
| AverageReturn        | 1863.1     |
| MinReturn            | 357.81     |
| MaxReturn            | 2049.3     |
| StdReturn            | 340.4      |
| AverageEpisodeLength | 939.49     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.99     |
| TotalNEpisodes       | 25771      |
| TotalNSamples        | 8.6908e+06 |
| ExplainedVariance    | 0.0043229  |
-------------------------------------
[2018-01-21 17:09:18.162082 UTC] Saving snapshot
[2018-01-21 17:09:18.162401 UTC] Starting iteration 1737
[2018-01-21 17:09:18.162637 UTC] Start collecting samples
[2018-01-21 17:09:23.043967 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:23.202866 UTC] Performing policy update
[2018-01-21 17:09:23.203945 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:23.331614 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:24.751629 UTC] Performing line search
[2018-01-21 17:09:24.959292 UTC] Updating baseline
[2018-01-21 17:09:28.171062 UTC] Computing logging information
-------------------------------------
| Iteration            | 1737       |
| ExpectedImprovement  | 0.018162   |
| ActualImprovement    | 0.017137   |
| ImprovementRatio     | 0.94356    |
| MeanKL               | 0.0085092  |
| Entropy              | -1.9115    |
| Perplexity           | 0.14786    |
| AveragePolicyStd     | 0.17824    |
| AveragePolicyStd[0]  | 0.20007    |
| AveragePolicyStd[1]  | 0.18974    |
| AveragePolicyStd[2]  | 0.14246    |
| AveragePolicyStd[3]  | 0.16905    |
| AveragePolicyStd[4]  | 0.14647    |
| AveragePolicyStd[5]  | 0.22163    |
| AverageReturn        | 1872.9     |
| MinReturn            | 357.81     |
| MaxReturn            | 2049.3     |
| StdReturn            | 329.21     |
| AverageEpisodeLength | 944.29     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.54     |
| TotalNEpisodes       | 25774      |
| TotalNSamples        | 8.6938e+06 |
| ExplainedVariance    | 0.0039123  |
-------------------------------------
[2018-01-21 17:09:29.036006 UTC] Saving snapshot
[2018-01-21 17:09:29.036254 UTC] Starting iteration 1738
[2018-01-21 17:09:29.036430 UTC] Start collecting samples
[2018-01-21 17:09:34.765512 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:34.939237 UTC] Performing policy update
[2018-01-21 17:09:34.939840 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:35.072761 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:36.571477 UTC] Performing line search
[2018-01-21 17:09:36.801376 UTC] Updating baseline
[2018-01-21 17:09:39.154384 UTC] Computing logging information
------------------------------------
| Iteration            | 1738      |
| ExpectedImprovement  | 0.020112  |
| ActualImprovement    | 0.019136  |
| ImprovementRatio     | 0.9515    |
| MeanKL               | 0.0076903 |
| Entropy              | -1.9152   |
| Perplexity           | 0.14731   |
| AveragePolicyStd     | 0.17811   |
| AveragePolicyStd[0]  | 0.19987   |
| AveragePolicyStd[1]  | 0.19022   |
| AveragePolicyStd[2]  | 0.1424    |
| AveragePolicyStd[3]  | 0.1689    |
| AveragePolicyStd[4]  | 0.14636   |
| AveragePolicyStd[5]  | 0.22093   |
| AverageReturn        | 1869.9    |
| MinReturn            | 357.81    |
| MaxReturn            | 2047      |
| StdReturn            | 348.06    |
| AverageEpisodeLength | 943.66    |
| MinEpisodeLength     | 220       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 166.89    |
| TotalNEpisodes       | 25783     |
| TotalNSamples        | 8.702e+06 |
| ExplainedVariance    | 0.056827  |
------------------------------------
[2018-01-21 17:09:40.049009 UTC] Saving snapshot
[2018-01-21 17:09:40.049311 UTC] Starting iteration 1739
[2018-01-21 17:09:40.049491 UTC] Start collecting samples
[2018-01-21 17:09:45.190051 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:45.331706 UTC] Performing policy update
[2018-01-21 17:09:45.332443 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:45.450741 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:46.998033 UTC] Performing line search
[2018-01-21 17:09:47.188157 UTC] Updating baseline
[2018-01-21 17:09:48.903920 UTC] Computing logging information
-------------------------------------
| Iteration            | 1739       |
| ExpectedImprovement  | 0.015814   |
| ActualImprovement    | 0.015218   |
| ImprovementRatio     | 0.96231    |
| MeanKL               | 0.007832   |
| Entropy              | -1.9166    |
| Perplexity           | 0.1471     |
| AveragePolicyStd     | 0.17805    |
| AveragePolicyStd[0]  | 0.19971    |
| AveragePolicyStd[1]  | 0.19037    |
| AveragePolicyStd[2]  | 0.14241    |
| AveragePolicyStd[3]  | 0.1688     |
| AveragePolicyStd[4]  | 0.14647    |
| AveragePolicyStd[5]  | 0.22058    |
| AverageReturn        | 1887.9     |
| MinReturn            | 357.81     |
| MaxReturn            | 2047       |
| StdReturn            | 328.99     |
| AverageEpisodeLength | 951.89     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.87     |
| TotalNEpisodes       | 25788      |
| TotalNSamples        | 8.707e+06  |
| ExplainedVariance    | -0.0057402 |
-------------------------------------
[2018-01-21 17:09:49.861107 UTC] Saving snapshot
[2018-01-21 17:09:49.861381 UTC] Starting iteration 1740
[2018-01-21 17:09:49.861583 UTC] Start collecting samples
[2018-01-21 17:09:55.053660 UTC] Computing input variables for policy optimization
[2018-01-21 17:09:55.183657 UTC] Performing policy update
[2018-01-21 17:09:55.184725 UTC] Computing gradient in Euclidean space
[2018-01-21 17:09:55.310336 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:09:56.754901 UTC] Performing line search
[2018-01-21 17:09:56.955679 UTC] Updating baseline
[2018-01-21 17:09:59.645923 UTC] Computing logging information
------------------------------------
| Iteration            | 1740      |
| ExpectedImprovement  | 0.018388  |
| ActualImprovement    | 0.016388  |
| ImprovementRatio     | 0.89127   |
| MeanKL               | 0.0073586 |
| Entropy              | -1.914    |
| Perplexity           | 0.14749   |
| AveragePolicyStd     | 0.17815   |
| AveragePolicyStd[0]  | 0.20002   |
| AveragePolicyStd[1]  | 0.19038   |
| AveragePolicyStd[2]  | 0.14236   |
| AveragePolicyStd[3]  | 0.16837   |
| AveragePolicyStd[4]  | 0.14678   |
| AveragePolicyStd[5]  | 0.22096   |
| AverageReturn        | 1887.2    |
| MinReturn            | 357.81    |
| MaxReturn            | 2047      |
| StdReturn            | 328.88    |
| AverageEpisodeLength | 951.54    |
| MinEpisodeLength     | 220       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 157.8     |
| TotalNEpisodes       | 25790     |
| TotalNSamples        | 8.709e+06 |
| ExplainedVariance    | 0.20394   |
------------------------------------
[2018-01-21 17:10:00.655117 UTC] Saving snapshot
[2018-01-21 17:10:00.665138 UTC] Starting iteration 1741
[2018-01-21 17:10:00.665370 UTC] Start collecting samples
[2018-01-21 17:10:05.594480 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:05.724535 UTC] Performing policy update
[2018-01-21 17:10:05.725133 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:05.853980 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:07.316715 UTC] Performing line search
[2018-01-21 17:10:07.503470 UTC] Updating baseline
[2018-01-21 17:10:09.831149 UTC] Computing logging information
------------------------------------
| Iteration            | 1741      |
| ExpectedImprovement  | 0.016695  |
| ActualImprovement    | 0.016345  |
| ImprovementRatio     | 0.97901   |
| MeanKL               | 0.008038  |
| Entropy              | -1.9162   |
| Perplexity           | 0.14716   |
| AveragePolicyStd     | 0.17804   |
| AveragePolicyStd[0]  | 0.20051   |
| AveragePolicyStd[1]  | 0.19005   |
| AveragePolicyStd[2]  | 0.14235   |
| AveragePolicyStd[3]  | 0.16829   |
| AveragePolicyStd[4]  | 0.14708   |
| AveragePolicyStd[5]  | 0.21998   |
| AverageReturn        | 1901.7    |
| MinReturn            | 357.81    |
| MaxReturn            | 2047      |
| StdReturn            | 298.51    |
| AverageEpisodeLength | 958.62    |
| MinEpisodeLength     | 220       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 143.27    |
| TotalNEpisodes       | 25798     |
| TotalNSamples        | 8.717e+06 |
| ExplainedVariance    | -0.020343 |
------------------------------------
[2018-01-21 17:10:10.759033 UTC] Saving snapshot
[2018-01-21 17:10:10.759228 UTC] Starting iteration 1742
[2018-01-21 17:10:10.759358 UTC] Start collecting samples
[2018-01-21 17:10:15.662378 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:15.790370 UTC] Performing policy update
[2018-01-21 17:10:15.790999 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:15.909010 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:17.282419 UTC] Performing line search
[2018-01-21 17:10:17.472604 UTC] Updating baseline
[2018-01-21 17:10:20.727355 UTC] Computing logging information
------------------------------------
| Iteration            | 1742      |
| ExpectedImprovement  | 0.019288  |
| ActualImprovement    | 0.018006  |
| ImprovementRatio     | 0.93352   |
| MeanKL               | 0.0078604 |
| Entropy              | -1.9132   |
| Perplexity           | 0.14761   |
| AveragePolicyStd     | 0.17813   |
| AveragePolicyStd[0]  | 0.20058   |
| AveragePolicyStd[1]  | 0.19002   |
| AveragePolicyStd[2]  | 0.14251   |
| AveragePolicyStd[3]  | 0.16835   |
| AveragePolicyStd[4]  | 0.14725   |
| AveragePolicyStd[5]  | 0.22004   |
| AverageReturn        | 1910.6    |
| MinReturn            | 357.81    |
| MaxReturn            | 2046.6    |
| StdReturn            | 284.14    |
| AverageEpisodeLength | 963.42    |
| MinEpisodeLength     | 220       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 136.37    |
| TotalNEpisodes       | 25804     |
| TotalNSamples        | 8.723e+06 |
| ExplainedVariance    | 0.0040664 |
------------------------------------
[2018-01-21 17:10:21.665260 UTC] Saving snapshot
[2018-01-21 17:10:21.665502 UTC] Starting iteration 1743
[2018-01-21 17:10:21.665720 UTC] Start collecting samples
[2018-01-21 17:10:26.534445 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:26.662686 UTC] Performing policy update
[2018-01-21 17:10:26.663358 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:26.791469 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:28.297056 UTC] Performing line search
[2018-01-21 17:10:28.495789 UTC] Updating baseline
[2018-01-21 17:10:31.370967 UTC] Computing logging information
------------------------------------
| Iteration            | 1743      |
| ExpectedImprovement  | 0.018986  |
| ActualImprovement    | 0.017361  |
| ImprovementRatio     | 0.91438   |
| MeanKL               | 0.0078913 |
| Entropy              | -1.9163   |
| Perplexity           | 0.14715   |
| AveragePolicyStd     | 0.17802   |
| AveragePolicyStd[0]  | 0.20016   |
| AveragePolicyStd[1]  | 0.18972   |
| AveragePolicyStd[2]  | 0.14222   |
| AveragePolicyStd[3]  | 0.16888   |
| AveragePolicyStd[4]  | 0.14727   |
| AveragePolicyStd[5]  | 0.21988   |
| AverageReturn        | 1917.3    |
| MinReturn            | 357.81    |
| MaxReturn            | 2046.6    |
| StdReturn            | 282.04    |
| AverageEpisodeLength | 965.49    |
| MinEpisodeLength     | 220       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 135.33    |
| TotalNEpisodes       | 25806     |
| TotalNSamples        | 8.725e+06 |
| ExplainedVariance    | 0.11864   |
------------------------------------
[2018-01-21 17:10:32.253370 UTC] Saving snapshot
[2018-01-21 17:10:32.253643 UTC] Starting iteration 1744
[2018-01-21 17:10:32.253874 UTC] Start collecting samples
[2018-01-21 17:10:37.439709 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:37.576265 UTC] Performing policy update
[2018-01-21 17:10:37.577013 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:37.703508 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:39.130706 UTC] Performing line search
[2018-01-21 17:10:39.323944 UTC] Updating baseline
[2018-01-21 17:10:41.668938 UTC] Computing logging information
-------------------------------------
| Iteration            | 1744       |
| ExpectedImprovement  | 0.01612    |
| ActualImprovement    | 0.015499   |
| ImprovementRatio     | 0.96148    |
| MeanKL               | 0.0093789  |
| Entropy              | -1.9191    |
| Perplexity           | 0.14673    |
| AveragePolicyStd     | 0.17793    |
| AveragePolicyStd[0]  | 0.19968    |
| AveragePolicyStd[1]  | 0.18913    |
| AveragePolicyStd[2]  | 0.14206    |
| AveragePolicyStd[3]  | 0.1689     |
| AveragePolicyStd[4]  | 0.14761    |
| AveragePolicyStd[5]  | 0.22019    |
| AverageReturn        | 1916.1     |
| MinReturn            | 357.81     |
| MaxReturn            | 2053.4     |
| StdReturn            | 282.72     |
| AverageEpisodeLength | 964.61     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.39     |
| TotalNEpisodes       | 25813      |
| TotalNSamples        | 8.7319e+06 |
| ExplainedVariance    | 0.013384   |
-------------------------------------
[2018-01-21 17:10:42.568167 UTC] Saving snapshot
[2018-01-21 17:10:42.568387 UTC] Starting iteration 1745
[2018-01-21 17:10:42.568539 UTC] Start collecting samples
[2018-01-21 17:10:47.400583 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:47.524958 UTC] Performing policy update
[2018-01-21 17:10:47.525854 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:47.677370 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:49.108045 UTC] Performing line search
[2018-01-21 17:10:49.309023 UTC] Updating baseline
[2018-01-21 17:10:51.475290 UTC] Computing logging information
-------------------------------------
| Iteration            | 1745       |
| ExpectedImprovement  | 0.016778   |
| ActualImprovement    | 0.01597    |
| ImprovementRatio     | 0.95186    |
| MeanKL               | 0.0073048  |
| Entropy              | -1.915     |
| Perplexity           | 0.14733    |
| AveragePolicyStd     | 0.17804    |
| AveragePolicyStd[0]  | 0.19992    |
| AveragePolicyStd[1]  | 0.1892     |
| AveragePolicyStd[2]  | 0.14234    |
| AveragePolicyStd[3]  | 0.16889    |
| AveragePolicyStd[4]  | 0.14764    |
| AveragePolicyStd[5]  | 0.22028    |
| AverageReturn        | 1912.3     |
| MinReturn            | 357.81     |
| MaxReturn            | 2053.4     |
| StdReturn            | 282.1      |
| AverageEpisodeLength | 963.74     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.44     |
| TotalNEpisodes       | 25819      |
| TotalNSamples        | 8.7378e+06 |
| ExplainedVariance    | 0.082283   |
-------------------------------------
[2018-01-21 17:10:52.410537 UTC] Saving snapshot
[2018-01-21 17:10:52.410826 UTC] Starting iteration 1746
[2018-01-21 17:10:52.411094 UTC] Start collecting samples
[2018-01-21 17:10:57.285189 UTC] Computing input variables for policy optimization
[2018-01-21 17:10:57.460655 UTC] Performing policy update
[2018-01-21 17:10:57.461406 UTC] Computing gradient in Euclidean space
[2018-01-21 17:10:57.576606 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:10:59.082941 UTC] Performing line search
[2018-01-21 17:10:59.291540 UTC] Updating baseline
[2018-01-21 17:11:02.205386 UTC] Computing logging information
-------------------------------------
| Iteration            | 1746       |
| ExpectedImprovement  | 0.01745    |
| ActualImprovement    | 0.015911   |
| ImprovementRatio     | 0.91181    |
| MeanKL               | 0.009024   |
| Entropy              | -1.9227    |
| Perplexity           | 0.14621    |
| AveragePolicyStd     | 0.17781    |
| AveragePolicyStd[0]  | 0.19916    |
| AveragePolicyStd[1]  | 0.18943    |
| AveragePolicyStd[2]  | 0.14207    |
| AveragePolicyStd[3]  | 0.16849    |
| AveragePolicyStd[4]  | 0.14776    |
| AveragePolicyStd[5]  | 0.21993    |
| AverageReturn        | 1907.7     |
| MinReturn            | 357.81     |
| MaxReturn            | 2053.4     |
| StdReturn            | 283.35     |
| AverageEpisodeLength | 962.1      |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.98     |
| TotalNEpisodes       | 25822      |
| TotalNSamples        | 8.7407e+06 |
| ExplainedVariance    | 0.31167    |
-------------------------------------
[2018-01-21 17:11:03.247683 UTC] Saving snapshot
[2018-01-21 17:11:03.247914 UTC] Starting iteration 1747
[2018-01-21 17:11:03.248066 UTC] Start collecting samples
[2018-01-21 17:11:08.432173 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:08.555551 UTC] Performing policy update
[2018-01-21 17:11:08.556302 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:08.689061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:11:10.121120 UTC] Performing line search
[2018-01-21 17:11:10.314630 UTC] Updating baseline
[2018-01-21 17:11:12.551879 UTC] Computing logging information
-------------------------------------
| Iteration            | 1747       |
| ExpectedImprovement  | 0.017255   |
| ActualImprovement    | 0.015842   |
| ImprovementRatio     | 0.91809    |
| MeanKL               | 0.0075747  |
| Entropy              | -1.9197    |
| Perplexity           | 0.14665    |
| AveragePolicyStd     | 0.1779     |
| AveragePolicyStd[0]  | 0.19947    |
| AveragePolicyStd[1]  | 0.18916    |
| AveragePolicyStd[2]  | 0.14222    |
| AveragePolicyStd[3]  | 0.16851    |
| AveragePolicyStd[4]  | 0.14781    |
| AveragePolicyStd[5]  | 0.22021    |
| AverageReturn        | 1912.2     |
| MinReturn            | 357.81     |
| MaxReturn            | 2053.4     |
| StdReturn            | 280.05     |
| AverageEpisodeLength | 964.37     |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.69     |
| TotalNEpisodes       | 25827      |
| TotalNSamples        | 8.7457e+06 |
| ExplainedVariance    | -0.033395  |
-------------------------------------
[2018-01-21 17:11:13.480297 UTC] Saving snapshot
[2018-01-21 17:11:13.480606 UTC] Starting iteration 1748
[2018-01-21 17:11:13.480801 UTC] Start collecting samples
[2018-01-21 17:11:18.669917 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:18.801717 UTC] Performing policy update
[2018-01-21 17:11:18.802518 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:18.949868 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:11:20.364971 UTC] Performing line search
[2018-01-21 17:11:20.549563 UTC] Updating baseline
[2018-01-21 17:11:22.694099 UTC] Computing logging information
-------------------------------------
| Iteration            | 1748       |
| ExpectedImprovement  | 0.017157   |
| ActualImprovement    | 0.015478   |
| ImprovementRatio     | 0.90213    |
| MeanKL               | 0.0081493  |
| Entropy              | -1.925     |
| Perplexity           | 0.14588    |
| AveragePolicyStd     | 0.17774    |
| AveragePolicyStd[0]  | 0.19885    |
| AveragePolicyStd[1]  | 0.18917    |
| AveragePolicyStd[2]  | 0.14202    |
| AveragePolicyStd[3]  | 0.1687     |
| AveragePolicyStd[4]  | 0.14759    |
| AveragePolicyStd[5]  | 0.22011    |
| AverageReturn        | 1929.4     |
| MinReturn            | 357.81     |
| MaxReturn            | 2053.4     |
| StdReturn            | 242.36     |
| AverageEpisodeLength | 972.8      |
| MinEpisodeLength     | 220        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 116.51     |
| TotalNEpisodes       | 25833      |
| TotalNSamples        | 8.7515e+06 |
| ExplainedVariance    | 0.12059    |
-------------------------------------
[2018-01-21 17:11:23.629631 UTC] Saving snapshot
[2018-01-21 17:11:23.629903 UTC] Starting iteration 1749
[2018-01-21 17:11:23.630117 UTC] Start collecting samples
[2018-01-21 17:11:28.624920 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:28.792848 UTC] Performing policy update
[2018-01-21 17:11:28.794150 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:28.919663 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:11:30.365318 UTC] Performing line search
[2018-01-21 17:11:30.561755 UTC] Updating baseline
[2018-01-21 17:11:32.579689 UTC] Computing logging information
-------------------------------------
| Iteration            | 1749       |
| ExpectedImprovement  | 0.017749   |
| ActualImprovement    | 0.01662    |
| ImprovementRatio     | 0.9364     |
| MeanKL               | 0.0080384  |
| Entropy              | -1.9286    |
| Perplexity           | 0.14536    |
| AveragePolicyStd     | 0.17767    |
| AveragePolicyStd[0]  | 0.19885    |
| AveragePolicyStd[1]  | 0.18924    |
| AveragePolicyStd[2]  | 0.14185    |
| AveragePolicyStd[3]  | 0.16881    |
| AveragePolicyStd[4]  | 0.14699    |
| AveragePolicyStd[5]  | 0.22027    |
| AverageReturn        | 1943.5     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 183.99     |
| AverageEpisodeLength | 980.6      |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 88.626     |
| TotalNEpisodes       | 25838      |
| TotalNSamples        | 8.7565e+06 |
| ExplainedVariance    | -0.0019361 |
-------------------------------------
[2018-01-21 17:11:33.527869 UTC] Saving snapshot
[2018-01-21 17:11:33.528217 UTC] Starting iteration 1750
[2018-01-21 17:11:33.528480 UTC] Start collecting samples
[2018-01-21 17:11:38.449438 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:38.579581 UTC] Performing policy update
[2018-01-21 17:11:38.580460 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:38.704543 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:11:40.084533 UTC] Performing line search
[2018-01-21 17:11:40.275765 UTC] Updating baseline
[2018-01-21 17:11:42.483309 UTC] Computing logging information
-------------------------------------
| Iteration            | 1750       |
| ExpectedImprovement  | 0.019556   |
| ActualImprovement    | 0.01824    |
| ImprovementRatio     | 0.93271    |
| MeanKL               | 0.0077627  |
| Entropy              | -1.9243    |
| Perplexity           | 0.14598    |
| AveragePolicyStd     | 0.17783    |
| AveragePolicyStd[0]  | 0.19941    |
| AveragePolicyStd[1]  | 0.18958    |
| AveragePolicyStd[2]  | 0.14152    |
| AveragePolicyStd[3]  | 0.16875    |
| AveragePolicyStd[4]  | 0.14713    |
| AveragePolicyStd[5]  | 0.22056    |
| AverageReturn        | 1943.9     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 184.15     |
| AverageEpisodeLength | 980.6      |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 88.626     |
| TotalNEpisodes       | 25840      |
| TotalNSamples        | 8.7585e+06 |
| ExplainedVariance    | 0.12408    |
-------------------------------------
[2018-01-21 17:11:43.450688 UTC] Saving snapshot
[2018-01-21 17:11:43.460904 UTC] Starting iteration 1751
[2018-01-21 17:11:43.461134 UTC] Start collecting samples
[2018-01-21 17:11:48.789961 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:48.954243 UTC] Performing policy update
[2018-01-21 17:11:48.954883 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:49.063874 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:11:50.466217 UTC] Performing line search
[2018-01-21 17:11:50.660087 UTC] Updating baseline
[2018-01-21 17:11:52.574848 UTC] Computing logging information
-------------------------------------
| Iteration            | 1751       |
| ExpectedImprovement  | 0.018189   |
| ActualImprovement    | 0.016849   |
| ImprovementRatio     | 0.92634    |
| MeanKL               | 0.0081453  |
| Entropy              | -1.9326    |
| Perplexity           | 0.14477    |
| AveragePolicyStd     | 0.17757    |
| AveragePolicyStd[0]  | 0.19906    |
| AveragePolicyStd[1]  | 0.18988    |
| AveragePolicyStd[2]  | 0.14129    |
| AveragePolicyStd[3]  | 0.16868    |
| AveragePolicyStd[4]  | 0.14688    |
| AveragePolicyStd[5]  | 0.2196     |
| AverageReturn        | 1941.3     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 204.4      |
| AverageEpisodeLength | 979.2      |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 97.411     |
| TotalNEpisodes       | 25850      |
| TotalNSamples        | 8.7677e+06 |
| ExplainedVariance    | 0.085279   |
-------------------------------------
[2018-01-21 17:11:53.483488 UTC] Saving snapshot
[2018-01-21 17:11:53.483832 UTC] Starting iteration 1752
[2018-01-21 17:11:53.484088 UTC] Start collecting samples
[2018-01-21 17:11:58.531159 UTC] Computing input variables for policy optimization
[2018-01-21 17:11:58.682506 UTC] Performing policy update
[2018-01-21 17:11:58.683584 UTC] Computing gradient in Euclidean space
[2018-01-21 17:11:58.811391 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:00.312524 UTC] Performing line search
[2018-01-21 17:12:00.573571 UTC] Updating baseline
[2018-01-21 17:12:02.663863 UTC] Computing logging information
-------------------------------------
| Iteration            | 1752       |
| ExpectedImprovement  | 0.017612   |
| ActualImprovement    | 0.016859   |
| ImprovementRatio     | 0.95721    |
| MeanKL               | 0.008564   |
| Entropy              | -1.9373    |
| Perplexity           | 0.1441     |
| AveragePolicyStd     | 0.17744    |
| AveragePolicyStd[0]  | 0.19911    |
| AveragePolicyStd[1]  | 0.18949    |
| AveragePolicyStd[2]  | 0.14128    |
| AveragePolicyStd[3]  | 0.16851    |
| AveragePolicyStd[4]  | 0.14659    |
| AveragePolicyStd[5]  | 0.21965    |
| AverageReturn        | 1930.7     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 233.22     |
| AverageEpisodeLength | 973.52     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.56     |
| TotalNEpisodes       | 25854      |
| TotalNSamples        | 8.7711e+06 |
| ExplainedVariance    | 0.14482    |
-------------------------------------
[2018-01-21 17:12:03.553261 UTC] Saving snapshot
[2018-01-21 17:12:03.553516 UTC] Starting iteration 1753
[2018-01-21 17:12:03.553676 UTC] Start collecting samples
[2018-01-21 17:12:09.542642 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:09.659368 UTC] Performing policy update
[2018-01-21 17:12:09.660086 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:09.784706 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:11.526147 UTC] Performing line search
[2018-01-21 17:12:11.771314 UTC] Updating baseline
[2018-01-21 17:12:13.910195 UTC] Computing logging information
-------------------------------------
| Iteration            | 1753       |
| ExpectedImprovement  | 0.020222   |
| ActualImprovement    | 0.018238   |
| ImprovementRatio     | 0.90189    |
| MeanKL               | 0.0088679  |
| Entropy              | -1.9422    |
| Perplexity           | 0.14339    |
| AveragePolicyStd     | 0.17727    |
| AveragePolicyStd[0]  | 0.19897    |
| AveragePolicyStd[1]  | 0.18951    |
| AveragePolicyStd[2]  | 0.14114    |
| AveragePolicyStd[3]  | 0.1687     |
| AveragePolicyStd[4]  | 0.14652    |
| AveragePolicyStd[5]  | 0.21876    |
| AverageReturn        | 1931.9     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 233.57     |
| AverageEpisodeLength | 973.52     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 111.56     |
| TotalNEpisodes       | 25856      |
| TotalNSamples        | 8.7731e+06 |
| ExplainedVariance    | -0.0057645 |
-------------------------------------
[2018-01-21 17:12:14.834459 UTC] Saving snapshot
[2018-01-21 17:12:14.835006 UTC] Starting iteration 1754
[2018-01-21 17:12:14.835436 UTC] Start collecting samples
[2018-01-21 17:12:20.357105 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:20.487493 UTC] Performing policy update
[2018-01-21 17:12:20.488192 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:20.613039 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:21.969767 UTC] Performing line search
[2018-01-21 17:12:22.159122 UTC] Updating baseline
[2018-01-21 17:12:23.887680 UTC] Computing logging information
--------------------------------------
| Iteration            | 1754        |
| ExpectedImprovement  | 0.018896    |
| ActualImprovement    | 0.01787     |
| ImprovementRatio     | 0.94573     |
| MeanKL               | 0.0078494   |
| Entropy              | -1.942      |
| Perplexity           | 0.14342     |
| AveragePolicyStd     | 0.17729     |
| AveragePolicyStd[0]  | 0.19932     |
| AveragePolicyStd[1]  | 0.1893      |
| AveragePolicyStd[2]  | 0.14128     |
| AveragePolicyStd[3]  | 0.16858     |
| AveragePolicyStd[4]  | 0.1463      |
| AveragePolicyStd[5]  | 0.21897     |
| AverageReturn        | 1934.1      |
| MinReturn            | 484.24      |
| MaxReturn            | 2053.4      |
| StdReturn            | 234.23      |
| AverageEpisodeLength | 973.52      |
| MinEpisodeLength     | 265         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 111.56      |
| TotalNEpisodes       | 25866       |
| TotalNSamples        | 8.7831e+06  |
| ExplainedVariance    | -0.00028855 |
--------------------------------------
[2018-01-21 17:12:24.738909 UTC] Saving snapshot
[2018-01-21 17:12:24.739144 UTC] Starting iteration 1755
[2018-01-21 17:12:24.739295 UTC] Start collecting samples
[2018-01-21 17:12:29.704408 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:29.862103 UTC] Performing policy update
[2018-01-21 17:12:29.862833 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:30.007566 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:31.453087 UTC] Performing line search
[2018-01-21 17:12:31.643919 UTC] Updating baseline
[2018-01-21 17:12:33.534211 UTC] Computing logging information
-------------------------------------
| Iteration            | 1755       |
| ExpectedImprovement  | 0.018591   |
| ActualImprovement    | 0.017736   |
| ImprovementRatio     | 0.954      |
| MeanKL               | 0.0079249  |
| Entropy              | -1.9468    |
| Perplexity           | 0.14272    |
| AveragePolicyStd     | 0.17717    |
| AveragePolicyStd[0]  | 0.19958    |
| AveragePolicyStd[1]  | 0.18929    |
| AveragePolicyStd[2]  | 0.14112    |
| AveragePolicyStd[3]  | 0.16804    |
| AveragePolicyStd[4]  | 0.14601    |
| AveragePolicyStd[5]  | 0.21899    |
| AverageReturn        | 1920.6     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 263.92     |
| AverageEpisodeLength | 966.76     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.81     |
| TotalNEpisodes       | 25871      |
| TotalNSamples        | 8.7875e+06 |
| ExplainedVariance    | 0.19545    |
-------------------------------------
[2018-01-21 17:12:34.479132 UTC] Saving snapshot
[2018-01-21 17:12:34.479382 UTC] Starting iteration 1756
[2018-01-21 17:12:34.479561 UTC] Start collecting samples
[2018-01-21 17:12:39.069279 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:39.195150 UTC] Performing policy update
[2018-01-21 17:12:39.195860 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:39.311891 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:40.741329 UTC] Performing line search
[2018-01-21 17:12:40.941212 UTC] Updating baseline
[2018-01-21 17:12:43.039653 UTC] Computing logging information
-------------------------------------
| Iteration            | 1756       |
| ExpectedImprovement  | 0.017624   |
| ActualImprovement    | 0.016546   |
| ImprovementRatio     | 0.93886    |
| MeanKL               | 0.0083167  |
| Entropy              | -1.9481    |
| Perplexity           | 0.14254    |
| AveragePolicyStd     | 0.17713    |
| AveragePolicyStd[0]  | 0.19977    |
| AveragePolicyStd[1]  | 0.18939    |
| AveragePolicyStd[2]  | 0.14115    |
| AveragePolicyStd[3]  | 0.16816    |
| AveragePolicyStd[4]  | 0.14586    |
| AveragePolicyStd[5]  | 0.21843    |
| AverageReturn        | 1901.7     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 297.87     |
| AverageEpisodeLength | 957.8      |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.78     |
| TotalNEpisodes       | 25875      |
| TotalNSamples        | 8.7906e+06 |
| ExplainedVariance    | 0.33468    |
-------------------------------------
[2018-01-21 17:12:43.889735 UTC] Saving snapshot
[2018-01-21 17:12:43.889977 UTC] Starting iteration 1757
[2018-01-21 17:12:43.890135 UTC] Start collecting samples
[2018-01-21 17:12:48.858218 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:49.019323 UTC] Performing policy update
[2018-01-21 17:12:49.020115 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:49.150754 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:12:50.594473 UTC] Performing line search
[2018-01-21 17:12:50.784354 UTC] Updating baseline
[2018-01-21 17:12:52.666051 UTC] Computing logging information
-------------------------------------
| Iteration            | 1757       |
| ExpectedImprovement  | 0.017954   |
| ActualImprovement    | 0.017169   |
| ImprovementRatio     | 0.95628    |
| MeanKL               | 0.0079517  |
| Entropy              | -1.9505    |
| Perplexity           | 0.14221    |
| AveragePolicyStd     | 0.17704    |
| AveragePolicyStd[0]  | 0.19956    |
| AveragePolicyStd[1]  | 0.18913    |
| AveragePolicyStd[2]  | 0.1412     |
| AveragePolicyStd[3]  | 0.16804    |
| AveragePolicyStd[4]  | 0.14601    |
| AveragePolicyStd[5]  | 0.2183     |
| AverageReturn        | 1901.3     |
| MinReturn            | 484.24     |
| MaxReturn            | 2053.4     |
| StdReturn            | 297.98     |
| AverageEpisodeLength | 957.19     |
| MinEpisodeLength     | 265        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.72     |
| TotalNEpisodes       | 25882      |
| TotalNSamples        | 8.7975e+06 |
| ExplainedVariance    | 0.084957   |
-------------------------------------
[2018-01-21 17:12:53.543455 UTC] Saving snapshot
[2018-01-21 17:12:53.543861 UTC] Starting iteration 1758
[2018-01-21 17:12:53.544195 UTC] Start collecting samples
[2018-01-21 17:12:58.567867 UTC] Computing input variables for policy optimization
[2018-01-21 17:12:58.698466 UTC] Performing policy update
[2018-01-21 17:12:58.699156 UTC] Computing gradient in Euclidean space
[2018-01-21 17:12:58.824153 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:00.262412 UTC] Performing line search
[2018-01-21 17:13:00.450386 UTC] Updating baseline
[2018-01-21 17:13:02.479051 UTC] Computing logging information
-------------------------------------
| Iteration            | 1758       |
| ExpectedImprovement  | 0.020555   |
| ActualImprovement    | 0.018784   |
| ImprovementRatio     | 0.91386    |
| MeanKL               | 0.007651   |
| Entropy              | -1.9565    |
| Perplexity           | 0.14135    |
| AveragePolicyStd     | 0.17685    |
| AveragePolicyStd[0]  | 0.19941    |
| AveragePolicyStd[1]  | 0.18886    |
| AveragePolicyStd[2]  | 0.14104    |
| AveragePolicyStd[3]  | 0.16768    |
| AveragePolicyStd[4]  | 0.14616    |
| AveragePolicyStd[5]  | 0.21794    |
| AverageReturn        | 1908.3     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.4     |
| StdReturn            | 274.13     |
| AverageEpisodeLength | 960.34     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.26     |
| TotalNEpisodes       | 25888      |
| TotalNSamples        | 8.8031e+06 |
| ExplainedVariance    | 0.092203   |
-------------------------------------
[2018-01-21 17:13:03.343832 UTC] Saving snapshot
[2018-01-21 17:13:03.344030 UTC] Starting iteration 1759
[2018-01-21 17:13:03.344134 UTC] Start collecting samples
[2018-01-21 17:13:08.206679 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:08.328938 UTC] Performing policy update
[2018-01-21 17:13:08.330045 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:08.448907 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:09.906091 UTC] Performing line search
[2018-01-21 17:13:10.111759 UTC] Updating baseline
[2018-01-21 17:13:12.604181 UTC] Computing logging information
--------------------------------------
| Iteration            | 1759        |
| ExpectedImprovement  | 0.01925     |
| ActualImprovement    | 0.01736     |
| ImprovementRatio     | 0.90181     |
| MeanKL               | 0.0084586   |
| Entropy              | -1.9591     |
| Perplexity           | 0.14099     |
| AveragePolicyStd     | 0.17677     |
| AveragePolicyStd[0]  | 0.19972     |
| AveragePolicyStd[1]  | 0.18899     |
| AveragePolicyStd[2]  | 0.14075     |
| AveragePolicyStd[3]  | 0.16753     |
| AveragePolicyStd[4]  | 0.14629     |
| AveragePolicyStd[5]  | 0.21733     |
| AverageReturn        | 1905.8      |
| MinReturn            | 625.9       |
| MaxReturn            | 2053.4      |
| StdReturn            | 274.76      |
| AverageEpisodeLength | 960.69      |
| MinEpisodeLength     | 336         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 129.32      |
| TotalNEpisodes       | 25890       |
| TotalNSamples        | 8.8051e+06  |
| ExplainedVariance    | -0.00055497 |
--------------------------------------
[2018-01-21 17:13:13.591169 UTC] Saving snapshot
[2018-01-21 17:13:13.591385 UTC] Starting iteration 1760
[2018-01-21 17:13:13.591568 UTC] Start collecting samples
[2018-01-21 17:13:19.219500 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:19.341680 UTC] Performing policy update
[2018-01-21 17:13:19.342273 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:19.467793 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:20.956315 UTC] Performing line search
[2018-01-21 17:13:21.152324 UTC] Updating baseline
[2018-01-21 17:13:23.218964 UTC] Computing logging information
-------------------------------------
| Iteration            | 1760       |
| ExpectedImprovement  | 0.015856   |
| ActualImprovement    | 0.015217   |
| ImprovementRatio     | 0.95975    |
| MeanKL               | 0.0090327  |
| Entropy              | -1.9561    |
| Perplexity           | 0.14141    |
| AveragePolicyStd     | 0.17687    |
| AveragePolicyStd[0]  | 0.19983    |
| AveragePolicyStd[1]  | 0.18921    |
| AveragePolicyStd[2]  | 0.14055    |
| AveragePolicyStd[3]  | 0.16797    |
| AveragePolicyStd[4]  | 0.14622    |
| AveragePolicyStd[5]  | 0.21747    |
| AverageReturn        | 1906.9     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.4     |
| StdReturn            | 275.09     |
| AverageEpisodeLength | 960.69     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.32     |
| TotalNEpisodes       | 25896      |
| TotalNSamples        | 8.8111e+06 |
| ExplainedVariance    | 0.0035835  |
-------------------------------------
[2018-01-21 17:13:24.166083 UTC] Saving snapshot
[2018-01-21 17:13:24.180416 UTC] Starting iteration 1761
[2018-01-21 17:13:24.181038 UTC] Start collecting samples
[2018-01-21 17:13:29.001354 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:29.147620 UTC] Performing policy update
[2018-01-21 17:13:29.148286 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:29.261930 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:30.798624 UTC] Performing line search
[2018-01-21 17:13:30.994018 UTC] Updating baseline
[2018-01-21 17:13:33.301889 UTC] Computing logging information
-------------------------------------
| Iteration            | 1761       |
| ExpectedImprovement  | 0.018931   |
| ActualImprovement    | 0.018094   |
| ImprovementRatio     | 0.95577    |
| MeanKL               | 0.0082492  |
| Entropy              | -1.9509    |
| Perplexity           | 0.14215    |
| AveragePolicyStd     | 0.177      |
| AveragePolicyStd[0]  | 0.19968    |
| AveragePolicyStd[1]  | 0.18926    |
| AveragePolicyStd[2]  | 0.141      |
| AveragePolicyStd[3]  | 0.16781    |
| AveragePolicyStd[4]  | 0.14659    |
| AveragePolicyStd[5]  | 0.21766    |
| AverageReturn        | 1907.3     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.4     |
| StdReturn            | 275.17     |
| AverageEpisodeLength | 960.69     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.32     |
| TotalNEpisodes       | 25902      |
| TotalNSamples        | 8.8171e+06 |
| ExplainedVariance    | 0.07211    |
-------------------------------------
[2018-01-21 17:13:34.222483 UTC] Saving snapshot
[2018-01-21 17:13:34.222780 UTC] Starting iteration 1762
[2018-01-21 17:13:34.222989 UTC] Start collecting samples
[2018-01-21 17:13:39.239689 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:39.358269 UTC] Performing policy update
[2018-01-21 17:13:39.358904 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:39.472192 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:40.952564 UTC] Performing line search
[2018-01-21 17:13:41.160833 UTC] Updating baseline
[2018-01-21 17:13:43.619635 UTC] Computing logging information
-------------------------------------
| Iteration            | 1762       |
| ExpectedImprovement  | 0.017609   |
| ActualImprovement    | 0.016325   |
| ImprovementRatio     | 0.92706    |
| MeanKL               | 0.009074   |
| Entropy              | -1.9517    |
| Perplexity           | 0.14203    |
| AveragePolicyStd     | 0.17698    |
| AveragePolicyStd[0]  | 0.19931    |
| AveragePolicyStd[1]  | 0.18919    |
| AveragePolicyStd[2]  | 0.14083    |
| AveragePolicyStd[3]  | 0.16802    |
| AveragePolicyStd[4]  | 0.1466     |
| AveragePolicyStd[5]  | 0.21794    |
| AverageReturn        | 1908.4     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.4     |
| StdReturn            | 275.57     |
| AverageEpisodeLength | 960.69     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 129.32     |
| TotalNEpisodes       | 25905      |
| TotalNSamples        | 8.8201e+06 |
| ExplainedVariance    | 0.0036051  |
-------------------------------------
[2018-01-21 17:13:44.557992 UTC] Saving snapshot
[2018-01-21 17:13:44.558241 UTC] Starting iteration 1763
[2018-01-21 17:13:44.558403 UTC] Start collecting samples
[2018-01-21 17:13:49.556135 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:49.714250 UTC] Performing policy update
[2018-01-21 17:13:49.714897 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:49.832877 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:13:51.271917 UTC] Performing line search
[2018-01-21 17:13:51.469886 UTC] Updating baseline
[2018-01-21 17:13:53.313360 UTC] Computing logging information
-------------------------------------
| Iteration            | 1763       |
| ExpectedImprovement  | 0.017352   |
| ActualImprovement    | 0.016487   |
| ImprovementRatio     | 0.95017    |
| MeanKL               | 0.0081811  |
| Entropy              | -1.9549    |
| Perplexity           | 0.14157    |
| AveragePolicyStd     | 0.17689    |
| AveragePolicyStd[0]  | 0.19936    |
| AveragePolicyStd[1]  | 0.18881    |
| AveragePolicyStd[2]  | 0.14082    |
| AveragePolicyStd[3]  | 0.16806    |
| AveragePolicyStd[4]  | 0.14642    |
| AveragePolicyStd[5]  | 0.21785    |
| AverageReturn        | 1899.2     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.4     |
| StdReturn            | 292.85     |
| AverageEpisodeLength | 956.29     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 25912      |
| TotalNSamples        | 8.8265e+06 |
| ExplainedVariance    | 0.12236    |
-------------------------------------
[2018-01-21 17:13:54.281595 UTC] Saving snapshot
[2018-01-21 17:13:54.281836 UTC] Starting iteration 1764
[2018-01-21 17:13:54.281982 UTC] Start collecting samples
[2018-01-21 17:13:59.160962 UTC] Computing input variables for policy optimization
[2018-01-21 17:13:59.290994 UTC] Performing policy update
[2018-01-21 17:13:59.291625 UTC] Computing gradient in Euclidean space
[2018-01-21 17:13:59.417329 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:00.972454 UTC] Performing line search
[2018-01-21 17:14:01.176450 UTC] Updating baseline
[2018-01-21 17:14:03.213189 UTC] Computing logging information
-------------------------------------
| Iteration            | 1764       |
| ExpectedImprovement  | 0.020739   |
| ActualImprovement    | 0.018838   |
| ImprovementRatio     | 0.90834    |
| MeanKL               | 0.0080869  |
| Entropy              | -1.9516    |
| Perplexity           | 0.14205    |
| AveragePolicyStd     | 0.17699    |
| AveragePolicyStd[0]  | 0.19972    |
| AveragePolicyStd[1]  | 0.18948    |
| AveragePolicyStd[2]  | 0.14106    |
| AveragePolicyStd[3]  | 0.1679     |
| AveragePolicyStd[4]  | 0.14626    |
| AveragePolicyStd[5]  | 0.21751    |
| AverageReturn        | 1901.6     |
| MinReturn            | 625.9      |
| MaxReturn            | 2053.7     |
| StdReturn            | 293.11     |
| AverageEpisodeLength | 957.16     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.09     |
| TotalNEpisodes       | 25917      |
| TotalNSamples        | 8.8315e+06 |
| ExplainedVariance    | 0.0031144  |
-------------------------------------
[2018-01-21 17:14:04.163996 UTC] Saving snapshot
[2018-01-21 17:14:04.164307 UTC] Starting iteration 1765
[2018-01-21 17:14:04.164539 UTC] Start collecting samples
[2018-01-21 17:14:09.115561 UTC] Computing input variables for policy optimization
[2018-01-21 17:14:09.256801 UTC] Performing policy update
[2018-01-21 17:14:09.257945 UTC] Computing gradient in Euclidean space
[2018-01-21 17:14:09.374723 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:10.796543 UTC] Performing line search
[2018-01-21 17:14:10.996995 UTC] Updating baseline
[2018-01-21 17:14:13.916855 UTC] Computing logging information
-------------------------------------
| Iteration            | 1765       |
| ExpectedImprovement  | 0.016133   |
| ActualImprovement    | 0.015045   |
| ImprovementRatio     | 0.93256    |
| MeanKL               | 0.0089307  |
| Entropy              | -1.9501    |
| Perplexity           | 0.14226    |
| AveragePolicyStd     | 0.17702    |
| AveragePolicyStd[0]  | 0.19909    |
| AveragePolicyStd[1]  | 0.18963    |
| AveragePolicyStd[2]  | 0.14122    |
| AveragePolicyStd[3]  | 0.16828    |
| AveragePolicyStd[4]  | 0.14625    |
| AveragePolicyStd[5]  | 0.21763    |
| AverageReturn        | 1906.5     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 292.13     |
| AverageEpisodeLength | 958.8      |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.61     |
| TotalNEpisodes       | 25921      |
| TotalNSamples        | 8.8355e+06 |
| ExplainedVariance    | 0.0067902  |
-------------------------------------
[2018-01-21 17:14:14.788799 UTC] Saving snapshot
[2018-01-21 17:14:14.789128 UTC] Starting iteration 1766
[2018-01-21 17:14:14.789369 UTC] Start collecting samples
[2018-01-21 17:14:19.924715 UTC] Computing input variables for policy optimization
[2018-01-21 17:14:20.048699 UTC] Performing policy update
[2018-01-21 17:14:20.049329 UTC] Computing gradient in Euclidean space
[2018-01-21 17:14:20.168993 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:21.593422 UTC] Performing line search
[2018-01-21 17:14:21.783142 UTC] Updating baseline
[2018-01-21 17:14:23.791156 UTC] Computing logging information
-------------------------------------
| Iteration            | 1766       |
| ExpectedImprovement  | 0.018897   |
| ActualImprovement    | 0.017622   |
| ImprovementRatio     | 0.93252    |
| MeanKL               | 0.0093456  |
| Entropy              | -1.9502    |
| Perplexity           | 0.14224    |
| AveragePolicyStd     | 0.17702    |
| AveragePolicyStd[0]  | 0.19962    |
| AveragePolicyStd[1]  | 0.18929    |
| AveragePolicyStd[2]  | 0.14138    |
| AveragePolicyStd[3]  | 0.16802    |
| AveragePolicyStd[4]  | 0.14609    |
| AveragePolicyStd[5]  | 0.21771    |
| AverageReturn        | 1905.9     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 292.07     |
| AverageEpisodeLength | 958.8      |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.61     |
| TotalNEpisodes       | 25926      |
| TotalNSamples        | 8.8405e+06 |
| ExplainedVariance    | 0.0031044  |
-------------------------------------
[2018-01-21 17:14:24.713457 UTC] Saving snapshot
[2018-01-21 17:14:24.713716 UTC] Starting iteration 1767
[2018-01-21 17:14:24.713932 UTC] Start collecting samples
[2018-01-21 17:14:29.555095 UTC] Computing input variables for policy optimization
[2018-01-21 17:14:29.678213 UTC] Performing policy update
[2018-01-21 17:14:29.679322 UTC] Computing gradient in Euclidean space
[2018-01-21 17:14:29.809074 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:31.341490 UTC] Performing line search
[2018-01-21 17:14:31.533791 UTC] Updating baseline
[2018-01-21 17:14:33.811925 UTC] Computing logging information
-------------------------------------
| Iteration            | 1767       |
| ExpectedImprovement  | 0.017117   |
| ActualImprovement    | 0.016179   |
| ImprovementRatio     | 0.94519    |
| MeanKL               | 0.0083602  |
| Entropy              | -1.9512    |
| Perplexity           | 0.14211    |
| AveragePolicyStd     | 0.17696    |
| AveragePolicyStd[0]  | 0.19926    |
| AveragePolicyStd[1]  | 0.1892     |
| AveragePolicyStd[2]  | 0.14159    |
| AveragePolicyStd[3]  | 0.16804    |
| AveragePolicyStd[4]  | 0.14628    |
| AveragePolicyStd[5]  | 0.21739    |
| AverageReturn        | 1910.3     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 290.36     |
| AverageEpisodeLength | 960.89     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.63     |
| TotalNEpisodes       | 25932      |
| TotalNSamples        | 8.8465e+06 |
| ExplainedVariance    | 0.0037895  |
-------------------------------------
[2018-01-21 17:14:34.861942 UTC] Saving snapshot
[2018-01-21 17:14:34.862479 UTC] Starting iteration 1768
[2018-01-21 17:14:34.862923 UTC] Start collecting samples
[2018-01-21 17:14:40.337775 UTC] Computing input variables for policy optimization
[2018-01-21 17:14:40.565748 UTC] Performing policy update
[2018-01-21 17:14:40.566491 UTC] Computing gradient in Euclidean space
[2018-01-21 17:14:40.687062 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:42.101403 UTC] Performing line search
[2018-01-21 17:14:42.295893 UTC] Updating baseline
[2018-01-21 17:14:44.572777 UTC] Computing logging information
-------------------------------------
| Iteration            | 1768       |
| ExpectedImprovement  | 0.018883   |
| ActualImprovement    | 0.017623   |
| ImprovementRatio     | 0.93327    |
| MeanKL               | 0.0076409  |
| Entropy              | -1.9451    |
| Perplexity           | 0.14297    |
| AveragePolicyStd     | 0.17715    |
| AveragePolicyStd[0]  | 0.19955    |
| AveragePolicyStd[1]  | 0.18945    |
| AveragePolicyStd[2]  | 0.14147    |
| AveragePolicyStd[3]  | 0.16832    |
| AveragePolicyStd[4]  | 0.14644    |
| AveragePolicyStd[5]  | 0.21768    |
| AverageReturn        | 1911.6     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 290.43     |
| AverageEpisodeLength | 960.89     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.63     |
| TotalNEpisodes       | 25936      |
| TotalNSamples        | 8.8505e+06 |
| ExplainedVariance    | 0.01931    |
-------------------------------------
[2018-01-21 17:14:45.479422 UTC] Saving snapshot
[2018-01-21 17:14:45.479696 UTC] Starting iteration 1769
[2018-01-21 17:14:45.479882 UTC] Start collecting samples
[2018-01-21 17:14:50.629195 UTC] Computing input variables for policy optimization
[2018-01-21 17:14:50.789143 UTC] Performing policy update
[2018-01-21 17:14:50.789802 UTC] Computing gradient in Euclidean space
[2018-01-21 17:14:50.907027 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:14:52.340747 UTC] Performing line search
[2018-01-21 17:14:52.544481 UTC] Updating baseline
[2018-01-21 17:14:54.453215 UTC] Computing logging information
-------------------------------------
| Iteration            | 1769       |
| ExpectedImprovement  | 0.017756   |
| ActualImprovement    | 0.016718   |
| ImprovementRatio     | 0.94151    |
| MeanKL               | 0.0073522  |
| Entropy              | -1.9478    |
| Perplexity           | 0.14259    |
| AveragePolicyStd     | 0.17706    |
| AveragePolicyStd[0]  | 0.19945    |
| AveragePolicyStd[1]  | 0.18937    |
| AveragePolicyStd[2]  | 0.14169    |
| AveragePolicyStd[3]  | 0.16797    |
| AveragePolicyStd[4]  | 0.14634    |
| AveragePolicyStd[5]  | 0.21756    |
| AverageReturn        | 1907.9     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 278.87     |
| AverageEpisodeLength | 958.67     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.82     |
| TotalNEpisodes       | 25942      |
| TotalNSamples        | 8.8558e+06 |
| ExplainedVariance    | 0.33488    |
-------------------------------------
[2018-01-21 17:14:55.325313 UTC] Saving snapshot
[2018-01-21 17:14:55.325552 UTC] Starting iteration 1770
[2018-01-21 17:14:55.325871 UTC] Start collecting samples
[2018-01-21 17:15:00.369985 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:00.518691 UTC] Performing policy update
[2018-01-21 17:15:00.519471 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:00.641700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:02.101090 UTC] Performing line search
[2018-01-21 17:15:02.313208 UTC] Updating baseline
[2018-01-21 17:15:04.324118 UTC] Computing logging information
-------------------------------------
| Iteration            | 1770       |
| ExpectedImprovement  | 0.018604   |
| ActualImprovement    | 0.017454   |
| ImprovementRatio     | 0.93821    |
| MeanKL               | 0.0080761  |
| Entropy              | -1.95      |
| Perplexity           | 0.14227    |
| AveragePolicyStd     | 0.17701    |
| AveragePolicyStd[0]  | 0.19943    |
| AveragePolicyStd[1]  | 0.18972    |
| AveragePolicyStd[2]  | 0.14142    |
| AveragePolicyStd[3]  | 0.16799    |
| AveragePolicyStd[4]  | 0.1462     |
| AveragePolicyStd[5]  | 0.2173     |
| AverageReturn        | 1909.2     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 279.18     |
| AverageEpisodeLength | 958.67     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.82     |
| TotalNEpisodes       | 25948      |
| TotalNSamples        | 8.8618e+06 |
| ExplainedVariance    | 0.0015156  |
-------------------------------------
[2018-01-21 17:15:05.263687 UTC] Saving snapshot
[2018-01-21 17:15:05.273331 UTC] Starting iteration 1771
[2018-01-21 17:15:05.273576 UTC] Start collecting samples
[2018-01-21 17:15:09.948501 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:10.092336 UTC] Performing policy update
[2018-01-21 17:15:10.092963 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:10.210862 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:11.655936 UTC] Performing line search
[2018-01-21 17:15:11.852356 UTC] Updating baseline
[2018-01-21 17:15:13.865971 UTC] Computing logging information
-------------------------------------
| Iteration            | 1771       |
| ExpectedImprovement  | 0.018259   |
| ActualImprovement    | 0.016832   |
| ImprovementRatio     | 0.92185    |
| MeanKL               | 0.0082306  |
| Entropy              | -1.9521    |
| Perplexity           | 0.14198    |
| AveragePolicyStd     | 0.17696    |
| AveragePolicyStd[0]  | 0.19972    |
| AveragePolicyStd[1]  | 0.18954    |
| AveragePolicyStd[2]  | 0.14125    |
| AveragePolicyStd[3]  | 0.16783    |
| AveragePolicyStd[4]  | 0.14611    |
| AveragePolicyStd[5]  | 0.21734    |
| AverageReturn        | 1911.8     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 277.16     |
| AverageEpisodeLength | 960.2      |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.99     |
| TotalNEpisodes       | 25952      |
| TotalNSamples        | 8.8657e+06 |
| ExplainedVariance    | 0.21451    |
-------------------------------------
[2018-01-21 17:15:14.710649 UTC] Saving snapshot
[2018-01-21 17:15:14.710910 UTC] Starting iteration 1772
[2018-01-21 17:15:14.711076 UTC] Start collecting samples
[2018-01-21 17:15:19.813510 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:19.975531 UTC] Performing policy update
[2018-01-21 17:15:19.976715 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:20.113685 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:21.532337 UTC] Performing line search
[2018-01-21 17:15:21.739381 UTC] Updating baseline
[2018-01-21 17:15:23.826508 UTC] Computing logging information
-------------------------------------
| Iteration            | 1772       |
| ExpectedImprovement  | 0.016521   |
| ActualImprovement    | 0.015835   |
| ImprovementRatio     | 0.95849    |
| MeanKL               | 0.0092818  |
| Entropy              | -1.9544    |
| Perplexity           | 0.14165    |
| AveragePolicyStd     | 0.17692    |
| AveragePolicyStd[0]  | 0.19975    |
| AveragePolicyStd[1]  | 0.18935    |
| AveragePolicyStd[2]  | 0.14109    |
| AveragePolicyStd[3]  | 0.1679     |
| AveragePolicyStd[4]  | 0.14589    |
| AveragePolicyStd[5]  | 0.21752    |
| AverageReturn        | 1920.7     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 254.31     |
| AverageEpisodeLength | 965.02     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.87     |
| TotalNEpisodes       | 25958      |
| TotalNSamples        | 8.8716e+06 |
| ExplainedVariance    | 0.014215   |
-------------------------------------
[2018-01-21 17:15:24.702531 UTC] Saving snapshot
[2018-01-21 17:15:24.702751 UTC] Starting iteration 1773
[2018-01-21 17:15:24.702934 UTC] Start collecting samples
[2018-01-21 17:15:29.797727 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:29.932056 UTC] Performing policy update
[2018-01-21 17:15:29.932721 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:30.058374 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:31.526345 UTC] Performing line search
[2018-01-21 17:15:31.738464 UTC] Updating baseline
[2018-01-21 17:15:33.936146 UTC] Computing logging information
-------------------------------------
| Iteration            | 1773       |
| ExpectedImprovement  | 0.019482   |
| ActualImprovement    | 0.018134   |
| ImprovementRatio     | 0.93085    |
| MeanKL               | 0.0085012  |
| Entropy              | -1.9592    |
| Perplexity           | 0.14097    |
| AveragePolicyStd     | 0.1768     |
| AveragePolicyStd[0]  | 0.20011    |
| AveragePolicyStd[1]  | 0.18896    |
| AveragePolicyStd[2]  | 0.14089    |
| AveragePolicyStd[3]  | 0.16755    |
| AveragePolicyStd[4]  | 0.14571    |
| AveragePolicyStd[5]  | 0.21755    |
| AverageReturn        | 1920.9     |
| MinReturn            | 625.9      |
| MaxReturn            | 2066.6     |
| StdReturn            | 254.39     |
| AverageEpisodeLength | 965.02     |
| MinEpisodeLength     | 336        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.87     |
| TotalNEpisodes       | 25963      |
| TotalNSamples        | 8.8766e+06 |
| ExplainedVariance    | 0.0016331  |
-------------------------------------
[2018-01-21 17:15:34.852396 UTC] Saving snapshot
[2018-01-21 17:15:34.852673 UTC] Starting iteration 1774
[2018-01-21 17:15:34.852840 UTC] Start collecting samples
[2018-01-21 17:15:40.162348 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:40.298498 UTC] Performing policy update
[2018-01-21 17:15:40.299607 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:40.429453 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:41.885298 UTC] Performing line search
[2018-01-21 17:15:42.094800 UTC] Updating baseline
[2018-01-21 17:15:44.367001 UTC] Computing logging information
------------------------------------
| Iteration            | 1774      |
| ExpectedImprovement  | 0.01954   |
| ActualImprovement    | 0.01817   |
| ImprovementRatio     | 0.92992   |
| MeanKL               | 0.0086491 |
| Entropy              | -1.9626   |
| Perplexity           | 0.1405    |
| AveragePolicyStd     | 0.1767    |
| AveragePolicyStd[0]  | 0.20019   |
| AveragePolicyStd[1]  | 0.18885   |
| AveragePolicyStd[2]  | 0.14081   |
| AveragePolicyStd[3]  | 0.16721   |
| AveragePolicyStd[4]  | 0.1457    |
| AveragePolicyStd[5]  | 0.21743   |
| AverageReturn        | 1900.6    |
| MinReturn            | 485.3     |
| MaxReturn            | 2066.6    |
| StdReturn            | 287.03    |
| AverageEpisodeLength | 955.46    |
| MinEpisodeLength     | 269       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 136.53    |
| TotalNEpisodes       | 25971     |
| TotalNSamples        | 8.883e+06 |
| ExplainedVariance    | 0.29299   |
------------------------------------
[2018-01-21 17:15:45.353391 UTC] Saving snapshot
[2018-01-21 17:15:45.353616 UTC] Starting iteration 1775
[2018-01-21 17:15:45.353836 UTC] Start collecting samples
[2018-01-21 17:15:50.641793 UTC] Computing input variables for policy optimization
[2018-01-21 17:15:50.772819 UTC] Performing policy update
[2018-01-21 17:15:50.773523 UTC] Computing gradient in Euclidean space
[2018-01-21 17:15:50.895918 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:15:52.342694 UTC] Performing line search
[2018-01-21 17:15:52.556003 UTC] Updating baseline
[2018-01-21 17:15:55.413376 UTC] Computing logging information
------------------------------------
| Iteration            | 1775      |
| ExpectedImprovement  | 0.018216  |
| ActualImprovement    | 0.017723  |
| ImprovementRatio     | 0.97296   |
| MeanKL               | 0.0075842 |
| Entropy              | -1.9613   |
| Perplexity           | 0.14067   |
| AveragePolicyStd     | 0.17675   |
| AveragePolicyStd[0]  | 0.20012   |
| AveragePolicyStd[1]  | 0.18894   |
| AveragePolicyStd[2]  | 0.14059   |
| AveragePolicyStd[3]  | 0.16736   |
| AveragePolicyStd[4]  | 0.14581   |
| AveragePolicyStd[5]  | 0.21764   |
| AverageReturn        | 1906.2    |
| MinReturn            | 485.3     |
| MaxReturn            | 2066.6    |
| StdReturn            | 282.67    |
| AverageEpisodeLength | 957.78    |
| MinEpisodeLength     | 269       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 135.29    |
| TotalNEpisodes       | 25974     |
| TotalNSamples        | 8.886e+06 |
| ExplainedVariance    | -0.071759 |
------------------------------------
[2018-01-21 17:15:56.313941 UTC] Saving snapshot
[2018-01-21 17:15:56.314131 UTC] Starting iteration 1776
[2018-01-21 17:15:56.314298 UTC] Start collecting samples
[2018-01-21 17:16:01.309313 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:01.448335 UTC] Performing policy update
[2018-01-21 17:16:01.449261 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:01.570113 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:03.004513 UTC] Performing line search
[2018-01-21 17:16:03.199502 UTC] Updating baseline
[2018-01-21 17:16:06.245662 UTC] Computing logging information
------------------------------------
| Iteration            | 1776      |
| ExpectedImprovement  | 0.019027  |
| ActualImprovement    | 0.017429  |
| ImprovementRatio     | 0.91603   |
| MeanKL               | 0.0084843 |
| Entropy              | -1.9614   |
| Perplexity           | 0.14067   |
| AveragePolicyStd     | 0.17673   |
| AveragePolicyStd[0]  | 0.19993   |
| AveragePolicyStd[1]  | 0.18899   |
| AveragePolicyStd[2]  | 0.1405    |
| AveragePolicyStd[3]  | 0.16736   |
| AveragePolicyStd[4]  | 0.14608   |
| AveragePolicyStd[5]  | 0.21754   |
| AverageReturn        | 1921.4    |
| MinReturn            | 485.3     |
| MaxReturn            | 2066.6    |
| StdReturn            | 251.92    |
| AverageEpisodeLength | 965.03    |
| MinEpisodeLength     | 269       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 120.07    |
| TotalNEpisodes       | 25978     |
| TotalNSamples        | 8.89e+06  |
| ExplainedVariance    | 0.0041096 |
------------------------------------
[2018-01-21 17:16:07.119466 UTC] Saving snapshot
[2018-01-21 17:16:07.119703 UTC] Starting iteration 1777
[2018-01-21 17:16:07.119863 UTC] Start collecting samples
[2018-01-21 17:16:12.311703 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:12.460730 UTC] Performing policy update
[2018-01-21 17:16:12.461382 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:12.582988 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:14.060341 UTC] Performing line search
[2018-01-21 17:16:14.292230 UTC] Updating baseline
[2018-01-21 17:16:16.307806 UTC] Computing logging information
-------------------------------------
| Iteration            | 1777       |
| ExpectedImprovement  | 0.019366   |
| ActualImprovement    | 0.018362   |
| ImprovementRatio     | 0.94812    |
| MeanKL               | 0.0077879  |
| Entropy              | -1.9606    |
| Perplexity           | 0.14077    |
| AveragePolicyStd     | 0.17673    |
| AveragePolicyStd[0]  | 0.19999    |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.1408     |
| AveragePolicyStd[3]  | 0.16756    |
| AveragePolicyStd[4]  | 0.14608    |
| AveragePolicyStd[5]  | 0.21707    |
| AverageReturn        | 1925.5     |
| MinReturn            | 485.3      |
| MaxReturn            | 2066.6     |
| StdReturn            | 240.85     |
| AverageEpisodeLength | 967.36     |
| MinEpisodeLength     | 269        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.71     |
| TotalNEpisodes       | 25986      |
| TotalNSamples        | 8.8978e+06 |
| ExplainedVariance    | 0.060274   |
-------------------------------------
[2018-01-21 17:16:17.170361 UTC] Saving snapshot
[2018-01-21 17:16:17.170616 UTC] Starting iteration 1778
[2018-01-21 17:16:17.170801 UTC] Start collecting samples
[2018-01-21 17:16:22.123980 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:22.246649 UTC] Performing policy update
[2018-01-21 17:16:22.247305 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:22.361277 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:23.815182 UTC] Performing line search
[2018-01-21 17:16:24.025666 UTC] Updating baseline
[2018-01-21 17:16:26.227253 UTC] Computing logging information
-------------------------------------
| Iteration            | 1778       |
| ExpectedImprovement  | 0.019638   |
| ActualImprovement    | 0.018711   |
| ImprovementRatio     | 0.95281    |
| MeanKL               | 0.0079559  |
| Entropy              | -1.9683    |
| Perplexity           | 0.13969    |
| AveragePolicyStd     | 0.17648    |
| AveragePolicyStd[0]  | 0.20017    |
| AveragePolicyStd[1]  | 0.18834    |
| AveragePolicyStd[2]  | 0.1405     |
| AveragePolicyStd[3]  | 0.16719    |
| AveragePolicyStd[4]  | 0.14636    |
| AveragePolicyStd[5]  | 0.21633    |
| AverageReturn        | 1913.2     |
| MinReturn            | 415.14     |
| MaxReturn            | 2066.6     |
| StdReturn            | 283.25     |
| AverageEpisodeLength | 959.75     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 135.63     |
| TotalNEpisodes       | 25990      |
| TotalNSamples        | 8.9010e+06 |
| ExplainedVariance    | 0.22325    |
-------------------------------------
[2018-01-21 17:16:27.114426 UTC] Saving snapshot
[2018-01-21 17:16:27.114718 UTC] Starting iteration 1779
[2018-01-21 17:16:27.114908 UTC] Start collecting samples
[2018-01-21 17:16:32.265006 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:32.454629 UTC] Performing policy update
[2018-01-21 17:16:32.457591 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:32.592678 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:33.989112 UTC] Performing line search
[2018-01-21 17:16:34.185344 UTC] Updating baseline
[2018-01-21 17:16:36.309211 UTC] Computing logging information
-------------------------------------
| Iteration            | 1779       |
| ExpectedImprovement  | 0.018458   |
| ActualImprovement    | 0.017256   |
| ImprovementRatio     | 0.93491    |
| MeanKL               | 0.0080439  |
| Entropy              | -1.9681    |
| Perplexity           | 0.13972    |
| AveragePolicyStd     | 0.17648    |
| AveragePolicyStd[0]  | 0.20013    |
| AveragePolicyStd[1]  | 0.18843    |
| AveragePolicyStd[2]  | 0.14035    |
| AveragePolicyStd[3]  | 0.16714    |
| AveragePolicyStd[4]  | 0.14662    |
| AveragePolicyStd[5]  | 0.21622    |
| AverageReturn        | 1892.9     |
| MinReturn            | 415.14     |
| MaxReturn            | 2066.6     |
| StdReturn            | 318.63     |
| AverageEpisodeLength | 950.1      |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.99     |
| TotalNEpisodes       | 25997      |
| TotalNSamples        | 8.9071e+06 |
| ExplainedVariance    | 0.08485    |
-------------------------------------
[2018-01-21 17:16:37.164539 UTC] Saving snapshot
[2018-01-21 17:16:37.164846 UTC] Starting iteration 1780
[2018-01-21 17:16:37.165053 UTC] Start collecting samples
[2018-01-21 17:16:42.460128 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:42.681176 UTC] Performing policy update
[2018-01-21 17:16:42.681922 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:42.808659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:44.310856 UTC] Performing line search
[2018-01-21 17:16:44.509096 UTC] Updating baseline
[2018-01-21 17:16:46.800938 UTC] Computing logging information
-------------------------------------
| Iteration            | 1780       |
| ExpectedImprovement  | 0.019607   |
| ActualImprovement    | 0.018331   |
| ImprovementRatio     | 0.93496    |
| MeanKL               | 0.0075167  |
| Entropy              | -1.9727    |
| Perplexity           | 0.13909    |
| AveragePolicyStd     | 0.17635    |
| AveragePolicyStd[0]  | 0.19984    |
| AveragePolicyStd[1]  | 0.18822    |
| AveragePolicyStd[2]  | 0.14013    |
| AveragePolicyStd[3]  | 0.16739    |
| AveragePolicyStd[4]  | 0.1464     |
| AveragePolicyStd[5]  | 0.21612    |
| AverageReturn        | 1874.2     |
| MinReturn            | 415.14     |
| MaxReturn            | 2066.6     |
| StdReturn            | 342.5      |
| AverageEpisodeLength | 941.4      |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.22     |
| TotalNEpisodes       | 26003      |
| TotalNSamples        | 8.9122e+06 |
| ExplainedVariance    | 0.26828    |
-------------------------------------
[2018-01-21 17:16:47.710805 UTC] Saving snapshot
[2018-01-21 17:16:47.720413 UTC] Starting iteration 1781
[2018-01-21 17:16:47.720648 UTC] Start collecting samples
[2018-01-21 17:16:52.579153 UTC] Computing input variables for policy optimization
[2018-01-21 17:16:52.762025 UTC] Performing policy update
[2018-01-21 17:16:52.762963 UTC] Computing gradient in Euclidean space
[2018-01-21 17:16:52.894590 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:16:54.372819 UTC] Performing line search
[2018-01-21 17:16:54.565288 UTC] Updating baseline
[2018-01-21 17:16:57.184196 UTC] Computing logging information
-------------------------------------
| Iteration            | 1781       |
| ExpectedImprovement  | 0.019626   |
| ActualImprovement    | 0.01911    |
| ImprovementRatio     | 0.97367    |
| MeanKL               | 0.0080614  |
| Entropy              | -1.9772    |
| Perplexity           | 0.13846    |
| AveragePolicyStd     | 0.1762     |
| AveragePolicyStd[0]  | 0.19946    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.14017    |
| AveragePolicyStd[3]  | 0.16722    |
| AveragePolicyStd[4]  | 0.14645    |
| AveragePolicyStd[5]  | 0.21576    |
| AverageReturn        | 1872.7     |
| MinReturn            | 415.14     |
| MaxReturn            | 2066.6     |
| StdReturn            | 341.98     |
| AverageEpisodeLength | 941.4      |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.22     |
| TotalNEpisodes       | 26006      |
| TotalNSamples        | 8.9152e+06 |
| ExplainedVariance    | 0.047788   |
-------------------------------------
[2018-01-21 17:16:58.069603 UTC] Saving snapshot
[2018-01-21 17:16:58.069846 UTC] Starting iteration 1782
[2018-01-21 17:16:58.070021 UTC] Start collecting samples
[2018-01-21 17:17:03.224968 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:03.364009 UTC] Performing policy update
[2018-01-21 17:17:03.364871 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:03.488808 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:04.964153 UTC] Performing line search
[2018-01-21 17:17:05.183816 UTC] Updating baseline
[2018-01-21 17:17:07.193274 UTC] Computing logging information
-------------------------------------
| Iteration            | 1782       |
| ExpectedImprovement  | 0.018127   |
| ActualImprovement    | 0.01732    |
| ImprovementRatio     | 0.9555     |
| MeanKL               | 0.008115   |
| Entropy              | -1.98      |
| Perplexity           | 0.13806    |
| AveragePolicyStd     | 0.17612    |
| AveragePolicyStd[0]  | 0.19947    |
| AveragePolicyStd[1]  | 0.18809    |
| AveragePolicyStd[2]  | 0.14014    |
| AveragePolicyStd[3]  | 0.16687    |
| AveragePolicyStd[4]  | 0.14639    |
| AveragePolicyStd[5]  | 0.21575    |
| AverageReturn        | 1868.7     |
| MinReturn            | 415.14     |
| MaxReturn            | 2066.6     |
| StdReturn            | 353.15     |
| AverageEpisodeLength | 939.74     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.7      |
| TotalNEpisodes       | 26013      |
| TotalNSamples        | 8.9215e+06 |
| ExplainedVariance    | 0.056036   |
-------------------------------------
[2018-01-21 17:17:08.117825 UTC] Saving snapshot
[2018-01-21 17:17:08.118136 UTC] Starting iteration 1783
[2018-01-21 17:17:08.118372 UTC] Start collecting samples
[2018-01-21 17:17:13.296462 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:13.443872 UTC] Performing policy update
[2018-01-21 17:17:13.444534 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:13.563331 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:15.104002 UTC] Performing line search
[2018-01-21 17:17:15.299203 UTC] Updating baseline
[2018-01-21 17:17:17.113927 UTC] Computing logging information
-------------------------------------
| Iteration            | 1783       |
| ExpectedImprovement  | 0.018578   |
| ActualImprovement    | 0.017554   |
| ImprovementRatio     | 0.9449     |
| MeanKL               | 0.0091399  |
| Entropy              | -1.9807    |
| Perplexity           | 0.13798    |
| AveragePolicyStd     | 0.17612    |
| AveragePolicyStd[0]  | 0.19947    |
| AveragePolicyStd[1]  | 0.18862    |
| AveragePolicyStd[2]  | 0.13991    |
| AveragePolicyStd[3]  | 0.16693    |
| AveragePolicyStd[4]  | 0.14621    |
| AveragePolicyStd[5]  | 0.21555    |
| AverageReturn        | 1852.7     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 378.27     |
| AverageEpisodeLength | 932.55     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.8      |
| TotalNEpisodes       | 26021      |
| TotalNSamples        | 8.9288e+06 |
| ExplainedVariance    | 0.11483    |
-------------------------------------
[2018-01-21 17:17:18.056845 UTC] Saving snapshot
[2018-01-21 17:17:18.057165 UTC] Starting iteration 1784
[2018-01-21 17:17:18.057381 UTC] Start collecting samples
[2018-01-21 17:17:22.784198 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:22.942274 UTC] Performing policy update
[2018-01-21 17:17:22.942954 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:23.079322 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:24.529251 UTC] Performing line search
[2018-01-21 17:17:24.728814 UTC] Updating baseline
[2018-01-21 17:17:27.085166 UTC] Computing logging information
-------------------------------------
| Iteration            | 1784       |
| ExpectedImprovement  | 0.016364   |
| ActualImprovement    | 0.016509   |
| ImprovementRatio     | 1.0088     |
| MeanKL               | 0.0079423  |
| Entropy              | -1.9909    |
| Perplexity           | 0.13657    |
| AveragePolicyStd     | 0.17578    |
| AveragePolicyStd[0]  | 0.19902    |
| AveragePolicyStd[1]  | 0.18806    |
| AveragePolicyStd[2]  | 0.14009    |
| AveragePolicyStd[3]  | 0.16668    |
| AveragePolicyStd[4]  | 0.14596    |
| AveragePolicyStd[5]  | 0.21488    |
| AverageReturn        | 1853.2     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 378.38     |
| AverageEpisodeLength | 932.55     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.8      |
| TotalNEpisodes       | 26023      |
| TotalNSamples        | 8.9308e+06 |
| ExplainedVariance    | -0.06579   |
-------------------------------------
[2018-01-21 17:17:28.051800 UTC] Saving snapshot
[2018-01-21 17:17:28.052040 UTC] Starting iteration 1785
[2018-01-21 17:17:28.052264 UTC] Start collecting samples
[2018-01-21 17:17:33.077190 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:33.216507 UTC] Performing policy update
[2018-01-21 17:17:33.217129 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:33.340784 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:34.846113 UTC] Performing line search
[2018-01-21 17:17:35.045475 UTC] Updating baseline
[2018-01-21 17:17:37.459133 UTC] Computing logging information
-------------------------------------
| Iteration            | 1785       |
| ExpectedImprovement  | 0.017548   |
| ActualImprovement    | 0.015725   |
| ImprovementRatio     | 0.89611    |
| MeanKL               | 0.0080897  |
| Entropy              | -1.9894    |
| Perplexity           | 0.13677    |
| AveragePolicyStd     | 0.17584    |
| AveragePolicyStd[0]  | 0.19925    |
| AveragePolicyStd[1]  | 0.18797    |
| AveragePolicyStd[2]  | 0.14034    |
| AveragePolicyStd[3]  | 0.16672    |
| AveragePolicyStd[4]  | 0.14564    |
| AveragePolicyStd[5]  | 0.2151     |
| AverageReturn        | 1846.5     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 382.14     |
| AverageEpisodeLength | 929.64     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.02     |
| TotalNEpisodes       | 26027      |
| TotalNSamples        | 8.9345e+06 |
| ExplainedVariance    | 0.16781    |
-------------------------------------
[2018-01-21 17:17:38.376408 UTC] Saving snapshot
[2018-01-21 17:17:38.376662 UTC] Starting iteration 1786
[2018-01-21 17:17:38.376797 UTC] Start collecting samples
[2018-01-21 17:17:44.136351 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:44.283177 UTC] Performing policy update
[2018-01-21 17:17:44.283891 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:44.483876 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:46.005874 UTC] Performing line search
[2018-01-21 17:17:46.233034 UTC] Updating baseline
[2018-01-21 17:17:48.407842 UTC] Computing logging information
-------------------------------------
| Iteration            | 1786       |
| ExpectedImprovement  | 0.018028   |
| ActualImprovement    | 0.017614   |
| ImprovementRatio     | 0.97703    |
| MeanKL               | 0.0078189  |
| Entropy              | -1.9894    |
| Perplexity           | 0.13677    |
| AveragePolicyStd     | 0.17581    |
| AveragePolicyStd[0]  | 0.19876    |
| AveragePolicyStd[1]  | 0.18796    |
| AveragePolicyStd[2]  | 0.14047    |
| AveragePolicyStd[3]  | 0.16685    |
| AveragePolicyStd[4]  | 0.14581    |
| AveragePolicyStd[5]  | 0.21504    |
| AverageReturn        | 1844.6     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 381.42     |
| AverageEpisodeLength | 929.45     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.96     |
| TotalNEpisodes       | 26036      |
| TotalNSamples        | 8.9435e+06 |
| ExplainedVariance    | 0.022894   |
-------------------------------------
[2018-01-21 17:17:49.339884 UTC] Saving snapshot
[2018-01-21 17:17:49.340114 UTC] Starting iteration 1787
[2018-01-21 17:17:49.340258 UTC] Start collecting samples
[2018-01-21 17:17:54.368620 UTC] Computing input variables for policy optimization
[2018-01-21 17:17:54.495286 UTC] Performing policy update
[2018-01-21 17:17:54.496028 UTC] Computing gradient in Euclidean space
[2018-01-21 17:17:54.628360 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:17:56.115245 UTC] Performing line search
[2018-01-21 17:17:56.320020 UTC] Updating baseline
[2018-01-21 17:17:59.384682 UTC] Computing logging information
-------------------------------------
| Iteration            | 1787       |
| ExpectedImprovement  | 0.019699   |
| ActualImprovement    | 0.017875   |
| ImprovementRatio     | 0.90739    |
| MeanKL               | 0.0080296  |
| Entropy              | -1.9924    |
| Perplexity           | 0.13636    |
| AveragePolicyStd     | 0.17574    |
| AveragePolicyStd[0]  | 0.19894    |
| AveragePolicyStd[1]  | 0.18802    |
| AveragePolicyStd[2]  | 0.14014    |
| AveragePolicyStd[3]  | 0.16641    |
| AveragePolicyStd[4]  | 0.14598    |
| AveragePolicyStd[5]  | 0.21494    |
| AverageReturn        | 1854.4     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 377.96     |
| AverageEpisodeLength | 934.53     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.14     |
| TotalNEpisodes       | 26039      |
| TotalNSamples        | 8.9465e+06 |
| ExplainedVariance    | 0.0039635  |
-------------------------------------
[2018-01-21 17:18:00.333277 UTC] Saving snapshot
[2018-01-21 17:18:00.333508 UTC] Starting iteration 1788
[2018-01-21 17:18:00.333666 UTC] Start collecting samples
[2018-01-21 17:18:05.289234 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:05.427443 UTC] Performing policy update
[2018-01-21 17:18:05.428469 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:05.554644 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:07.021710 UTC] Performing line search
[2018-01-21 17:18:07.223757 UTC] Updating baseline
[2018-01-21 17:18:09.701733 UTC] Computing logging information
-------------------------------------
| Iteration            | 1788       |
| ExpectedImprovement  | 0.017852   |
| ActualImprovement    | 0.01668    |
| ImprovementRatio     | 0.93432    |
| MeanKL               | 0.0083699  |
| Entropy              | -2.0008    |
| Perplexity           | 0.13523    |
| AveragePolicyStd     | 0.17547    |
| AveragePolicyStd[0]  | 0.19829    |
| AveragePolicyStd[1]  | 0.18796    |
| AveragePolicyStd[2]  | 0.13998    |
| AveragePolicyStd[3]  | 0.16609    |
| AveragePolicyStd[4]  | 0.14606    |
| AveragePolicyStd[5]  | 0.21446    |
| AverageReturn        | 1859.3     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 375        |
| AverageEpisodeLength | 937.41     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.86     |
| TotalNEpisodes       | 26043      |
| TotalNSamples        | 8.9505e+06 |
| ExplainedVariance    | 0.0026352  |
-------------------------------------
[2018-01-21 17:18:10.597478 UTC] Saving snapshot
[2018-01-21 17:18:10.597688 UTC] Starting iteration 1789
[2018-01-21 17:18:10.597897 UTC] Start collecting samples
[2018-01-21 17:18:15.466386 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:15.609775 UTC] Performing policy update
[2018-01-21 17:18:15.610495 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:15.741880 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:17.493187 UTC] Performing line search
[2018-01-21 17:18:17.702560 UTC] Updating baseline
[2018-01-21 17:18:19.893458 UTC] Computing logging information
-------------------------------------
| Iteration            | 1789       |
| ExpectedImprovement  | 0.018033   |
| ActualImprovement    | 0.017026   |
| ImprovementRatio     | 0.94419    |
| MeanKL               | 0.0090178  |
| Entropy              | -2.0036    |
| Perplexity           | 0.13485    |
| AveragePolicyStd     | 0.17538    |
| AveragePolicyStd[0]  | 0.1981     |
| AveragePolicyStd[1]  | 0.18782    |
| AveragePolicyStd[2]  | 0.13997    |
| AveragePolicyStd[3]  | 0.16611    |
| AveragePolicyStd[4]  | 0.14604    |
| AveragePolicyStd[5]  | 0.21425    |
| AverageReturn        | 1858.5     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 374.63     |
| AverageEpisodeLength | 937.76     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.95     |
| TotalNEpisodes       | 26050      |
| TotalNSamples        | 8.9575e+06 |
| ExplainedVariance    | 0.00016252 |
-------------------------------------
[2018-01-21 17:18:20.747406 UTC] Saving snapshot
[2018-01-21 17:18:20.747676 UTC] Starting iteration 1790
[2018-01-21 17:18:20.747875 UTC] Start collecting samples
[2018-01-21 17:18:26.183036 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:26.316491 UTC] Performing policy update
[2018-01-21 17:18:26.317818 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:26.448199 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:27.921639 UTC] Performing line search
[2018-01-21 17:18:28.126200 UTC] Updating baseline
[2018-01-21 17:18:31.380598 UTC] Computing logging information
-------------------------------------
| Iteration            | 1790       |
| ExpectedImprovement  | 0.016051   |
| ActualImprovement    | 0.015172   |
| ImprovementRatio     | 0.94523    |
| MeanKL               | 0.0095883  |
| Entropy              | -2.0028    |
| Perplexity           | 0.13495    |
| AveragePolicyStd     | 0.1754     |
| AveragePolicyStd[0]  | 0.19844    |
| AveragePolicyStd[1]  | 0.18755    |
| AveragePolicyStd[2]  | 0.13985    |
| AveragePolicyStd[3]  | 0.16575    |
| AveragePolicyStd[4]  | 0.14646    |
| AveragePolicyStd[5]  | 0.21437    |
| AverageReturn        | 1859.5     |
| MinReturn            | 415.14     |
| MaxReturn            | 2043       |
| StdReturn            | 374.65     |
| AverageEpisodeLength | 938.62     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.04     |
| TotalNEpisodes       | 26054      |
| TotalNSamples        | 8.9615e+06 |
| ExplainedVariance    | 0.0047648  |
-------------------------------------
[2018-01-21 17:18:32.285807 UTC] Saving snapshot
[2018-01-21 17:18:32.295172 UTC] Starting iteration 1791
[2018-01-21 17:18:32.295394 UTC] Start collecting samples
[2018-01-21 17:18:36.169355 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:36.257849 UTC] Performing policy update
[2018-01-21 17:18:36.258544 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:36.338600 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:37.304058 UTC] Performing line search
[2018-01-21 17:18:37.443650 UTC] Updating baseline
[2018-01-21 17:18:39.032259 UTC] Computing logging information
-------------------------------------
| Iteration            | 1791       |
| ExpectedImprovement  | 0.017436   |
| ActualImprovement    | 0.015722   |
| ImprovementRatio     | 0.90167    |
| MeanKL               | 0.0083832  |
| Entropy              | -1.9899    |
| Perplexity           | 0.13671    |
| AveragePolicyStd     | 0.17578    |
| AveragePolicyStd[0]  | 0.19907    |
| AveragePolicyStd[1]  | 0.18797    |
| AveragePolicyStd[2]  | 0.14017    |
| AveragePolicyStd[3]  | 0.16657    |
| AveragePolicyStd[4]  | 0.1464     |
| AveragePolicyStd[5]  | 0.21451    |
| AverageReturn        | 1858       |
| MinReturn            | 415.14     |
| MaxReturn            | 2036.7     |
| StdReturn            | 374.1      |
| AverageEpisodeLength | 938.62     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.04     |
| TotalNEpisodes       | 26058      |
| TotalNSamples        | 8.9655e+06 |
| ExplainedVariance    | 0.0043984  |
-------------------------------------
[2018-01-21 17:18:39.572815 UTC] Saving snapshot
[2018-01-21 17:18:39.573028 UTC] Starting iteration 1792
[2018-01-21 17:18:39.573180 UTC] Start collecting samples
[2018-01-21 17:18:41.995394 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:42.070001 UTC] Performing policy update
[2018-01-21 17:18:42.070562 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:42.152218 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:43.019146 UTC] Performing line search
[2018-01-21 17:18:43.137785 UTC] Updating baseline
[2018-01-21 17:18:44.657991 UTC] Computing logging information
-------------------------------------
| Iteration            | 1792       |
| ExpectedImprovement  | 0.016399   |
| ActualImprovement    | 0.015918   |
| ImprovementRatio     | 0.97062    |
| MeanKL               | 0.0080578  |
| Entropy              | -1.9901    |
| Perplexity           | 0.13668    |
| AveragePolicyStd     | 0.17581    |
| AveragePolicyStd[0]  | 0.19906    |
| AveragePolicyStd[1]  | 0.18798    |
| AveragePolicyStd[2]  | 0.14003    |
| AveragePolicyStd[3]  | 0.16635    |
| AveragePolicyStd[4]  | 0.14623    |
| AveragePolicyStd[5]  | 0.21523    |
| AverageReturn        | 1855.9     |
| MinReturn            | 415.14     |
| MaxReturn            | 2036.7     |
| StdReturn            | 373.4      |
| AverageEpisodeLength | 938.62     |
| MinEpisodeLength     | 239        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.04     |
| TotalNEpisodes       | 26064      |
| TotalNSamples        | 8.9715e+06 |
| ExplainedVariance    | 0.0040533  |
-------------------------------------
[2018-01-21 17:18:45.176851 UTC] Saving snapshot
[2018-01-21 17:18:45.177041 UTC] Starting iteration 1793
[2018-01-21 17:18:45.177188 UTC] Start collecting samples
[2018-01-21 17:18:47.651955 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:47.729955 UTC] Performing policy update
[2018-01-21 17:18:47.730488 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:47.803284 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:48.687125 UTC] Performing line search
[2018-01-21 17:18:48.809017 UTC] Updating baseline
[2018-01-21 17:18:50.030421 UTC] Computing logging information
-------------------------------------
| Iteration            | 1793       |
| ExpectedImprovement  | 0.017935   |
| ActualImprovement    | 0.017226   |
| ImprovementRatio     | 0.96048    |
| MeanKL               | 0.0077951  |
| Entropy              | -1.9882    |
| Perplexity           | 0.13694    |
| AveragePolicyStd     | 0.17586    |
| AveragePolicyStd[0]  | 0.19843    |
| AveragePolicyStd[1]  | 0.18839    |
| AveragePolicyStd[2]  | 0.14028    |
| AveragePolicyStd[3]  | 0.16624    |
| AveragePolicyStd[4]  | 0.14635    |
| AveragePolicyStd[5]  | 0.21544    |
| AverageReturn        | 1870.9     |
| MinReturn            | 353.12     |
| MaxReturn            | 2036.7     |
| StdReturn            | 364.26     |
| AverageEpisodeLength | 947.07     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.37     |
| TotalNEpisodes       | 26070      |
| TotalNSamples        | 8.9767e+06 |
| ExplainedVariance    | 0.12595    |
-------------------------------------
[2018-01-21 17:18:50.553417 UTC] Saving snapshot
[2018-01-21 17:18:50.553612 UTC] Starting iteration 1794
[2018-01-21 17:18:50.553751 UTC] Start collecting samples
[2018-01-21 17:18:53.040002 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:53.113065 UTC] Performing policy update
[2018-01-21 17:18:53.113584 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:53.189332 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:54.058237 UTC] Performing line search
[2018-01-21 17:18:54.180352 UTC] Updating baseline
[2018-01-21 17:18:55.595911 UTC] Computing logging information
-------------------------------------
| Iteration            | 1794       |
| ExpectedImprovement  | 0.01751    |
| ActualImprovement    | 0.016509   |
| ImprovementRatio     | 0.94283    |
| MeanKL               | 0.0089223  |
| Entropy              | -1.9907    |
| Perplexity           | 0.1366     |
| AveragePolicyStd     | 0.17577    |
| AveragePolicyStd[0]  | 0.19806    |
| AveragePolicyStd[1]  | 0.18838    |
| AveragePolicyStd[2]  | 0.14022    |
| AveragePolicyStd[3]  | 0.16615    |
| AveragePolicyStd[4]  | 0.14649    |
| AveragePolicyStd[5]  | 0.21533    |
| AverageReturn        | 1869.5     |
| MinReturn            | 353.12     |
| MaxReturn            | 2036.7     |
| StdReturn            | 363.88     |
| AverageEpisodeLength | 947.07     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.37     |
| TotalNEpisodes       | 26075      |
| TotalNSamples        | 8.9817e+06 |
| ExplainedVariance    | -0.0059762 |
-------------------------------------
[2018-01-21 17:18:56.116976 UTC] Saving snapshot
[2018-01-21 17:18:56.117168 UTC] Starting iteration 1795
[2018-01-21 17:18:56.117300 UTC] Start collecting samples
[2018-01-21 17:18:58.439072 UTC] Computing input variables for policy optimization
[2018-01-21 17:18:58.517049 UTC] Performing policy update
[2018-01-21 17:18:58.517665 UTC] Computing gradient in Euclidean space
[2018-01-21 17:18:58.595218 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:18:59.511237 UTC] Performing line search
[2018-01-21 17:18:59.649044 UTC] Updating baseline
[2018-01-21 17:19:01.043158 UTC] Computing logging information
-------------------------------------
| Iteration            | 1795       |
| ExpectedImprovement  | 0.019215   |
| ActualImprovement    | 0.017884   |
| ImprovementRatio     | 0.93069    |
| MeanKL               | 0.0094898  |
| Entropy              | -1.9928    |
| Perplexity           | 0.13632    |
| AveragePolicyStd     | 0.17571    |
| AveragePolicyStd[0]  | 0.19744    |
| AveragePolicyStd[1]  | 0.18845    |
| AveragePolicyStd[2]  | 0.14046    |
| AveragePolicyStd[3]  | 0.1657     |
| AveragePolicyStd[4]  | 0.14641    |
| AveragePolicyStd[5]  | 0.21581    |
| AverageReturn        | 1867       |
| MinReturn            | 353.12     |
| MaxReturn            | 2036.7     |
| StdReturn            | 363.11     |
| AverageEpisodeLength | 947.07     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 175.37     |
| TotalNEpisodes       | 26079      |
| TotalNSamples        | 8.9857e+06 |
| ExplainedVariance    | 0.0013677  |
-------------------------------------
[2018-01-21 17:19:01.566704 UTC] Saving snapshot
[2018-01-21 17:19:01.566909 UTC] Starting iteration 1796
[2018-01-21 17:19:01.567045 UTC] Start collecting samples
[2018-01-21 17:19:03.944000 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:04.018656 UTC] Performing policy update
[2018-01-21 17:19:04.019177 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:04.092889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:04.965787 UTC] Performing line search
[2018-01-21 17:19:05.092715 UTC] Updating baseline
[2018-01-21 17:19:06.094848 UTC] Computing logging information
-------------------------------------
| Iteration            | 1796       |
| ExpectedImprovement  | 0.015646   |
| ActualImprovement    | 0.015076   |
| ImprovementRatio     | 0.96358    |
| MeanKL               | 0.0083969  |
| Entropy              | -1.9912    |
| Perplexity           | 0.13653    |
| AveragePolicyStd     | 0.17578    |
| AveragePolicyStd[0]  | 0.19739    |
| AveragePolicyStd[1]  | 0.18835    |
| AveragePolicyStd[2]  | 0.14022    |
| AveragePolicyStd[3]  | 0.16572    |
| AveragePolicyStd[4]  | 0.14657    |
| AveragePolicyStd[5]  | 0.21642    |
| AverageReturn        | 1869.9     |
| MinReturn            | 353.12     |
| MaxReturn            | 2036.7     |
| StdReturn            | 361.76     |
| AverageEpisodeLength | 948.94     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.93     |
| TotalNEpisodes       | 26086      |
| TotalNSamples        | 8.9927e+06 |
| ExplainedVariance    | 0.0023414  |
-------------------------------------
[2018-01-21 17:19:06.639995 UTC] Saving snapshot
[2018-01-21 17:19:06.640186 UTC] Starting iteration 1797
[2018-01-21 17:19:06.640310 UTC] Start collecting samples
[2018-01-21 17:19:08.971932 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:09.043342 UTC] Performing policy update
[2018-01-21 17:19:09.043850 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:09.117867 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:10.023645 UTC] Performing line search
[2018-01-21 17:19:10.142922 UTC] Updating baseline
[2018-01-21 17:19:11.369323 UTC] Computing logging information
------------------------------------
| Iteration            | 1797      |
| ExpectedImprovement  | 0.018574  |
| ActualImprovement    | 0.016967  |
| ImprovementRatio     | 0.91346   |
| MeanKL               | 0.0080797 |
| Entropy              | -1.9807   |
| Perplexity           | 0.13797   |
| AveragePolicyStd     | 0.17609   |
| AveragePolicyStd[0]  | 0.19745   |
| AveragePolicyStd[1]  | 0.18835   |
| AveragePolicyStd[2]  | 0.14055   |
| AveragePolicyStd[3]  | 0.16629   |
| AveragePolicyStd[4]  | 0.14663   |
| AveragePolicyStd[5]  | 0.21729   |
| AverageReturn        | 1868.8    |
| MinReturn            | 353.12    |
| MaxReturn            | 2036.7    |
| StdReturn            | 360.15    |
| AverageEpisodeLength | 949.08    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 174.37    |
| TotalNEpisodes       | 26089     |
| TotalNSamples        | 8.995e+06 |
| ExplainedVariance    | 0.18177   |
------------------------------------
[2018-01-21 17:19:11.888802 UTC] Saving snapshot
[2018-01-21 17:19:11.889002 UTC] Starting iteration 1798
[2018-01-21 17:19:11.889145 UTC] Start collecting samples
[2018-01-21 17:19:14.528957 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:14.604730 UTC] Performing policy update
[2018-01-21 17:19:14.605268 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:14.679788 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:15.590645 UTC] Performing line search
[2018-01-21 17:19:15.717873 UTC] Updating baseline
[2018-01-21 17:19:16.967032 UTC] Computing logging information
-------------------------------------
| Iteration            | 1798       |
| ExpectedImprovement  | 0.018761   |
| ActualImprovement    | 0.017504   |
| ImprovementRatio     | 0.93301    |
| MeanKL               | 0.0080929  |
| Entropy              | -1.9825    |
| Perplexity           | 0.13772    |
| AveragePolicyStd     | 0.17606    |
| AveragePolicyStd[0]  | 0.19788    |
| AveragePolicyStd[1]  | 0.18836    |
| AveragePolicyStd[2]  | 0.14047    |
| AveragePolicyStd[3]  | 0.16596    |
| AveragePolicyStd[4]  | 0.14637    |
| AveragePolicyStd[5]  | 0.21733    |
| AverageReturn        | 1880.8     |
| MinReturn            | 353.12     |
| MaxReturn            | 2036.7     |
| StdReturn            | 331.97     |
| AverageEpisodeLength | 956.38     |
| MinEpisodeLength     | 213        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.51     |
| TotalNEpisodes       | 26095      |
| TotalNSamples        | 9.001e+06  |
| ExplainedVariance    | -0.0035334 |
-------------------------------------
[2018-01-21 17:19:17.481836 UTC] Saving snapshot
[2018-01-21 17:19:17.482048 UTC] Starting iteration 1799
[2018-01-21 17:19:17.482203 UTC] Start collecting samples
[2018-01-21 17:19:19.808038 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:19.885819 UTC] Performing policy update
[2018-01-21 17:19:19.886336 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:19.992444 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:20.916327 UTC] Performing line search
[2018-01-21 17:19:21.033992 UTC] Updating baseline
[2018-01-21 17:19:22.568150 UTC] Computing logging information
------------------------------------
| Iteration            | 1799      |
| ExpectedImprovement  | 0.016123  |
| ActualImprovement    | 0.015478  |
| ImprovementRatio     | 0.96004   |
| MeanKL               | 0.0076882 |
| Entropy              | -1.9836   |
| Perplexity           | 0.13757   |
| AveragePolicyStd     | 0.17602   |
| AveragePolicyStd[0]  | 0.19789   |
| AveragePolicyStd[1]  | 0.18802   |
| AveragePolicyStd[2]  | 0.14045   |
| AveragePolicyStd[3]  | 0.16604   |
| AveragePolicyStd[4]  | 0.1465    |
| AveragePolicyStd[5]  | 0.21723   |
| AverageReturn        | 1898.5    |
| MinReturn            | 353.12    |
| MaxReturn            | 2023.8    |
| StdReturn            | 304.09    |
| AverageEpisodeLength | 966.18    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 147.05    |
| TotalNEpisodes       | 26102     |
| TotalNSamples        | 9.008e+06 |
| ExplainedVariance    | 0.0012872 |
------------------------------------
[2018-01-21 17:19:23.096530 UTC] Saving snapshot
[2018-01-21 17:19:23.096726 UTC] Starting iteration 1800
[2018-01-21 17:19:23.096846 UTC] Start collecting samples
[2018-01-21 17:19:25.653486 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:25.773313 UTC] Performing policy update
[2018-01-21 17:19:25.773947 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:25.849911 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:26.748342 UTC] Performing line search
[2018-01-21 17:19:26.875005 UTC] Updating baseline
[2018-01-21 17:19:28.419605 UTC] Computing logging information
------------------------------------
| Iteration            | 1800      |
| ExpectedImprovement  | 0.016391  |
| ActualImprovement    | 0.015221  |
| ImprovementRatio     | 0.9286    |
| MeanKL               | 0.0087708 |
| Entropy              | -1.9813   |
| Perplexity           | 0.13788   |
| AveragePolicyStd     | 0.17609   |
| AveragePolicyStd[0]  | 0.19774   |
| AveragePolicyStd[1]  | 0.18825   |
| AveragePolicyStd[2]  | 0.14059   |
| AveragePolicyStd[3]  | 0.16566   |
| AveragePolicyStd[4]  | 0.14671   |
| AveragePolicyStd[5]  | 0.21759   |
| AverageReturn        | 1900.4    |
| MinReturn            | 353.12    |
| MaxReturn            | 2023.8    |
| StdReturn            | 303.42    |
| AverageEpisodeLength | 967.43    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 146.8     |
| TotalNEpisodes       | 26105     |
| TotalNSamples        | 9.011e+06 |
| ExplainedVariance    | -0.019379 |
------------------------------------
[2018-01-21 17:19:28.954652 UTC] Saving snapshot
[2018-01-21 17:19:28.960247 UTC] Starting iteration 1801
[2018-01-21 17:19:28.960418 UTC] Start collecting samples
[2018-01-21 17:19:31.251272 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:31.322111 UTC] Performing policy update
[2018-01-21 17:19:31.322646 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:31.397041 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:32.266599 UTC] Performing line search
[2018-01-21 17:19:32.384226 UTC] Updating baseline
[2018-01-21 17:19:33.854896 UTC] Computing logging information
--------------------------------------
| Iteration            | 1801        |
| ExpectedImprovement  | 0.018186    |
| ActualImprovement    | 0.017017    |
| ImprovementRatio     | 0.93575     |
| MeanKL               | 0.0087835   |
| Entropy              | -1.9771     |
| Perplexity           | 0.13847     |
| AveragePolicyStd     | 0.17622     |
| AveragePolicyStd[0]  | 0.19795     |
| AveragePolicyStd[1]  | 0.18829     |
| AveragePolicyStd[2]  | 0.14079     |
| AveragePolicyStd[3]  | 0.16591     |
| AveragePolicyStd[4]  | 0.14656     |
| AveragePolicyStd[5]  | 0.21782     |
| AverageReturn        | 1914.1      |
| MinReturn            | 353.12      |
| MaxReturn            | 2023.8      |
| StdReturn            | 271.4       |
| AverageEpisodeLength | 974.37      |
| MinEpisodeLength     | 213         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 130.91      |
| TotalNEpisodes       | 26109       |
| TotalNSamples        | 9.015e+06   |
| ExplainedVariance    | -3.5784e-06 |
--------------------------------------
[2018-01-21 17:19:34.381700 UTC] Saving snapshot
[2018-01-21 17:19:34.381896 UTC] Starting iteration 1802
[2018-01-21 17:19:34.382023 UTC] Start collecting samples
[2018-01-21 17:19:36.868114 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:36.947438 UTC] Performing policy update
[2018-01-21 17:19:36.948035 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:37.022319 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:37.884220 UTC] Performing line search
[2018-01-21 17:19:38.002340 UTC] Updating baseline
[2018-01-21 17:19:39.588724 UTC] Computing logging information
--------------------------------------
| Iteration            | 1802        |
| ExpectedImprovement  | 0.017147    |
| ActualImprovement    | 0.016552    |
| ImprovementRatio     | 0.96531     |
| MeanKL               | 0.009204    |
| Entropy              | -1.9753     |
| Perplexity           | 0.13872     |
| AveragePolicyStd     | 0.17629     |
| AveragePolicyStd[0]  | 0.1976      |
| AveragePolicyStd[1]  | 0.1884      |
| AveragePolicyStd[2]  | 0.1407      |
| AveragePolicyStd[3]  | 0.16597     |
| AveragePolicyStd[4]  | 0.14665     |
| AveragePolicyStd[5]  | 0.2184      |
| AverageReturn        | 1926.8      |
| MinReturn            | 353.12      |
| MaxReturn            | 2023.8      |
| StdReturn            | 230.07      |
| AverageEpisodeLength | 981.56      |
| MinEpisodeLength     | 213         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 110.83      |
| TotalNEpisodes       | 26117       |
| TotalNSamples        | 9.023e+06   |
| ExplainedVariance    | -9.2533e-05 |
--------------------------------------
[2018-01-21 17:19:40.101356 UTC] Saving snapshot
[2018-01-21 17:19:40.101544 UTC] Starting iteration 1803
[2018-01-21 17:19:40.101685 UTC] Start collecting samples
[2018-01-21 17:19:42.521361 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:42.593382 UTC] Performing policy update
[2018-01-21 17:19:42.593978 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:42.669061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:43.558959 UTC] Performing line search
[2018-01-21 17:19:43.681188 UTC] Updating baseline
[2018-01-21 17:19:44.972088 UTC] Computing logging information
------------------------------------
| Iteration            | 1803      |
| ExpectedImprovement  | 0.017994  |
| ActualImprovement    | 0.016664  |
| ImprovementRatio     | 0.92607   |
| MeanKL               | 0.0074683 |
| Entropy              | -1.9782   |
| Perplexity           | 0.13831   |
| AveragePolicyStd     | 0.1762    |
| AveragePolicyStd[0]  | 0.19722   |
| AveragePolicyStd[1]  | 0.18866   |
| AveragePolicyStd[2]  | 0.14043   |
| AveragePolicyStd[3]  | 0.1659    |
| AveragePolicyStd[4]  | 0.14674   |
| AveragePolicyStd[5]  | 0.21825   |
| AverageReturn        | 1925.2    |
| MinReturn            | 353.12    |
| MaxReturn            | 2020      |
| StdReturn            | 229.66    |
| AverageEpisodeLength | 981.56    |
| MinEpisodeLength     | 213       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 110.83    |
| TotalNEpisodes       | 26121     |
| TotalNSamples        | 9.027e+06 |
| ExplainedVariance    | -0.006146 |
------------------------------------
[2018-01-21 17:19:45.485334 UTC] Saving snapshot
[2018-01-21 17:19:45.485530 UTC] Starting iteration 1804
[2018-01-21 17:19:45.485650 UTC] Start collecting samples
[2018-01-21 17:19:47.811143 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:47.887660 UTC] Performing policy update
[2018-01-21 17:19:47.888160 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:47.963490 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:48.898082 UTC] Performing line search
[2018-01-21 17:19:49.022222 UTC] Updating baseline
[2018-01-21 17:19:50.599524 UTC] Computing logging information
-------------------------------------
| Iteration            | 1804       |
| ExpectedImprovement  | 0.016622   |
| ActualImprovement    | 0.015663   |
| ImprovementRatio     | 0.9423     |
| MeanKL               | 0.0080315  |
| Entropy              | -1.9782    |
| Perplexity           | 0.13832    |
| AveragePolicyStd     | 0.17621    |
| AveragePolicyStd[0]  | 0.19701    |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.13997    |
| AveragePolicyStd[3]  | 0.16635    |
| AveragePolicyStd[4]  | 0.14689    |
| AveragePolicyStd[5]  | 0.21815    |
| AverageReturn        | 1912       |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 285.08     |
| AverageEpisodeLength | 975.48     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.77     |
| TotalNEpisodes       | 26126      |
| TotalNSamples        | 9.0311e+06 |
| ExplainedVariance    | 0.12273    |
-------------------------------------
[2018-01-21 17:19:51.115851 UTC] Saving snapshot
[2018-01-21 17:19:51.116035 UTC] Starting iteration 1805
[2018-01-21 17:19:51.116155 UTC] Start collecting samples
[2018-01-21 17:19:53.465711 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:53.538059 UTC] Performing policy update
[2018-01-21 17:19:53.538643 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:53.612127 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:54.477564 UTC] Performing line search
[2018-01-21 17:19:54.604471 UTC] Updating baseline
[2018-01-21 17:19:55.734674 UTC] Computing logging information
-------------------------------------
| Iteration            | 1805       |
| ExpectedImprovement  | 0.018072   |
| ActualImprovement    | 0.017347   |
| ImprovementRatio     | 0.95986    |
| MeanKL               | 0.008409   |
| Entropy              | -1.9831    |
| Perplexity           | 0.13764    |
| AveragePolicyStd     | 0.17607    |
| AveragePolicyStd[0]  | 0.19656    |
| AveragePolicyStd[1]  | 0.18881    |
| AveragePolicyStd[2]  | 0.13961    |
| AveragePolicyStd[3]  | 0.16644    |
| AveragePolicyStd[4]  | 0.1469     |
| AveragePolicyStd[5]  | 0.21807    |
| AverageReturn        | 1911.4     |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 284.91     |
| AverageEpisodeLength | 975.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.79     |
| TotalNEpisodes       | 26132      |
| TotalNSamples        | 9.0371e+06 |
| ExplainedVariance    | 0.0002242  |
-------------------------------------
[2018-01-21 17:19:56.261935 UTC] Saving snapshot
[2018-01-21 17:19:56.262142 UTC] Starting iteration 1806
[2018-01-21 17:19:56.262263 UTC] Start collecting samples
[2018-01-21 17:19:58.563624 UTC] Computing input variables for policy optimization
[2018-01-21 17:19:58.636793 UTC] Performing policy update
[2018-01-21 17:19:58.637394 UTC] Computing gradient in Euclidean space
[2018-01-21 17:19:58.711709 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:19:59.601723 UTC] Performing line search
[2018-01-21 17:19:59.723051 UTC] Updating baseline
[2018-01-21 17:20:00.955525 UTC] Computing logging information
-------------------------------------
| Iteration            | 1806       |
| ExpectedImprovement  | 0.017231   |
| ActualImprovement    | 0.016331   |
| ImprovementRatio     | 0.94774    |
| MeanKL               | 0.0087695  |
| Entropy              | -1.9827    |
| Perplexity           | 0.1377     |
| AveragePolicyStd     | 0.17607    |
| AveragePolicyStd[0]  | 0.19661    |
| AveragePolicyStd[1]  | 0.18873    |
| AveragePolicyStd[2]  | 0.13958    |
| AveragePolicyStd[3]  | 0.16686    |
| AveragePolicyStd[4]  | 0.14681    |
| AveragePolicyStd[5]  | 0.21783    |
| AverageReturn        | 1910.2     |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 284.64     |
| AverageEpisodeLength | 975.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.79     |
| TotalNEpisodes       | 26137      |
| TotalNSamples        | 9.0421e+06 |
| ExplainedVariance    | 0.0033473  |
-------------------------------------
[2018-01-21 17:20:01.469875 UTC] Saving snapshot
[2018-01-21 17:20:01.470102 UTC] Starting iteration 1807
[2018-01-21 17:20:01.470230 UTC] Start collecting samples
[2018-01-21 17:20:03.872094 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:03.947737 UTC] Performing policy update
[2018-01-21 17:20:03.948262 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:04.023253 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:04.902067 UTC] Performing line search
[2018-01-21 17:20:05.022945 UTC] Updating baseline
[2018-01-21 17:20:06.627779 UTC] Computing logging information
-------------------------------------
| Iteration            | 1807       |
| ExpectedImprovement  | 0.01839    |
| ActualImprovement    | 0.016769   |
| ImprovementRatio     | 0.91185    |
| MeanKL               | 0.0082716  |
| Entropy              | -1.9779    |
| Perplexity           | 0.13835    |
| AveragePolicyStd     | 0.17622    |
| AveragePolicyStd[0]  | 0.19622    |
| AveragePolicyStd[1]  | 0.18927    |
| AveragePolicyStd[2]  | 0.1394     |
| AveragePolicyStd[3]  | 0.16712    |
| AveragePolicyStd[4]  | 0.14708    |
| AveragePolicyStd[5]  | 0.21821    |
| AverageReturn        | 1910.2     |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 284.7      |
| AverageEpisodeLength | 975.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 138.79     |
| TotalNEpisodes       | 26141      |
| TotalNSamples        | 9.0461e+06 |
| ExplainedVariance    | 0.0064861  |
-------------------------------------
[2018-01-21 17:20:07.150965 UTC] Saving snapshot
[2018-01-21 17:20:07.151151 UTC] Starting iteration 1808
[2018-01-21 17:20:07.151308 UTC] Start collecting samples
[2018-01-21 17:20:09.497909 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:09.582631 UTC] Performing policy update
[2018-01-21 17:20:09.583156 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:09.664966 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:10.565925 UTC] Performing line search
[2018-01-21 17:20:10.689397 UTC] Updating baseline
[2018-01-21 17:20:11.897886 UTC] Computing logging information
-------------------------------------
| Iteration            | 1808       |
| ExpectedImprovement  | 0.018532   |
| ActualImprovement    | 0.017434   |
| ImprovementRatio     | 0.94074    |
| MeanKL               | 0.0082604  |
| Entropy              | -1.9794    |
| Perplexity           | 0.13815    |
| AveragePolicyStd     | 0.17617    |
| AveragePolicyStd[0]  | 0.19589    |
| AveragePolicyStd[1]  | 0.18963    |
| AveragePolicyStd[2]  | 0.13944    |
| AveragePolicyStd[3]  | 0.16698    |
| AveragePolicyStd[4]  | 0.147      |
| AveragePolicyStd[5]  | 0.21808    |
| AverageReturn        | 1895.1     |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 311.3      |
| AverageEpisodeLength | 969.26     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.72     |
| TotalNEpisodes       | 26147      |
| TotalNSamples        | 9.0514e+06 |
| ExplainedVariance    | 0.087453   |
-------------------------------------
[2018-01-21 17:20:12.414553 UTC] Saving snapshot
[2018-01-21 17:20:12.414743 UTC] Starting iteration 1809
[2018-01-21 17:20:12.414887 UTC] Start collecting samples
[2018-01-21 17:20:14.798561 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:14.907086 UTC] Performing policy update
[2018-01-21 17:20:14.907687 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:14.983572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:15.860484 UTC] Performing line search
[2018-01-21 17:20:15.979916 UTC] Updating baseline
[2018-01-21 17:20:17.182302 UTC] Computing logging information
-------------------------------------
| Iteration            | 1809       |
| ExpectedImprovement  | 0.018344   |
| ActualImprovement    | 0.017465   |
| ImprovementRatio     | 0.95208    |
| MeanKL               | 0.007739   |
| Entropy              | -1.9788    |
| Perplexity           | 0.13823    |
| AveragePolicyStd     | 0.17622    |
| AveragePolicyStd[0]  | 0.19599    |
| AveragePolicyStd[1]  | 0.18988    |
| AveragePolicyStd[2]  | 0.13933    |
| AveragePolicyStd[3]  | 0.16693    |
| AveragePolicyStd[4]  | 0.14676    |
| AveragePolicyStd[5]  | 0.21842    |
| AverageReturn        | 1877.9     |
| MinReturn            | 117.95     |
| MaxReturn            | 2020       |
| StdReturn            | 350.68     |
| AverageEpisodeLength | 961.02     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 170.98     |
| TotalNEpisodes       | 26154      |
| TotalNSamples        | 9.0576e+06 |
| ExplainedVariance    | 0.12597    |
-------------------------------------
[2018-01-21 17:20:17.694575 UTC] Saving snapshot
[2018-01-21 17:20:17.694788 UTC] Starting iteration 1810
[2018-01-21 17:20:17.694945 UTC] Start collecting samples
[2018-01-21 17:20:19.966525 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:20.039330 UTC] Performing policy update
[2018-01-21 17:20:20.039911 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:20.115721 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:21.000634 UTC] Performing line search
[2018-01-21 17:20:21.119190 UTC] Updating baseline
[2018-01-21 17:20:22.438946 UTC] Computing logging information
------------------------------------
| Iteration            | 1810      |
| ExpectedImprovement  | 0.016046  |
| ActualImprovement    | 0.015501  |
| ImprovementRatio     | 0.96599   |
| MeanKL               | 0.0086383 |
| Entropy              | -1.9826   |
| Perplexity           | 0.13771   |
| AveragePolicyStd     | 0.1761    |
| AveragePolicyStd[0]  | 0.19556   |
| AveragePolicyStd[1]  | 0.18957   |
| AveragePolicyStd[2]  | 0.13942   |
| AveragePolicyStd[3]  | 0.16661   |
| AveragePolicyStd[4]  | 0.14687   |
| AveragePolicyStd[5]  | 0.21855   |
| AverageReturn        | 1866.3    |
| MinReturn            | 117.95    |
| MaxReturn            | 2020      |
| StdReturn            | 367.53    |
| AverageEpisodeLength | 955.16    |
| MinEpisodeLength     | 101       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 179.38    |
| TotalNEpisodes       | 26159     |
| TotalNSamples        | 9.062e+06 |
| ExplainedVariance    | 0.14761   |
------------------------------------
[2018-01-21 17:20:22.955096 UTC] Saving snapshot
[2018-01-21 17:20:22.960705 UTC] Starting iteration 1811
[2018-01-21 17:20:22.960892 UTC] Start collecting samples
[2018-01-21 17:20:25.443024 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:25.516810 UTC] Performing policy update
[2018-01-21 17:20:25.517320 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:25.590865 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:26.545758 UTC] Performing line search
[2018-01-21 17:20:26.663752 UTC] Updating baseline
[2018-01-21 17:20:27.962100 UTC] Computing logging information
------------------------------------
| Iteration            | 1811      |
| ExpectedImprovement  | 0.018321  |
| ActualImprovement    | 0.017496  |
| ImprovementRatio     | 0.95498   |
| MeanKL               | 0.0083478 |
| Entropy              | -1.9759   |
| Perplexity           | 0.13864   |
| AveragePolicyStd     | 0.17628   |
| AveragePolicyStd[0]  | 0.19527   |
| AveragePolicyStd[1]  | 0.18938   |
| AveragePolicyStd[2]  | 0.1395    |
| AveragePolicyStd[3]  | 0.1669    |
| AveragePolicyStd[4]  | 0.14752   |
| AveragePolicyStd[5]  | 0.21909   |
| AverageReturn        | 1864.9    |
| MinReturn            | 117.95    |
| MaxReturn            | 2018.9    |
| StdReturn            | 367.17    |
| AverageEpisodeLength | 955.16    |
| MinEpisodeLength     | 101       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 179.38    |
| TotalNEpisodes       | 26164     |
| TotalNSamples        | 9.067e+06 |
| ExplainedVariance    | -0.018183 |
------------------------------------
[2018-01-21 17:20:28.473151 UTC] Saving snapshot
[2018-01-21 17:20:28.473359 UTC] Starting iteration 1812
[2018-01-21 17:20:28.473511 UTC] Start collecting samples
[2018-01-21 17:20:31.007755 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:31.080796 UTC] Performing policy update
[2018-01-21 17:20:31.081337 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:31.162971 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:32.028815 UTC] Performing line search
[2018-01-21 17:20:32.155440 UTC] Updating baseline
[2018-01-21 17:20:33.546165 UTC] Computing logging information
-------------------------------------
| Iteration            | 1812       |
| ExpectedImprovement  | 0.01757    |
| ActualImprovement    | 0.016462   |
| ImprovementRatio     | 0.93691    |
| MeanKL               | 0.0075819  |
| Entropy              | -1.977     |
| Perplexity           | 0.13848    |
| AveragePolicyStd     | 0.17624    |
| AveragePolicyStd[0]  | 0.19498    |
| AveragePolicyStd[1]  | 0.18948    |
| AveragePolicyStd[2]  | 0.13977    |
| AveragePolicyStd[3]  | 0.16695    |
| AveragePolicyStd[4]  | 0.14718    |
| AveragePolicyStd[5]  | 0.21908    |
| AverageReturn        | 1880.5     |
| MinReturn            | 117.95     |
| MaxReturn            | 2026.2     |
| StdReturn            | 334.75     |
| AverageEpisodeLength | 961.99     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.27     |
| TotalNEpisodes       | 26169      |
| TotalNSamples        | 9.0719e+06 |
| ExplainedVariance    | 0.10615    |
-------------------------------------
[2018-01-21 17:20:34.221718 UTC] Saving snapshot
[2018-01-21 17:20:34.221919 UTC] Starting iteration 1813
[2018-01-21 17:20:34.222047 UTC] Start collecting samples
[2018-01-21 17:20:36.591146 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:36.717452 UTC] Performing policy update
[2018-01-21 17:20:36.718115 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:36.840174 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:37.871332 UTC] Performing line search
[2018-01-21 17:20:37.990143 UTC] Updating baseline
[2018-01-21 17:20:39.323911 UTC] Computing logging information
-------------------------------------
| Iteration            | 1813       |
| ExpectedImprovement  | 0.016679   |
| ActualImprovement    | 0.016257   |
| ImprovementRatio     | 0.97467    |
| MeanKL               | 0.0077777  |
| Entropy              | -1.982     |
| Perplexity           | 0.13779    |
| AveragePolicyStd     | 0.1761     |
| AveragePolicyStd[0]  | 0.1945     |
| AveragePolicyStd[1]  | 0.18922    |
| AveragePolicyStd[2]  | 0.1397     |
| AveragePolicyStd[3]  | 0.16707    |
| AveragePolicyStd[4]  | 0.14684    |
| AveragePolicyStd[5]  | 0.21928    |
| AverageReturn        | 1869.8     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 356.35     |
| AverageEpisodeLength | 955.82     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.08     |
| TotalNEpisodes       | 26175      |
| TotalNSamples        | 9.0773e+06 |
| ExplainedVariance    | 0.08544    |
-------------------------------------
[2018-01-21 17:20:39.834667 UTC] Saving snapshot
[2018-01-21 17:20:39.834880 UTC] Starting iteration 1814
[2018-01-21 17:20:39.835010 UTC] Start collecting samples
[2018-01-21 17:20:42.467660 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:42.549044 UTC] Performing policy update
[2018-01-21 17:20:42.549630 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:42.625100 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:43.519917 UTC] Performing line search
[2018-01-21 17:20:43.637526 UTC] Updating baseline
[2018-01-21 17:20:45.001739 UTC] Computing logging information
-------------------------------------
| Iteration            | 1814       |
| ExpectedImprovement  | 0.017028   |
| ActualImprovement    | 0.015719   |
| ImprovementRatio     | 0.92315    |
| MeanKL               | 0.0084413  |
| Entropy              | -1.9799    |
| Perplexity           | 0.13809    |
| AveragePolicyStd     | 0.17618    |
| AveragePolicyStd[0]  | 0.19457    |
| AveragePolicyStd[1]  | 0.1893     |
| AveragePolicyStd[2]  | 0.13972    |
| AveragePolicyStd[3]  | 0.16714    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.21962    |
| AverageReturn        | 1870.8     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 356.58     |
| AverageEpisodeLength | 955.82     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.08     |
| TotalNEpisodes       | 26179      |
| TotalNSamples        | 9.0813e+06 |
| ExplainedVariance    | 0.031904   |
-------------------------------------
[2018-01-21 17:20:45.537247 UTC] Saving snapshot
[2018-01-21 17:20:45.537439 UTC] Starting iteration 1815
[2018-01-21 17:20:45.537578 UTC] Start collecting samples
[2018-01-21 17:20:48.182303 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:48.257582 UTC] Performing policy update
[2018-01-21 17:20:48.258030 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:48.331973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:49.316391 UTC] Performing line search
[2018-01-21 17:20:49.441546 UTC] Updating baseline
[2018-01-21 17:20:50.642307 UTC] Computing logging information
-------------------------------------
| Iteration            | 1815       |
| ExpectedImprovement  | 0.018099   |
| ActualImprovement    | 0.017032   |
| ImprovementRatio     | 0.94107    |
| MeanKL               | 0.008605   |
| Entropy              | -1.9837    |
| Perplexity           | 0.13755    |
| AveragePolicyStd     | 0.17605    |
| AveragePolicyStd[0]  | 0.19511    |
| AveragePolicyStd[1]  | 0.18885    |
| AveragePolicyStd[2]  | 0.13995    |
| AveragePolicyStd[3]  | 0.16706    |
| AveragePolicyStd[4]  | 0.1464     |
| AveragePolicyStd[5]  | 0.2189     |
| AverageReturn        | 1872.4     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 356.98     |
| AverageEpisodeLength | 955.82     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.08     |
| TotalNEpisodes       | 26185      |
| TotalNSamples        | 9.0873e+06 |
| ExplainedVariance    | 0.015628   |
-------------------------------------
[2018-01-21 17:20:51.164017 UTC] Saving snapshot
[2018-01-21 17:20:51.164211 UTC] Starting iteration 1816
[2018-01-21 17:20:51.164380 UTC] Start collecting samples
[2018-01-21 17:20:53.860386 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:53.941983 UTC] Performing policy update
[2018-01-21 17:20:53.942551 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:54.017020 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:20:54.924989 UTC] Performing line search
[2018-01-21 17:20:55.042508 UTC] Updating baseline
[2018-01-21 17:20:56.129444 UTC] Computing logging information
-------------------------------------
| Iteration            | 1816       |
| ExpectedImprovement  | 0.017943   |
| ActualImprovement    | 0.017594   |
| ImprovementRatio     | 0.98052    |
| MeanKL               | 0.0078221  |
| Entropy              | -1.9806    |
| Perplexity           | 0.13799    |
| AveragePolicyStd     | 0.17613    |
| AveragePolicyStd[0]  | 0.19493    |
| AveragePolicyStd[1]  | 0.18906    |
| AveragePolicyStd[2]  | 0.14005    |
| AveragePolicyStd[3]  | 0.16692    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.2191     |
| AverageReturn        | 1874.2     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 340.32     |
| AverageEpisodeLength | 956.14     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 164.22     |
| TotalNEpisodes       | 26190      |
| TotalNSamples        | 9.0916e+06 |
| ExplainedVariance    | 0.18248    |
-------------------------------------
[2018-01-21 17:20:56.641156 UTC] Saving snapshot
[2018-01-21 17:20:56.641341 UTC] Starting iteration 1817
[2018-01-21 17:20:56.641472 UTC] Start collecting samples
[2018-01-21 17:20:59.226213 UTC] Computing input variables for policy optimization
[2018-01-21 17:20:59.300255 UTC] Performing policy update
[2018-01-21 17:20:59.300780 UTC] Computing gradient in Euclidean space
[2018-01-21 17:20:59.374043 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:00.249809 UTC] Performing line search
[2018-01-21 17:21:00.367030 UTC] Updating baseline
[2018-01-21 17:21:01.618530 UTC] Computing logging information
-------------------------------------
| Iteration            | 1817       |
| ExpectedImprovement  | 0.015567   |
| ActualImprovement    | 0.014897   |
| ImprovementRatio     | 0.95697    |
| MeanKL               | 0.0081338  |
| Entropy              | -1.9694    |
| Perplexity           | 0.13954    |
| AveragePolicyStd     | 0.1765     |
| AveragePolicyStd[0]  | 0.19518    |
| AveragePolicyStd[1]  | 0.18985    |
| AveragePolicyStd[2]  | 0.14023    |
| AveragePolicyStd[3]  | 0.16695    |
| AveragePolicyStd[4]  | 0.14669    |
| AveragePolicyStd[5]  | 0.2201     |
| AverageReturn        | 1868.2     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 345.01     |
| AverageEpisodeLength | 953.05     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26194      |
| TotalNSamples        | 9.0953e+06 |
| ExplainedVariance    | 0.20508    |
-------------------------------------
[2018-01-21 17:21:02.132676 UTC] Saving snapshot
[2018-01-21 17:21:02.132860 UTC] Starting iteration 1818
[2018-01-21 17:21:02.132997 UTC] Start collecting samples
[2018-01-21 17:21:04.680794 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:04.754166 UTC] Performing policy update
[2018-01-21 17:21:04.754722 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:04.830000 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:05.758787 UTC] Performing line search
[2018-01-21 17:21:05.891465 UTC] Updating baseline
[2018-01-21 17:21:07.182063 UTC] Computing logging information
-------------------------------------
| Iteration            | 1818       |
| ExpectedImprovement  | 0.017074   |
| ActualImprovement    | 0.016151   |
| ImprovementRatio     | 0.94593    |
| MeanKL               | 0.0082823  |
| Entropy              | -1.9731    |
| Perplexity           | 0.13903    |
| AveragePolicyStd     | 0.1764     |
| AveragePolicyStd[0]  | 0.19494    |
| AveragePolicyStd[1]  | 0.18947    |
| AveragePolicyStd[2]  | 0.14003    |
| AveragePolicyStd[3]  | 0.16706    |
| AveragePolicyStd[4]  | 0.14658    |
| AveragePolicyStd[5]  | 0.22033    |
| AverageReturn        | 1869.1     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 345.18     |
| AverageEpisodeLength | 953.05     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26201      |
| TotalNSamples        | 9.1023e+06 |
| ExplainedVariance    | -0.017554  |
-------------------------------------
[2018-01-21 17:21:07.711941 UTC] Saving snapshot
[2018-01-21 17:21:07.712121 UTC] Starting iteration 1819
[2018-01-21 17:21:07.712260 UTC] Start collecting samples
[2018-01-21 17:21:10.121459 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:10.194294 UTC] Performing policy update
[2018-01-21 17:21:10.194868 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:10.270095 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:11.152565 UTC] Performing line search
[2018-01-21 17:21:11.270685 UTC] Updating baseline
[2018-01-21 17:21:12.769921 UTC] Computing logging information
-------------------------------------
| Iteration            | 1819       |
| ExpectedImprovement  | 0.017269   |
| ActualImprovement    | 0.015993   |
| ImprovementRatio     | 0.92611    |
| MeanKL               | 0.0086393  |
| Entropy              | -1.9748    |
| Perplexity           | 0.13878    |
| AveragePolicyStd     | 0.17636    |
| AveragePolicyStd[0]  | 0.19509    |
| AveragePolicyStd[1]  | 0.18916    |
| AveragePolicyStd[2]  | 0.13982    |
| AveragePolicyStd[3]  | 0.16681    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.22059    |
| AverageReturn        | 1854.8     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 374.04     |
| AverageEpisodeLength | 945.59     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.14     |
| TotalNEpisodes       | 26206      |
| TotalNSamples        | 9.1065e+06 |
| ExplainedVariance    | 0.089433   |
-------------------------------------
[2018-01-21 17:21:13.295536 UTC] Saving snapshot
[2018-01-21 17:21:13.295735 UTC] Starting iteration 1820
[2018-01-21 17:21:13.295860 UTC] Start collecting samples
[2018-01-21 17:21:15.602415 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:15.674653 UTC] Performing policy update
[2018-01-21 17:21:15.675185 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:15.756985 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:16.696913 UTC] Performing line search
[2018-01-21 17:21:16.815600 UTC] Updating baseline
[2018-01-21 17:21:18.420144 UTC] Computing logging information
-------------------------------------
| Iteration            | 1820       |
| ExpectedImprovement  | 0.018696   |
| ActualImprovement    | 0.017541   |
| ImprovementRatio     | 0.9382     |
| MeanKL               | 0.0084662  |
| Entropy              | -1.9789    |
| Perplexity           | 0.13822    |
| AveragePolicyStd     | 0.17625    |
| AveragePolicyStd[0]  | 0.19518    |
| AveragePolicyStd[1]  | 0.18895    |
| AveragePolicyStd[2]  | 0.13989    |
| AveragePolicyStd[3]  | 0.16642    |
| AveragePolicyStd[4]  | 0.14647    |
| AveragePolicyStd[5]  | 0.2206     |
| AverageReturn        | 1846.6     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 382.02     |
| AverageEpisodeLength | 941.34     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.78     |
| TotalNEpisodes       | 26212      |
| TotalNSamples        | 9.1121e+06 |
| ExplainedVariance    | 0.11887    |
-------------------------------------
[2018-01-21 17:21:18.939650 UTC] Saving snapshot
[2018-01-21 17:21:18.945353 UTC] Starting iteration 1821
[2018-01-21 17:21:18.945545 UTC] Start collecting samples
[2018-01-21 17:21:21.462138 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:21.534201 UTC] Performing policy update
[2018-01-21 17:21:21.534732 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:21.611229 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:22.544635 UTC] Performing line search
[2018-01-21 17:21:22.668603 UTC] Updating baseline
[2018-01-21 17:21:23.890010 UTC] Computing logging information
-------------------------------------
| Iteration            | 1821       |
| ExpectedImprovement  | 0.019359   |
| ActualImprovement    | 0.018594   |
| ImprovementRatio     | 0.96051    |
| MeanKL               | 0.008319   |
| Entropy              | -1.983     |
| Perplexity           | 0.13765    |
| AveragePolicyStd     | 0.17611    |
| AveragePolicyStd[0]  | 0.1948     |
| AveragePolicyStd[1]  | 0.18889    |
| AveragePolicyStd[2]  | 0.14008    |
| AveragePolicyStd[3]  | 0.16588    |
| AveragePolicyStd[4]  | 0.14658    |
| AveragePolicyStd[5]  | 0.22044    |
| AverageReturn        | 1847.9     |
| MinReturn            | 117.95     |
| MaxReturn            | 2040.5     |
| StdReturn            | 382.42     |
| AverageEpisodeLength | 941.34     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.78     |
| TotalNEpisodes       | 26217      |
| TotalNSamples        | 9.1171e+06 |
| ExplainedVariance    | 0.11926    |
-------------------------------------
[2018-01-21 17:21:24.410969 UTC] Saving snapshot
[2018-01-21 17:21:24.411156 UTC] Starting iteration 1822
[2018-01-21 17:21:24.411277 UTC] Start collecting samples
[2018-01-21 17:21:26.918756 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:26.996859 UTC] Performing policy update
[2018-01-21 17:21:26.997372 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:27.074002 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:27.985793 UTC] Performing line search
[2018-01-21 17:21:28.103561 UTC] Updating baseline
[2018-01-21 17:21:29.342008 UTC] Computing logging information
-------------------------------------
| Iteration            | 1822       |
| ExpectedImprovement  | 0.018161   |
| ActualImprovement    | 0.016672   |
| ImprovementRatio     | 0.91802    |
| MeanKL               | 0.008545   |
| Entropy              | -1.986     |
| Perplexity           | 0.13724    |
| AveragePolicyStd     | 0.17603    |
| AveragePolicyStd[0]  | 0.19477    |
| AveragePolicyStd[1]  | 0.18832    |
| AveragePolicyStd[2]  | 0.14034    |
| AveragePolicyStd[3]  | 0.16537    |
| AveragePolicyStd[4]  | 0.1465     |
| AveragePolicyStd[5]  | 0.22088    |
| AverageReturn        | 1855.8     |
| MinReturn            | 268.42     |
| MaxReturn            | 2040.5     |
| StdReturn            | 348.99     |
| AverageEpisodeLength | 944.64     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.41     |
| TotalNEpisodes       | 26223      |
| TotalNSamples        | 9.1225e+06 |
| ExplainedVariance    | 0.13141    |
-------------------------------------
[2018-01-21 17:21:29.853627 UTC] Saving snapshot
[2018-01-21 17:21:29.853813 UTC] Starting iteration 1823
[2018-01-21 17:21:29.853949 UTC] Start collecting samples
[2018-01-21 17:21:32.183103 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:32.255560 UTC] Performing policy update
[2018-01-21 17:21:32.256135 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:32.330639 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:33.228242 UTC] Performing line search
[2018-01-21 17:21:33.351029 UTC] Updating baseline
[2018-01-21 17:21:34.979265 UTC] Computing logging information
-------------------------------------
| Iteration            | 1823       |
| ExpectedImprovement  | 0.019106   |
| ActualImprovement    | 0.018056   |
| ImprovementRatio     | 0.94504    |
| MeanKL               | 0.0084101  |
| Entropy              | -1.9923    |
| Perplexity           | 0.13638    |
| AveragePolicyStd     | 0.17584    |
| AveragePolicyStd[0]  | 0.19447    |
| AveragePolicyStd[1]  | 0.18828    |
| AveragePolicyStd[2]  | 0.14033    |
| AveragePolicyStd[3]  | 0.16519    |
| AveragePolicyStd[4]  | 0.1462     |
| AveragePolicyStd[5]  | 0.2206     |
| AverageReturn        | 1842.6     |
| MinReturn            | 268.42     |
| MaxReturn            | 2040.5     |
| StdReturn            | 376.97     |
| AverageEpisodeLength | 937.15     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.04     |
| TotalNEpisodes       | 26229      |
| TotalNSamples        | 9.1278e+06 |
| ExplainedVariance    | 0.086676   |
-------------------------------------
[2018-01-21 17:21:35.552824 UTC] Saving snapshot
[2018-01-21 17:21:35.553007 UTC] Starting iteration 1824
[2018-01-21 17:21:35.553151 UTC] Start collecting samples
[2018-01-21 17:21:37.954987 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:38.030576 UTC] Performing policy update
[2018-01-21 17:21:38.031082 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:38.105527 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:39.033562 UTC] Performing line search
[2018-01-21 17:21:39.151544 UTC] Updating baseline
[2018-01-21 17:21:40.555809 UTC] Computing logging information
-------------------------------------
| Iteration            | 1824       |
| ExpectedImprovement  | 0.019544   |
| ActualImprovement    | 0.018553   |
| ImprovementRatio     | 0.94933    |
| MeanKL               | 0.0082833  |
| Entropy              | -1.9961    |
| Perplexity           | 0.13587    |
| AveragePolicyStd     | 0.17572    |
| AveragePolicyStd[0]  | 0.1944     |
| AveragePolicyStd[1]  | 0.18808    |
| AveragePolicyStd[2]  | 0.14054    |
| AveragePolicyStd[3]  | 0.16478    |
| AveragePolicyStd[4]  | 0.1462     |
| AveragePolicyStd[5]  | 0.22029    |
| AverageReturn        | 1832.3     |
| MinReturn            | 268.42     |
| MaxReturn            | 2040.5     |
| StdReturn            | 392.33     |
| AverageEpisodeLength | 931.3      |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.27     |
| TotalNEpisodes       | 26233      |
| TotalNSamples        | 9.1312e+06 |
| ExplainedVariance    | 0.16014    |
-------------------------------------
[2018-01-21 17:21:41.103159 UTC] Saving snapshot
[2018-01-21 17:21:41.103350 UTC] Starting iteration 1825
[2018-01-21 17:21:41.103492 UTC] Start collecting samples
[2018-01-21 17:21:43.496253 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:43.578502 UTC] Performing policy update
[2018-01-21 17:21:43.579029 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:43.654249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:44.561899 UTC] Performing line search
[2018-01-21 17:21:44.677958 UTC] Updating baseline
[2018-01-21 17:21:46.106675 UTC] Computing logging information
-------------------------------------
| Iteration            | 1825       |
| ExpectedImprovement  | 0.018405   |
| ActualImprovement    | 0.017558   |
| ImprovementRatio     | 0.95398    |
| MeanKL               | 0.0082602  |
| Entropy              | -1.997     |
| Perplexity           | 0.13575    |
| AveragePolicyStd     | 0.17567    |
| AveragePolicyStd[0]  | 0.19432    |
| AveragePolicyStd[1]  | 0.18805    |
| AveragePolicyStd[2]  | 0.14065    |
| AveragePolicyStd[3]  | 0.16478    |
| AveragePolicyStd[4]  | 0.14629    |
| AveragePolicyStd[5]  | 0.21991    |
| AverageReturn        | 1833.8     |
| MinReturn            | 268.42     |
| MaxReturn            | 2040.5     |
| StdReturn            | 392.87     |
| AverageEpisodeLength | 931.3      |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.27     |
| TotalNEpisodes       | 26239      |
| TotalNSamples        | 9.1372e+06 |
| ExplainedVariance    | -0.0094756 |
-------------------------------------
[2018-01-21 17:21:46.622558 UTC] Saving snapshot
[2018-01-21 17:21:46.622759 UTC] Starting iteration 1826
[2018-01-21 17:21:46.622900 UTC] Start collecting samples
[2018-01-21 17:21:49.123296 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:49.196054 UTC] Performing policy update
[2018-01-21 17:21:49.196539 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:49.271228 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:50.167393 UTC] Performing line search
[2018-01-21 17:21:50.284910 UTC] Updating baseline
[2018-01-21 17:21:51.648965 UTC] Computing logging information
-------------------------------------
| Iteration            | 1826       |
| ExpectedImprovement  | 0.016676   |
| ActualImprovement    | 0.015947   |
| ImprovementRatio     | 0.95627    |
| MeanKL               | 0.0086516  |
| Entropy              | -1.9997    |
| Perplexity           | 0.13538    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19432    |
| AveragePolicyStd[1]  | 0.18809    |
| AveragePolicyStd[2]  | 0.14007    |
| AveragePolicyStd[3]  | 0.16462    |
| AveragePolicyStd[4]  | 0.14631    |
| AveragePolicyStd[5]  | 0.22036    |
| AverageReturn        | 1820.8     |
| MinReturn            | 268.42     |
| MaxReturn            | 2040.5     |
| StdReturn            | 414.14     |
| AverageEpisodeLength | 923.79     |
| MinEpisodeLength     | 176        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.68     |
| TotalNEpisodes       | 26246      |
| TotalNSamples        | 9.1434e+06 |
| ExplainedVariance    | 0.17555    |
-------------------------------------
[2018-01-21 17:21:52.170898 UTC] Saving snapshot
[2018-01-21 17:21:52.171100 UTC] Starting iteration 1827
[2018-01-21 17:21:52.171252 UTC] Start collecting samples
[2018-01-21 17:21:54.577885 UTC] Computing input variables for policy optimization
[2018-01-21 17:21:54.672240 UTC] Performing policy update
[2018-01-21 17:21:54.672856 UTC] Computing gradient in Euclidean space
[2018-01-21 17:21:54.758613 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:21:55.674669 UTC] Performing line search
[2018-01-21 17:21:55.794875 UTC] Updating baseline
[2018-01-21 17:21:57.240469 UTC] Computing logging information
-------------------------------------
| Iteration            | 1827       |
| ExpectedImprovement  | 0.017026   |
| ActualImprovement    | 0.016142   |
| ImprovementRatio     | 0.94809    |
| MeanKL               | 0.0095526  |
| Entropy              | -2.0043    |
| Perplexity           | 0.13475    |
| AveragePolicyStd     | 0.1755     |
| AveragePolicyStd[0]  | 0.19407    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.13989    |
| AveragePolicyStd[3]  | 0.16411    |
| AveragePolicyStd[4]  | 0.14646    |
| AveragePolicyStd[5]  | 0.22033    |
| AverageReturn        | 1810.4     |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 431.59     |
| AverageEpisodeLength | 918.24     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.36     |
| TotalNEpisodes       | 26252      |
| TotalNSamples        | 9.1482e+06 |
| ExplainedVariance    | 0.19115    |
-------------------------------------
[2018-01-21 17:21:57.755960 UTC] Saving snapshot
[2018-01-21 17:21:57.756152 UTC] Starting iteration 1828
[2018-01-21 17:21:57.756288 UTC] Start collecting samples
[2018-01-21 17:22:00.064984 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:00.140966 UTC] Performing policy update
[2018-01-21 17:22:00.141501 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:00.217495 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:01.122164 UTC] Performing line search
[2018-01-21 17:22:01.243037 UTC] Updating baseline
[2018-01-21 17:22:03.720016 UTC] Computing logging information
-------------------------------------
| Iteration            | 1828       |
| ExpectedImprovement  | 0.017324   |
| ActualImprovement    | 0.0165     |
| ImprovementRatio     | 0.95245    |
| MeanKL               | 0.0090806  |
| Entropy              | -2.0006    |
| Perplexity           | 0.13525    |
| AveragePolicyStd     | 0.17562    |
| AveragePolicyStd[0]  | 0.19425    |
| AveragePolicyStd[1]  | 0.18845    |
| AveragePolicyStd[2]  | 0.13988    |
| AveragePolicyStd[3]  | 0.16401    |
| AveragePolicyStd[4]  | 0.14652    |
| AveragePolicyStd[5]  | 0.22061    |
| AverageReturn        | 1828.2     |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 403.34     |
| AverageEpisodeLength | 926.48     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 192.55     |
| TotalNEpisodes       | 26255      |
| TotalNSamples        | 9.1512e+06 |
| ExplainedVariance    | -0.028554  |
-------------------------------------
[2018-01-21 17:22:04.244024 UTC] Saving snapshot
[2018-01-21 17:22:04.244202 UTC] Starting iteration 1829
[2018-01-21 17:22:04.244304 UTC] Start collecting samples
[2018-01-21 17:22:06.579302 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:06.651205 UTC] Performing policy update
[2018-01-21 17:22:06.651701 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:06.725739 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:07.608370 UTC] Performing line search
[2018-01-21 17:22:07.725103 UTC] Updating baseline
[2018-01-21 17:22:08.811643 UTC] Computing logging information
-------------------------------------
| Iteration            | 1829       |
| ExpectedImprovement  | 0.016995   |
| ActualImprovement    | 0.016189   |
| ImprovementRatio     | 0.95255    |
| MeanKL               | 0.0081095  |
| Entropy              | -2.0007    |
| Perplexity           | 0.13524    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19417    |
| AveragePolicyStd[1]  | 0.18851    |
| AveragePolicyStd[2]  | 0.14001    |
| AveragePolicyStd[3]  | 0.1643     |
| AveragePolicyStd[4]  | 0.14611    |
| AveragePolicyStd[5]  | 0.22065    |
| AverageReturn        | 1834.9     |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 393.45     |
| AverageEpisodeLength | 929.08     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.3      |
| TotalNEpisodes       | 26261      |
| TotalNSamples        | 9.1569e+06 |
| ExplainedVariance    | 0.11317    |
-------------------------------------
[2018-01-21 17:22:09.334025 UTC] Saving snapshot
[2018-01-21 17:22:09.334240 UTC] Starting iteration 1830
[2018-01-21 17:22:09.334382 UTC] Start collecting samples
[2018-01-21 17:22:11.651432 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:11.724277 UTC] Performing policy update
[2018-01-21 17:22:11.724798 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:11.799210 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:12.690396 UTC] Performing line search
[2018-01-21 17:22:12.808434 UTC] Updating baseline
[2018-01-21 17:22:14.164035 UTC] Computing logging information
-------------------------------------
| Iteration            | 1830       |
| ExpectedImprovement  | 0.018889   |
| ActualImprovement    | 0.01751    |
| ImprovementRatio     | 0.92701    |
| MeanKL               | 0.0078987  |
| Entropy              | -2.0005    |
| Perplexity           | 0.13526    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19413    |
| AveragePolicyStd[1]  | 0.18857    |
| AveragePolicyStd[2]  | 0.1401     |
| AveragePolicyStd[3]  | 0.16422    |
| AveragePolicyStd[4]  | 0.14608    |
| AveragePolicyStd[5]  | 0.22067    |
| AverageReturn        | 1829.6     |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 396.15     |
| AverageEpisodeLength | 926.26     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.33     |
| TotalNEpisodes       | 26266      |
| TotalNSamples        | 9.1616e+06 |
| ExplainedVariance    | 0.13086    |
-------------------------------------
[2018-01-21 17:22:14.680764 UTC] Saving snapshot
[2018-01-21 17:22:14.686362 UTC] Starting iteration 1831
[2018-01-21 17:22:14.686568 UTC] Start collecting samples
[2018-01-21 17:22:17.162149 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:17.233416 UTC] Performing policy update
[2018-01-21 17:22:17.233930 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:17.308611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:18.195321 UTC] Performing line search
[2018-01-21 17:22:18.311236 UTC] Updating baseline
[2018-01-21 17:22:19.663658 UTC] Computing logging information
-------------------------------------
| Iteration            | 1831       |
| ExpectedImprovement  | 0.018475   |
| ActualImprovement    | 0.016989   |
| ImprovementRatio     | 0.91954    |
| MeanKL               | 0.0086174  |
| Entropy              | -2.0032    |
| Perplexity           | 0.1349     |
| AveragePolicyStd     | 0.17555    |
| AveragePolicyStd[0]  | 0.19389    |
| AveragePolicyStd[1]  | 0.18791    |
| AveragePolicyStd[2]  | 0.13991    |
| AveragePolicyStd[3]  | 0.1643     |
| AveragePolicyStd[4]  | 0.14624    |
| AveragePolicyStd[5]  | 0.22107    |
| AverageReturn        | 1831.7     |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 396.42     |
| AverageEpisodeLength | 927.3      |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 188.45     |
| TotalNEpisodes       | 26271      |
| TotalNSamples        | 9.1666e+06 |
| ExplainedVariance    | 0.12002    |
-------------------------------------
[2018-01-21 17:22:20.179883 UTC] Saving snapshot
[2018-01-21 17:22:20.180085 UTC] Starting iteration 1832
[2018-01-21 17:22:20.180230 UTC] Start collecting samples
[2018-01-21 17:22:22.877461 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:22.952259 UTC] Performing policy update
[2018-01-21 17:22:22.952764 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:23.027651 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:23.985441 UTC] Performing line search
[2018-01-21 17:22:24.104980 UTC] Updating baseline
[2018-01-21 17:22:26.678653 UTC] Computing logging information
-------------------------------------
| Iteration            | 1832       |
| ExpectedImprovement  | 0.016515   |
| ActualImprovement    | 0.015917   |
| ImprovementRatio     | 0.96378    |
| MeanKL               | 0.0085302  |
| Entropy              | -2.0045    |
| Perplexity           | 0.13473    |
| AveragePolicyStd     | 0.17552    |
| AveragePolicyStd[0]  | 0.19403    |
| AveragePolicyStd[1]  | 0.18789    |
| AveragePolicyStd[2]  | 0.13996    |
| AveragePolicyStd[3]  | 0.16403    |
| AveragePolicyStd[4]  | 0.14622    |
| AveragePolicyStd[5]  | 0.22098    |
| AverageReturn        | 1837       |
| MinReturn            | 211.01     |
| MaxReturn            | 2040.5     |
| StdReturn            | 383.15     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.3      |
| TotalNEpisodes       | 26274      |
| TotalNSamples        | 9.1693e+06 |
| ExplainedVariance    | 0.038283   |
-------------------------------------
[2018-01-21 17:22:27.196462 UTC] Saving snapshot
[2018-01-21 17:22:27.196678 UTC] Starting iteration 1833
[2018-01-21 17:22:27.196803 UTC] Start collecting samples
[2018-01-21 17:22:29.749535 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:29.833370 UTC] Performing policy update
[2018-01-21 17:22:29.833924 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:29.915556 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:30.850858 UTC] Performing line search
[2018-01-21 17:22:30.979383 UTC] Updating baseline
[2018-01-21 17:22:32.444323 UTC] Computing logging information
-------------------------------------
| Iteration            | 1833       |
| ExpectedImprovement  | 0.019255   |
| ActualImprovement    | 0.018218   |
| ImprovementRatio     | 0.94617    |
| MeanKL               | 0.0084739  |
| Entropy              | -2.0016    |
| Perplexity           | 0.13512    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19411    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.13974    |
| AveragePolicyStd[3]  | 0.1637     |
| AveragePolicyStd[4]  | 0.14653    |
| AveragePolicyStd[5]  | 0.22157    |
| AverageReturn        | 1836.5     |
| MinReturn            | 211.01     |
| MaxReturn            | 2039.5     |
| StdReturn            | 382.98     |
| AverageEpisodeLength | 930.11     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.3      |
| TotalNEpisodes       | 26281      |
| TotalNSamples        | 9.1763e+06 |
| ExplainedVariance    | 0.0064524  |
-------------------------------------
[2018-01-21 17:22:32.957864 UTC] Saving snapshot
[2018-01-21 17:22:32.958049 UTC] Starting iteration 1834
[2018-01-21 17:22:32.958198 UTC] Start collecting samples
[2018-01-21 17:22:35.539688 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:35.640202 UTC] Performing policy update
[2018-01-21 17:22:35.640746 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:35.716149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:36.685088 UTC] Performing line search
[2018-01-21 17:22:36.814474 UTC] Updating baseline
[2018-01-21 17:22:38.398046 UTC] Computing logging information
--------------------------------------
| Iteration            | 1834        |
| ExpectedImprovement  | 0.019242    |
| ActualImprovement    | 0.017531    |
| ImprovementRatio     | 0.91106     |
| MeanKL               | 0.0090327   |
| Entropy              | -2          |
| Perplexity           | 0.13534     |
| AveragePolicyStd     | 0.17568     |
| AveragePolicyStd[0]  | 0.19461     |
| AveragePolicyStd[1]  | 0.18817     |
| AveragePolicyStd[2]  | 0.13981     |
| AveragePolicyStd[3]  | 0.16361     |
| AveragePolicyStd[4]  | 0.14647     |
| AveragePolicyStd[5]  | 0.22141     |
| AverageReturn        | 1846.1      |
| MinReturn            | 211.01      |
| MaxReturn            | 2039.5      |
| StdReturn            | 377.64      |
| AverageEpisodeLength | 933.84      |
| MinEpisodeLength     | 143         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 179.86      |
| TotalNEpisodes       | 26287       |
| TotalNSamples        | 9.1823e+06  |
| ExplainedVariance    | -5.4867e-05 |
--------------------------------------
[2018-01-21 17:22:38.926476 UTC] Saving snapshot
[2018-01-21 17:22:38.926655 UTC] Starting iteration 1835
[2018-01-21 17:22:38.926758 UTC] Start collecting samples
[2018-01-21 17:22:41.298690 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:41.370389 UTC] Performing policy update
[2018-01-21 17:22:41.370921 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:41.444826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:42.325242 UTC] Performing line search
[2018-01-21 17:22:42.442418 UTC] Updating baseline
[2018-01-21 17:22:43.904908 UTC] Computing logging information
-------------------------------------
| Iteration            | 1835       |
| ExpectedImprovement  | 0.015437   |
| ActualImprovement    | 0.014869   |
| ImprovementRatio     | 0.96321    |
| MeanKL               | 0.0086062  |
| Entropy              | -2.0022    |
| Perplexity           | 0.13504    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19467    |
| AveragePolicyStd[1]  | 0.18823    |
| AveragePolicyStd[2]  | 0.13971    |
| AveragePolicyStd[3]  | 0.16337    |
| AveragePolicyStd[4]  | 0.14635    |
| AveragePolicyStd[5]  | 0.22143    |
| AverageReturn        | 1852.4     |
| MinReturn            | 211.01     |
| MaxReturn            | 2039.5     |
| StdReturn            | 373.34     |
| AverageEpisodeLength | 937.26     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.83     |
| TotalNEpisodes       | 26290      |
| TotalNSamples        | 9.1853e+06 |
| ExplainedVariance    | 0.00086699 |
-------------------------------------
[2018-01-21 17:22:44.425267 UTC] Saving snapshot
[2018-01-21 17:22:44.425468 UTC] Starting iteration 1836
[2018-01-21 17:22:44.425624 UTC] Start collecting samples
[2018-01-21 17:22:46.834674 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:46.910047 UTC] Performing policy update
[2018-01-21 17:22:46.910598 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:46.988915 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:47.884558 UTC] Performing line search
[2018-01-21 17:22:48.011898 UTC] Updating baseline
[2018-01-21 17:22:49.569730 UTC] Computing logging information
-------------------------------------
| Iteration            | 1836       |
| ExpectedImprovement  | 0.017516   |
| ActualImprovement    | 0.016085   |
| ImprovementRatio     | 0.91826    |
| MeanKL               | 0.0087053  |
| Entropy              | -2.0013    |
| Perplexity           | 0.13516    |
| AveragePolicyStd     | 0.17563    |
| AveragePolicyStd[0]  | 0.19447    |
| AveragePolicyStd[1]  | 0.18805    |
| AveragePolicyStd[2]  | 0.14003    |
| AveragePolicyStd[3]  | 0.16328    |
| AveragePolicyStd[4]  | 0.1465     |
| AveragePolicyStd[5]  | 0.22148    |
| AverageReturn        | 1858.6     |
| MinReturn            | 211.01     |
| MaxReturn            | 2039.5     |
| StdReturn            | 369.3      |
| AverageEpisodeLength | 940.35     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.2      |
| TotalNEpisodes       | 26297      |
| TotalNSamples        | 9.1923e+06 |
| ExplainedVariance    | 0.00049241 |
-------------------------------------
[2018-01-21 17:22:50.089513 UTC] Saving snapshot
[2018-01-21 17:22:50.089699 UTC] Starting iteration 1837
[2018-01-21 17:22:50.089836 UTC] Start collecting samples
[2018-01-21 17:22:52.528626 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:52.610120 UTC] Performing policy update
[2018-01-21 17:22:52.610676 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:52.686225 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:53.608109 UTC] Performing line search
[2018-01-21 17:22:53.728915 UTC] Updating baseline
[2018-01-21 17:22:55.146749 UTC] Computing logging information
-------------------------------------
| Iteration            | 1837       |
| ExpectedImprovement  | 0.019667   |
| ActualImprovement    | 0.018004   |
| ImprovementRatio     | 0.91545    |
| MeanKL               | 0.0088671  |
| Entropy              | -1.9999    |
| Perplexity           | 0.13535    |
| AveragePolicyStd     | 0.17566    |
| AveragePolicyStd[0]  | 0.19448    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.14018    |
| AveragePolicyStd[3]  | 0.1629     |
| AveragePolicyStd[4]  | 0.14688    |
| AveragePolicyStd[5]  | 0.2214     |
| AverageReturn        | 1860.4     |
| MinReturn            | 211.01     |
| MaxReturn            | 2039.5     |
| StdReturn            | 369.83     |
| AverageEpisodeLength | 940.35     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.2      |
| TotalNEpisodes       | 26303      |
| TotalNSamples        | 9.1983e+06 |
| ExplainedVariance    | 0.00067456 |
-------------------------------------
[2018-01-21 17:22:55.662702 UTC] Saving snapshot
[2018-01-21 17:22:55.662897 UTC] Starting iteration 1838
[2018-01-21 17:22:55.663024 UTC] Start collecting samples
[2018-01-21 17:22:58.121320 UTC] Computing input variables for policy optimization
[2018-01-21 17:22:58.193455 UTC] Performing policy update
[2018-01-21 17:22:58.193984 UTC] Computing gradient in Euclidean space
[2018-01-21 17:22:58.272702 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:22:59.205353 UTC] Performing line search
[2018-01-21 17:22:59.325652 UTC] Updating baseline
[2018-01-21 17:23:02.127066 UTC] Computing logging information
-------------------------------------
| Iteration            | 1838       |
| ExpectedImprovement  | 0.017835   |
| ActualImprovement    | 0.016923   |
| ImprovementRatio     | 0.94888    |
| MeanKL               | 0.0085254  |
| Entropy              | -2.0019    |
| Perplexity           | 0.13508    |
| AveragePolicyStd     | 0.1756     |
| AveragePolicyStd[0]  | 0.19427    |
| AveragePolicyStd[1]  | 0.18851    |
| AveragePolicyStd[2]  | 0.14028    |
| AveragePolicyStd[3]  | 0.16261    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.22121    |
| AverageReturn        | 1876.7     |
| MinReturn            | 211.01     |
| MaxReturn            | 2044.4     |
| StdReturn            | 340.89     |
| AverageEpisodeLength | 947.81     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.22     |
| TotalNEpisodes       | 26306      |
| TotalNSamples        | 9.2013e+06 |
| ExplainedVariance    | 0.0044964  |
-------------------------------------
[2018-01-21 17:23:02.647596 UTC] Saving snapshot
[2018-01-21 17:23:02.647789 UTC] Starting iteration 1839
[2018-01-21 17:23:02.647938 UTC] Start collecting samples
[2018-01-21 17:23:05.272185 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:05.354259 UTC] Performing policy update
[2018-01-21 17:23:05.354796 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:05.432507 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:06.438633 UTC] Performing line search
[2018-01-21 17:23:06.642986 UTC] Updating baseline
[2018-01-21 17:23:07.875732 UTC] Computing logging information
-------------------------------------
| Iteration            | 1839       |
| ExpectedImprovement  | 0.01802    |
| ActualImprovement    | 0.016922   |
| ImprovementRatio     | 0.93908    |
| MeanKL               | 0.008525   |
| Entropy              | -2.0006    |
| Perplexity           | 0.13525    |
| AveragePolicyStd     | 0.17562    |
| AveragePolicyStd[0]  | 0.19414    |
| AveragePolicyStd[1]  | 0.18841    |
| AveragePolicyStd[2]  | 0.14044    |
| AveragePolicyStd[3]  | 0.16253    |
| AveragePolicyStd[4]  | 0.147      |
| AveragePolicyStd[5]  | 0.22119    |
| AverageReturn        | 1885.2     |
| MinReturn            | 211.01     |
| MaxReturn            | 2044.4     |
| StdReturn            | 331.23     |
| AverageEpisodeLength | 952.06     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.9      |
| TotalNEpisodes       | 26312      |
| TotalNSamples        | 9.2073e+06 |
| ExplainedVariance    | 0.0016157  |
-------------------------------------
[2018-01-21 17:23:08.421636 UTC] Saving snapshot
[2018-01-21 17:23:08.421823 UTC] Starting iteration 1840
[2018-01-21 17:23:08.421944 UTC] Start collecting samples
[2018-01-21 17:23:10.952213 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:11.028483 UTC] Performing policy update
[2018-01-21 17:23:11.029050 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:11.104989 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:12.103286 UTC] Performing line search
[2018-01-21 17:23:12.223593 UTC] Updating baseline
[2018-01-21 17:23:13.710304 UTC] Computing logging information
-------------------------------------
| Iteration            | 1840       |
| ExpectedImprovement  | 0.016437   |
| ActualImprovement    | 0.015516   |
| ImprovementRatio     | 0.94397    |
| MeanKL               | 0.0092346  |
| Entropy              | -2.0046    |
| Perplexity           | 0.13472    |
| AveragePolicyStd     | 0.17552    |
| AveragePolicyStd[0]  | 0.19366    |
| AveragePolicyStd[1]  | 0.18864    |
| AveragePolicyStd[2]  | 0.14035    |
| AveragePolicyStd[3]  | 0.16226    |
| AveragePolicyStd[4]  | 0.14679    |
| AveragePolicyStd[5]  | 0.22142    |
| AverageReturn        | 1884.9     |
| MinReturn            | 211.01     |
| MaxReturn            | 2044.4     |
| StdReturn            | 331.2      |
| AverageEpisodeLength | 952.06     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.9      |
| TotalNEpisodes       | 26316      |
| TotalNSamples        | 9.2113e+06 |
| ExplainedVariance    | 0.0041985  |
-------------------------------------
[2018-01-21 17:23:14.239896 UTC] Saving snapshot
[2018-01-21 17:23:14.245563 UTC] Starting iteration 1841
[2018-01-21 17:23:14.245724 UTC] Start collecting samples
[2018-01-21 17:23:16.752051 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:16.824100 UTC] Performing policy update
[2018-01-21 17:23:16.824628 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:16.898749 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:17.839363 UTC] Performing line search
[2018-01-21 17:23:17.957400 UTC] Updating baseline
[2018-01-21 17:23:19.456875 UTC] Computing logging information
-------------------------------------
| Iteration            | 1841       |
| ExpectedImprovement  | 0.018798   |
| ActualImprovement    | 0.017326   |
| ImprovementRatio     | 0.9217     |
| MeanKL               | 0.0081742  |
| Entropy              | -2.0013    |
| Perplexity           | 0.13516    |
| AveragePolicyStd     | 0.17561    |
| AveragePolicyStd[0]  | 0.1935     |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.14042    |
| AveragePolicyStd[3]  | 0.16249    |
| AveragePolicyStd[4]  | 0.14684    |
| AveragePolicyStd[5]  | 0.22154    |
| AverageReturn        | 1896.7     |
| MinReturn            | 211.01     |
| MaxReturn            | 2044.4     |
| StdReturn            | 321.89     |
| AverageEpisodeLength | 957.75     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.36     |
| TotalNEpisodes       | 26322      |
| TotalNSamples        | 9.2173e+06 |
| ExplainedVariance    | 0.0027478  |
-------------------------------------
[2018-01-21 17:23:19.988397 UTC] Saving snapshot
[2018-01-21 17:23:19.988586 UTC] Starting iteration 1842
[2018-01-21 17:23:19.988736 UTC] Start collecting samples
[2018-01-21 17:23:22.872337 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:22.949585 UTC] Performing policy update
[2018-01-21 17:23:22.950126 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:23.039789 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:23.936337 UTC] Performing line search
[2018-01-21 17:23:24.056901 UTC] Updating baseline
[2018-01-21 17:23:25.872945 UTC] Computing logging information
-------------------------------------
| Iteration            | 1842       |
| ExpectedImprovement  | 0.01874    |
| ActualImprovement    | 0.017685   |
| ImprovementRatio     | 0.9437     |
| MeanKL               | 0.0083836  |
| Entropy              | -2.0077    |
| Perplexity           | 0.1343     |
| AveragePolicyStd     | 0.17543    |
| AveragePolicyStd[0]  | 0.19309    |
| AveragePolicyStd[1]  | 0.18872    |
| AveragePolicyStd[2]  | 0.14017    |
| AveragePolicyStd[3]  | 0.16232    |
| AveragePolicyStd[4]  | 0.14672    |
| AveragePolicyStd[5]  | 0.22158    |
| AverageReturn        | 1911.4     |
| MinReturn            | 211.01     |
| MaxReturn            | 2044.4     |
| StdReturn            | 286.34     |
| AverageEpisodeLength | 965.24     |
| MinEpisodeLength     | 143        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 137.09     |
| TotalNEpisodes       | 26328      |
| TotalNSamples        | 9.2233e+06 |
| ExplainedVariance    | 0.036949   |
-------------------------------------
[2018-01-21 17:23:26.405300 UTC] Saving snapshot
[2018-01-21 17:23:26.405502 UTC] Starting iteration 1843
[2018-01-21 17:23:26.405621 UTC] Start collecting samples
[2018-01-21 17:23:29.234593 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:29.307352 UTC] Performing policy update
[2018-01-21 17:23:29.307857 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:29.383979 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:30.338137 UTC] Performing line search
[2018-01-21 17:23:30.455129 UTC] Updating baseline
[2018-01-21 17:23:32.500186 UTC] Computing logging information
-------------------------------------
| Iteration            | 1843       |
| ExpectedImprovement  | 0.018375   |
| ActualImprovement    | 0.017627   |
| ImprovementRatio     | 0.95929    |
| MeanKL               | 0.008649   |
| Entropy              | -2.007     |
| Perplexity           | 0.13439    |
| AveragePolicyStd     | 0.17547    |
| AveragePolicyStd[0]  | 0.19313    |
| AveragePolicyStd[1]  | 0.18831    |
| AveragePolicyStd[2]  | 0.14022    |
| AveragePolicyStd[3]  | 0.16218    |
| AveragePolicyStd[4]  | 0.14679    |
| AveragePolicyStd[5]  | 0.22218    |
| AverageReturn        | 1903.2     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 320.5      |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.11     |
| TotalNEpisodes       | 26333      |
| TotalNSamples        | 9.2274e+06 |
| ExplainedVariance    | 0.10117    |
-------------------------------------
[2018-01-21 17:23:33.068940 UTC] Saving snapshot
[2018-01-21 17:23:33.069127 UTC] Starting iteration 1844
[2018-01-21 17:23:33.069272 UTC] Start collecting samples
[2018-01-21 17:23:35.497000 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:35.569053 UTC] Performing policy update
[2018-01-21 17:23:35.569574 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:35.645850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:36.606451 UTC] Performing line search
[2018-01-21 17:23:36.731279 UTC] Updating baseline
[2018-01-21 17:23:37.962730 UTC] Computing logging information
-------------------------------------
| Iteration            | 1844       |
| ExpectedImprovement  | 0.016184   |
| ActualImprovement    | 0.015066   |
| ImprovementRatio     | 0.93093    |
| MeanKL               | 0.0086985  |
| Entropy              | -2.0034    |
| Perplexity           | 0.13488    |
| AveragePolicyStd     | 0.17557    |
| AveragePolicyStd[0]  | 0.19303    |
| AveragePolicyStd[1]  | 0.18856    |
| AveragePolicyStd[2]  | 0.1404     |
| AveragePolicyStd[3]  | 0.16207    |
| AveragePolicyStd[4]  | 0.14699    |
| AveragePolicyStd[5]  | 0.22237    |
| AverageReturn        | 1903.4     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 320.57     |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.11     |
| TotalNEpisodes       | 26337      |
| TotalNSamples        | 9.2314e+06 |
| ExplainedVariance    | 3.7578e-06 |
-------------------------------------
[2018-01-21 17:23:38.484352 UTC] Saving snapshot
[2018-01-21 17:23:38.484540 UTC] Starting iteration 1845
[2018-01-21 17:23:38.484669 UTC] Start collecting samples
[2018-01-21 17:23:41.297327 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:41.373491 UTC] Performing policy update
[2018-01-21 17:23:41.374044 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:41.450631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:42.351992 UTC] Performing line search
[2018-01-21 17:23:42.469081 UTC] Updating baseline
[2018-01-21 17:23:43.926450 UTC] Computing logging information
-------------------------------------
| Iteration            | 1845       |
| ExpectedImprovement  | 0.016879   |
| ActualImprovement    | 0.016386   |
| ImprovementRatio     | 0.97083    |
| MeanKL               | 0.0080954  |
| Entropy              | -1.9982    |
| Perplexity           | 0.13557    |
| AveragePolicyStd     | 0.17572    |
| AveragePolicyStd[0]  | 0.19332    |
| AveragePolicyStd[1]  | 0.18832    |
| AveragePolicyStd[2]  | 0.14081    |
| AveragePolicyStd[3]  | 0.16206    |
| AveragePolicyStd[4]  | 0.14702    |
| AveragePolicyStd[5]  | 0.22277    |
| AverageReturn        | 1901.8     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 320.11     |
| AverageEpisodeLength | 961.8      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.11     |
| TotalNEpisodes       | 26342      |
| TotalNSamples        | 9.2364e+06 |
| ExplainedVariance    | 0.0017007  |
-------------------------------------
[2018-01-21 17:23:44.557006 UTC] Saving snapshot
[2018-01-21 17:23:44.557197 UTC] Starting iteration 1846
[2018-01-21 17:23:44.557334 UTC] Start collecting samples
[2018-01-21 17:23:47.367878 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:47.444260 UTC] Performing policy update
[2018-01-21 17:23:47.444760 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:47.528356 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:48.454712 UTC] Performing line search
[2018-01-21 17:23:48.629931 UTC] Updating baseline
[2018-01-21 17:23:50.201012 UTC] Computing logging information
-------------------------------------
| Iteration            | 1846       |
| ExpectedImprovement  | 0.018433   |
| ActualImprovement    | 0.017077   |
| ImprovementRatio     | 0.92644    |
| MeanKL               | 0.0088623  |
| Entropy              | -1.9986    |
| Perplexity           | 0.13552    |
| AveragePolicyStd     | 0.17572    |
| AveragePolicyStd[0]  | 0.19322    |
| AveragePolicyStd[1]  | 0.18845    |
| AveragePolicyStd[2]  | 0.1407     |
| AveragePolicyStd[3]  | 0.16237    |
| AveragePolicyStd[4]  | 0.14676    |
| AveragePolicyStd[5]  | 0.22282    |
| AverageReturn        | 1913.6     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 282.34     |
| AverageEpisodeLength | 968.09     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.3      |
| TotalNEpisodes       | 26349      |
| TotalNSamples        | 9.2424e+06 |
| ExplainedVariance    | 0.15713    |
-------------------------------------
[2018-01-21 17:23:50.737915 UTC] Saving snapshot
[2018-01-21 17:23:50.738108 UTC] Starting iteration 1847
[2018-01-21 17:23:50.738233 UTC] Start collecting samples
[2018-01-21 17:23:53.337634 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:53.410145 UTC] Performing policy update
[2018-01-21 17:23:53.410691 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:53.485686 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:23:54.554718 UTC] Performing line search
[2018-01-21 17:23:54.713038 UTC] Updating baseline
[2018-01-21 17:23:56.068429 UTC] Computing logging information
-------------------------------------
| Iteration            | 1847       |
| ExpectedImprovement  | 0.016636   |
| ActualImprovement    | 0.015426   |
| ImprovementRatio     | 0.92727    |
| MeanKL               | 0.008426   |
| Entropy              | -2.004     |
| Perplexity           | 0.13479    |
| AveragePolicyStd     | 0.17557    |
| AveragePolicyStd[0]  | 0.19333    |
| AveragePolicyStd[1]  | 0.18816    |
| AveragePolicyStd[2]  | 0.14041    |
| AveragePolicyStd[3]  | 0.16227    |
| AveragePolicyStd[4]  | 0.14667    |
| AveragePolicyStd[5]  | 0.22256    |
| AverageReturn        | 1920.4     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 274.8      |
| AverageEpisodeLength | 971.48     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.74     |
| TotalNEpisodes       | 26354      |
| TotalNSamples        | 9.2474e+06 |
| ExplainedVariance    | 0.14976    |
-------------------------------------
[2018-01-21 17:23:56.585692 UTC] Saving snapshot
[2018-01-21 17:23:56.585874 UTC] Starting iteration 1848
[2018-01-21 17:23:56.586011 UTC] Start collecting samples
[2018-01-21 17:23:59.162698 UTC] Computing input variables for policy optimization
[2018-01-21 17:23:59.239321 UTC] Performing policy update
[2018-01-21 17:23:59.239857 UTC] Computing gradient in Euclidean space
[2018-01-21 17:23:59.316368 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:00.236625 UTC] Performing line search
[2018-01-21 17:24:00.376352 UTC] Updating baseline
[2018-01-21 17:24:01.567076 UTC] Computing logging information
-------------------------------------
| Iteration            | 1848       |
| ExpectedImprovement  | 0.017604   |
| ActualImprovement    | 0.01686    |
| ImprovementRatio     | 0.95775    |
| MeanKL               | 0.0088274  |
| Entropy              | -2.0082    |
| Perplexity           | 0.13423    |
| AveragePolicyStd     | 0.17542    |
| AveragePolicyStd[0]  | 0.1935     |
| AveragePolicyStd[1]  | 0.18832    |
| AveragePolicyStd[2]  | 0.14046    |
| AveragePolicyStd[3]  | 0.16208    |
| AveragePolicyStd[4]  | 0.14651    |
| AveragePolicyStd[5]  | 0.22167    |
| AverageReturn        | 1906       |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 306.74     |
| AverageEpisodeLength | 964.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.27     |
| TotalNEpisodes       | 26359      |
| TotalNSamples        | 9.2514e+06 |
| ExplainedVariance    | 0.24015    |
-------------------------------------
[2018-01-21 17:24:02.096157 UTC] Saving snapshot
[2018-01-21 17:24:02.096371 UTC] Starting iteration 1849
[2018-01-21 17:24:02.096548 UTC] Start collecting samples
[2018-01-21 17:24:04.499444 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:04.573926 UTC] Performing policy update
[2018-01-21 17:24:04.574528 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:04.652216 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:05.568403 UTC] Performing line search
[2018-01-21 17:24:05.686246 UTC] Updating baseline
[2018-01-21 17:24:06.977579 UTC] Computing logging information
-------------------------------------
| Iteration            | 1849       |
| ExpectedImprovement  | 0.018387   |
| ActualImprovement    | 0.017288   |
| ImprovementRatio     | 0.94022    |
| MeanKL               | 0.0077904  |
| Entropy              | -2.0139    |
| Perplexity           | 0.13347    |
| AveragePolicyStd     | 0.17524    |
| AveragePolicyStd[0]  | 0.19341    |
| AveragePolicyStd[1]  | 0.18813    |
| AveragePolicyStd[2]  | 0.14063    |
| AveragePolicyStd[3]  | 0.16146    |
| AveragePolicyStd[4]  | 0.14651    |
| AveragePolicyStd[5]  | 0.22131    |
| AverageReturn        | 1885.5     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 352.76     |
| AverageEpisodeLength | 954.48     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.52     |
| TotalNEpisodes       | 26367      |
| TotalNSamples        | 9.2581e+06 |
| ExplainedVariance    | 0.16445    |
-------------------------------------
[2018-01-21 17:24:07.496095 UTC] Saving snapshot
[2018-01-21 17:24:07.496299 UTC] Starting iteration 1850
[2018-01-21 17:24:07.496430 UTC] Start collecting samples
[2018-01-21 17:24:09.937867 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:10.034636 UTC] Performing policy update
[2018-01-21 17:24:10.035184 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:10.110344 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:11.018542 UTC] Performing line search
[2018-01-21 17:24:11.135799 UTC] Updating baseline
[2018-01-21 17:24:12.473955 UTC] Computing logging information
-------------------------------------
| Iteration            | 1850       |
| ExpectedImprovement  | 0.017674   |
| ActualImprovement    | 0.016278   |
| ImprovementRatio     | 0.92098    |
| MeanKL               | 0.0092366  |
| Entropy              | -2.0139    |
| Perplexity           | 0.13346    |
| AveragePolicyStd     | 0.17525    |
| AveragePolicyStd[0]  | 0.19316    |
| AveragePolicyStd[1]  | 0.18809    |
| AveragePolicyStd[2]  | 0.14086    |
| AveragePolicyStd[3]  | 0.1612     |
| AveragePolicyStd[4]  | 0.14643    |
| AveragePolicyStd[5]  | 0.22175    |
| AverageReturn        | 1886.7     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 353.13     |
| AverageEpisodeLength | 954.48     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.52     |
| TotalNEpisodes       | 26371      |
| TotalNSamples        | 9.2621e+06 |
| ExplainedVariance    | -0.001297  |
-------------------------------------
[2018-01-21 17:24:12.991709 UTC] Saving snapshot
[2018-01-21 17:24:12.997527 UTC] Starting iteration 1851
[2018-01-21 17:24:12.997724 UTC] Start collecting samples
[2018-01-21 17:24:15.515487 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:15.588685 UTC] Performing policy update
[2018-01-21 17:24:15.589285 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:15.666183 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:16.619974 UTC] Performing line search
[2018-01-21 17:24:16.747402 UTC] Updating baseline
[2018-01-21 17:24:18.081134 UTC] Computing logging information
-------------------------------------
| Iteration            | 1851       |
| ExpectedImprovement  | 0.017658   |
| ActualImprovement    | 0.016442   |
| ImprovementRatio     | 0.93113    |
| MeanKL               | 0.0080383  |
| Entropy              | -2.0166    |
| Perplexity           | 0.13311    |
| AveragePolicyStd     | 0.17517    |
| AveragePolicyStd[0]  | 0.19356    |
| AveragePolicyStd[1]  | 0.18783    |
| AveragePolicyStd[2]  | 0.14094    |
| AveragePolicyStd[3]  | 0.16114    |
| AveragePolicyStd[4]  | 0.14617    |
| AveragePolicyStd[5]  | 0.22138    |
| AverageReturn        | 1880.5     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 360.7      |
| AverageEpisodeLength | 950.6      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.9      |
| TotalNEpisodes       | 26377      |
| TotalNSamples        | 9.2674e+06 |
| ExplainedVariance    | 0.24595    |
-------------------------------------
[2018-01-21 17:24:18.603035 UTC] Saving snapshot
[2018-01-21 17:24:18.603219 UTC] Starting iteration 1852
[2018-01-21 17:24:18.603348 UTC] Start collecting samples
[2018-01-21 17:24:20.960252 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:21.035603 UTC] Performing policy update
[2018-01-21 17:24:21.036164 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:21.112901 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:22.093459 UTC] Performing line search
[2018-01-21 17:24:22.219022 UTC] Updating baseline
[2018-01-21 17:24:23.633104 UTC] Computing logging information
-------------------------------------
| Iteration            | 1852       |
| ExpectedImprovement  | 0.018265   |
| ActualImprovement    | 0.018118   |
| ImprovementRatio     | 0.99191    |
| MeanKL               | 0.0084405  |
| Entropy              | -2.0214    |
| Perplexity           | 0.13247    |
| AveragePolicyStd     | 0.17503    |
| AveragePolicyStd[0]  | 0.19336    |
| AveragePolicyStd[1]  | 0.18754    |
| AveragePolicyStd[2]  | 0.14106    |
| AveragePolicyStd[3]  | 0.16095    |
| AveragePolicyStd[4]  | 0.1459     |
| AveragePolicyStd[5]  | 0.22134    |
| AverageReturn        | 1880.8     |
| MinReturn            | 63.293     |
| MaxReturn            | 2044.4     |
| StdReturn            | 360.81     |
| AverageEpisodeLength | 950.6      |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.9      |
| TotalNEpisodes       | 26380      |
| TotalNSamples        | 9.2704e+06 |
| ExplainedVariance    | -0.077963  |
-------------------------------------
[2018-01-21 17:24:24.154067 UTC] Saving snapshot
[2018-01-21 17:24:24.154246 UTC] Starting iteration 1853
[2018-01-21 17:24:24.154351 UTC] Start collecting samples
[2018-01-21 17:24:26.518167 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:26.592552 UTC] Performing policy update
[2018-01-21 17:24:26.593191 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:26.671381 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:27.636196 UTC] Performing line search
[2018-01-21 17:24:27.777067 UTC] Updating baseline
[2018-01-21 17:24:29.337790 UTC] Computing logging information
-------------------------------------
| Iteration            | 1853       |
| ExpectedImprovement  | 0.016563   |
| ActualImprovement    | 0.015792   |
| ImprovementRatio     | 0.95343    |
| MeanKL               | 0.0083035  |
| Entropy              | -2.0238    |
| Perplexity           | 0.13215    |
| AveragePolicyStd     | 0.17496    |
| AveragePolicyStd[0]  | 0.1931     |
| AveragePolicyStd[1]  | 0.18741    |
| AveragePolicyStd[2]  | 0.14119    |
| AveragePolicyStd[3]  | 0.16093    |
| AveragePolicyStd[4]  | 0.14563    |
| AveragePolicyStd[5]  | 0.22151    |
| AverageReturn        | 1877.1     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 364.48     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26388      |
| TotalNSamples        | 9.2781e+06 |
| ExplainedVariance    | 0.094525   |
-------------------------------------
[2018-01-21 17:24:29.854804 UTC] Saving snapshot
[2018-01-21 17:24:29.855000 UTC] Starting iteration 1854
[2018-01-21 17:24:29.855147 UTC] Start collecting samples
[2018-01-21 17:24:32.535336 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:32.610534 UTC] Performing policy update
[2018-01-21 17:24:32.611110 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:32.691252 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:33.701054 UTC] Performing line search
[2018-01-21 17:24:33.831506 UTC] Updating baseline
[2018-01-21 17:24:35.027392 UTC] Computing logging information
-------------------------------------
| Iteration            | 1854       |
| ExpectedImprovement  | 0.021508   |
| ActualImprovement    | 0.019032   |
| ImprovementRatio     | 0.88487    |
| MeanKL               | 0.0082617  |
| Entropy              | -2.0149    |
| Perplexity           | 0.13333    |
| AveragePolicyStd     | 0.17522    |
| AveragePolicyStd[0]  | 0.19332    |
| AveragePolicyStd[1]  | 0.1874     |
| AveragePolicyStd[2]  | 0.14137    |
| AveragePolicyStd[3]  | 0.16143    |
| AveragePolicyStd[4]  | 0.14586    |
| AveragePolicyStd[5]  | 0.22191    |
| AverageReturn        | 1878.1     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 364.81     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26392      |
| TotalNSamples        | 9.2821e+06 |
| ExplainedVariance    | -0.0075134 |
-------------------------------------
[2018-01-21 17:24:35.760802 UTC] Saving snapshot
[2018-01-21 17:24:35.761102 UTC] Starting iteration 1855
[2018-01-21 17:24:35.761291 UTC] Start collecting samples
[2018-01-21 17:24:38.439753 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:38.513564 UTC] Performing policy update
[2018-01-21 17:24:38.514096 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:38.600015 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:39.509081 UTC] Performing line search
[2018-01-21 17:24:39.626447 UTC] Updating baseline
[2018-01-21 17:24:41.424350 UTC] Computing logging information
-------------------------------------
| Iteration            | 1855       |
| ExpectedImprovement  | 0.01665    |
| ActualImprovement    | 0.015388   |
| ImprovementRatio     | 0.92423    |
| MeanKL               | 0.0080014  |
| Entropy              | -2.0181    |
| Perplexity           | 0.13291    |
| AveragePolicyStd     | 0.17515    |
| AveragePolicyStd[0]  | 0.19333    |
| AveragePolicyStd[1]  | 0.18755    |
| AveragePolicyStd[2]  | 0.14124    |
| AveragePolicyStd[3]  | 0.16097    |
| AveragePolicyStd[4]  | 0.14567    |
| AveragePolicyStd[5]  | 0.22215    |
| AverageReturn        | 1880       |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 365.48     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26396      |
| TotalNSamples        | 9.2861e+06 |
| ExplainedVariance    | 0.0061641  |
-------------------------------------
[2018-01-21 17:24:41.974559 UTC] Saving snapshot
[2018-01-21 17:24:41.974748 UTC] Starting iteration 1856
[2018-01-21 17:24:41.974898 UTC] Start collecting samples
[2018-01-21 17:24:45.011326 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:45.087336 UTC] Performing policy update
[2018-01-21 17:24:45.087860 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:45.163749 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:46.077984 UTC] Performing line search
[2018-01-21 17:24:46.195878 UTC] Updating baseline
[2018-01-21 17:24:47.832809 UTC] Computing logging information
-------------------------------------
| Iteration            | 1856       |
| ExpectedImprovement  | 0.016649   |
| ActualImprovement    | 0.016052   |
| ImprovementRatio     | 0.96414    |
| MeanKL               | 0.0082638  |
| Entropy              | -2.0228    |
| Perplexity           | 0.13228    |
| AveragePolicyStd     | 0.17508    |
| AveragePolicyStd[0]  | 0.19382    |
| AveragePolicyStd[1]  | 0.18715    |
| AveragePolicyStd[2]  | 0.14103    |
| AveragePolicyStd[3]  | 0.16097    |
| AveragePolicyStd[4]  | 0.14468    |
| AveragePolicyStd[5]  | 0.22285    |
| AverageReturn        | 1882.6     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 366.31     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26402      |
| TotalNSamples        | 9.2921e+06 |
| ExplainedVariance    | 0.0020278  |
-------------------------------------
[2018-01-21 17:24:48.353687 UTC] Saving snapshot
[2018-01-21 17:24:48.353879 UTC] Starting iteration 1857
[2018-01-21 17:24:48.354029 UTC] Start collecting samples
[2018-01-21 17:24:51.193117 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:51.272992 UTC] Performing policy update
[2018-01-21 17:24:51.273531 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:51.375825 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:52.435216 UTC] Performing line search
[2018-01-21 17:24:52.627503 UTC] Updating baseline
[2018-01-21 17:24:54.190902 UTC] Computing logging information
-------------------------------------
| Iteration            | 1857       |
| ExpectedImprovement  | 0.016048   |
| ActualImprovement    | 0.015043   |
| ImprovementRatio     | 0.93737    |
| MeanKL               | 0.0094228  |
| Entropy              | -2.0175    |
| Perplexity           | 0.13299    |
| AveragePolicyStd     | 0.1752     |
| AveragePolicyStd[0]  | 0.19304    |
| AveragePolicyStd[1]  | 0.18707    |
| AveragePolicyStd[2]  | 0.14141    |
| AveragePolicyStd[3]  | 0.16134    |
| AveragePolicyStd[4]  | 0.14518    |
| AveragePolicyStd[5]  | 0.22316    |
| AverageReturn        | 1885.1     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 367.11     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26408      |
| TotalNSamples        | 9.2981e+06 |
| ExplainedVariance    | 0.0019156  |
-------------------------------------
[2018-01-21 17:24:54.714514 UTC] Saving snapshot
[2018-01-21 17:24:54.714710 UTC] Starting iteration 1858
[2018-01-21 17:24:54.714828 UTC] Start collecting samples
[2018-01-21 17:24:57.457338 UTC] Computing input variables for policy optimization
[2018-01-21 17:24:57.538916 UTC] Performing policy update
[2018-01-21 17:24:57.539437 UTC] Computing gradient in Euclidean space
[2018-01-21 17:24:57.621561 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:24:58.736235 UTC] Performing line search
[2018-01-21 17:24:58.864859 UTC] Updating baseline
[2018-01-21 17:25:00.666572 UTC] Computing logging information
-------------------------------------
| Iteration            | 1858       |
| ExpectedImprovement  | 0.017246   |
| ActualImprovement    | 0.016186   |
| ImprovementRatio     | 0.93856    |
| MeanKL               | 0.0083252  |
| Entropy              | -2.0211    |
| Perplexity           | 0.13251    |
| AveragePolicyStd     | 0.17512    |
| AveragePolicyStd[0]  | 0.19304    |
| AveragePolicyStd[1]  | 0.18736    |
| AveragePolicyStd[2]  | 0.14141    |
| AveragePolicyStd[3]  | 0.16096    |
| AveragePolicyStd[4]  | 0.14479    |
| AveragePolicyStd[5]  | 0.22314    |
| AverageReturn        | 1886.3     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 367.48     |
| AverageEpisodeLength | 947.78     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 26411      |
| TotalNSamples        | 9.3011e+06 |
| ExplainedVariance    | 0.0041731  |
-------------------------------------
[2018-01-21 17:25:01.192654 UTC] Saving snapshot
[2018-01-21 17:25:01.192844 UTC] Starting iteration 1859
[2018-01-21 17:25:01.192970 UTC] Start collecting samples
[2018-01-21 17:25:04.721048 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:04.803823 UTC] Performing policy update
[2018-01-21 17:25:04.804348 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:04.899989 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:05.921138 UTC] Performing line search
[2018-01-21 17:25:06.050707 UTC] Updating baseline
[2018-01-21 17:25:07.258040 UTC] Computing logging information
-------------------------------------
| Iteration            | 1859       |
| ExpectedImprovement  | 0.017651   |
| ActualImprovement    | 0.016874   |
| ImprovementRatio     | 0.95601    |
| MeanKL               | 0.0090511  |
| Entropy              | -2.0218    |
| Perplexity           | 0.13241    |
| AveragePolicyStd     | 0.1751     |
| AveragePolicyStd[0]  | 0.19315    |
| AveragePolicyStd[1]  | 0.1873     |
| AveragePolicyStd[2]  | 0.14126    |
| AveragePolicyStd[3]  | 0.16091    |
| AveragePolicyStd[4]  | 0.14484    |
| AveragePolicyStd[5]  | 0.22315    |
| AverageReturn        | 1868       |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 389.02     |
| AverageEpisodeLength | 938.06     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.67     |
| TotalNEpisodes       | 26420      |
| TotalNSamples        | 9.3091e+06 |
| ExplainedVariance    | 0.10985    |
-------------------------------------
[2018-01-21 17:25:07.943381 UTC] Saving snapshot
[2018-01-21 17:25:07.943639 UTC] Starting iteration 1860
[2018-01-21 17:25:07.943806 UTC] Start collecting samples
[2018-01-21 17:25:12.164259 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:12.287678 UTC] Performing policy update
[2018-01-21 17:25:12.288463 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:12.397558 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:13.435945 UTC] Performing line search
[2018-01-21 17:25:13.567986 UTC] Updating baseline
[2018-01-21 17:25:15.572402 UTC] Computing logging information
-------------------------------------
| Iteration            | 1860       |
| ExpectedImprovement  | 0.018869   |
| ActualImprovement    | 0.018028   |
| ImprovementRatio     | 0.9554     |
| MeanKL               | 0.007955   |
| Entropy              | -2.0203    |
| Perplexity           | 0.13262    |
| AveragePolicyStd     | 0.17513    |
| AveragePolicyStd[0]  | 0.19309    |
| AveragePolicyStd[1]  | 0.18716    |
| AveragePolicyStd[2]  | 0.14141    |
| AveragePolicyStd[3]  | 0.16096    |
| AveragePolicyStd[4]  | 0.14505    |
| AveragePolicyStd[5]  | 0.22309    |
| AverageReturn        | 1869.2     |
| MinReturn            | 63.293     |
| MaxReturn            | 2073.1     |
| StdReturn            | 389.39     |
| AverageEpisodeLength | 937.66     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 183.58     |
| TotalNEpisodes       | 26424      |
| TotalNSamples        | 9.3131e+06 |
| ExplainedVariance    | 0.05378    |
-------------------------------------
[2018-01-21 17:25:16.413875 UTC] Saving snapshot
[2018-01-21 17:25:16.420512 UTC] Starting iteration 1861
[2018-01-21 17:25:16.420699 UTC] Start collecting samples
[2018-01-21 17:25:20.369543 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:20.452604 UTC] Performing policy update
[2018-01-21 17:25:20.453067 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:20.535042 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:21.466530 UTC] Performing line search
[2018-01-21 17:25:21.607546 UTC] Updating baseline
[2018-01-21 17:25:22.887500 UTC] Computing logging information
-------------------------------------
| Iteration            | 1861       |
| ExpectedImprovement  | 0.018309   |
| ActualImprovement    | 0.016057   |
| ImprovementRatio     | 0.87702    |
| MeanKL               | 0.0075877  |
| Entropy              | -2.021     |
| Perplexity           | 0.13253    |
| AveragePolicyStd     | 0.17511    |
| AveragePolicyStd[0]  | 0.19278    |
| AveragePolicyStd[1]  | 0.18729    |
| AveragePolicyStd[2]  | 0.14135    |
| AveragePolicyStd[3]  | 0.16076    |
| AveragePolicyStd[4]  | 0.14522    |
| AveragePolicyStd[5]  | 0.22325    |
| AverageReturn        | 1864.9     |
| MinReturn            | 63.293     |
| MaxReturn            | 2076.6     |
| StdReturn            | 393.02     |
| AverageEpisodeLength | 934.61     |
| MinEpisodeLength     | 71         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 185.04     |
| TotalNEpisodes       | 26428      |
| TotalNSamples        | 9.3168e+06 |
| ExplainedVariance    | 0.11724    |
-------------------------------------
[2018-01-21 17:25:23.418713 UTC] Saving snapshot
[2018-01-21 17:25:23.418942 UTC] Starting iteration 1862
[2018-01-21 17:25:23.419072 UTC] Start collecting samples
[2018-01-21 17:25:25.871561 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:25.944439 UTC] Performing policy update
[2018-01-21 17:25:25.945978 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:26.024366 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:26.950872 UTC] Performing line search
[2018-01-21 17:25:27.082522 UTC] Updating baseline
[2018-01-21 17:25:28.228305 UTC] Computing logging information
-------------------------------------
| Iteration            | 1862       |
| ExpectedImprovement  | 0.016497   |
| ActualImprovement    | 0.015508   |
| ImprovementRatio     | 0.94009    |
| MeanKL               | 0.0087642  |
| Entropy              | -2.0241    |
| Perplexity           | 0.13211    |
| AveragePolicyStd     | 0.17506    |
| AveragePolicyStd[0]  | 0.19282    |
| AveragePolicyStd[1]  | 0.18698    |
| AveragePolicyStd[2]  | 0.14081    |
| AveragePolicyStd[3]  | 0.16093    |
| AveragePolicyStd[4]  | 0.145      |
| AveragePolicyStd[5]  | 0.22384    |
| AverageReturn        | 1886.8     |
| MinReturn            | 382.47     |
| MaxReturn            | 2076.6     |
| StdReturn            | 349.81     |
| AverageEpisodeLength | 943.9      |
| MinEpisodeLength     | 237        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 163.52     |
| TotalNEpisodes       | 26433      |
| TotalNSamples        | 9.3218e+06 |
| ExplainedVariance    | 0.040621   |
-------------------------------------
[2018-01-21 17:25:28.750386 UTC] Saving snapshot
[2018-01-21 17:25:28.750602 UTC] Starting iteration 1863
[2018-01-21 17:25:28.750740 UTC] Start collecting samples
[2018-01-21 17:25:31.218194 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:31.293808 UTC] Performing policy update
[2018-01-21 17:25:31.294310 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:31.374770 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:32.348866 UTC] Performing line search
[2018-01-21 17:25:32.475246 UTC] Updating baseline
[2018-01-21 17:25:33.611749 UTC] Computing logging information
-------------------------------------
| Iteration            | 1863       |
| ExpectedImprovement  | 0.017715   |
| ActualImprovement    | 0.01677    |
| ImprovementRatio     | 0.94668    |
| MeanKL               | 0.0085179  |
| Entropy              | -2.0235    |
| Perplexity           | 0.13219    |
| AveragePolicyStd     | 0.17507    |
| AveragePolicyStd[0]  | 0.19219    |
| AveragePolicyStd[1]  | 0.18707    |
| AveragePolicyStd[2]  | 0.14096    |
| AveragePolicyStd[3]  | 0.16106    |
| AveragePolicyStd[4]  | 0.14502    |
| AveragePolicyStd[5]  | 0.22415    |
| AverageReturn        | 1882.9     |
| MinReturn            | 382.47     |
| MaxReturn            | 2079.2     |
| StdReturn            | 358.27     |
| AverageEpisodeLength | 940.09     |
| MinEpisodeLength     | 237        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.58     |
| TotalNEpisodes       | 26440      |
| TotalNSamples        | 9.3284e+06 |
| ExplainedVariance    | 0.1587     |
-------------------------------------
[2018-01-21 17:25:34.148036 UTC] Saving snapshot
[2018-01-21 17:25:34.148223 UTC] Starting iteration 1864
[2018-01-21 17:25:34.148352 UTC] Start collecting samples
[2018-01-21 17:25:36.633013 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:36.702765 UTC] Performing policy update
[2018-01-21 17:25:36.703287 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:36.779870 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:37.728083 UTC] Performing line search
[2018-01-21 17:25:37.845911 UTC] Updating baseline
[2018-01-21 17:25:39.516718 UTC] Computing logging information
-------------------------------------
| Iteration            | 1864       |
| ExpectedImprovement  | 0.018405   |
| ActualImprovement    | 0.01667    |
| ImprovementRatio     | 0.90571    |
| MeanKL               | 0.0084997  |
| Entropy              | -2.0254    |
| Perplexity           | 0.13194    |
| AveragePolicyStd     | 0.17503    |
| AveragePolicyStd[0]  | 0.19204    |
| AveragePolicyStd[1]  | 0.18699    |
| AveragePolicyStd[2]  | 0.14058    |
| AveragePolicyStd[3]  | 0.16137    |
| AveragePolicyStd[4]  | 0.14501    |
| AveragePolicyStd[5]  | 0.22418    |
| AverageReturn        | 1884.3     |
| MinReturn            | 382.47     |
| MaxReturn            | 2114.5     |
| StdReturn            | 358.9      |
| AverageEpisodeLength | 940.09     |
| MinEpisodeLength     | 237        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.58     |
| TotalNEpisodes       | 26441      |
| TotalNSamples        | 9.3294e+06 |
| ExplainedVariance    | -0.037839  |
-------------------------------------
[2018-01-21 17:25:40.035457 UTC] Saving snapshot
[2018-01-21 17:25:40.035659 UTC] Starting iteration 1865
[2018-01-21 17:25:40.035792 UTC] Start collecting samples
[2018-01-21 17:25:42.526462 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:42.603184 UTC] Performing policy update
[2018-01-21 17:25:42.603719 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:42.679545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:43.576261 UTC] Performing line search
[2018-01-21 17:25:43.728532 UTC] Updating baseline
[2018-01-21 17:25:45.277941 UTC] Computing logging information
-------------------------------------
| Iteration            | 1865       |
| ExpectedImprovement  | 0.016813   |
| ActualImprovement    | 0.016718   |
| ImprovementRatio     | 0.99433    |
| MeanKL               | 0.0093613  |
| Entropy              | -2.0231    |
| Perplexity           | 0.13224    |
| AveragePolicyStd     | 0.1751     |
| AveragePolicyStd[0]  | 0.19232    |
| AveragePolicyStd[1]  | 0.18709    |
| AveragePolicyStd[2]  | 0.14053    |
| AveragePolicyStd[3]  | 0.1615     |
| AveragePolicyStd[4]  | 0.14502    |
| AveragePolicyStd[5]  | 0.22412    |
| AverageReturn        | 1911.2     |
| MinReturn            | 510.14     |
| MaxReturn            | 2114.5     |
| StdReturn            | 325.01     |
| AverageEpisodeLength | 949.88     |
| MinEpisodeLength     | 294        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 150.13     |
| TotalNEpisodes       | 26448      |
| TotalNSamples        | 9.3364e+06 |
| ExplainedVariance    | 0.0024657  |
-------------------------------------
[2018-01-21 17:25:45.814014 UTC] Saving snapshot
[2018-01-21 17:25:45.814218 UTC] Starting iteration 1866
[2018-01-21 17:25:45.814445 UTC] Start collecting samples
[2018-01-21 17:25:48.499307 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:48.573878 UTC] Performing policy update
[2018-01-21 17:25:48.574398 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:48.653292 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:49.615202 UTC] Performing line search
[2018-01-21 17:25:49.732518 UTC] Updating baseline
[2018-01-21 17:25:51.041589 UTC] Computing logging information
-------------------------------------
| Iteration            | 1866       |
| ExpectedImprovement  | 0.018003   |
| ActualImprovement    | 0.017284   |
| ImprovementRatio     | 0.96006    |
| MeanKL               | 0.0081941  |
| Entropy              | -2.0133    |
| Perplexity           | 0.13355    |
| AveragePolicyStd     | 0.17539    |
| AveragePolicyStd[0]  | 0.19246    |
| AveragePolicyStd[1]  | 0.18753    |
| AveragePolicyStd[2]  | 0.14083    |
| AveragePolicyStd[3]  | 0.162      |
| AveragePolicyStd[4]  | 0.14492    |
| AveragePolicyStd[5]  | 0.22463    |
| AverageReturn        | 1904.4     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 345.75     |
| AverageEpisodeLength | 944.84     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.71     |
| TotalNEpisodes       | 26458      |
| TotalNSamples        | 9.3449e+06 |
| ExplainedVariance    | 0.18561    |
-------------------------------------
[2018-01-21 17:25:51.561147 UTC] Saving snapshot
[2018-01-21 17:25:51.561353 UTC] Starting iteration 1867
[2018-01-21 17:25:51.561500 UTC] Start collecting samples
[2018-01-21 17:25:53.937877 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:54.009214 UTC] Performing policy update
[2018-01-21 17:25:54.009847 UTC] Computing gradient in Euclidean space
[2018-01-21 17:25:54.110748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:25:55.060184 UTC] Performing line search
[2018-01-21 17:25:55.193358 UTC] Updating baseline
[2018-01-21 17:25:56.986607 UTC] Computing logging information
-------------------------------------
| Iteration            | 1867       |
| ExpectedImprovement  | 0.017952   |
| ActualImprovement    | 0.017086   |
| ImprovementRatio     | 0.95176    |
| MeanKL               | 0.007582   |
| Entropy              | -2.0208    |
| Perplexity           | 0.13255    |
| AveragePolicyStd     | 0.17515    |
| AveragePolicyStd[0]  | 0.19204    |
| AveragePolicyStd[1]  | 0.18736    |
| AveragePolicyStd[2]  | 0.14067    |
| AveragePolicyStd[3]  | 0.16173    |
| AveragePolicyStd[4]  | 0.14511    |
| AveragePolicyStd[5]  | 0.22398    |
| AverageReturn        | 1894.8     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 357.96     |
| AverageEpisodeLength | 939.8      |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.91     |
| TotalNEpisodes       | 26460      |
| TotalNSamples        | 9.3464e+06 |
| ExplainedVariance    | 0.042788   |
-------------------------------------
[2018-01-21 17:25:57.518103 UTC] Saving snapshot
[2018-01-21 17:25:57.518296 UTC] Starting iteration 1868
[2018-01-21 17:25:57.518440 UTC] Start collecting samples
[2018-01-21 17:25:59.870814 UTC] Computing input variables for policy optimization
[2018-01-21 17:25:59.962502 UTC] Performing policy update
[2018-01-21 17:25:59.963123 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:00.048077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:00.976945 UTC] Performing line search
[2018-01-21 17:26:01.095434 UTC] Updating baseline
[2018-01-21 17:26:02.484913 UTC] Computing logging information
-------------------------------------
| Iteration            | 1868       |
| ExpectedImprovement  | 0.020856   |
| ActualImprovement    | 0.01833    |
| ImprovementRatio     | 0.87889    |
| MeanKL               | 0.0074915  |
| Entropy              | -2.0274    |
| Perplexity           | 0.13167    |
| AveragePolicyStd     | 0.17496    |
| AveragePolicyStd[0]  | 0.19133    |
| AveragePolicyStd[1]  | 0.18729    |
| AveragePolicyStd[2]  | 0.1409     |
| AveragePolicyStd[3]  | 0.16121    |
| AveragePolicyStd[4]  | 0.14477    |
| AveragePolicyStd[5]  | 0.22428    |
| AverageReturn        | 1906       |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 337.63     |
| AverageEpisodeLength | 944.95     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 156.13     |
| TotalNEpisodes       | 26463      |
| TotalNSamples        | 9.3493e+06 |
| ExplainedVariance    | 0.17179    |
-------------------------------------
[2018-01-21 17:26:03.054423 UTC] Saving snapshot
[2018-01-21 17:26:03.054685 UTC] Starting iteration 1869
[2018-01-21 17:26:03.054883 UTC] Start collecting samples
[2018-01-21 17:26:05.878843 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:05.963287 UTC] Performing policy update
[2018-01-21 17:26:05.963826 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:06.044513 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:07.030155 UTC] Performing line search
[2018-01-21 17:26:07.198290 UTC] Updating baseline
[2018-01-21 17:26:08.640784 UTC] Computing logging information
-------------------------------------
| Iteration            | 1869       |
| ExpectedImprovement  | 0.018985   |
| ActualImprovement    | 0.018153   |
| ImprovementRatio     | 0.95621    |
| MeanKL               | 0.0086873  |
| Entropy              | -2.0321    |
| Perplexity           | 0.13106    |
| AveragePolicyStd     | 0.17482    |
| AveragePolicyStd[0]  | 0.19094    |
| AveragePolicyStd[1]  | 0.18748    |
| AveragePolicyStd[2]  | 0.14084    |
| AveragePolicyStd[3]  | 0.16084    |
| AveragePolicyStd[4]  | 0.14483    |
| AveragePolicyStd[5]  | 0.22396    |
| AverageReturn        | 1923       |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 313.27     |
| AverageEpisodeLength | 950.96     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 144.62     |
| TotalNEpisodes       | 26473      |
| TotalNSamples        | 9.3588e+06 |
| ExplainedVariance    | 0.11386    |
-------------------------------------
[2018-01-21 17:26:09.238757 UTC] Saving snapshot
[2018-01-21 17:26:09.238959 UTC] Starting iteration 1870
[2018-01-21 17:26:09.239086 UTC] Start collecting samples
[2018-01-21 17:26:11.674801 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:11.746892 UTC] Performing policy update
[2018-01-21 17:26:11.747415 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:11.827004 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:12.722201 UTC] Performing line search
[2018-01-21 17:26:12.839171 UTC] Updating baseline
[2018-01-21 17:26:14.416357 UTC] Computing logging information
-------------------------------------
| Iteration            | 1870       |
| ExpectedImprovement  | 0.019272   |
| ActualImprovement    | 0.017934   |
| ImprovementRatio     | 0.93059    |
| MeanKL               | 0.0074951  |
| Entropy              | -2.029     |
| Perplexity           | 0.13146    |
| AveragePolicyStd     | 0.17493    |
| AveragePolicyStd[0]  | 0.19094    |
| AveragePolicyStd[1]  | 0.18785    |
| AveragePolicyStd[2]  | 0.14092    |
| AveragePolicyStd[3]  | 0.16108    |
| AveragePolicyStd[4]  | 0.14445    |
| AveragePolicyStd[5]  | 0.22436    |
| AverageReturn        | 1931.3     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 307.42     |
| AverageEpisodeLength | 954.2      |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.02     |
| TotalNEpisodes       | 26477      |
| TotalNSamples        | 9.3628e+06 |
| ExplainedVariance    | -0.01831   |
-------------------------------------
[2018-01-21 17:26:14.964614 UTC] Saving snapshot
[2018-01-21 17:26:14.970990 UTC] Starting iteration 1871
[2018-01-21 17:26:14.971184 UTC] Start collecting samples
[2018-01-21 17:26:17.477778 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:17.548284 UTC] Performing policy update
[2018-01-21 17:26:17.548768 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:17.623488 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:18.527869 UTC] Performing line search
[2018-01-21 17:26:18.645565 UTC] Updating baseline
[2018-01-21 17:26:20.052325 UTC] Computing logging information
-------------------------------------
| Iteration            | 1871       |
| ExpectedImprovement  | 0.017041   |
| ActualImprovement    | 0.015499   |
| ImprovementRatio     | 0.90954    |
| MeanKL               | 0.0086887  |
| Entropy              | -2.0329    |
| Perplexity           | 0.13096    |
| AveragePolicyStd     | 0.17482    |
| AveragePolicyStd[0]  | 0.1909     |
| AveragePolicyStd[1]  | 0.18772    |
| AveragePolicyStd[2]  | 0.14056    |
| AveragePolicyStd[3]  | 0.16087    |
| AveragePolicyStd[4]  | 0.14469    |
| AveragePolicyStd[5]  | 0.22419    |
| AverageReturn        | 1932.2     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 307.66     |
| AverageEpisodeLength | 954.2      |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.02     |
| TotalNEpisodes       | 26479      |
| TotalNSamples        | 9.3648e+06 |
| ExplainedVariance    | 0.0042741  |
-------------------------------------
[2018-01-21 17:26:20.579870 UTC] Saving snapshot
[2018-01-21 17:26:20.580085 UTC] Starting iteration 1872
[2018-01-21 17:26:20.580218 UTC] Start collecting samples
[2018-01-21 17:26:23.210764 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:23.283961 UTC] Performing policy update
[2018-01-21 17:26:23.284444 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:23.389330 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:24.295291 UTC] Performing line search
[2018-01-21 17:26:24.419923 UTC] Updating baseline
[2018-01-21 17:26:25.647784 UTC] Computing logging information
-------------------------------------
| Iteration            | 1872       |
| ExpectedImprovement  | 0.017151   |
| ActualImprovement    | 0.016403   |
| ImprovementRatio     | 0.95639    |
| MeanKL               | 0.0084046  |
| Entropy              | -2.0351    |
| Perplexity           | 0.13066    |
| AveragePolicyStd     | 0.17476    |
| AveragePolicyStd[0]  | 0.19032    |
| AveragePolicyStd[1]  | 0.18807    |
| AveragePolicyStd[2]  | 0.14053    |
| AveragePolicyStd[3]  | 0.16129    |
| AveragePolicyStd[4]  | 0.14424    |
| AveragePolicyStd[5]  | 0.2241     |
| AverageReturn        | 1932.4     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 307.21     |
| AverageEpisodeLength | 953.25     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.65     |
| TotalNEpisodes       | 26487      |
| TotalNSamples        | 9.3724e+06 |
| ExplainedVariance    | 0.12761    |
-------------------------------------
[2018-01-21 17:26:26.173939 UTC] Saving snapshot
[2018-01-21 17:26:26.174136 UTC] Starting iteration 1873
[2018-01-21 17:26:26.174263 UTC] Start collecting samples
[2018-01-21 17:26:28.848300 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:28.924533 UTC] Performing policy update
[2018-01-21 17:26:28.925099 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:29.003343 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:29.899681 UTC] Performing line search
[2018-01-21 17:26:30.026811 UTC] Updating baseline
[2018-01-21 17:26:31.341637 UTC] Computing logging information
-------------------------------------
| Iteration            | 1873       |
| ExpectedImprovement  | 0.018901   |
| ActualImprovement    | 0.017577   |
| ImprovementRatio     | 0.92998    |
| MeanKL               | 0.0082986  |
| Entropy              | -2.0298    |
| Perplexity           | 0.13136    |
| AveragePolicyStd     | 0.17488    |
| AveragePolicyStd[0]  | 0.19081    |
| AveragePolicyStd[1]  | 0.18794    |
| AveragePolicyStd[2]  | 0.14114    |
| AveragePolicyStd[3]  | 0.16129    |
| AveragePolicyStd[4]  | 0.14436    |
| AveragePolicyStd[5]  | 0.22372    |
| AverageReturn        | 1921.2     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 332.6      |
| AverageEpisodeLength | 946.84     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.4      |
| TotalNEpisodes       | 26494      |
| TotalNSamples        | 9.3788e+06 |
| ExplainedVariance    | 0.10423    |
-------------------------------------
[2018-01-21 17:26:31.860527 UTC] Saving snapshot
[2018-01-21 17:26:31.860712 UTC] Starting iteration 1874
[2018-01-21 17:26:31.860814 UTC] Start collecting samples
[2018-01-21 17:26:34.137581 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:34.209157 UTC] Performing policy update
[2018-01-21 17:26:34.209692 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:34.285108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:35.249898 UTC] Performing line search
[2018-01-21 17:26:35.373006 UTC] Updating baseline
[2018-01-21 17:26:36.614417 UTC] Computing logging information
-------------------------------------
| Iteration            | 1874       |
| ExpectedImprovement  | 0.017423   |
| ActualImprovement    | 0.016458   |
| ImprovementRatio     | 0.94457    |
| MeanKL               | 0.0080583  |
| Entropy              | -2.0333    |
| Perplexity           | 0.1309     |
| AveragePolicyStd     | 0.1748     |
| AveragePolicyStd[0]  | 0.19062    |
| AveragePolicyStd[1]  | 0.18835    |
| AveragePolicyStd[2]  | 0.14086    |
| AveragePolicyStd[3]  | 0.16086    |
| AveragePolicyStd[4]  | 0.14433    |
| AveragePolicyStd[5]  | 0.22375    |
| AverageReturn        | 1900.5     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 361        |
| AverageEpisodeLength | 936.95     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26497      |
| TotalNSamples        | 9.3808e+06 |
| ExplainedVariance    | 0.46799    |
-------------------------------------
[2018-01-21 17:26:37.171557 UTC] Saving snapshot
[2018-01-21 17:26:37.171755 UTC] Starting iteration 1875
[2018-01-21 17:26:37.171878 UTC] Start collecting samples
[2018-01-21 17:26:39.561188 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:39.634158 UTC] Performing policy update
[2018-01-21 17:26:39.634707 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:39.712217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:40.613430 UTC] Performing line search
[2018-01-21 17:26:40.732448 UTC] Updating baseline
[2018-01-21 17:26:42.416310 UTC] Computing logging information
-------------------------------------
| Iteration            | 1875       |
| ExpectedImprovement  | 0.018205   |
| ActualImprovement    | 0.017514   |
| ImprovementRatio     | 0.96208    |
| MeanKL               | 0.0092603  |
| Entropy              | -2.0386    |
| Perplexity           | 0.13021    |
| AveragePolicyStd     | 0.17465    |
| AveragePolicyStd[0]  | 0.19055    |
| AveragePolicyStd[1]  | 0.18804    |
| AveragePolicyStd[2]  | 0.1406     |
| AveragePolicyStd[3]  | 0.16086    |
| AveragePolicyStd[4]  | 0.14414    |
| AveragePolicyStd[5]  | 0.22374    |
| AverageReturn        | 1901.7     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 361.43     |
| AverageEpisodeLength | 936.95     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26503      |
| TotalNSamples        | 9.3868e+06 |
| ExplainedVariance    | -0.0165    |
-------------------------------------
[2018-01-21 17:26:42.935844 UTC] Saving snapshot
[2018-01-21 17:26:42.936036 UTC] Starting iteration 1876
[2018-01-21 17:26:42.936171 UTC] Start collecting samples
[2018-01-21 17:26:45.280327 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:45.353926 UTC] Performing policy update
[2018-01-21 17:26:45.354465 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:45.431198 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:46.322188 UTC] Performing line search
[2018-01-21 17:26:46.441094 UTC] Updating baseline
[2018-01-21 17:26:47.573793 UTC] Computing logging information
-------------------------------------
| Iteration            | 1876       |
| ExpectedImprovement  | 0.018403   |
| ActualImprovement    | 0.017632   |
| ImprovementRatio     | 0.95812    |
| MeanKL               | 0.0085321  |
| Entropy              | -2.037     |
| Perplexity           | 0.13042    |
| AveragePolicyStd     | 0.17468    |
| AveragePolicyStd[0]  | 0.19012    |
| AveragePolicyStd[1]  | 0.18824    |
| AveragePolicyStd[2]  | 0.14091    |
| AveragePolicyStd[3]  | 0.16079    |
| AveragePolicyStd[4]  | 0.14425    |
| AveragePolicyStd[5]  | 0.22378    |
| AverageReturn        | 1902.2     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 361.64     |
| AverageEpisodeLength | 936.95     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26510      |
| TotalNSamples        | 9.3938e+06 |
| ExplainedVariance    | 0.07648    |
-------------------------------------
[2018-01-21 17:26:48.103660 UTC] Saving snapshot
[2018-01-21 17:26:48.103855 UTC] Starting iteration 1877
[2018-01-21 17:26:48.103976 UTC] Start collecting samples
[2018-01-21 17:26:50.512532 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:50.584868 UTC] Performing policy update
[2018-01-21 17:26:50.585387 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:50.661094 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:51.556035 UTC] Performing line search
[2018-01-21 17:26:51.674217 UTC] Updating baseline
[2018-01-21 17:26:53.886587 UTC] Computing logging information
-------------------------------------
| Iteration            | 1877       |
| ExpectedImprovement  | 0.018379   |
| ActualImprovement    | 0.017099   |
| ImprovementRatio     | 0.93034    |
| MeanKL               | 0.0084473  |
| Entropy              | -2.0341    |
| Perplexity           | 0.1308     |
| AveragePolicyStd     | 0.17476    |
| AveragePolicyStd[0]  | 0.19023    |
| AveragePolicyStd[1]  | 0.18857    |
| AveragePolicyStd[2]  | 0.14097    |
| AveragePolicyStd[3]  | 0.16101    |
| AveragePolicyStd[4]  | 0.14424    |
| AveragePolicyStd[5]  | 0.22353    |
| AverageReturn        | 1902.6     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 361.8      |
| AverageEpisodeLength | 936.95     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.26     |
| TotalNEpisodes       | 26513      |
| TotalNSamples        | 9.3968e+06 |
| ExplainedVariance    | -0.002004  |
-------------------------------------
[2018-01-21 17:26:54.407915 UTC] Saving snapshot
[2018-01-21 17:26:54.408113 UTC] Starting iteration 1878
[2018-01-21 17:26:54.408241 UTC] Start collecting samples
[2018-01-21 17:26:56.831886 UTC] Computing input variables for policy optimization
[2018-01-21 17:26:56.903644 UTC] Performing policy update
[2018-01-21 17:26:56.904292 UTC] Computing gradient in Euclidean space
[2018-01-21 17:26:56.981271 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:26:57.892426 UTC] Performing line search
[2018-01-21 17:26:58.019389 UTC] Updating baseline
[2018-01-21 17:26:59.954250 UTC] Computing logging information
-------------------------------------
| Iteration            | 1878       |
| ExpectedImprovement  | 0.017851   |
| ActualImprovement    | 0.016456   |
| ImprovementRatio     | 0.92184    |
| MeanKL               | 0.0081278  |
| Entropy              | -2.0354    |
| Perplexity           | 0.13063    |
| AveragePolicyStd     | 0.17469    |
| AveragePolicyStd[0]  | 0.19042    |
| AveragePolicyStd[1]  | 0.18834    |
| AveragePolicyStd[2]  | 0.1409     |
| AveragePolicyStd[3]  | 0.16132    |
| AveragePolicyStd[4]  | 0.14437    |
| AveragePolicyStd[5]  | 0.22279    |
| AverageReturn        | 1915       |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 347.31     |
| AverageEpisodeLength | 942.36     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 159.27     |
| TotalNEpisodes       | 26517      |
| TotalNSamples        | 9.4008e+06 |
| ExplainedVariance    | 0.0074061  |
-------------------------------------
[2018-01-21 17:27:00.480883 UTC] Saving snapshot
[2018-01-21 17:27:00.481070 UTC] Starting iteration 1879
[2018-01-21 17:27:00.481198 UTC] Start collecting samples
[2018-01-21 17:27:02.913066 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:03.000949 UTC] Performing policy update
[2018-01-21 17:27:03.001450 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:03.089989 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:04.068452 UTC] Performing line search
[2018-01-21 17:27:04.190938 UTC] Updating baseline
[2018-01-21 17:27:05.715741 UTC] Computing logging information
-------------------------------------
| Iteration            | 1879       |
| ExpectedImprovement  | 0.01773    |
| ActualImprovement    | 0.016823   |
| ImprovementRatio     | 0.94888    |
| MeanKL               | 0.0077361  |
| Entropy              | -2.0407    |
| Perplexity           | 0.12994    |
| AveragePolicyStd     | 0.17451    |
| AveragePolicyStd[0]  | 0.19056    |
| AveragePolicyStd[1]  | 0.18795    |
| AveragePolicyStd[2]  | 0.1409     |
| AveragePolicyStd[3]  | 0.16113    |
| AveragePolicyStd[4]  | 0.14438    |
| AveragePolicyStd[5]  | 0.22214    |
| AverageReturn        | 1915.2     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 354.87     |
| AverageEpisodeLength | 941.43     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 162.99     |
| TotalNEpisodes       | 26526      |
| TotalNSamples        | 9.4092e+06 |
| ExplainedVariance    | 0.069596   |
-------------------------------------
[2018-01-21 17:27:06.273063 UTC] Saving snapshot
[2018-01-21 17:27:06.273253 UTC] Starting iteration 1880
[2018-01-21 17:27:06.273374 UTC] Start collecting samples
[2018-01-21 17:27:08.717427 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:08.788479 UTC] Performing policy update
[2018-01-21 17:27:08.789105 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:08.865609 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:09.811075 UTC] Performing line search
[2018-01-21 17:27:09.955120 UTC] Updating baseline
[2018-01-21 17:27:11.666777 UTC] Computing logging information
-------------------------------------
| Iteration            | 1880       |
| ExpectedImprovement  | 0.021022   |
| ActualImprovement    | 0.018626   |
| ImprovementRatio     | 0.886      |
| MeanKL               | 0.0069028  |
| Entropy              | -2.0471    |
| Perplexity           | 0.12911    |
| AveragePolicyStd     | 0.17432    |
| AveragePolicyStd[0]  | 0.19019    |
| AveragePolicyStd[1]  | 0.18802    |
| AveragePolicyStd[2]  | 0.1408     |
| AveragePolicyStd[3]  | 0.16081    |
| AveragePolicyStd[4]  | 0.14434    |
| AveragePolicyStd[5]  | 0.22175    |
| AverageReturn        | 1921.1     |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 350.62     |
| AverageEpisodeLength | 944.48     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.2      |
| TotalNEpisodes       | 26528      |
| TotalNSamples        | 9.4112e+06 |
| ExplainedVariance    | -0.025717  |
-------------------------------------
[2018-01-21 17:27:12.196215 UTC] Saving snapshot
[2018-01-21 17:27:12.201743 UTC] Starting iteration 1881
[2018-01-21 17:27:12.201903 UTC] Start collecting samples
[2018-01-21 17:27:14.547696 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:14.620696 UTC] Performing policy update
[2018-01-21 17:27:14.621274 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:14.697689 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:15.613880 UTC] Performing line search
[2018-01-21 17:27:15.737189 UTC] Updating baseline
[2018-01-21 17:27:17.744814 UTC] Computing logging information
-------------------------------------
| Iteration            | 1881       |
| ExpectedImprovement  | 0.018234   |
| ActualImprovement    | 0.016842   |
| ImprovementRatio     | 0.92366    |
| MeanKL               | 0.0084347  |
| Entropy              | -2.0485    |
| Perplexity           | 0.12893    |
| AveragePolicyStd     | 0.17427    |
| AveragePolicyStd[0]  | 0.1904     |
| AveragePolicyStd[1]  | 0.18809    |
| AveragePolicyStd[2]  | 0.14072    |
| AveragePolicyStd[3]  | 0.16082    |
| AveragePolicyStd[4]  | 0.14426    |
| AveragePolicyStd[5]  | 0.22135    |
| AverageReturn        | 1921       |
| MinReturn            | 367.76     |
| MaxReturn            | 2114.5     |
| StdReturn            | 350.67     |
| AverageEpisodeLength | 944.48     |
| MinEpisodeLength     | 229        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.2      |
| TotalNEpisodes       | 26533      |
| TotalNSamples        | 9.4162e+06 |
| ExplainedVariance    | -0.019683  |
-------------------------------------
[2018-01-21 17:27:18.274937 UTC] Saving snapshot
[2018-01-21 17:27:18.275184 UTC] Starting iteration 1882
[2018-01-21 17:27:18.275391 UTC] Start collecting samples
[2018-01-21 17:27:20.696189 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:20.771895 UTC] Performing policy update
[2018-01-21 17:27:20.772402 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:20.846926 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:21.733348 UTC] Performing line search
[2018-01-21 17:27:21.849683 UTC] Updating baseline
[2018-01-21 17:27:22.869036 UTC] Computing logging information
-------------------------------------
| Iteration            | 1882       |
| ExpectedImprovement  | 0.018484   |
| ActualImprovement    | 0.01763    |
| ImprovementRatio     | 0.95383    |
| MeanKL               | 0.0095288  |
| Entropy              | -2.0548    |
| Perplexity           | 0.12811    |
| AveragePolicyStd     | 0.17411    |
| AveragePolicyStd[0]  | 0.19045    |
| AveragePolicyStd[1]  | 0.18778    |
| AveragePolicyStd[2]  | 0.1405     |
| AveragePolicyStd[3]  | 0.16077    |
| AveragePolicyStd[4]  | 0.14378    |
| AveragePolicyStd[5]  | 0.2214     |
| AverageReturn        | 1907.6     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 380.62     |
| AverageEpisodeLength | 939.71     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.04     |
| TotalNEpisodes       | 26542      |
| TotalNSamples        | 9.4243e+06 |
| ExplainedVariance    | 0.04971    |
-------------------------------------
[2018-01-21 17:27:23.385721 UTC] Saving snapshot
[2018-01-21 17:27:23.385918 UTC] Starting iteration 1883
[2018-01-21 17:27:23.386058 UTC] Start collecting samples
[2018-01-21 17:27:25.826804 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:25.903270 UTC] Performing policy update
[2018-01-21 17:27:25.903921 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:25.995660 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:26.911483 UTC] Performing line search
[2018-01-21 17:27:27.032650 UTC] Updating baseline
[2018-01-21 17:27:28.228532 UTC] Computing logging information
------------------------------------
| Iteration            | 1883      |
| ExpectedImprovement  | 0.019102  |
| ActualImprovement    | 0.018525  |
| ImprovementRatio     | 0.96979   |
| MeanKL               | 0.0087082 |
| Entropy              | -2.0478   |
| Perplexity           | 0.12902   |
| AveragePolicyStd     | 0.17431   |
| AveragePolicyStd[0]  | 0.18992   |
| AveragePolicyStd[1]  | 0.18855   |
| AveragePolicyStd[2]  | 0.14081   |
| AveragePolicyStd[3]  | 0.16122   |
| AveragePolicyStd[4]  | 0.1437    |
| AveragePolicyStd[5]  | 0.22168   |
| AverageReturn        | 1877.6    |
| MinReturn            | 232.15    |
| MaxReturn            | 2093.5    |
| StdReturn            | 424.09    |
| AverageEpisodeLength | 926.18    |
| MinEpisodeLength     | 142       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 197.36    |
| TotalNEpisodes       | 26546     |
| TotalNSamples        | 9.427e+06 |
| ExplainedVariance    | 0.36241   |
------------------------------------
[2018-01-21 17:27:28.761045 UTC] Saving snapshot
[2018-01-21 17:27:28.761249 UTC] Starting iteration 1884
[2018-01-21 17:27:28.761390 UTC] Start collecting samples
[2018-01-21 17:27:31.143760 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:31.228475 UTC] Performing policy update
[2018-01-21 17:27:31.229051 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:31.307436 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:32.199106 UTC] Performing line search
[2018-01-21 17:27:32.317108 UTC] Updating baseline
[2018-01-21 17:27:33.611919 UTC] Computing logging information
-------------------------------------
| Iteration            | 1884       |
| ExpectedImprovement  | 0.019262   |
| ActualImprovement    | 0.017505   |
| ImprovementRatio     | 0.90879    |
| MeanKL               | 0.0082079  |
| Entropy              | -2.0511    |
| Perplexity           | 0.12859    |
| AveragePolicyStd     | 0.17424    |
| AveragePolicyStd[0]  | 0.18949    |
| AveragePolicyStd[1]  | 0.18846    |
| AveragePolicyStd[2]  | 0.14051    |
| AveragePolicyStd[3]  | 0.16095    |
| AveragePolicyStd[4]  | 0.14385    |
| AveragePolicyStd[5]  | 0.22216    |
| AverageReturn        | 1876.6     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 423.77     |
| AverageEpisodeLength | 926.18     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 197.36     |
| TotalNEpisodes       | 26550      |
| TotalNSamples        | 9.431e+06  |
| ExplainedVariance    | -0.0070915 |
-------------------------------------
[2018-01-21 17:27:34.133732 UTC] Saving snapshot
[2018-01-21 17:27:34.133925 UTC] Starting iteration 1885
[2018-01-21 17:27:34.134085 UTC] Start collecting samples
[2018-01-21 17:27:36.660120 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:36.739205 UTC] Performing policy update
[2018-01-21 17:27:36.739823 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:36.816152 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:37.768150 UTC] Performing line search
[2018-01-21 17:27:37.906274 UTC] Updating baseline
[2018-01-21 17:27:39.341987 UTC] Computing logging information
-------------------------------------
| Iteration            | 1885       |
| ExpectedImprovement  | 0.017144   |
| ActualImprovement    | 0.016321   |
| ImprovementRatio     | 0.95203    |
| MeanKL               | 0.0094837  |
| Entropy              | -2.0574    |
| Perplexity           | 0.12779    |
| AveragePolicyStd     | 0.17408    |
| AveragePolicyStd[0]  | 0.18928    |
| AveragePolicyStd[1]  | 0.18894    |
| AveragePolicyStd[2]  | 0.14043    |
| AveragePolicyStd[3]  | 0.16066    |
| AveragePolicyStd[4]  | 0.1433     |
| AveragePolicyStd[5]  | 0.22185    |
| AverageReturn        | 1908.9     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 381.12     |
| AverageEpisodeLength | 941.12     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 177.89     |
| TotalNEpisodes       | 26558      |
| TotalNSamples        | 9.439e+06  |
| ExplainedVariance    | -0.0049903 |
-------------------------------------
[2018-01-21 17:27:39.860319 UTC] Saving snapshot
[2018-01-21 17:27:39.860506 UTC] Starting iteration 1886
[2018-01-21 17:27:39.860621 UTC] Start collecting samples
[2018-01-21 17:27:42.167191 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:42.240462 UTC] Performing policy update
[2018-01-21 17:27:42.241022 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:42.318049 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:43.223458 UTC] Performing line search
[2018-01-21 17:27:43.340524 UTC] Updating baseline
[2018-01-21 17:27:45.158765 UTC] Computing logging information
-------------------------------------
| Iteration            | 1886       |
| ExpectedImprovement  | 0.01728    |
| ActualImprovement    | 0.016053   |
| ImprovementRatio     | 0.929      |
| MeanKL               | 0.0085384  |
| Entropy              | -2.057     |
| Perplexity           | 0.12784    |
| AveragePolicyStd     | 0.17409    |
| AveragePolicyStd[0]  | 0.18946    |
| AveragePolicyStd[1]  | 0.18932    |
| AveragePolicyStd[2]  | 0.14023    |
| AveragePolicyStd[3]  | 0.16032    |
| AveragePolicyStd[4]  | 0.14351    |
| AveragePolicyStd[5]  | 0.22173    |
| AverageReturn        | 1913.2     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 374.88     |
| AverageEpisodeLength | 943.61     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.83     |
| TotalNEpisodes       | 26563      |
| TotalNSamples        | 9.4436e+06 |
| ExplainedVariance    | 0.090416   |
-------------------------------------
[2018-01-21 17:27:45.692233 UTC] Saving snapshot
[2018-01-21 17:27:45.692429 UTC] Starting iteration 1887
[2018-01-21 17:27:45.692569 UTC] Start collecting samples
[2018-01-21 17:27:47.956874 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:48.028390 UTC] Performing policy update
[2018-01-21 17:27:48.028931 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:48.113002 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:49.029427 UTC] Performing line search
[2018-01-21 17:27:49.158701 UTC] Updating baseline
[2018-01-21 17:27:50.469236 UTC] Computing logging information
-------------------------------------
| Iteration            | 1887       |
| ExpectedImprovement  | 0.017621   |
| ActualImprovement    | 0.01633    |
| ImprovementRatio     | 0.92669    |
| MeanKL               | 0.0086903  |
| Entropy              | -2.0583    |
| Perplexity           | 0.12767    |
| AveragePolicyStd     | 0.17404    |
| AveragePolicyStd[0]  | 0.18903    |
| AveragePolicyStd[1]  | 0.18876    |
| AveragePolicyStd[2]  | 0.14024    |
| AveragePolicyStd[3]  | 0.16077    |
| AveragePolicyStd[4]  | 0.14353    |
| AveragePolicyStd[5]  | 0.22192    |
| AverageReturn        | 1913.8     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 375.08     |
| AverageEpisodeLength | 943.61     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.83     |
| TotalNEpisodes       | 26566      |
| TotalNSamples        | 9.4466e+06 |
| ExplainedVariance    | -0.016272  |
-------------------------------------
[2018-01-21 17:27:50.998385 UTC] Saving snapshot
[2018-01-21 17:27:50.998622 UTC] Starting iteration 1888
[2018-01-21 17:27:50.998761 UTC] Start collecting samples
[2018-01-21 17:27:53.568845 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:53.645865 UTC] Performing policy update
[2018-01-21 17:27:53.646424 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:53.750741 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:27:54.656088 UTC] Performing line search
[2018-01-21 17:27:54.774506 UTC] Updating baseline
[2018-01-21 17:27:56.259869 UTC] Computing logging information
-------------------------------------
| Iteration            | 1888       |
| ExpectedImprovement  | 0.017351   |
| ActualImprovement    | 0.015919   |
| ImprovementRatio     | 0.91745    |
| MeanKL               | 0.0085023  |
| Entropy              | -2.0562    |
| Perplexity           | 0.12794    |
| AveragePolicyStd     | 0.17408    |
| AveragePolicyStd[0]  | 0.18875    |
| AveragePolicyStd[1]  | 0.18882    |
| AveragePolicyStd[2]  | 0.14061    |
| AveragePolicyStd[3]  | 0.16085    |
| AveragePolicyStd[4]  | 0.14357    |
| AveragePolicyStd[5]  | 0.2219     |
| AverageReturn        | 1913.1     |
| MinReturn            | 232.15     |
| MaxReturn            | 2093.5     |
| StdReturn            | 374.83     |
| AverageEpisodeLength | 943.73     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 174.86     |
| TotalNEpisodes       | 26570      |
| TotalNSamples        | 9.4506e+06 |
| ExplainedVariance    | 0.0048141  |
-------------------------------------
[2018-01-21 17:27:56.782916 UTC] Saving snapshot
[2018-01-21 17:27:56.783110 UTC] Starting iteration 1889
[2018-01-21 17:27:56.783277 UTC] Start collecting samples
[2018-01-21 17:27:59.371996 UTC] Computing input variables for policy optimization
[2018-01-21 17:27:59.445351 UTC] Performing policy update
[2018-01-21 17:27:59.445879 UTC] Computing gradient in Euclidean space
[2018-01-21 17:27:59.521438 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:00.530117 UTC] Performing line search
[2018-01-21 17:28:00.652585 UTC] Updating baseline
[2018-01-21 17:28:02.452653 UTC] Computing logging information
-------------------------------------
| Iteration            | 1889       |
| ExpectedImprovement  | 0.015593   |
| ActualImprovement    | 0.014716   |
| ImprovementRatio     | 0.94373    |
| MeanKL               | 0.0085353  |
| Entropy              | -2.0589    |
| Perplexity           | 0.12759    |
| AveragePolicyStd     | 0.17398    |
| AveragePolicyStd[0]  | 0.18898    |
| AveragePolicyStd[1]  | 0.18868    |
| AveragePolicyStd[2]  | 0.14066    |
| AveragePolicyStd[3]  | 0.16087    |
| AveragePolicyStd[4]  | 0.14348    |
| AveragePolicyStd[5]  | 0.22122    |
| AverageReturn        | 1919.8     |
| MinReturn            | 232.15     |
| MaxReturn            | 2087.3     |
| StdReturn            | 362.18     |
| AverageEpisodeLength | 948.51     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.72     |
| TotalNEpisodes       | 26578      |
| TotalNSamples        | 9.4586e+06 |
| ExplainedVariance    | -0.0022578 |
-------------------------------------
[2018-01-21 17:28:02.975440 UTC] Saving snapshot
[2018-01-21 17:28:02.975636 UTC] Starting iteration 1890
[2018-01-21 17:28:02.975761 UTC] Start collecting samples
[2018-01-21 17:28:05.425315 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:05.497496 UTC] Performing policy update
[2018-01-21 17:28:05.498045 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:05.572804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:06.520642 UTC] Performing line search
[2018-01-21 17:28:06.642950 UTC] Updating baseline
[2018-01-21 17:28:07.836186 UTC] Computing logging information
-------------------------------------
| Iteration            | 1890       |
| ExpectedImprovement  | 0.01984    |
| ActualImprovement    | 0.017975   |
| ImprovementRatio     | 0.90596    |
| MeanKL               | 0.0081485  |
| Entropy              | -2.0597    |
| Perplexity           | 0.1275     |
| AveragePolicyStd     | 0.17396    |
| AveragePolicyStd[0]  | 0.18933    |
| AveragePolicyStd[1]  | 0.18875    |
| AveragePolicyStd[2]  | 0.14054    |
| AveragePolicyStd[3]  | 0.16072    |
| AveragePolicyStd[4]  | 0.14355    |
| AveragePolicyStd[5]  | 0.22085    |
| AverageReturn        | 1924.1     |
| MinReturn            | 232.15     |
| MaxReturn            | 2087.3     |
| StdReturn            | 359.47     |
| AverageEpisodeLength | 950.97     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.67     |
| TotalNEpisodes       | 26582      |
| TotalNSamples        | 9.4626e+06 |
| ExplainedVariance    | 0.0035707  |
-------------------------------------
[2018-01-21 17:28:08.358002 UTC] Saving snapshot
[2018-01-21 17:28:08.363757 UTC] Starting iteration 1891
[2018-01-21 17:28:08.363927 UTC] Start collecting samples
[2018-01-21 17:28:10.623063 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:10.695899 UTC] Performing policy update
[2018-01-21 17:28:10.696372 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:10.771061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:11.662402 UTC] Performing line search
[2018-01-21 17:28:11.779520 UTC] Updating baseline
[2018-01-21 17:28:14.450445 UTC] Computing logging information
-------------------------------------
| Iteration            | 1891       |
| ExpectedImprovement  | 0.015807   |
| ActualImprovement    | 0.014884   |
| ImprovementRatio     | 0.94162    |
| MeanKL               | 0.0081218  |
| Entropy              | -2.0637    |
| Perplexity           | 0.12699    |
| AveragePolicyStd     | 0.17385    |
| AveragePolicyStd[0]  | 0.18916    |
| AveragePolicyStd[1]  | 0.18844    |
| AveragePolicyStd[2]  | 0.14029    |
| AveragePolicyStd[3]  | 0.16056    |
| AveragePolicyStd[4]  | 0.14358    |
| AveragePolicyStd[5]  | 0.22109    |
| AverageReturn        | 1923.8     |
| MinReturn            | 232.15     |
| MaxReturn            | 2087.3     |
| StdReturn            | 359.36     |
| AverageEpisodeLength | 950.97     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.67     |
| TotalNEpisodes       | 26584      |
| TotalNSamples        | 9.4646e+06 |
| ExplainedVariance    | 0.0049569  |
-------------------------------------
[2018-01-21 17:28:14.970181 UTC] Saving snapshot
[2018-01-21 17:28:14.970372 UTC] Starting iteration 1892
[2018-01-21 17:28:14.970529 UTC] Start collecting samples
[2018-01-21 17:28:17.694323 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:17.768931 UTC] Performing policy update
[2018-01-21 17:28:17.769458 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:17.843717 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:18.762819 UTC] Performing line search
[2018-01-21 17:28:18.879229 UTC] Updating baseline
[2018-01-21 17:28:20.134091 UTC] Computing logging information
-------------------------------------
| Iteration            | 1892       |
| ExpectedImprovement  | 0.01767    |
| ActualImprovement    | 0.016667   |
| ImprovementRatio     | 0.94326    |
| MeanKL               | 0.0088233  |
| Entropy              | -2.0664    |
| Perplexity           | 0.12664    |
| AveragePolicyStd     | 0.17376    |
| AveragePolicyStd[0]  | 0.18859    |
| AveragePolicyStd[1]  | 0.18775    |
| AveragePolicyStd[2]  | 0.14054    |
| AveragePolicyStd[3]  | 0.16077    |
| AveragePolicyStd[4]  | 0.1435     |
| AveragePolicyStd[5]  | 0.22141    |
| AverageReturn        | 1922.1     |
| MinReturn            | 232.15     |
| MaxReturn            | 2083.7     |
| StdReturn            | 357.92     |
| AverageEpisodeLength | 952.28     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 168.53     |
| TotalNEpisodes       | 26593      |
| TotalNSamples        | 9.4736e+06 |
| ExplainedVariance    | -0.0034439 |
-------------------------------------
[2018-01-21 17:28:20.680901 UTC] Saving snapshot
[2018-01-21 17:28:20.681120 UTC] Starting iteration 1893
[2018-01-21 17:28:20.681282 UTC] Start collecting samples
[2018-01-21 17:28:23.112208 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:23.187748 UTC] Performing policy update
[2018-01-21 17:28:23.188296 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:23.263203 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:24.153084 UTC] Performing line search
[2018-01-21 17:28:24.273280 UTC] Updating baseline
[2018-01-21 17:28:25.670644 UTC] Computing logging information
-------------------------------------
| Iteration            | 1893       |
| ExpectedImprovement  | 0.017603   |
| ActualImprovement    | 0.016399   |
| ImprovementRatio     | 0.9316     |
| MeanKL               | 0.0098813  |
| Entropy              | -2.0648    |
| Perplexity           | 0.12684    |
| AveragePolicyStd     | 0.17381    |
| AveragePolicyStd[0]  | 0.18879    |
| AveragePolicyStd[1]  | 0.18743    |
| AveragePolicyStd[2]  | 0.14064    |
| AveragePolicyStd[3]  | 0.16122    |
| AveragePolicyStd[4]  | 0.14322    |
| AveragePolicyStd[5]  | 0.22154    |
| AverageReturn        | 1957       |
| MinReturn            | 232.15     |
| MaxReturn            | 2083.7     |
| StdReturn            | 302.05     |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26598      |
| TotalNSamples        | 9.4786e+06 |
| ExplainedVariance    | -0.0017462 |
-------------------------------------
[2018-01-21 17:28:26.297966 UTC] Saving snapshot
[2018-01-21 17:28:26.298157 UTC] Starting iteration 1894
[2018-01-21 17:28:26.298329 UTC] Start collecting samples
[2018-01-21 17:28:28.942692 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:29.014775 UTC] Performing policy update
[2018-01-21 17:28:29.015316 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:29.090213 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:29.995639 UTC] Performing line search
[2018-01-21 17:28:30.114664 UTC] Updating baseline
[2018-01-21 17:28:31.487531 UTC] Computing logging information
-------------------------------------
| Iteration            | 1894       |
| ExpectedImprovement  | 0.017545   |
| ActualImprovement    | 0.0156     |
| ImprovementRatio     | 0.88911    |
| MeanKL               | 0.0099534  |
| Entropy              | -2.0628    |
| Perplexity           | 0.12709    |
| AveragePolicyStd     | 0.17388    |
| AveragePolicyStd[0]  | 0.18861    |
| AveragePolicyStd[1]  | 0.18792    |
| AveragePolicyStd[2]  | 0.14095    |
| AveragePolicyStd[3]  | 0.16096    |
| AveragePolicyStd[4]  | 0.14307    |
| AveragePolicyStd[5]  | 0.22176    |
| AverageReturn        | 1956.3     |
| MinReturn            | 232.15     |
| MaxReturn            | 2083.7     |
| StdReturn            | 301.9      |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26600      |
| TotalNSamples        | 9.4806e+06 |
| ExplainedVariance    | 0.0037147  |
-------------------------------------
[2018-01-21 17:28:32.009697 UTC] Saving snapshot
[2018-01-21 17:28:32.009900 UTC] Starting iteration 1895
[2018-01-21 17:28:32.010042 UTC] Start collecting samples
[2018-01-21 17:28:34.778867 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:34.853753 UTC] Performing policy update
[2018-01-21 17:28:34.854261 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:34.929722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:35.818617 UTC] Performing line search
[2018-01-21 17:28:35.937463 UTC] Updating baseline
[2018-01-21 17:28:37.134879 UTC] Computing logging information
-------------------------------------
| Iteration            | 1895       |
| ExpectedImprovement  | 0.017307   |
| ActualImprovement    | 0.016451   |
| ImprovementRatio     | 0.95058    |
| MeanKL               | 0.0089039  |
| Entropy              | -2.0653    |
| Perplexity           | 0.12679    |
| AveragePolicyStd     | 0.17379    |
| AveragePolicyStd[0]  | 0.18861    |
| AveragePolicyStd[1]  | 0.18762    |
| AveragePolicyStd[2]  | 0.14091    |
| AveragePolicyStd[3]  | 0.1608     |
| AveragePolicyStd[4]  | 0.14328    |
| AveragePolicyStd[5]  | 0.22151    |
| AverageReturn        | 1955.2     |
| MinReturn            | 232.15     |
| MaxReturn            | 2083.7     |
| StdReturn            | 301.54     |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26608      |
| TotalNSamples        | 9.4886e+06 |
| ExplainedVariance    | 0.001095   |
-------------------------------------
[2018-01-21 17:28:37.662515 UTC] Saving snapshot
[2018-01-21 17:28:37.662706 UTC] Starting iteration 1896
[2018-01-21 17:28:37.662840 UTC] Start collecting samples
[2018-01-21 17:28:40.064349 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:40.137131 UTC] Performing policy update
[2018-01-21 17:28:40.137674 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:40.217646 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:41.152689 UTC] Performing line search
[2018-01-21 17:28:41.280769 UTC] Updating baseline
[2018-01-21 17:28:42.553041 UTC] Computing logging information
-------------------------------------
| Iteration            | 1896       |
| ExpectedImprovement  | 0.017297   |
| ActualImprovement    | 0.016253   |
| ImprovementRatio     | 0.93961    |
| MeanKL               | 0.008173   |
| Entropy              | -2.0664    |
| Perplexity           | 0.12664    |
| AveragePolicyStd     | 0.17376    |
| AveragePolicyStd[0]  | 0.18863    |
| AveragePolicyStd[1]  | 0.18743    |
| AveragePolicyStd[2]  | 0.14085    |
| AveragePolicyStd[3]  | 0.16067    |
| AveragePolicyStd[4]  | 0.14329    |
| AveragePolicyStd[5]  | 0.22172    |
| AverageReturn        | 1954       |
| MinReturn            | 232.15     |
| MaxReturn            | 2080.2     |
| StdReturn            | 301.18     |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26612      |
| TotalNSamples        | 9.4926e+06 |
| ExplainedVariance    | 0.0040368  |
-------------------------------------
[2018-01-21 17:28:43.072733 UTC] Saving snapshot
[2018-01-21 17:28:43.072929 UTC] Starting iteration 1897
[2018-01-21 17:28:43.073082 UTC] Start collecting samples
[2018-01-21 17:28:45.373242 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:45.443681 UTC] Performing policy update
[2018-01-21 17:28:45.444242 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:45.518869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:46.414538 UTC] Performing line search
[2018-01-21 17:28:46.537150 UTC] Updating baseline
[2018-01-21 17:28:50.844789 UTC] Computing logging information
-------------------------------------
| Iteration            | 1897       |
| ExpectedImprovement  | 0.017735   |
| ActualImprovement    | 0.016174   |
| ImprovementRatio     | 0.91193    |
| MeanKL               | 0.0087218  |
| Entropy              | -2.0696    |
| Perplexity           | 0.12624    |
| AveragePolicyStd     | 0.17366    |
| AveragePolicyStd[0]  | 0.18827    |
| AveragePolicyStd[1]  | 0.18732    |
| AveragePolicyStd[2]  | 0.14109    |
| AveragePolicyStd[3]  | 0.16042    |
| AveragePolicyStd[4]  | 0.14318    |
| AveragePolicyStd[5]  | 0.2217     |
| AverageReturn        | 1954.1     |
| MinReturn            | 232.15     |
| MaxReturn            | 2080.2     |
| StdReturn            | 301.17     |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26615      |
| TotalNSamples        | 9.4956e+06 |
| ExplainedVariance    | 0.0086936  |
-------------------------------------
[2018-01-21 17:28:51.372719 UTC] Saving snapshot
[2018-01-21 17:28:51.372932 UTC] Starting iteration 1898
[2018-01-21 17:28:51.373080 UTC] Start collecting samples
[2018-01-21 17:28:53.743025 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:53.817491 UTC] Performing policy update
[2018-01-21 17:28:53.818115 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:53.894833 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:28:54.800049 UTC] Performing line search
[2018-01-21 17:28:54.927601 UTC] Updating baseline
[2018-01-21 17:28:56.842697 UTC] Computing logging information
-------------------------------------
| Iteration            | 1898       |
| ExpectedImprovement  | 0.018434   |
| ActualImprovement    | 0.017599   |
| ImprovementRatio     | 0.95467    |
| MeanKL               | 0.0079697  |
| Entropy              | -2.0724    |
| Perplexity           | 0.12589    |
| AveragePolicyStd     | 0.17361    |
| AveragePolicyStd[0]  | 0.18819    |
| AveragePolicyStd[1]  | 0.18761    |
| AveragePolicyStd[2]  | 0.14059    |
| AveragePolicyStd[3]  | 0.16057    |
| AveragePolicyStd[4]  | 0.14303    |
| AveragePolicyStd[5]  | 0.22166    |
| AverageReturn        | 1951.9     |
| MinReturn            | 232.15     |
| MaxReturn            | 2080.2     |
| StdReturn            | 300.64     |
| AverageEpisodeLength | 968.58     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 142.57     |
| TotalNEpisodes       | 26623      |
| TotalNSamples        | 9.5036e+06 |
| ExplainedVariance    | -0.0023943 |
-------------------------------------
[2018-01-21 17:28:57.364194 UTC] Saving snapshot
[2018-01-21 17:28:57.364395 UTC] Starting iteration 1899
[2018-01-21 17:28:57.364562 UTC] Start collecting samples
[2018-01-21 17:28:59.716651 UTC] Computing input variables for policy optimization
[2018-01-21 17:28:59.795046 UTC] Performing policy update
[2018-01-21 17:28:59.795627 UTC] Computing gradient in Euclidean space
[2018-01-21 17:28:59.870787 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:00.768589 UTC] Performing line search
[2018-01-21 17:29:00.886128 UTC] Updating baseline
[2018-01-21 17:29:03.137837 UTC] Computing logging information
-------------------------------------
| Iteration            | 1899       |
| ExpectedImprovement  | 0.017311   |
| ActualImprovement    | 0.016148   |
| ImprovementRatio     | 0.93279    |
| MeanKL               | 0.0095795  |
| Entropy              | -2.0719    |
| Perplexity           | 0.12595    |
| AveragePolicyStd     | 0.17362    |
| AveragePolicyStd[0]  | 0.18826    |
| AveragePolicyStd[1]  | 0.18781    |
| AveragePolicyStd[2]  | 0.14048    |
| AveragePolicyStd[3]  | 0.16014    |
| AveragePolicyStd[4]  | 0.14335    |
| AveragePolicyStd[5]  | 0.22171    |
| AverageReturn        | 1963.6     |
| MinReturn            | 232.15     |
| MaxReturn            | 2080.2     |
| StdReturn            | 279.15     |
| AverageEpisodeLength | 974.22     |
| MinEpisodeLength     | 142        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.17     |
| TotalNEpisodes       | 26627      |
| TotalNSamples        | 9.5076e+06 |
| ExplainedVariance    | 0.0027172  |
-------------------------------------
[2018-01-21 17:29:03.665815 UTC] Saving snapshot
[2018-01-21 17:29:03.666017 UTC] Starting iteration 1900
[2018-01-21 17:29:03.666176 UTC] Start collecting samples
[2018-01-21 17:29:06.029705 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:06.106869 UTC] Performing policy update
[2018-01-21 17:29:06.107402 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:06.185737 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:07.086468 UTC] Performing line search
[2018-01-21 17:29:07.211007 UTC] Updating baseline
[2018-01-21 17:29:08.571445 UTC] Computing logging information
------------------------------------
| Iteration            | 1900      |
| ExpectedImprovement  | 0.018652  |
| ActualImprovement    | 0.017543  |
| ImprovementRatio     | 0.94054   |
| MeanKL               | 0.0075233 |
| Entropy              | -2.0741   |
| Perplexity           | 0.12567   |
| AveragePolicyStd     | 0.17359   |
| AveragePolicyStd[0]  | 0.18836   |
| AveragePolicyStd[1]  | 0.1883    |
| AveragePolicyStd[2]  | 0.14016   |
| AveragePolicyStd[3]  | 0.15973   |
| AveragePolicyStd[4]  | 0.14326   |
| AveragePolicyStd[5]  | 0.22174   |
| AverageReturn        | 1928.2    |
| MinReturn            | 232.15    |
| MaxReturn            | 2080.2    |
| StdReturn            | 342.2     |
| AverageEpisodeLength | 957.69    |
| MinEpisodeLength     | 142       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 161.48    |
| TotalNEpisodes       | 26633     |
| TotalNSamples        | 9.512e+06 |
| ExplainedVariance    | 0.23148   |
------------------------------------
[2018-01-21 17:29:09.132070 UTC] Saving snapshot
[2018-01-21 17:29:09.138607 UTC] Starting iteration 1901
[2018-01-21 17:29:09.138776 UTC] Start collecting samples
[2018-01-21 17:29:11.495261 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:11.573454 UTC] Performing policy update
[2018-01-21 17:29:11.574016 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:11.650011 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:12.557935 UTC] Performing line search
[2018-01-21 17:29:12.677139 UTC] Updating baseline
[2018-01-21 17:29:14.044750 UTC] Computing logging information
------------------------------------
| Iteration            | 1901      |
| ExpectedImprovement  | 0.016458  |
| ActualImprovement    | 0.015754  |
| ImprovementRatio     | 0.95723   |
| MeanKL               | 0.0075104 |
| Entropy              | -2.0751   |
| Perplexity           | 0.12555   |
| AveragePolicyStd     | 0.17355   |
| AveragePolicyStd[0]  | 0.18788   |
| AveragePolicyStd[1]  | 0.18841   |
| AveragePolicyStd[2]  | 0.14017   |
| AveragePolicyStd[3]  | 0.1596    |
| AveragePolicyStd[4]  | 0.14347   |
| AveragePolicyStd[5]  | 0.22178   |
| AverageReturn        | 1945.9    |
| MinReturn            | 356.2     |
| MaxReturn            | 2080.2    |
| StdReturn            | 296.89    |
| AverageEpisodeLength | 966.27    |
| MinEpisodeLength     | 207       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 139.17    |
| TotalNEpisodes       | 26640     |
| TotalNSamples        | 9.519e+06 |
| ExplainedVariance    | -0.037713 |
------------------------------------
[2018-01-21 17:29:14.622122 UTC] Saving snapshot
[2018-01-21 17:29:14.622318 UTC] Starting iteration 1902
[2018-01-21 17:29:14.622481 UTC] Start collecting samples
[2018-01-21 17:29:17.101073 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:17.176716 UTC] Performing policy update
[2018-01-21 17:29:17.177250 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:17.251445 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:18.196543 UTC] Performing line search
[2018-01-21 17:29:18.335123 UTC] Updating baseline
[2018-01-21 17:29:20.244654 UTC] Computing logging information
------------------------------------
| Iteration            | 1902      |
| ExpectedImprovement  | 0.017048  |
| ActualImprovement    | 0.015884  |
| ImprovementRatio     | 0.93167   |
| MeanKL               | 0.0084126 |
| Entropy              | -2.0844   |
| Perplexity           | 0.12438   |
| AveragePolicyStd     | 0.1733    |
| AveragePolicyStd[0]  | 0.18752   |
| AveragePolicyStd[1]  | 0.18818   |
| AveragePolicyStd[2]  | 0.13985   |
| AveragePolicyStd[3]  | 0.15924   |
| AveragePolicyStd[4]  | 0.14326   |
| AveragePolicyStd[5]  | 0.22175   |
| AverageReturn        | 1946      |
| MinReturn            | 356.2     |
| MaxReturn            | 2080.2    |
| StdReturn            | 296.95    |
| AverageEpisodeLength | 966.27    |
| MinEpisodeLength     | 207       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 139.17    |
| TotalNEpisodes       | 26643     |
| TotalNSamples        | 9.522e+06 |
| ExplainedVariance    | 0.0038195 |
------------------------------------
[2018-01-21 17:29:20.766169 UTC] Saving snapshot
[2018-01-21 17:29:20.766362 UTC] Starting iteration 1903
[2018-01-21 17:29:20.766516 UTC] Start collecting samples
[2018-01-21 17:29:23.466105 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:23.539832 UTC] Performing policy update
[2018-01-21 17:29:23.540363 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:23.615502 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:24.541430 UTC] Performing line search
[2018-01-21 17:29:24.659547 UTC] Updating baseline
[2018-01-21 17:29:26.178696 UTC] Computing logging information
-------------------------------------
| Iteration            | 1903       |
| ExpectedImprovement  | 0.018234   |
| ActualImprovement    | 0.017149   |
| ImprovementRatio     | 0.94049    |
| MeanKL               | 0.0081846  |
| Entropy              | -2.0852    |
| Perplexity           | 0.12428    |
| AveragePolicyStd     | 0.17327    |
| AveragePolicyStd[0]  | 0.1875     |
| AveragePolicyStd[1]  | 0.18793    |
| AveragePolicyStd[2]  | 0.14016    |
| AveragePolicyStd[3]  | 0.1592     |
| AveragePolicyStd[4]  | 0.14308    |
| AveragePolicyStd[5]  | 0.22175    |
| AverageReturn        | 1964.5     |
| MinReturn            | 433.04     |
| MaxReturn            | 2080.2     |
| StdReturn            | 241.79     |
| AverageEpisodeLength | 974.92     |
| MinEpisodeLength     | 266        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.28     |
| TotalNEpisodes       | 26650      |
| TotalNSamples        | 9.5285e+06 |
| ExplainedVariance    | 0.075319   |
-------------------------------------
[2018-01-21 17:29:26.750791 UTC] Saving snapshot
[2018-01-21 17:29:26.751008 UTC] Starting iteration 1904
[2018-01-21 17:29:26.751161 UTC] Start collecting samples
[2018-01-21 17:29:29.189480 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:29.264233 UTC] Performing policy update
[2018-01-21 17:29:29.264782 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:29.343861 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:30.300278 UTC] Performing line search
[2018-01-21 17:29:30.450273 UTC] Updating baseline
[2018-01-21 17:29:32.066098 UTC] Computing logging information
-------------------------------------
| Iteration            | 1904       |
| ExpectedImprovement  | 0.018007   |
| ActualImprovement    | 0.01654    |
| ImprovementRatio     | 0.91852    |
| MeanKL               | 0.0094262  |
| Entropy              | -2.0872    |
| Perplexity           | 0.12403    |
| AveragePolicyStd     | 0.17319    |
| AveragePolicyStd[0]  | 0.18713    |
| AveragePolicyStd[1]  | 0.18784    |
| AveragePolicyStd[2]  | 0.14001    |
| AveragePolicyStd[3]  | 0.15942    |
| AveragePolicyStd[4]  | 0.14323    |
| AveragePolicyStd[5]  | 0.22151    |
| AverageReturn        | 1964       |
| MinReturn            | 433.04     |
| MaxReturn            | 2080.2     |
| StdReturn            | 241.66     |
| AverageEpisodeLength | 974.92     |
| MinEpisodeLength     | 266        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.28     |
| TotalNEpisodes       | 26653      |
| TotalNSamples        | 9.5315e+06 |
| ExplainedVariance    | 0.0022407  |
-------------------------------------
[2018-01-21 17:29:32.600387 UTC] Saving snapshot
[2018-01-21 17:29:32.600580 UTC] Starting iteration 1905
[2018-01-21 17:29:32.600705 UTC] Start collecting samples
[2018-01-21 17:29:34.989625 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:35.062517 UTC] Performing policy update
[2018-01-21 17:29:35.063040 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:35.140636 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:36.032017 UTC] Performing line search
[2018-01-21 17:29:36.149936 UTC] Updating baseline
[2018-01-21 17:29:37.642984 UTC] Computing logging information
-------------------------------------
| Iteration            | 1905       |
| ExpectedImprovement  | 0.01858    |
| ActualImprovement    | 0.017626   |
| ImprovementRatio     | 0.94866    |
| MeanKL               | 0.0079759  |
| Entropy              | -2.0886    |
| Perplexity           | 0.12386    |
| AveragePolicyStd     | 0.17313    |
| AveragePolicyStd[0]  | 0.18713    |
| AveragePolicyStd[1]  | 0.18797    |
| AveragePolicyStd[2]  | 0.14007    |
| AveragePolicyStd[3]  | 0.15961    |
| AveragePolicyStd[4]  | 0.14306    |
| AveragePolicyStd[5]  | 0.22096    |
| AverageReturn        | 1961.4     |
| MinReturn            | 433.04     |
| MaxReturn            | 2080.2     |
| StdReturn            | 241.19     |
| AverageEpisodeLength | 974.92     |
| MinEpisodeLength     | 266        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 113.28     |
| TotalNEpisodes       | 26659      |
| TotalNSamples        | 9.5375e+06 |
| ExplainedVariance    | -0.006507  |
-------------------------------------
[2018-01-21 17:29:38.161128 UTC] Saving snapshot
[2018-01-21 17:29:38.161342 UTC] Starting iteration 1906
[2018-01-21 17:29:38.161501 UTC] Start collecting samples
[2018-01-21 17:29:40.574729 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:40.650932 UTC] Performing policy update
[2018-01-21 17:29:40.651365 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:40.725964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:41.624276 UTC] Performing line search
[2018-01-21 17:29:41.742678 UTC] Updating baseline
[2018-01-21 17:29:42.952547 UTC] Computing logging information
-------------------------------------
| Iteration            | 1906       |
| ExpectedImprovement  | 0.016018   |
| ActualImprovement    | 0.015421   |
| ImprovementRatio     | 0.96275    |
| MeanKL               | 0.0084499  |
| Entropy              | -2.0846    |
| Perplexity           | 0.12436    |
| AveragePolicyStd     | 0.17327    |
| AveragePolicyStd[0]  | 0.18707    |
| AveragePolicyStd[1]  | 0.18817    |
| AveragePolicyStd[2]  | 0.14016    |
| AveragePolicyStd[3]  | 0.15914    |
| AveragePolicyStd[4]  | 0.14341    |
| AveragePolicyStd[5]  | 0.22167    |
| AverageReturn        | 1945       |
| MinReturn            | 433.04     |
| MaxReturn            | 2080.2     |
| StdReturn            | 280.15     |
| AverageEpisodeLength | 967.54     |
| MinEpisodeLength     | 266        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 132.2      |
| TotalNEpisodes       | 26668      |
| TotalNSamples        | 9.5454e+06 |
| ExplainedVariance    | 0.11533    |
-------------------------------------
[2018-01-21 17:29:43.477530 UTC] Saving snapshot
[2018-01-21 17:29:43.477707 UTC] Starting iteration 1907
[2018-01-21 17:29:43.477840 UTC] Start collecting samples
[2018-01-21 17:29:45.868209 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:45.948373 UTC] Performing policy update
[2018-01-21 17:29:45.948964 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:46.024640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:46.916532 UTC] Performing line search
[2018-01-21 17:29:47.034054 UTC] Updating baseline
[2018-01-21 17:29:48.313937 UTC] Computing logging information
-------------------------------------
| Iteration            | 1907       |
| ExpectedImprovement  | 0.021768   |
| ActualImprovement    | 0.019229   |
| ImprovementRatio     | 0.88335    |
| MeanKL               | 0.007365   |
| Entropy              | -2.0868    |
| Perplexity           | 0.12408    |
| AveragePolicyStd     | 0.17318    |
| AveragePolicyStd[0]  | 0.18682    |
| AveragePolicyStd[1]  | 0.18846    |
| AveragePolicyStd[2]  | 0.1404     |
| AveragePolicyStd[3]  | 0.15881    |
| AveragePolicyStd[4]  | 0.1435     |
| AveragePolicyStd[5]  | 0.22108    |
| AverageReturn        | 1919.1     |
| MinReturn            | 289.36     |
| MaxReturn            | 2080.2     |
| StdReturn            | 333.91     |
| AverageEpisodeLength | 955.23     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.67     |
| TotalNEpisodes       | 26671      |
| TotalNSamples        | 9.5472e+06 |
| ExplainedVariance    | 0.42575    |
-------------------------------------
[2018-01-21 17:29:48.843578 UTC] Saving snapshot
[2018-01-21 17:29:48.843761 UTC] Starting iteration 1908
[2018-01-21 17:29:48.843916 UTC] Start collecting samples
[2018-01-21 17:29:51.299026 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:51.376892 UTC] Performing policy update
[2018-01-21 17:29:51.377418 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:51.455915 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:52.364452 UTC] Performing line search
[2018-01-21 17:29:52.487494 UTC] Updating baseline
[2018-01-21 17:29:53.712141 UTC] Computing logging information
-------------------------------------
| Iteration            | 1908       |
| ExpectedImprovement  | 0.018402   |
| ActualImprovement    | 0.017889   |
| ImprovementRatio     | 0.97212    |
| MeanKL               | 0.0092506  |
| Entropy              | -2.09      |
| Perplexity           | 0.12369    |
| AveragePolicyStd     | 0.17309    |
| AveragePolicyStd[0]  | 0.18685    |
| AveragePolicyStd[1]  | 0.18854    |
| AveragePolicyStd[2]  | 0.14009    |
| AveragePolicyStd[3]  | 0.15865    |
| AveragePolicyStd[4]  | 0.14362    |
| AveragePolicyStd[5]  | 0.22077    |
| AverageReturn        | 1901.4     |
| MinReturn            | 289.36     |
| MaxReturn            | 2080.2     |
| StdReturn            | 351.06     |
| AverageEpisodeLength | 947.14     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.58     |
| TotalNEpisodes       | 26677      |
| TotalNSamples        | 9.5523e+06 |
| ExplainedVariance    | 0.20544    |
-------------------------------------
[2018-01-21 17:29:54.235768 UTC] Saving snapshot
[2018-01-21 17:29:54.235971 UTC] Starting iteration 1909
[2018-01-21 17:29:54.236121 UTC] Start collecting samples
[2018-01-21 17:29:56.550707 UTC] Computing input variables for policy optimization
[2018-01-21 17:29:56.625273 UTC] Performing policy update
[2018-01-21 17:29:56.625802 UTC] Computing gradient in Euclidean space
[2018-01-21 17:29:56.702186 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:29:57.589594 UTC] Performing line search
[2018-01-21 17:29:57.708505 UTC] Updating baseline
[2018-01-21 17:29:59.318010 UTC] Computing logging information
-------------------------------------
| Iteration            | 1909       |
| ExpectedImprovement  | 0.018104   |
| ActualImprovement    | 0.017483   |
| ImprovementRatio     | 0.96567    |
| MeanKL               | 0.0077205  |
| Entropy              | -2.0852    |
| Perplexity           | 0.12428    |
| AveragePolicyStd     | 0.17325    |
| AveragePolicyStd[0]  | 0.18719    |
| AveragePolicyStd[1]  | 0.18866    |
| AveragePolicyStd[2]  | 0.1401     |
| AveragePolicyStd[3]  | 0.15865    |
| AveragePolicyStd[4]  | 0.14359    |
| AveragePolicyStd[5]  | 0.22133    |
| AverageReturn        | 1885.9     |
| MinReturn            | 289.36     |
| MaxReturn            | 2080.2     |
| StdReturn            | 380.67     |
| AverageEpisodeLength | 939.52     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.89     |
| TotalNEpisodes       | 26683      |
| TotalNSamples        | 9.5576e+06 |
| ExplainedVariance    | 0.080825   |
-------------------------------------
[2018-01-21 17:29:59.834709 UTC] Saving snapshot
[2018-01-21 17:29:59.834900 UTC] Starting iteration 1910
[2018-01-21 17:29:59.835049 UTC] Start collecting samples
[2018-01-21 17:30:02.145667 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:02.218255 UTC] Performing policy update
[2018-01-21 17:30:02.218797 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:02.292380 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:03.183184 UTC] Performing line search
[2018-01-21 17:30:03.299837 UTC] Updating baseline
[2018-01-21 17:30:04.698803 UTC] Computing logging information
-------------------------------------
| Iteration            | 1910       |
| ExpectedImprovement  | 0.018269   |
| ActualImprovement    | 0.016944   |
| ImprovementRatio     | 0.92751    |
| MeanKL               | 0.0089534  |
| Entropy              | -2.0877    |
| Perplexity           | 0.12397    |
| AveragePolicyStd     | 0.17318    |
| AveragePolicyStd[0]  | 0.18713    |
| AveragePolicyStd[1]  | 0.18826    |
| AveragePolicyStd[2]  | 0.14018    |
| AveragePolicyStd[3]  | 0.15837    |
| AveragePolicyStd[4]  | 0.1436     |
| AveragePolicyStd[5]  | 0.22156    |
| AverageReturn        | 1869.1     |
| MinReturn            | 289.36     |
| MaxReturn            | 2080.2     |
| StdReturn            | 408.84     |
| AverageEpisodeLength | 931.61     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.9      |
| TotalNEpisodes       | 26689      |
| TotalNSamples        | 9.5628e+06 |
| ExplainedVariance    | 0.11176    |
-------------------------------------
[2018-01-21 17:30:05.273637 UTC] Saving snapshot
[2018-01-21 17:30:05.279780 UTC] Starting iteration 1911
[2018-01-21 17:30:05.279954 UTC] Start collecting samples
[2018-01-21 17:30:07.903966 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:07.977186 UTC] Performing policy update
[2018-01-21 17:30:07.977687 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:08.054627 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:08.972108 UTC] Performing line search
[2018-01-21 17:30:09.089448 UTC] Updating baseline
[2018-01-21 17:30:10.424793 UTC] Computing logging information
-------------------------------------
| Iteration            | 1911       |
| ExpectedImprovement  | 0.015443   |
| ActualImprovement    | 0.014824   |
| ImprovementRatio     | 0.95996    |
| MeanKL               | 0.0088282  |
| Entropy              | -2.0911    |
| Perplexity           | 0.12355    |
| AveragePolicyStd     | 0.1731     |
| AveragePolicyStd[0]  | 0.18684    |
| AveragePolicyStd[1]  | 0.18829    |
| AveragePolicyStd[2]  | 0.13997    |
| AveragePolicyStd[3]  | 0.15837    |
| AveragePolicyStd[4]  | 0.14339    |
| AveragePolicyStd[5]  | 0.22175    |
| AverageReturn        | 1867.8     |
| MinReturn            | 289.36     |
| MaxReturn            | 2080.2     |
| StdReturn            | 408.42     |
| AverageEpisodeLength | 931.61     |
| MinEpisodeLength     | 190        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.9      |
| TotalNEpisodes       | 26694      |
| TotalNSamples        | 9.5678e+06 |
| ExplainedVariance    | -0.0061785 |
-------------------------------------
[2018-01-21 17:30:10.954849 UTC] Saving snapshot
[2018-01-21 17:30:10.955052 UTC] Starting iteration 1912
[2018-01-21 17:30:10.955210 UTC] Start collecting samples
[2018-01-21 17:30:13.549860 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:13.625105 UTC] Performing policy update
[2018-01-21 17:30:13.625675 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:13.700669 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:14.648051 UTC] Performing line search
[2018-01-21 17:30:14.764285 UTC] Updating baseline
[2018-01-21 17:30:16.126245 UTC] Computing logging information
-------------------------------------
| Iteration            | 1912       |
| ExpectedImprovement  | 0.016147   |
| ActualImprovement    | 0.015594   |
| ImprovementRatio     | 0.96576    |
| MeanKL               | 0.0083104  |
| Entropy              | -2.0898    |
| Perplexity           | 0.12371    |
| AveragePolicyStd     | 0.17314    |
| AveragePolicyStd[0]  | 0.1869     |
| AveragePolicyStd[1]  | 0.18869    |
| AveragePolicyStd[2]  | 0.13979    |
| AveragePolicyStd[3]  | 0.15833    |
| AveragePolicyStd[4]  | 0.14362    |
| AveragePolicyStd[5]  | 0.22151    |
| AverageReturn        | 1846.3     |
| MinReturn            | 68.182     |
| MaxReturn            | 2067.9     |
| StdReturn            | 444.92     |
| AverageEpisodeLength | 922.27     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.03     |
| TotalNEpisodes       | 26700      |
| TotalNSamples        | 9.5729e+06 |
| ExplainedVariance    | 0.10341    |
-------------------------------------
[2018-01-21 17:30:16.654213 UTC] Saving snapshot
[2018-01-21 17:30:16.654408 UTC] Starting iteration 1913
[2018-01-21 17:30:16.654567 UTC] Start collecting samples
[2018-01-21 17:30:19.176872 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:19.261303 UTC] Performing policy update
[2018-01-21 17:30:19.261893 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:19.341697 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:20.253299 UTC] Performing line search
[2018-01-21 17:30:20.371947 UTC] Updating baseline
[2018-01-21 17:30:21.675520 UTC] Computing logging information
------------------------------------
| Iteration            | 1913      |
| ExpectedImprovement  | 0.018304  |
| ActualImprovement    | 0.017109  |
| ImprovementRatio     | 0.93471   |
| MeanKL               | 0.0079769 |
| Entropy              | -2.0855   |
| Perplexity           | 0.12425   |
| AveragePolicyStd     | 0.17325   |
| AveragePolicyStd[0]  | 0.18667   |
| AveragePolicyStd[1]  | 0.18903   |
| AveragePolicyStd[2]  | 0.13979   |
| AveragePolicyStd[3]  | 0.15824   |
| AveragePolicyStd[4]  | 0.14415   |
| AveragePolicyStd[5]  | 0.22163   |
| AverageReturn        | 1825.7    |
| MinReturn            | 68.182    |
| MaxReturn            | 2067.9    |
| StdReturn            | 460.16    |
| AverageEpisodeLength | 914.17    |
| MinEpisodeLength     | 66        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 219.83    |
| TotalNEpisodes       | 26707     |
| TotalNSamples        | 9.579e+06 |
| ExplainedVariance    | 0.16572   |
------------------------------------
[2018-01-21 17:30:22.201362 UTC] Saving snapshot
[2018-01-21 17:30:22.201577 UTC] Starting iteration 1914
[2018-01-21 17:30:22.201732 UTC] Start collecting samples
[2018-01-21 17:30:24.506241 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:24.579017 UTC] Performing policy update
[2018-01-21 17:30:24.579510 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:24.656462 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:25.556240 UTC] Performing line search
[2018-01-21 17:30:25.674208 UTC] Updating baseline
[2018-01-21 17:30:26.945699 UTC] Computing logging information
------------------------------------
| Iteration            | 1914      |
| ExpectedImprovement  | 0.016     |
| ActualImprovement    | 0.014771  |
| ImprovementRatio     | 0.92317   |
| MeanKL               | 0.0089957 |
| Entropy              | -2.0901   |
| Perplexity           | 0.12367   |
| AveragePolicyStd     | 0.17312   |
| AveragePolicyStd[0]  | 0.18685   |
| AveragePolicyStd[1]  | 0.1889    |
| AveragePolicyStd[2]  | 0.13963   |
| AveragePolicyStd[3]  | 0.15792   |
| AveragePolicyStd[4]  | 0.14415   |
| AveragePolicyStd[5]  | 0.22126   |
| AverageReturn        | 1825.2    |
| MinReturn            | 68.182    |
| MaxReturn            | 2067.9    |
| StdReturn            | 460.02    |
| AverageEpisodeLength | 914.17    |
| MinEpisodeLength     | 66        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 219.83    |
| TotalNEpisodes       | 26711     |
| TotalNSamples        | 9.583e+06 |
| ExplainedVariance    | -0.049134 |
------------------------------------
[2018-01-21 17:30:27.469019 UTC] Saving snapshot
[2018-01-21 17:30:27.469225 UTC] Starting iteration 1915
[2018-01-21 17:30:27.469394 UTC] Start collecting samples
[2018-01-21 17:30:29.809112 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:29.880768 UTC] Performing policy update
[2018-01-21 17:30:29.881243 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:29.958693 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:30.849940 UTC] Performing line search
[2018-01-21 17:30:30.971381 UTC] Updating baseline
[2018-01-21 17:30:32.324575 UTC] Computing logging information
-------------------------------------
| Iteration            | 1915       |
| ExpectedImprovement  | 0.018887   |
| ActualImprovement    | 0.017232   |
| ImprovementRatio     | 0.9124     |
| MeanKL               | 0.0082829  |
| Entropy              | -2.0944    |
| Perplexity           | 0.12314    |
| AveragePolicyStd     | 0.17302    |
| AveragePolicyStd[0]  | 0.18641    |
| AveragePolicyStd[1]  | 0.1891     |
| AveragePolicyStd[2]  | 0.13935    |
| AveragePolicyStd[3]  | 0.15775    |
| AveragePolicyStd[4]  | 0.14398    |
| AveragePolicyStd[5]  | 0.22152    |
| AverageReturn        | 1804.2     |
| MinReturn            | 68.182     |
| MaxReturn            | 2067.9     |
| StdReturn            | 472.69     |
| AverageEpisodeLength | 905.13     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.38     |
| TotalNEpisodes       | 26716      |
| TotalNSamples        | 9.5871e+06 |
| ExplainedVariance    | 0.24114    |
-------------------------------------
[2018-01-21 17:30:32.858123 UTC] Saving snapshot
[2018-01-21 17:30:32.858309 UTC] Starting iteration 1916
[2018-01-21 17:30:32.858460 UTC] Start collecting samples
[2018-01-21 17:30:35.413780 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:35.491725 UTC] Performing policy update
[2018-01-21 17:30:35.492315 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:35.570259 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:36.471826 UTC] Performing line search
[2018-01-21 17:30:36.609225 UTC] Updating baseline
[2018-01-21 17:30:37.881673 UTC] Computing logging information
-------------------------------------
| Iteration            | 1916       |
| ExpectedImprovement  | 0.020024   |
| ActualImprovement    | 0.019611   |
| ImprovementRatio     | 0.97935    |
| MeanKL               | 0.0076792  |
| Entropy              | -2.0974    |
| Perplexity           | 0.12277    |
| AveragePolicyStd     | 0.17297    |
| AveragePolicyStd[0]  | 0.18661    |
| AveragePolicyStd[1]  | 0.18886    |
| AveragePolicyStd[2]  | 0.13903    |
| AveragePolicyStd[3]  | 0.15769    |
| AveragePolicyStd[4]  | 0.14374    |
| AveragePolicyStd[5]  | 0.22188    |
| AverageReturn        | 1803.3     |
| MinReturn            | 68.182     |
| MaxReturn            | 2067.9     |
| StdReturn            | 472.33     |
| AverageEpisodeLength | 905.13     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.38     |
| TotalNEpisodes       | 26720      |
| TotalNSamples        | 9.5911e+06 |
| ExplainedVariance    | 0.0081088  |
-------------------------------------
[2018-01-21 17:30:38.440584 UTC] Saving snapshot
[2018-01-21 17:30:38.440859 UTC] Starting iteration 1917
[2018-01-21 17:30:38.441038 UTC] Start collecting samples
[2018-01-21 17:30:41.502490 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:41.583418 UTC] Performing policy update
[2018-01-21 17:30:41.583935 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:41.665016 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:42.645681 UTC] Performing line search
[2018-01-21 17:30:42.771070 UTC] Updating baseline
[2018-01-21 17:30:44.303736 UTC] Computing logging information
-------------------------------------
| Iteration            | 1917       |
| ExpectedImprovement  | 0.017483   |
| ActualImprovement    | 0.016868   |
| ImprovementRatio     | 0.96482    |
| MeanKL               | 0.0088699  |
| Entropy              | -2.1051    |
| Perplexity           | 0.12183    |
| AveragePolicyStd     | 0.17276    |
| AveragePolicyStd[0]  | 0.18648    |
| AveragePolicyStd[1]  | 0.18896    |
| AveragePolicyStd[2]  | 0.13876    |
| AveragePolicyStd[3]  | 0.15732    |
| AveragePolicyStd[4]  | 0.14346    |
| AveragePolicyStd[5]  | 0.22159    |
| AverageReturn        | 1787.5     |
| MinReturn            | 68.182     |
| MaxReturn            | 2067.9     |
| StdReturn            | 485.28     |
| AverageEpisodeLength | 898.36     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 232.17     |
| TotalNEpisodes       | 26728      |
| TotalNSamples        | 9.5985e+06 |
| ExplainedVariance    | 0.13412    |
-------------------------------------
[2018-01-21 17:30:44.864042 UTC] Saving snapshot
[2018-01-21 17:30:44.864241 UTC] Starting iteration 1918
[2018-01-21 17:30:44.864392 UTC] Start collecting samples
[2018-01-21 17:30:47.358162 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:47.432157 UTC] Performing policy update
[2018-01-21 17:30:47.432682 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:47.510487 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:48.429749 UTC] Performing line search
[2018-01-21 17:30:48.578805 UTC] Updating baseline
[2018-01-21 17:30:50.206811 UTC] Computing logging information
-------------------------------------
| Iteration            | 1918       |
| ExpectedImprovement  | 0.017523   |
| ActualImprovement    | 0.016624   |
| ImprovementRatio     | 0.94867    |
| MeanKL               | 0.0087747  |
| Entropy              | -2.1034    |
| Perplexity           | 0.12204    |
| AveragePolicyStd     | 0.17281    |
| AveragePolicyStd[0]  | 0.18644    |
| AveragePolicyStd[1]  | 0.1887     |
| AveragePolicyStd[2]  | 0.13886    |
| AveragePolicyStd[3]  | 0.15732    |
| AveragePolicyStd[4]  | 0.14366    |
| AveragePolicyStd[5]  | 0.22186    |
| AverageReturn        | 1820.7     |
| MinReturn            | 68.182     |
| MaxReturn            | 2067.9     |
| StdReturn            | 453.75     |
| AverageEpisodeLength | 914.07     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.22     |
| TotalNEpisodes       | 26733      |
| TotalNSamples        | 9.6034e+06 |
| ExplainedVariance    | 0.12289    |
-------------------------------------
[2018-01-21 17:30:50.727091 UTC] Saving snapshot
[2018-01-21 17:30:50.727266 UTC] Starting iteration 1919
[2018-01-21 17:30:50.727390 UTC] Start collecting samples
[2018-01-21 17:30:53.104002 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:53.177100 UTC] Performing policy update
[2018-01-21 17:30:53.177635 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:53.253498 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:54.262018 UTC] Performing line search
[2018-01-21 17:30:54.403933 UTC] Updating baseline
[2018-01-21 17:30:55.576800 UTC] Computing logging information
-------------------------------------
| Iteration            | 1919       |
| ExpectedImprovement  | 0.01916    |
| ActualImprovement    | 0.017852   |
| ImprovementRatio     | 0.93174    |
| MeanKL               | 0.0085748  |
| Entropy              | -2.0995    |
| Perplexity           | 0.12252    |
| AveragePolicyStd     | 0.17293    |
| AveragePolicyStd[0]  | 0.18659    |
| AveragePolicyStd[1]  | 0.18897    |
| AveragePolicyStd[2]  | 0.13903    |
| AveragePolicyStd[3]  | 0.15694    |
| AveragePolicyStd[4]  | 0.14383    |
| AveragePolicyStd[5]  | 0.22223    |
| AverageReturn        | 1816       |
| MinReturn            | 68.182     |
| MaxReturn            | 2054.9     |
| StdReturn            | 453.78     |
| AverageEpisodeLength | 912.01     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.37     |
| TotalNEpisodes       | 26737      |
| TotalNSamples        | 9.6072e+06 |
| ExplainedVariance    | 0.16134    |
-------------------------------------
[2018-01-21 17:30:56.105793 UTC] Saving snapshot
[2018-01-21 17:30:56.106005 UTC] Starting iteration 1920
[2018-01-21 17:30:56.106129 UTC] Start collecting samples
[2018-01-21 17:30:58.512447 UTC] Computing input variables for policy optimization
[2018-01-21 17:30:58.588766 UTC] Performing policy update
[2018-01-21 17:30:58.589317 UTC] Computing gradient in Euclidean space
[2018-01-21 17:30:58.666313 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:30:59.584053 UTC] Performing line search
[2018-01-21 17:30:59.721602 UTC] Updating baseline
[2018-01-21 17:31:01.066085 UTC] Computing logging information
-------------------------------------
| Iteration            | 1920       |
| ExpectedImprovement  | 0.017738   |
| ActualImprovement    | 0.016198   |
| ImprovementRatio     | 0.91314    |
| MeanKL               | 0.0083967  |
| Entropy              | -2.1035    |
| Perplexity           | 0.12202    |
| AveragePolicyStd     | 0.17281    |
| AveragePolicyStd[0]  | 0.18645    |
| AveragePolicyStd[1]  | 0.18884    |
| AveragePolicyStd[2]  | 0.13927    |
| AveragePolicyStd[3]  | 0.15675    |
| AveragePolicyStd[4]  | 0.14348    |
| AveragePolicyStd[5]  | 0.22208    |
| AverageReturn        | 1808.5     |
| MinReturn            | 68.182     |
| MaxReturn            | 2048.1     |
| StdReturn            | 455.84     |
| AverageEpisodeLength | 908.8      |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.41     |
| TotalNEpisodes       | 26742      |
| TotalNSamples        | 9.6119e+06 |
| ExplainedVariance    | 0.11752    |
-------------------------------------
[2018-01-21 17:31:01.622951 UTC] Saving snapshot
[2018-01-21 17:31:01.628808 UTC] Starting iteration 1921
[2018-01-21 17:31:01.628975 UTC] Start collecting samples
[2018-01-21 17:31:04.133571 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:04.215287 UTC] Performing policy update
[2018-01-21 17:31:04.215852 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:04.291612 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:05.192586 UTC] Performing line search
[2018-01-21 17:31:05.309355 UTC] Updating baseline
[2018-01-21 17:31:06.552494 UTC] Computing logging information
-------------------------------------
| Iteration            | 1921       |
| ExpectedImprovement  | 0.017132   |
| ActualImprovement    | 0.016657   |
| ImprovementRatio     | 0.97225    |
| MeanKL               | 0.0088284  |
| Entropy              | -2.105     |
| Perplexity           | 0.12185    |
| AveragePolicyStd     | 0.17278    |
| AveragePolicyStd[0]  | 0.18705    |
| AveragePolicyStd[1]  | 0.18789    |
| AveragePolicyStd[2]  | 0.13904    |
| AveragePolicyStd[3]  | 0.157      |
| AveragePolicyStd[4]  | 0.14338    |
| AveragePolicyStd[5]  | 0.22231    |
| AverageReturn        | 1788       |
| MinReturn            | 68.182     |
| MaxReturn            | 2048.1     |
| StdReturn            | 480.09     |
| AverageEpisodeLength | 899.3      |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 230.61     |
| TotalNEpisodes       | 26750      |
| TotalNSamples        | 9.6184e+06 |
| ExplainedVariance    | 0.24684    |
-------------------------------------
[2018-01-21 17:31:07.084232 UTC] Saving snapshot
[2018-01-21 17:31:07.084407 UTC] Starting iteration 1922
[2018-01-21 17:31:07.084510 UTC] Start collecting samples
[2018-01-21 17:31:09.563689 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:09.645225 UTC] Performing policy update
[2018-01-21 17:31:09.645931 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:09.720940 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:10.613800 UTC] Performing line search
[2018-01-21 17:31:10.730533 UTC] Updating baseline
[2018-01-21 17:31:11.891725 UTC] Computing logging information
-------------------------------------
| Iteration            | 1922       |
| ExpectedImprovement  | 0.019021   |
| ActualImprovement    | 0.019009   |
| ImprovementRatio     | 0.99937    |
| MeanKL               | 0.0083454  |
| Entropy              | -2.0959    |
| Perplexity           | 0.12296    |
| AveragePolicyStd     | 0.17303    |
| AveragePolicyStd[0]  | 0.18752    |
| AveragePolicyStd[1]  | 0.18796    |
| AveragePolicyStd[2]  | 0.13958    |
| AveragePolicyStd[3]  | 0.15693    |
| AveragePolicyStd[4]  | 0.1437     |
| AveragePolicyStd[5]  | 0.22247    |
| AverageReturn        | 1778.6     |
| MinReturn            | 68.182     |
| MaxReturn            | 2048.1     |
| StdReturn            | 486.14     |
| AverageEpisodeLength | 894.33     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.41     |
| TotalNEpisodes       | 26756      |
| TotalNSamples        | 9.6239e+06 |
| ExplainedVariance    | 0.21585    |
-------------------------------------
[2018-01-21 17:31:12.422884 UTC] Saving snapshot
[2018-01-21 17:31:12.423070 UTC] Starting iteration 1923
[2018-01-21 17:31:12.423195 UTC] Start collecting samples
[2018-01-21 17:31:14.712852 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:14.790766 UTC] Performing policy update
[2018-01-21 17:31:14.791307 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:14.872285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:15.834267 UTC] Performing line search
[2018-01-21 17:31:16.033826 UTC] Updating baseline
[2018-01-21 17:31:17.521930 UTC] Computing logging information
-------------------------------------
| Iteration            | 1923       |
| ExpectedImprovement  | 0.019541   |
| ActualImprovement    | 0.017857   |
| ImprovementRatio     | 0.91383    |
| MeanKL               | 0.0078743  |
| Entropy              | -2.0923    |
| Perplexity           | 0.1234     |
| AveragePolicyStd     | 0.17314    |
| AveragePolicyStd[0]  | 0.18781    |
| AveragePolicyStd[1]  | 0.18791    |
| AveragePolicyStd[2]  | 0.13979    |
| AveragePolicyStd[3]  | 0.15674    |
| AveragePolicyStd[4]  | 0.14376    |
| AveragePolicyStd[5]  | 0.22282    |
| AverageReturn        | 1778.6     |
| MinReturn            | 68.182     |
| MaxReturn            | 2048.1     |
| StdReturn            | 486.16     |
| AverageEpisodeLength | 894.33     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.41     |
| TotalNEpisodes       | 26758      |
| TotalNSamples        | 9.6259e+06 |
| ExplainedVariance    | 0.024304   |
-------------------------------------
[2018-01-21 17:31:18.090835 UTC] Saving snapshot
[2018-01-21 17:31:18.091042 UTC] Starting iteration 1924
[2018-01-21 17:31:18.091187 UTC] Start collecting samples
[2018-01-21 17:31:20.560062 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:20.645122 UTC] Performing policy update
[2018-01-21 17:31:20.645603 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:20.719985 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:21.693023 UTC] Performing line search
[2018-01-21 17:31:21.852045 UTC] Updating baseline
[2018-01-21 17:31:23.205269 UTC] Computing logging information
-------------------------------------
| Iteration            | 1924       |
| ExpectedImprovement  | 0.01736    |
| ActualImprovement    | 0.016569   |
| ImprovementRatio     | 0.95443    |
| MeanKL               | 0.0083782  |
| Entropy              | -2.0928    |
| Perplexity           | 0.12335    |
| AveragePolicyStd     | 0.17313    |
| AveragePolicyStd[0]  | 0.18797    |
| AveragePolicyStd[1]  | 0.18807    |
| AveragePolicyStd[2]  | 0.13968    |
| AveragePolicyStd[3]  | 0.15655    |
| AveragePolicyStd[4]  | 0.14383    |
| AveragePolicyStd[5]  | 0.22267    |
| AverageReturn        | 1776       |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 485.34     |
| AverageEpisodeLength | 894.16     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 233.28     |
| TotalNEpisodes       | 26768      |
| TotalNSamples        | 9.6348e+06 |
| ExplainedVariance    | 0.11487    |
-------------------------------------
[2018-01-21 17:31:23.723885 UTC] Saving snapshot
[2018-01-21 17:31:23.724089 UTC] Starting iteration 1925
[2018-01-21 17:31:23.724248 UTC] Start collecting samples
[2018-01-21 17:31:26.140795 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:26.217131 UTC] Performing policy update
[2018-01-21 17:31:26.217763 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:26.292838 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:27.216607 UTC] Performing line search
[2018-01-21 17:31:27.334064 UTC] Updating baseline
[2018-01-21 17:31:28.910535 UTC] Computing logging information
-------------------------------------
| Iteration            | 1925       |
| ExpectedImprovement  | 0.018309   |
| ActualImprovement    | 0.018649   |
| ImprovementRatio     | 1.0186     |
| MeanKL               | 0.0085125  |
| Entropy              | -2.0925    |
| Perplexity           | 0.12338    |
| AveragePolicyStd     | 0.17316    |
| AveragePolicyStd[0]  | 0.18782    |
| AveragePolicyStd[1]  | 0.18797    |
| AveragePolicyStd[2]  | 0.13968    |
| AveragePolicyStd[3]  | 0.15669    |
| AveragePolicyStd[4]  | 0.14353    |
| AveragePolicyStd[5]  | 0.22328    |
| AverageReturn        | 1789.1     |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 469.76     |
| AverageEpisodeLength | 900.43     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 225.88     |
| TotalNEpisodes       | 26772      |
| TotalNSamples        | 9.6382e+06 |
| ExplainedVariance    | 0.12946    |
-------------------------------------
[2018-01-21 17:31:29.472945 UTC] Saving snapshot
[2018-01-21 17:31:29.473127 UTC] Starting iteration 1926
[2018-01-21 17:31:29.473270 UTC] Start collecting samples
[2018-01-21 17:31:31.956117 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:32.033727 UTC] Performing policy update
[2018-01-21 17:31:32.034386 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:32.120150 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:33.016664 UTC] Performing line search
[2018-01-21 17:31:33.141562 UTC] Updating baseline
[2018-01-21 17:31:34.560317 UTC] Computing logging information
-------------------------------------
| Iteration            | 1926       |
| ExpectedImprovement  | 0.020152   |
| ActualImprovement    | 0.01848    |
| ImprovementRatio     | 0.91707    |
| MeanKL               | 0.007744   |
| Entropy              | -2.0903    |
| Perplexity           | 0.12365    |
| AveragePolicyStd     | 0.17324    |
| AveragePolicyStd[0]  | 0.18824    |
| AveragePolicyStd[1]  | 0.18814    |
| AveragePolicyStd[2]  | 0.13967    |
| AveragePolicyStd[3]  | 0.15663    |
| AveragePolicyStd[4]  | 0.14342    |
| AveragePolicyStd[5]  | 0.22336    |
| AverageReturn        | 1799.2     |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 463.77     |
| AverageEpisodeLength | 905.09     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 223.06     |
| TotalNEpisodes       | 26775      |
| TotalNSamples        | 9.6412e+06 |
| ExplainedVariance    | -0.069967  |
-------------------------------------
[2018-01-21 17:31:35.088014 UTC] Saving snapshot
[2018-01-21 17:31:35.088200 UTC] Starting iteration 1927
[2018-01-21 17:31:35.088323 UTC] Start collecting samples
[2018-01-21 17:31:37.800326 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:37.879407 UTC] Performing policy update
[2018-01-21 17:31:37.880016 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:37.960276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:38.860302 UTC] Performing line search
[2018-01-21 17:31:38.978956 UTC] Updating baseline
[2018-01-21 17:31:40.267870 UTC] Computing logging information
-------------------------------------
| Iteration            | 1927       |
| ExpectedImprovement  | 0.018603   |
| ActualImprovement    | 0.017493   |
| ImprovementRatio     | 0.94033    |
| MeanKL               | 0.0078948  |
| Entropy              | -2.0954    |
| Perplexity           | 0.12302    |
| AveragePolicyStd     | 0.17309    |
| AveragePolicyStd[0]  | 0.1881     |
| AveragePolicyStd[1]  | 0.18778    |
| AveragePolicyStd[2]  | 0.13983    |
| AveragePolicyStd[3]  | 0.15636    |
| AveragePolicyStd[4]  | 0.14321    |
| AveragePolicyStd[5]  | 0.22328    |
| AverageReturn        | 1821.7     |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 439.87     |
| AverageEpisodeLength | 916.14     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.54     |
| TotalNEpisodes       | 26781      |
| TotalNSamples        | 9.6472e+06 |
| ExplainedVariance    | 0.0007088  |
-------------------------------------
[2018-01-21 17:31:40.794916 UTC] Saving snapshot
[2018-01-21 17:31:40.795118 UTC] Starting iteration 1928
[2018-01-21 17:31:40.795270 UTC] Start collecting samples
[2018-01-21 17:31:43.242971 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:43.326319 UTC] Performing policy update
[2018-01-21 17:31:43.326877 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:43.404584 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:44.292975 UTC] Performing line search
[2018-01-21 17:31:44.412427 UTC] Updating baseline
[2018-01-21 17:31:45.684760 UTC] Computing logging information
-------------------------------------
| Iteration            | 1928       |
| ExpectedImprovement  | 0.016945   |
| ActualImprovement    | 0.015778   |
| ImprovementRatio     | 0.93113    |
| MeanKL               | 0.0085251  |
| Entropy              | -2.1009    |
| Perplexity           | 0.12235    |
| AveragePolicyStd     | 0.17295    |
| AveragePolicyStd[0]  | 0.18733    |
| AveragePolicyStd[1]  | 0.18796    |
| AveragePolicyStd[2]  | 0.13964    |
| AveragePolicyStd[3]  | 0.15632    |
| AveragePolicyStd[4]  | 0.143      |
| AveragePolicyStd[5]  | 0.22342    |
| AverageReturn        | 1815.8     |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 440.96     |
| AverageEpisodeLength | 913.59     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 212.05     |
| TotalNEpisodes       | 26787      |
| TotalNSamples        | 9.6529e+06 |
| ExplainedVariance    | 0.1233     |
-------------------------------------
[2018-01-21 17:31:46.206531 UTC] Saving snapshot
[2018-01-21 17:31:46.206724 UTC] Starting iteration 1929
[2018-01-21 17:31:46.206858 UTC] Start collecting samples
[2018-01-21 17:31:48.585955 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:48.659822 UTC] Performing policy update
[2018-01-21 17:31:48.660441 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:48.736766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:49.608949 UTC] Performing line search
[2018-01-21 17:31:49.725903 UTC] Updating baseline
[2018-01-21 17:31:51.110032 UTC] Computing logging information
-------------------------------------
| Iteration            | 1929       |
| ExpectedImprovement  | 0.018307   |
| ActualImprovement    | 0.017342   |
| ImprovementRatio     | 0.94731    |
| MeanKL               | 0.0086916  |
| Entropy              | -2.1007    |
| Perplexity           | 0.12237    |
| AveragePolicyStd     | 0.1729     |
| AveragePolicyStd[0]  | 0.18731    |
| AveragePolicyStd[1]  | 0.1875     |
| AveragePolicyStd[2]  | 0.14004    |
| AveragePolicyStd[3]  | 0.15672    |
| AveragePolicyStd[4]  | 0.14302    |
| AveragePolicyStd[5]  | 0.2228     |
| AverageReturn        | 1825.6     |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 419.33     |
| AverageEpisodeLength | 918.61     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 200.96     |
| TotalNEpisodes       | 26792      |
| TotalNSamples        | 9.6577e+06 |
| ExplainedVariance    | 0.13332    |
-------------------------------------
[2018-01-21 17:31:51.633277 UTC] Saving snapshot
[2018-01-21 17:31:51.633468 UTC] Starting iteration 1930
[2018-01-21 17:31:51.633584 UTC] Start collecting samples
[2018-01-21 17:31:53.930902 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:54.003466 UTC] Performing policy update
[2018-01-21 17:31:54.003983 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:54.080238 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:31:54.951530 UTC] Performing line search
[2018-01-21 17:31:55.070539 UTC] Updating baseline
[2018-01-21 17:31:56.214795 UTC] Computing logging information
-------------------------------------
| Iteration            | 1930       |
| ExpectedImprovement  | 0.01713    |
| ActualImprovement    | 0.01606    |
| ImprovementRatio     | 0.93754    |
| MeanKL               | 0.0079009  |
| Entropy              | -2.1046    |
| Perplexity           | 0.12189    |
| AveragePolicyStd     | 0.17279    |
| AveragePolicyStd[0]  | 0.18704    |
| AveragePolicyStd[1]  | 0.18733    |
| AveragePolicyStd[2]  | 0.13989    |
| AveragePolicyStd[3]  | 0.1566     |
| AveragePolicyStd[4]  | 0.143      |
| AveragePolicyStd[5]  | 0.22289    |
| AverageReturn        | 1814       |
| MinReturn            | 68.182     |
| MaxReturn            | 2072.1     |
| StdReturn            | 430.63     |
| AverageEpisodeLength | 913.07     |
| MinEpisodeLength     | 66         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.21     |
| TotalNEpisodes       | 26797      |
| TotalNSamples        | 9.6621e+06 |
| ExplainedVariance    | 0.1341     |
-------------------------------------
[2018-01-21 17:31:56.733408 UTC] Saving snapshot
[2018-01-21 17:31:56.739226 UTC] Starting iteration 1931
[2018-01-21 17:31:56.739413 UTC] Start collecting samples
[2018-01-21 17:31:59.195940 UTC] Computing input variables for policy optimization
[2018-01-21 17:31:59.270025 UTC] Performing policy update
[2018-01-21 17:31:59.270604 UTC] Computing gradient in Euclidean space
[2018-01-21 17:31:59.346484 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:00.248096 UTC] Performing line search
[2018-01-21 17:32:00.370292 UTC] Updating baseline
[2018-01-21 17:32:01.592413 UTC] Computing logging information
-------------------------------------
| Iteration            | 1931       |
| ExpectedImprovement  | 0.018169   |
| ActualImprovement    | 0.017558   |
| ImprovementRatio     | 0.96641    |
| MeanKL               | 0.0085285  |
| Entropy              | -2.113     |
| Perplexity           | 0.12088    |
| AveragePolicyStd     | 0.17257    |
| AveragePolicyStd[0]  | 0.1868     |
| AveragePolicyStd[1]  | 0.18712    |
| AveragePolicyStd[2]  | 0.13966    |
| AveragePolicyStd[3]  | 0.15627    |
| AveragePolicyStd[4]  | 0.14275    |
| AveragePolicyStd[5]  | 0.22279    |
| AverageReturn        | 1830.3     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 399.69     |
| AverageEpisodeLength | 919.9      |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.44     |
| TotalNEpisodes       | 26804      |
| TotalNSamples        | 9.6687e+06 |
| ExplainedVariance    | 0.098701   |
-------------------------------------
[2018-01-21 17:32:02.117795 UTC] Saving snapshot
[2018-01-21 17:32:02.117998 UTC] Starting iteration 1932
[2018-01-21 17:32:02.118135 UTC] Start collecting samples
[2018-01-21 17:32:04.528298 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:04.603014 UTC] Performing policy update
[2018-01-21 17:32:04.603523 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:04.680889 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:05.567253 UTC] Performing line search
[2018-01-21 17:32:05.689004 UTC] Updating baseline
[2018-01-21 17:32:07.218980 UTC] Computing logging information
-------------------------------------
| Iteration            | 1932       |
| ExpectedImprovement  | 0.016795   |
| ActualImprovement    | 0.015929   |
| ImprovementRatio     | 0.94846    |
| MeanKL               | 0.0083804  |
| Entropy              | -2.1175    |
| Perplexity           | 0.12033    |
| AveragePolicyStd     | 0.17242    |
| AveragePolicyStd[0]  | 0.18699    |
| AveragePolicyStd[1]  | 0.18668    |
| AveragePolicyStd[2]  | 0.13995    |
| AveragePolicyStd[3]  | 0.15612    |
| AveragePolicyStd[4]  | 0.14239    |
| AveragePolicyStd[5]  | 0.2224     |
| AverageReturn        | 1831.6     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 398.22     |
| AverageEpisodeLength | 920.26     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.35     |
| TotalNEpisodes       | 26809      |
| TotalNSamples        | 9.6731e+06 |
| ExplainedVariance    | 0.1641     |
-------------------------------------
[2018-01-21 17:32:07.739443 UTC] Saving snapshot
[2018-01-21 17:32:07.739657 UTC] Starting iteration 1933
[2018-01-21 17:32:07.739789 UTC] Start collecting samples
[2018-01-21 17:32:10.044535 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:10.122262 UTC] Performing policy update
[2018-01-21 17:32:10.122935 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:10.203817 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:11.130469 UTC] Performing line search
[2018-01-21 17:32:11.262930 UTC] Updating baseline
[2018-01-21 17:32:12.555598 UTC] Computing logging information
-------------------------------------
| Iteration            | 1933       |
| ExpectedImprovement  | 0.019415   |
| ActualImprovement    | 0.018689   |
| ImprovementRatio     | 0.96262    |
| MeanKL               | 0.0085055  |
| Entropy              | -2.1195    |
| Perplexity           | 0.12009    |
| AveragePolicyStd     | 0.17239    |
| AveragePolicyStd[0]  | 0.18737    |
| AveragePolicyStd[1]  | 0.18701    |
| AveragePolicyStd[2]  | 0.13977    |
| AveragePolicyStd[3]  | 0.15599    |
| AveragePolicyStd[4]  | 0.14203    |
| AveragePolicyStd[5]  | 0.22215    |
| AverageReturn        | 1834       |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 391.37     |
| AverageEpisodeLength | 922.74     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.44     |
| TotalNEpisodes       | 26813      |
| TotalNSamples        | 9.6769e+06 |
| ExplainedVariance    | 0.1464     |
-------------------------------------
[2018-01-21 17:32:13.079860 UTC] Saving snapshot
[2018-01-21 17:32:13.080051 UTC] Starting iteration 1934
[2018-01-21 17:32:13.080205 UTC] Start collecting samples
[2018-01-21 17:32:15.454130 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:15.536738 UTC] Performing policy update
[2018-01-21 17:32:15.537283 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:15.617538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:16.534185 UTC] Performing line search
[2018-01-21 17:32:16.653598 UTC] Updating baseline
[2018-01-21 17:32:17.953852 UTC] Computing logging information
-------------------------------------
| Iteration            | 1934       |
| ExpectedImprovement  | 0.017769   |
| ActualImprovement    | 0.016524   |
| ImprovementRatio     | 0.9299     |
| MeanKL               | 0.0083309  |
| Entropy              | -2.1257    |
| Perplexity           | 0.11935    |
| AveragePolicyStd     | 0.17218    |
| AveragePolicyStd[0]  | 0.18685    |
| AveragePolicyStd[1]  | 0.18686    |
| AveragePolicyStd[2]  | 0.13988    |
| AveragePolicyStd[3]  | 0.15573    |
| AveragePolicyStd[4]  | 0.14204    |
| AveragePolicyStd[5]  | 0.22174    |
| AverageReturn        | 1842.4     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 380.68     |
| AverageEpisodeLength | 927.55     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 182.12     |
| TotalNEpisodes       | 26820      |
| TotalNSamples        | 9.6839e+06 |
| ExplainedVariance    | -0.0039622 |
-------------------------------------
[2018-01-21 17:32:18.528503 UTC] Saving snapshot
[2018-01-21 17:32:18.528704 UTC] Starting iteration 1935
[2018-01-21 17:32:18.528825 UTC] Start collecting samples
[2018-01-21 17:32:20.819362 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:20.891710 UTC] Performing policy update
[2018-01-21 17:32:20.892228 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:20.967829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:21.857105 UTC] Performing line search
[2018-01-21 17:32:21.974186 UTC] Updating baseline
[2018-01-21 17:32:23.193218 UTC] Computing logging information
-------------------------------------
| Iteration            | 1935       |
| ExpectedImprovement  | 0.018735   |
| ActualImprovement    | 0.017862   |
| ImprovementRatio     | 0.95342    |
| MeanKL               | 0.008011   |
| Entropy              | -2.1264    |
| Perplexity           | 0.11926    |
| AveragePolicyStd     | 0.17213    |
| AveragePolicyStd[0]  | 0.18664    |
| AveragePolicyStd[1]  | 0.18701    |
| AveragePolicyStd[2]  | 0.14034    |
| AveragePolicyStd[3]  | 0.15578    |
| AveragePolicyStd[4]  | 0.1418     |
| AveragePolicyStd[5]  | 0.22123    |
| AverageReturn        | 1856.2     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 360.77     |
| AverageEpisodeLength | 934.32     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.23     |
| TotalNEpisodes       | 26824      |
| TotalNSamples        | 9.6879e+06 |
| ExplainedVariance    | -0.0021843 |
-------------------------------------
[2018-01-21 17:32:23.713504 UTC] Saving snapshot
[2018-01-21 17:32:23.713718 UTC] Starting iteration 1936
[2018-01-21 17:32:23.713832 UTC] Start collecting samples
[2018-01-21 17:32:26.170634 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:26.247554 UTC] Performing policy update
[2018-01-21 17:32:26.248229 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:26.324511 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:27.254258 UTC] Performing line search
[2018-01-21 17:32:27.377154 UTC] Updating baseline
[2018-01-21 17:32:28.922542 UTC] Computing logging information
-------------------------------------
| Iteration            | 1936       |
| ExpectedImprovement  | 0.020949   |
| ActualImprovement    | 0.019108   |
| ImprovementRatio     | 0.91212    |
| MeanKL               | 0.0077667  |
| Entropy              | -2.1277    |
| Perplexity           | 0.11911    |
| AveragePolicyStd     | 0.17208    |
| AveragePolicyStd[0]  | 0.18697    |
| AveragePolicyStd[1]  | 0.18661    |
| AveragePolicyStd[2]  | 0.14041    |
| AveragePolicyStd[3]  | 0.15586    |
| AveragePolicyStd[4]  | 0.14171    |
| AveragePolicyStd[5]  | 0.22093    |
| AverageReturn        | 1856.6     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 360.9      |
| AverageEpisodeLength | 934.32     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.23     |
| TotalNEpisodes       | 26826      |
| TotalNSamples        | 9.6899e+06 |
| ExplainedVariance    | 0.13678    |
-------------------------------------
[2018-01-21 17:32:29.500307 UTC] Saving snapshot
[2018-01-21 17:32:29.500480 UTC] Starting iteration 1937
[2018-01-21 17:32:29.500583 UTC] Start collecting samples
[2018-01-21 17:32:32.002205 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:32.115464 UTC] Performing policy update
[2018-01-21 17:32:32.116181 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:32.234629 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:33.178925 UTC] Performing line search
[2018-01-21 17:32:33.353606 UTC] Updating baseline
[2018-01-21 17:32:34.838755 UTC] Computing logging information
-------------------------------------
| Iteration            | 1937       |
| ExpectedImprovement  | 0.020331   |
| ActualImprovement    | 0.018893   |
| ImprovementRatio     | 0.92924    |
| MeanKL               | 0.0084591  |
| Entropy              | -2.1317    |
| Perplexity           | 0.11863    |
| AveragePolicyStd     | 0.17198    |
| AveragePolicyStd[0]  | 0.18688    |
| AveragePolicyStd[1]  | 0.18675    |
| AveragePolicyStd[2]  | 0.14028    |
| AveragePolicyStd[3]  | 0.15533    |
| AveragePolicyStd[4]  | 0.14177    |
| AveragePolicyStd[5]  | 0.22086    |
| AverageReturn        | 1855.8     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 360.2      |
| AverageEpisodeLength | 934.89     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 172.27     |
| TotalNEpisodes       | 26834      |
| TotalNSamples        | 9.6979e+06 |
| ExplainedVariance    | 0.05972    |
-------------------------------------
[2018-01-21 17:32:35.369003 UTC] Saving snapshot
[2018-01-21 17:32:35.369197 UTC] Starting iteration 1938
[2018-01-21 17:32:35.369330 UTC] Start collecting samples
[2018-01-21 17:32:37.791926 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:37.865873 UTC] Performing policy update
[2018-01-21 17:32:37.866449 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:37.944285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:38.853132 UTC] Performing line search
[2018-01-21 17:32:38.981687 UTC] Updating baseline
[2018-01-21 17:32:40.333841 UTC] Computing logging information
-------------------------------------
| Iteration            | 1938       |
| ExpectedImprovement  | 0.020253   |
| ActualImprovement    | 0.019045   |
| ImprovementRatio     | 0.94038    |
| MeanKL               | 0.0086999  |
| Entropy              | -2.1391    |
| Perplexity           | 0.11776    |
| AveragePolicyStd     | 0.17178    |
| AveragePolicyStd[0]  | 0.18673    |
| AveragePolicyStd[1]  | 0.18658    |
| AveragePolicyStd[2]  | 0.13995    |
| AveragePolicyStd[3]  | 0.15525    |
| AveragePolicyStd[4]  | 0.14147    |
| AveragePolicyStd[5]  | 0.22073    |
| AverageReturn        | 1866.2     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 355.22     |
| AverageEpisodeLength | 940.16     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 169.95     |
| TotalNEpisodes       | 26840      |
| TotalNSamples        | 9.7039e+06 |
| ExplainedVariance    | -0.010047  |
-------------------------------------
[2018-01-21 17:32:40.877580 UTC] Saving snapshot
[2018-01-21 17:32:40.877773 UTC] Starting iteration 1939
[2018-01-21 17:32:40.877913 UTC] Start collecting samples
[2018-01-21 17:32:43.221083 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:43.292107 UTC] Performing policy update
[2018-01-21 17:32:43.292640 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:43.369099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:44.272601 UTC] Performing line search
[2018-01-21 17:32:44.388994 UTC] Updating baseline
[2018-01-21 17:32:45.905396 UTC] Computing logging information
-------------------------------------
| Iteration            | 1939       |
| ExpectedImprovement  | 0.018043   |
| ActualImprovement    | 0.016212   |
| ImprovementRatio     | 0.89848    |
| MeanKL               | 0.0082491  |
| Entropy              | -2.1422    |
| Perplexity           | 0.1174     |
| AveragePolicyStd     | 0.17171    |
| AveragePolicyStd[0]  | 0.18656    |
| AveragePolicyStd[1]  | 0.18664    |
| AveragePolicyStd[2]  | 0.13997    |
| AveragePolicyStd[3]  | 0.15513    |
| AveragePolicyStd[4]  | 0.14117    |
| AveragePolicyStd[5]  | 0.22077    |
| AverageReturn        | 1854.3     |
| MinReturn            | 305.57     |
| MaxReturn            | 2072.1     |
| StdReturn            | 367.89     |
| AverageEpisodeLength | 934.86     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 176.15     |
| TotalNEpisodes       | 26842      |
| TotalNSamples        | 9.7053e+06 |
| ExplainedVariance    | 0.24136    |
-------------------------------------
[2018-01-21 17:32:46.474977 UTC] Saving snapshot
[2018-01-21 17:32:46.475163 UTC] Starting iteration 1940
[2018-01-21 17:32:46.475280 UTC] Start collecting samples
[2018-01-21 17:32:48.868975 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:48.944093 UTC] Performing policy update
[2018-01-21 17:32:48.944615 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:49.019550 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:49.930968 UTC] Performing line search
[2018-01-21 17:32:50.049245 UTC] Updating baseline
[2018-01-21 17:32:51.435247 UTC] Computing logging information
-------------------------------------
| Iteration            | 1940       |
| ExpectedImprovement  | 0.019058   |
| ActualImprovement    | 0.017169   |
| ImprovementRatio     | 0.90089    |
| MeanKL               | 0.0078712  |
| Entropy              | -2.1411    |
| Perplexity           | 0.11753    |
| AveragePolicyStd     | 0.17176    |
| AveragePolicyStd[0]  | 0.18692    |
| AveragePolicyStd[1]  | 0.18675    |
| AveragePolicyStd[2]  | 0.13994    |
| AveragePolicyStd[3]  | 0.15522    |
| AveragePolicyStd[4]  | 0.14088    |
| AveragePolicyStd[5]  | 0.22083    |
| AverageReturn        | 1883.3     |
| MinReturn            | 656.27     |
| MaxReturn            | 2072.1     |
| StdReturn            | 319.11     |
| AverageEpisodeLength | 949.24     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.71     |
| TotalNEpisodes       | 26850      |
| TotalNSamples        | 9.7133e+06 |
| ExplainedVariance    | 0.0019131  |
-------------------------------------
[2018-01-21 17:32:51.964450 UTC] Saving snapshot
[2018-01-21 17:32:51.970242 UTC] Starting iteration 1941
[2018-01-21 17:32:51.970405 UTC] Start collecting samples
[2018-01-21 17:32:54.415273 UTC] Computing input variables for policy optimization
[2018-01-21 17:32:54.488573 UTC] Performing policy update
[2018-01-21 17:32:54.489114 UTC] Computing gradient in Euclidean space
[2018-01-21 17:32:54.565370 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:32:55.620051 UTC] Performing line search
[2018-01-21 17:32:55.745854 UTC] Updating baseline
[2018-01-21 17:32:57.371029 UTC] Computing logging information
-------------------------------------
| Iteration            | 1941       |
| ExpectedImprovement  | 0.018581   |
| ActualImprovement    | 0.017924   |
| ImprovementRatio     | 0.96465    |
| MeanKL               | 0.0096266  |
| Entropy              | -2.1484    |
| Perplexity           | 0.11667    |
| AveragePolicyStd     | 0.17157    |
| AveragePolicyStd[0]  | 0.18664    |
| AveragePolicyStd[1]  | 0.1869     |
| AveragePolicyStd[2]  | 0.13994    |
| AveragePolicyStd[3]  | 0.15485    |
| AveragePolicyStd[4]  | 0.14039    |
| AveragePolicyStd[5]  | 0.22068    |
| AverageReturn        | 1889.7     |
| MinReturn            | 656.27     |
| MaxReturn            | 2072.1     |
| StdReturn            | 307.61     |
| AverageEpisodeLength | 952.7      |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.8      |
| TotalNEpisodes       | 26856      |
| TotalNSamples        | 9.7192e+06 |
| ExplainedVariance    | 0.10791    |
-------------------------------------
[2018-01-21 17:32:57.937007 UTC] Saving snapshot
[2018-01-21 17:32:57.937211 UTC] Starting iteration 1942
[2018-01-21 17:32:57.937355 UTC] Start collecting samples
[2018-01-21 17:33:00.395339 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:00.465728 UTC] Performing policy update
[2018-01-21 17:33:00.466275 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:00.542482 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:01.485254 UTC] Performing line search
[2018-01-21 17:33:01.605350 UTC] Updating baseline
[2018-01-21 17:33:03.113492 UTC] Computing logging information
-------------------------------------
| Iteration            | 1942       |
| ExpectedImprovement  | 0.018589   |
| ActualImprovement    | 0.017339   |
| ImprovementRatio     | 0.93276    |
| MeanKL               | 0.0075416  |
| Entropy              | -2.147     |
| Perplexity           | 0.11683    |
| AveragePolicyStd     | 0.17163    |
| AveragePolicyStd[0]  | 0.18663    |
| AveragePolicyStd[1]  | 0.18687    |
| AveragePolicyStd[2]  | 0.14006    |
| AveragePolicyStd[3]  | 0.15456    |
| AveragePolicyStd[4]  | 0.14035    |
| AveragePolicyStd[5]  | 0.22129    |
| AverageReturn        | 1889.3     |
| MinReturn            | 656.27     |
| MaxReturn            | 2072.1     |
| StdReturn            | 307.5      |
| AverageEpisodeLength | 952.7      |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 145.8      |
| TotalNEpisodes       | 26858      |
| TotalNSamples        | 9.7212e+06 |
| ExplainedVariance    | -0.055004  |
-------------------------------------
[2018-01-21 17:33:03.641292 UTC] Saving snapshot
[2018-01-21 17:33:03.641510 UTC] Starting iteration 1943
[2018-01-21 17:33:03.641688 UTC] Start collecting samples
[2018-01-21 17:33:06.020254 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:06.093104 UTC] Performing policy update
[2018-01-21 17:33:06.093683 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:06.169035 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:07.108566 UTC] Performing line search
[2018-01-21 17:33:07.243949 UTC] Updating baseline
[2018-01-21 17:33:08.668847 UTC] Computing logging information
-------------------------------------
| Iteration            | 1943       |
| ExpectedImprovement  | 0.016673   |
| ActualImprovement    | 0.015927   |
| ImprovementRatio     | 0.95525    |
| MeanKL               | 0.0083647  |
| Entropy              | -2.1513    |
| Perplexity           | 0.11633    |
| AveragePolicyStd     | 0.17148    |
| AveragePolicyStd[0]  | 0.18627    |
| AveragePolicyStd[1]  | 0.18678    |
| AveragePolicyStd[2]  | 0.14019    |
| AveragePolicyStd[3]  | 0.15444    |
| AveragePolicyStd[4]  | 0.14034    |
| AveragePolicyStd[5]  | 0.22089    |
| AverageReturn        | 1911.8     |
| MinReturn            | 656.27     |
| MaxReturn            | 2072.1     |
| StdReturn            | 267.03     |
| AverageEpisodeLength | 963.92     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.71     |
| TotalNEpisodes       | 26865      |
| TotalNSamples        | 9.7282e+06 |
| ExplainedVariance    | 0.0016611  |
-------------------------------------
[2018-01-21 17:33:09.193548 UTC] Saving snapshot
[2018-01-21 17:33:09.193739 UTC] Starting iteration 1944
[2018-01-21 17:33:09.193859 UTC] Start collecting samples
[2018-01-21 17:33:11.546387 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:11.622941 UTC] Performing policy update
[2018-01-21 17:33:11.623480 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:11.701108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:12.603092 UTC] Performing line search
[2018-01-21 17:33:12.724809 UTC] Updating baseline
[2018-01-21 17:33:14.203835 UTC] Computing logging information
-------------------------------------
| Iteration            | 1944       |
| ExpectedImprovement  | 0.018127   |
| ActualImprovement    | 0.016545   |
| ImprovementRatio     | 0.91271    |
| MeanKL               | 0.008208   |
| Entropy              | -2.1483    |
| Perplexity           | 0.11668    |
| AveragePolicyStd     | 0.17154    |
| AveragePolicyStd[0]  | 0.18626    |
| AveragePolicyStd[1]  | 0.1868     |
| AveragePolicyStd[2]  | 0.14026    |
| AveragePolicyStd[3]  | 0.15456    |
| AveragePolicyStd[4]  | 0.14074    |
| AveragePolicyStd[5]  | 0.22061    |
| AverageReturn        | 1904.3     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 270.81     |
| AverageEpisodeLength | 961.07     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 128.07     |
| TotalNEpisodes       | 26871      |
| TotalNSamples        | 9.7339e+06 |
| ExplainedVariance    | 0.089604   |
-------------------------------------
[2018-01-21 17:33:14.750624 UTC] Saving snapshot
[2018-01-21 17:33:14.750807 UTC] Starting iteration 1945
[2018-01-21 17:33:14.750939 UTC] Start collecting samples
[2018-01-21 17:33:17.238911 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:17.310616 UTC] Performing policy update
[2018-01-21 17:33:17.311137 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:17.387563 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:18.329644 UTC] Performing line search
[2018-01-21 17:33:18.452422 UTC] Updating baseline
[2018-01-21 17:33:19.937820 UTC] Computing logging information
-------------------------------------
| Iteration            | 1945       |
| ExpectedImprovement  | 0.018627   |
| ActualImprovement    | 0.017485   |
| ImprovementRatio     | 0.93867    |
| MeanKL               | 0.0079062  |
| Entropy              | -2.1543    |
| Perplexity           | 0.11599    |
| AveragePolicyStd     | 0.17137    |
| AveragePolicyStd[0]  | 0.18607    |
| AveragePolicyStd[1]  | 0.18618    |
| AveragePolicyStd[2]  | 0.14009    |
| AveragePolicyStd[3]  | 0.15427    |
| AveragePolicyStd[4]  | 0.14088    |
| AveragePolicyStd[5]  | 0.22072    |
| AverageReturn        | 1904       |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 257.71     |
| AverageEpisodeLength | 960.67     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.67     |
| TotalNEpisodes       | 26876      |
| TotalNSamples        | 9.7383e+06 |
| ExplainedVariance    | 0.20443    |
-------------------------------------
[2018-01-21 17:33:20.469867 UTC] Saving snapshot
[2018-01-21 17:33:20.470060 UTC] Starting iteration 1946
[2018-01-21 17:33:20.470212 UTC] Start collecting samples
[2018-01-21 17:33:23.296539 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:23.406554 UTC] Performing policy update
[2018-01-21 17:33:23.407195 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:23.485170 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:24.408751 UTC] Performing line search
[2018-01-21 17:33:24.528600 UTC] Updating baseline
[2018-01-21 17:33:25.844681 UTC] Computing logging information
-------------------------------------
| Iteration            | 1946       |
| ExpectedImprovement  | 0.018597   |
| ActualImprovement    | 0.017529   |
| ImprovementRatio     | 0.94257    |
| MeanKL               | 0.0086124  |
| Entropy              | -2.1537    |
| Perplexity           | 0.11605    |
| AveragePolicyStd     | 0.17137    |
| AveragePolicyStd[0]  | 0.18623    |
| AveragePolicyStd[1]  | 0.18589    |
| AveragePolicyStd[2]  | 0.14025    |
| AveragePolicyStd[3]  | 0.15433    |
| AveragePolicyStd[4]  | 0.14094    |
| AveragePolicyStd[5]  | 0.22054    |
| AverageReturn        | 1904       |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 257.8      |
| AverageEpisodeLength | 960.67     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.67     |
| TotalNEpisodes       | 26880      |
| TotalNSamples        | 9.7423e+06 |
| ExplainedVariance    | 0.0027232  |
-------------------------------------
[2018-01-21 17:33:26.398945 UTC] Saving snapshot
[2018-01-21 17:33:26.399195 UTC] Starting iteration 1947
[2018-01-21 17:33:26.399346 UTC] Start collecting samples
[2018-01-21 17:33:28.912284 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:29.003148 UTC] Performing policy update
[2018-01-21 17:33:29.003712 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:29.079192 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:30.190338 UTC] Performing line search
[2018-01-21 17:33:30.317229 UTC] Updating baseline
[2018-01-21 17:33:31.985168 UTC] Computing logging information
-------------------------------------
| Iteration            | 1947       |
| ExpectedImprovement  | 0.018399   |
| ActualImprovement    | 0.017601   |
| ImprovementRatio     | 0.9566     |
| MeanKL               | 0.0089045  |
| Entropy              | -2.1551    |
| Perplexity           | 0.11589    |
| AveragePolicyStd     | 0.17131    |
| AveragePolicyStd[0]  | 0.18613    |
| AveragePolicyStd[1]  | 0.18555    |
| AveragePolicyStd[2]  | 0.14038    |
| AveragePolicyStd[3]  | 0.15453    |
| AveragePolicyStd[4]  | 0.14082    |
| AveragePolicyStd[5]  | 0.22047    |
| AverageReturn        | 1903.1     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 257.46     |
| AverageEpisodeLength | 960.67     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 121.67     |
| TotalNEpisodes       | 26886      |
| TotalNSamples        | 9.7483e+06 |
| ExplainedVariance    | 0.0029315  |
-------------------------------------
[2018-01-21 17:33:32.512909 UTC] Saving snapshot
[2018-01-21 17:33:32.513114 UTC] Starting iteration 1948
[2018-01-21 17:33:32.513245 UTC] Start collecting samples
[2018-01-21 17:33:34.918725 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:35.002447 UTC] Performing policy update
[2018-01-21 17:33:35.002978 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:35.085259 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:36.043788 UTC] Performing line search
[2018-01-21 17:33:36.165799 UTC] Updating baseline
[2018-01-21 17:33:37.502533 UTC] Computing logging information
-------------------------------------
| Iteration            | 1948       |
| ExpectedImprovement  | 0.020559   |
| ActualImprovement    | 0.01912    |
| ImprovementRatio     | 0.93       |
| MeanKL               | 0.0074618  |
| Entropy              | -2.1553    |
| Perplexity           | 0.11586    |
| AveragePolicyStd     | 0.17131    |
| AveragePolicyStd[0]  | 0.18615    |
| AveragePolicyStd[1]  | 0.18592    |
| AveragePolicyStd[2]  | 0.14023    |
| AveragePolicyStd[3]  | 0.1543     |
| AveragePolicyStd[4]  | 0.14096    |
| AveragePolicyStd[5]  | 0.22031    |
| AverageReturn        | 1903.3     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 266.16     |
| AverageEpisodeLength | 960.51     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 125.66     |
| TotalNEpisodes       | 26893      |
| TotalNSamples        | 9.7547e+06 |
| ExplainedVariance    | 0.19252    |
-------------------------------------
[2018-01-21 17:33:38.030554 UTC] Saving snapshot
[2018-01-21 17:33:38.030748 UTC] Starting iteration 1949
[2018-01-21 17:33:38.030871 UTC] Start collecting samples
[2018-01-21 17:33:40.422154 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:40.491599 UTC] Performing policy update
[2018-01-21 17:33:40.492098 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:40.571086 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:41.558793 UTC] Performing line search
[2018-01-21 17:33:41.677617 UTC] Updating baseline
[2018-01-21 17:33:43.454033 UTC] Computing logging information
-------------------------------------
| Iteration            | 1949       |
| ExpectedImprovement  | 0.017072   |
| ActualImprovement    | 0.016208   |
| ImprovementRatio     | 0.94939    |
| MeanKL               | 0.0082619  |
| Entropy              | -2.1568    |
| Perplexity           | 0.1157     |
| AveragePolicyStd     | 0.17126    |
| AveragePolicyStd[0]  | 0.18594    |
| AveragePolicyStd[1]  | 0.18613    |
| AveragePolicyStd[2]  | 0.14047    |
| AveragePolicyStd[3]  | 0.15422    |
| AveragePolicyStd[4]  | 0.14077    |
| AveragePolicyStd[5]  | 0.22002    |
| AverageReturn        | 1915.9     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 243.64     |
| AverageEpisodeLength | 966.05     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.57     |
| TotalNEpisodes       | 26895      |
| TotalNSamples        | 9.7567e+06 |
| ExplainedVariance    | -0.01544   |
-------------------------------------
[2018-01-21 17:33:44.023635 UTC] Saving snapshot
[2018-01-21 17:33:44.023857 UTC] Starting iteration 1950
[2018-01-21 17:33:44.024009 UTC] Start collecting samples
[2018-01-21 17:33:46.461444 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:46.533689 UTC] Performing policy update
[2018-01-21 17:33:46.534216 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:46.610213 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:47.576279 UTC] Performing line search
[2018-01-21 17:33:47.703982 UTC] Updating baseline
[2018-01-21 17:33:49.373513 UTC] Computing logging information
-------------------------------------
| Iteration            | 1950       |
| ExpectedImprovement  | 0.018796   |
| ActualImprovement    | 0.016941   |
| ImprovementRatio     | 0.90132    |
| MeanKL               | 0.0078951  |
| Entropy              | -2.1563    |
| Perplexity           | 0.11575    |
| AveragePolicyStd     | 0.17123    |
| AveragePolicyStd[0]  | 0.18562    |
| AveragePolicyStd[1]  | 0.18606    |
| AveragePolicyStd[2]  | 0.14101    |
| AveragePolicyStd[3]  | 0.15436    |
| AveragePolicyStd[4]  | 0.14072    |
| AveragePolicyStd[5]  | 0.21962    |
| AverageReturn        | 1915.1     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 243.37     |
| AverageEpisodeLength | 966.05     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 114.57     |
| TotalNEpisodes       | 26900      |
| TotalNSamples        | 9.7617e+06 |
| ExplainedVariance    | 0.0020623  |
-------------------------------------
[2018-01-21 17:33:49.907841 UTC] Saving snapshot
[2018-01-21 17:33:49.913361 UTC] Starting iteration 1951
[2018-01-21 17:33:49.913518 UTC] Start collecting samples
[2018-01-21 17:33:52.352050 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:52.427910 UTC] Performing policy update
[2018-01-21 17:33:52.428514 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:52.506448 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:53.418009 UTC] Performing line search
[2018-01-21 17:33:53.589918 UTC] Updating baseline
[2018-01-21 17:33:54.973876 UTC] Computing logging information
-------------------------------------
| Iteration            | 1951       |
| ExpectedImprovement  | 0.017663   |
| ActualImprovement    | 0.017072   |
| ImprovementRatio     | 0.96653    |
| MeanKL               | 0.0084672  |
| Entropy              | -2.1586    |
| Perplexity           | 0.11549    |
| AveragePolicyStd     | 0.17118    |
| AveragePolicyStd[0]  | 0.18546    |
| AveragePolicyStd[1]  | 0.18633    |
| AveragePolicyStd[2]  | 0.1409     |
| AveragePolicyStd[3]  | 0.15438    |
| AveragePolicyStd[4]  | 0.14043    |
| AveragePolicyStd[5]  | 0.2196     |
| AverageReturn        | 1924.3     |
| MinReturn            | 656.27     |
| MaxReturn            | 2054.3     |
| StdReturn            | 232.14     |
| AverageEpisodeLength | 969.95     |
| MinEpisodeLength     | 365        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 108.88     |
| TotalNEpisodes       | 26907      |
| TotalNSamples        | 9.7687e+06 |
| ExplainedVariance    | 0.0044909  |
-------------------------------------
[2018-01-21 17:33:55.499296 UTC] Saving snapshot
[2018-01-21 17:33:55.499492 UTC] Starting iteration 1952
[2018-01-21 17:33:55.499641 UTC] Start collecting samples
[2018-01-21 17:33:57.900430 UTC] Computing input variables for policy optimization
[2018-01-21 17:33:57.973159 UTC] Performing policy update
[2018-01-21 17:33:57.973669 UTC] Computing gradient in Euclidean space
[2018-01-21 17:33:58.050613 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:33:58.981986 UTC] Performing line search
[2018-01-21 17:33:59.108007 UTC] Updating baseline
[2018-01-21 17:34:00.632448 UTC] Computing logging information
-------------------------------------
| Iteration            | 1952       |
| ExpectedImprovement  | 0.020562   |
| ActualImprovement    | 0.018585   |
| ImprovementRatio     | 0.90385    |
| MeanKL               | 0.0081634  |
| Entropy              | -2.155     |
| Perplexity           | 0.1159     |
| AveragePolicyStd     | 0.17129    |
| AveragePolicyStd[0]  | 0.1857     |
| AveragePolicyStd[1]  | 0.18599    |
| AveragePolicyStd[2]  | 0.14062    |
| AveragePolicyStd[3]  | 0.1549     |
| AveragePolicyStd[4]  | 0.1406     |
| AveragePolicyStd[5]  | 0.21991    |
| AverageReturn        | 1925.3     |
| MinReturn            | 284.79     |
| MaxReturn            | 2054.3     |
| StdReturn            | 251.25     |
| AverageEpisodeLength | 969.88     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 119.11     |
| TotalNEpisodes       | 26911      |
| TotalNSamples        | 9.7719e+06 |
| ExplainedVariance    | 0.20187    |
-------------------------------------
[2018-01-21 17:34:01.160682 UTC] Saving snapshot
[2018-01-21 17:34:01.160880 UTC] Starting iteration 1953
[2018-01-21 17:34:01.161018 UTC] Start collecting samples
[2018-01-21 17:34:03.609245 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:03.688701 UTC] Performing policy update
[2018-01-21 17:34:03.689141 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:03.768603 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:04.673098 UTC] Performing line search
[2018-01-21 17:34:04.790595 UTC] Updating baseline
[2018-01-21 17:34:06.200438 UTC] Computing logging information
-------------------------------------
| Iteration            | 1953       |
| ExpectedImprovement  | 0.019671   |
| ActualImprovement    | 0.018344   |
| ImprovementRatio     | 0.93251    |
| MeanKL               | 0.0090667  |
| Entropy              | -2.1609    |
| Perplexity           | 0.11522    |
| AveragePolicyStd     | 0.17113    |
| AveragePolicyStd[0]  | 0.18601    |
| AveragePolicyStd[1]  | 0.1857     |
| AveragePolicyStd[2]  | 0.14052    |
| AveragePolicyStd[3]  | 0.15457    |
| AveragePolicyStd[4]  | 0.1403     |
| AveragePolicyStd[5]  | 0.2197     |
| AverageReturn        | 1914.2     |
| MinReturn            | 284.79     |
| MaxReturn            | 2054.3     |
| StdReturn            | 279.6      |
| AverageEpisodeLength | 963.88     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.87     |
| TotalNEpisodes       | 26916      |
| TotalNSamples        | 9.7763e+06 |
| ExplainedVariance    | 0.1512     |
-------------------------------------
[2018-01-21 17:34:06.728677 UTC] Saving snapshot
[2018-01-21 17:34:06.728866 UTC] Starting iteration 1954
[2018-01-21 17:34:06.729000 UTC] Start collecting samples
[2018-01-21 17:34:09.067725 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:09.140770 UTC] Performing policy update
[2018-01-21 17:34:09.141362 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:09.217374 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:10.162304 UTC] Performing line search
[2018-01-21 17:34:10.290774 UTC] Updating baseline
[2018-01-21 17:34:11.886995 UTC] Computing logging information
-------------------------------------
| Iteration            | 1954       |
| ExpectedImprovement  | 0.01788    |
| ActualImprovement    | 0.016451   |
| ImprovementRatio     | 0.92011    |
| MeanKL               | 0.0098403  |
| Entropy              | -2.1676    |
| Perplexity           | 0.11446    |
| AveragePolicyStd     | 0.17094    |
| AveragePolicyStd[0]  | 0.18598    |
| AveragePolicyStd[1]  | 0.18561    |
| AveragePolicyStd[2]  | 0.14028    |
| AveragePolicyStd[3]  | 0.15469    |
| AveragePolicyStd[4]  | 0.13994    |
| AveragePolicyStd[5]  | 0.21915    |
| AverageReturn        | 1916.2     |
| MinReturn            | 284.79     |
| MaxReturn            | 2056.4     |
| StdReturn            | 280.16     |
| AverageEpisodeLength | 963.88     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.87     |
| TotalNEpisodes       | 26923      |
| TotalNSamples        | 9.7833e+06 |
| ExplainedVariance    | -0.011353  |
-------------------------------------
[2018-01-21 17:34:12.412619 UTC] Saving snapshot
[2018-01-21 17:34:12.412824 UTC] Starting iteration 1955
[2018-01-21 17:34:12.412967 UTC] Start collecting samples
[2018-01-21 17:34:14.891877 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:14.965871 UTC] Performing policy update
[2018-01-21 17:34:14.966406 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:15.041858 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:15.957119 UTC] Performing line search
[2018-01-21 17:34:16.085781 UTC] Updating baseline
[2018-01-21 17:34:17.626676 UTC] Computing logging information
-------------------------------------
| Iteration            | 1955       |
| ExpectedImprovement  | 0.01955    |
| ActualImprovement    | 0.018231   |
| ImprovementRatio     | 0.93251    |
| MeanKL               | 0.009147   |
| Entropy              | -2.1665    |
| Perplexity           | 0.11458    |
| AveragePolicyStd     | 0.17095    |
| AveragePolicyStd[0]  | 0.18604    |
| AveragePolicyStd[1]  | 0.18591    |
| AveragePolicyStd[2]  | 0.14053    |
| AveragePolicyStd[3]  | 0.15432    |
| AveragePolicyStd[4]  | 0.14014    |
| AveragePolicyStd[5]  | 0.2188     |
| AverageReturn        | 1915.8     |
| MinReturn            | 284.79     |
| MaxReturn            | 2056.4     |
| StdReturn            | 280.21     |
| AverageEpisodeLength | 963.47     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.82     |
| TotalNEpisodes       | 26926      |
| TotalNSamples        | 9.7862e+06 |
| ExplainedVariance    | 0.23444    |
-------------------------------------
[2018-01-21 17:34:18.155996 UTC] Saving snapshot
[2018-01-21 17:34:18.156203 UTC] Starting iteration 1956
[2018-01-21 17:34:18.156336 UTC] Start collecting samples
[2018-01-21 17:34:20.930630 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:21.013018 UTC] Performing policy update
[2018-01-21 17:34:21.013614 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:21.089910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:22.019958 UTC] Performing line search
[2018-01-21 17:34:22.145934 UTC] Updating baseline
[2018-01-21 17:34:23.225921 UTC] Computing logging information
------------------------------------
| Iteration            | 1956      |
| ExpectedImprovement  | 0.018198  |
| ActualImprovement    | 0.016972  |
| ImprovementRatio     | 0.93261   |
| MeanKL               | 0.008588  |
| Entropy              | -2.1681   |
| Perplexity           | 0.11439   |
| AveragePolicyStd     | 0.17092   |
| AveragePolicyStd[0]  | 0.18603   |
| AveragePolicyStd[1]  | 0.18591   |
| AveragePolicyStd[2]  | 0.14043   |
| AveragePolicyStd[3]  | 0.15396   |
| AveragePolicyStd[4]  | 0.14025   |
| AveragePolicyStd[5]  | 0.21894   |
| AverageReturn        | 1892.4    |
| MinReturn            | 284.79    |
| MaxReturn            | 2056.4    |
| StdReturn            | 319.44    |
| AverageEpisodeLength | 951.64    |
| MinEpisodeLength     | 183       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 150.4     |
| TotalNEpisodes       | 26935     |
| TotalNSamples        | 9.794e+06 |
| ExplainedVariance    | 0.18157   |
------------------------------------
[2018-01-21 17:34:23.752884 UTC] Saving snapshot
[2018-01-21 17:34:23.753083 UTC] Starting iteration 1957
[2018-01-21 17:34:23.753241 UTC] Start collecting samples
[2018-01-21 17:34:26.388915 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:26.460667 UTC] Performing policy update
[2018-01-21 17:34:26.461190 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:26.537924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:27.493610 UTC] Performing line search
[2018-01-21 17:34:27.610768 UTC] Updating baseline
[2018-01-21 17:34:28.884585 UTC] Computing logging information
------------------------------------
| Iteration            | 1957      |
| ExpectedImprovement  | 0.018147  |
| ActualImprovement    | 0.017297  |
| ImprovementRatio     | 0.95312   |
| MeanKL               | 0.0077977 |
| Entropy              | -2.1692   |
| Perplexity           | 0.11427   |
| AveragePolicyStd     | 0.1709    |
| AveragePolicyStd[0]  | 0.18604   |
| AveragePolicyStd[1]  | 0.18609   |
| AveragePolicyStd[2]  | 0.14037   |
| AveragePolicyStd[3]  | 0.15369   |
| AveragePolicyStd[4]  | 0.14027   |
| AveragePolicyStd[5]  | 0.21893   |
| AverageReturn        | 1892.8    |
| MinReturn            | 284.79    |
| MaxReturn            | 2056.4    |
| StdReturn            | 319.6     |
| AverageEpisodeLength | 951.32    |
| MinEpisodeLength     | 183       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 150.33    |
| TotalNEpisodes       | 26940     |
| TotalNSamples        | 9.799e+06 |
| ExplainedVariance    | 0.1743    |
------------------------------------
[2018-01-21 17:34:29.449628 UTC] Saving snapshot
[2018-01-21 17:34:29.449834 UTC] Starting iteration 1958
[2018-01-21 17:34:29.449964 UTC] Start collecting samples
[2018-01-21 17:34:31.838704 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:31.916772 UTC] Performing policy update
[2018-01-21 17:34:31.917335 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:31.992611 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:32.919379 UTC] Performing line search
[2018-01-21 17:34:33.037322 UTC] Updating baseline
[2018-01-21 17:34:34.652538 UTC] Computing logging information
-------------------------------------
| Iteration            | 1958       |
| ExpectedImprovement  | 0.016219   |
| ActualImprovement    | 0.015341   |
| ImprovementRatio     | 0.94587    |
| MeanKL               | 0.0079797  |
| Entropy              | -2.1649    |
| Perplexity           | 0.11476    |
| AveragePolicyStd     | 0.17099    |
| AveragePolicyStd[0]  | 0.1861     |
| AveragePolicyStd[1]  | 0.18607    |
| AveragePolicyStd[2]  | 0.14055    |
| AveragePolicyStd[3]  | 0.15394    |
| AveragePolicyStd[4]  | 0.14053    |
| AveragePolicyStd[5]  | 0.21877    |
| AverageReturn        | 1891.4     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 325.88     |
| AverageEpisodeLength | 950.4      |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.52     |
| TotalNEpisodes       | 26943      |
| TotalNSamples        | 9.8014e+06 |
| ExplainedVariance    | 0.052979   |
-------------------------------------
[2018-01-21 17:34:35.192960 UTC] Saving snapshot
[2018-01-21 17:34:35.193137 UTC] Starting iteration 1959
[2018-01-21 17:34:35.193240 UTC] Start collecting samples
[2018-01-21 17:34:37.641786 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:37.724802 UTC] Performing policy update
[2018-01-21 17:34:37.725448 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:37.800734 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:38.751096 UTC] Performing line search
[2018-01-21 17:34:38.869839 UTC] Updating baseline
[2018-01-21 17:34:40.190094 UTC] Computing logging information
-------------------------------------
| Iteration            | 1959       |
| ExpectedImprovement  | 0.018341   |
| ActualImprovement    | 0.017315   |
| ImprovementRatio     | 0.94404    |
| MeanKL               | 0.0081198  |
| Entropy              | -2.1613    |
| Perplexity           | 0.11518    |
| AveragePolicyStd     | 0.17109    |
| AveragePolicyStd[0]  | 0.18627    |
| AveragePolicyStd[1]  | 0.18646    |
| AveragePolicyStd[2]  | 0.14072    |
| AveragePolicyStd[3]  | 0.15394    |
| AveragePolicyStd[4]  | 0.14059    |
| AveragePolicyStd[5]  | 0.21855    |
| AverageReturn        | 1888.6     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 326.78     |
| AverageEpisodeLength | 948.66     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.85     |
| TotalNEpisodes       | 26950      |
| TotalNSamples        | 9.8082e+06 |
| ExplainedVariance    | 0.1478     |
-------------------------------------
[2018-01-21 17:34:40.719413 UTC] Saving snapshot
[2018-01-21 17:34:40.719609 UTC] Starting iteration 1960
[2018-01-21 17:34:40.719749 UTC] Start collecting samples
[2018-01-21 17:34:43.390054 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:43.471488 UTC] Performing policy update
[2018-01-21 17:34:43.472043 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:43.549354 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:44.456246 UTC] Performing line search
[2018-01-21 17:34:44.573668 UTC] Updating baseline
[2018-01-21 17:34:45.935027 UTC] Computing logging information
-------------------------------------
| Iteration            | 1960       |
| ExpectedImprovement  | 0.019029   |
| ActualImprovement    | 0.017987   |
| ImprovementRatio     | 0.94526    |
| MeanKL               | 0.0089044  |
| Entropy              | -2.1576    |
| Perplexity           | 0.1156     |
| AveragePolicyStd     | 0.1712     |
| AveragePolicyStd[0]  | 0.18658    |
| AveragePolicyStd[1]  | 0.18627    |
| AveragePolicyStd[2]  | 0.14069    |
| AveragePolicyStd[3]  | 0.15411    |
| AveragePolicyStd[4]  | 0.14077    |
| AveragePolicyStd[5]  | 0.21877    |
| AverageReturn        | 1863.8     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 362.52     |
| AverageEpisodeLength | 935.44     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 171.56     |
| TotalNEpisodes       | 26957      |
| TotalNSamples        | 9.8137e+06 |
| ExplainedVariance    | 0.31589    |
-------------------------------------
[2018-01-21 17:34:46.485042 UTC] Saving snapshot
[2018-01-21 17:34:46.490536 UTC] Starting iteration 1961
[2018-01-21 17:34:46.490696 UTC] Start collecting samples
[2018-01-21 17:34:48.968080 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:49.041180 UTC] Performing policy update
[2018-01-21 17:34:49.041772 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:49.116139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:50.034134 UTC] Performing line search
[2018-01-21 17:34:50.150283 UTC] Updating baseline
[2018-01-21 17:34:51.453214 UTC] Computing logging information
-------------------------------------
| Iteration            | 1961       |
| ExpectedImprovement  | 0.01791    |
| ActualImprovement    | 0.019309   |
| ImprovementRatio     | 1.0781     |
| MeanKL               | 0.007892   |
| Entropy              | -2.1606    |
| Perplexity           | 0.11526    |
| AveragePolicyStd     | 0.17115    |
| AveragePolicyStd[0]  | 0.18679    |
| AveragePolicyStd[1]  | 0.18624    |
| AveragePolicyStd[2]  | 0.14053    |
| AveragePolicyStd[3]  | 0.15364    |
| AveragePolicyStd[4]  | 0.14048    |
| AveragePolicyStd[5]  | 0.21924    |
| AverageReturn        | 1848.1     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 380.64     |
| AverageEpisodeLength | 926.27     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.92     |
| TotalNEpisodes       | 26963      |
| TotalNSamples        | 9.8188e+06 |
| ExplainedVariance    | 0.18877    |
-------------------------------------
[2018-01-21 17:34:52.014329 UTC] Saving snapshot
[2018-01-21 17:34:52.014534 UTC] Starting iteration 1962
[2018-01-21 17:34:52.014672 UTC] Start collecting samples
[2018-01-21 17:34:54.554253 UTC] Computing input variables for policy optimization
[2018-01-21 17:34:54.626954 UTC] Performing policy update
[2018-01-21 17:34:54.627496 UTC] Computing gradient in Euclidean space
[2018-01-21 17:34:54.709029 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:34:55.596567 UTC] Performing line search
[2018-01-21 17:34:55.715370 UTC] Updating baseline
[2018-01-21 17:34:57.149137 UTC] Computing logging information
-------------------------------------
| Iteration            | 1962       |
| ExpectedImprovement  | 0.020724   |
| ActualImprovement    | 0.018907   |
| ImprovementRatio     | 0.91233    |
| MeanKL               | 0.0085504  |
| Entropy              | -2.1633    |
| Perplexity           | 0.11494    |
| AveragePolicyStd     | 0.17109    |
| AveragePolicyStd[0]  | 0.18723    |
| AveragePolicyStd[1]  | 0.18634    |
| AveragePolicyStd[2]  | 0.13994    |
| AveragePolicyStd[3]  | 0.15372    |
| AveragePolicyStd[4]  | 0.14044    |
| AveragePolicyStd[5]  | 0.21889    |
| AverageReturn        | 1830.3     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 397.5      |
| AverageEpisodeLength | 917.05     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 187.67     |
| TotalNEpisodes       | 26968      |
| TotalNSamples        | 9.8229e+06 |
| ExplainedVariance    | 0.26873    |
-------------------------------------
[2018-01-21 17:34:57.673212 UTC] Saving snapshot
[2018-01-21 17:34:57.673429 UTC] Starting iteration 1963
[2018-01-21 17:34:57.673596 UTC] Start collecting samples
[2018-01-21 17:35:00.354713 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:00.459115 UTC] Performing policy update
[2018-01-21 17:35:00.459725 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:00.535290 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:01.439329 UTC] Performing line search
[2018-01-21 17:35:01.557544 UTC] Updating baseline
[2018-01-21 17:35:02.983558 UTC] Computing logging information
-------------------------------------
| Iteration            | 1963       |
| ExpectedImprovement  | 0.018438   |
| ActualImprovement    | 0.018562   |
| ImprovementRatio     | 1.0068     |
| MeanKL               | 0.0080148  |
| Entropy              | -2.1658    |
| Perplexity           | 0.11466    |
| AveragePolicyStd     | 0.17102    |
| AveragePolicyStd[0]  | 0.18727    |
| AveragePolicyStd[1]  | 0.18623    |
| AveragePolicyStd[2]  | 0.13985    |
| AveragePolicyStd[3]  | 0.15357    |
| AveragePolicyStd[4]  | 0.1405     |
| AveragePolicyStd[5]  | 0.21871    |
| AverageReturn        | 1830.3     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 402.58     |
| AverageEpisodeLength | 915.84     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.35     |
| TotalNEpisodes       | 26974      |
| TotalNSamples        | 9.8285e+06 |
| ExplainedVariance    | 0.098558   |
-------------------------------------
[2018-01-21 17:35:03.510617 UTC] Saving snapshot
[2018-01-21 17:35:03.510811 UTC] Starting iteration 1964
[2018-01-21 17:35:03.510951 UTC] Start collecting samples
[2018-01-21 17:35:05.822548 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:05.901401 UTC] Performing policy update
[2018-01-21 17:35:05.902279 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:05.988223 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:06.962770 UTC] Performing line search
[2018-01-21 17:35:07.082904 UTC] Updating baseline
[2018-01-21 17:35:08.681111 UTC] Computing logging information
-------------------------------------
| Iteration            | 1964       |
| ExpectedImprovement  | 0.018337   |
| ActualImprovement    | 0.017221   |
| ImprovementRatio     | 0.93913    |
| MeanKL               | 0.0086431  |
| Entropy              | -2.1673    |
| Perplexity           | 0.11449    |
| AveragePolicyStd     | 0.17098    |
| AveragePolicyStd[0]  | 0.18716    |
| AveragePolicyStd[1]  | 0.18648    |
| AveragePolicyStd[2]  | 0.13996    |
| AveragePolicyStd[3]  | 0.15312    |
| AveragePolicyStd[4]  | 0.14046    |
| AveragePolicyStd[5]  | 0.21871    |
| AverageReturn        | 1846       |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 397.71     |
| AverageEpisodeLength | 922.28     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 186.59     |
| TotalNEpisodes       | 26979      |
| TotalNSamples        | 9.8335e+06 |
| ExplainedVariance    | 0.0059546  |
-------------------------------------
[2018-01-21 17:35:09.200730 UTC] Saving snapshot
[2018-01-21 17:35:09.200918 UTC] Starting iteration 1965
[2018-01-21 17:35:09.201071 UTC] Start collecting samples
[2018-01-21 17:35:11.589480 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:11.662174 UTC] Performing policy update
[2018-01-21 17:35:11.662713 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:11.739900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:12.731453 UTC] Performing line search
[2018-01-21 17:35:12.886859 UTC] Updating baseline
[2018-01-21 17:35:14.199722 UTC] Computing logging information
-------------------------------------
| Iteration            | 1965       |
| ExpectedImprovement  | 0.01699    |
| ActualImprovement    | 0.01637    |
| ImprovementRatio     | 0.96353    |
| MeanKL               | 0.0082081  |
| Entropy              | -2.1654    |
| Perplexity           | 0.11471    |
| AveragePolicyStd     | 0.17104    |
| AveragePolicyStd[0]  | 0.18706    |
| AveragePolicyStd[1]  | 0.18706    |
| AveragePolicyStd[2]  | 0.14018    |
| AveragePolicyStd[3]  | 0.15265    |
| AveragePolicyStd[4]  | 0.14058    |
| AveragePolicyStd[5]  | 0.21872    |
| AverageReturn        | 1833.2     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 414.77     |
| AverageEpisodeLength | 915.62     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 194.81     |
| TotalNEpisodes       | 26984      |
| TotalNSamples        | 9.8378e+06 |
| ExplainedVariance    | 0.31338    |
-------------------------------------
[2018-01-21 17:35:14.727993 UTC] Saving snapshot
[2018-01-21 17:35:14.728171 UTC] Starting iteration 1966
[2018-01-21 17:35:14.728299 UTC] Start collecting samples
[2018-01-21 17:35:17.180486 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:17.253327 UTC] Performing policy update
[2018-01-21 17:35:17.254324 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:17.330321 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:18.281141 UTC] Performing line search
[2018-01-21 17:35:18.398760 UTC] Updating baseline
[2018-01-21 17:35:19.704605 UTC] Computing logging information
-------------------------------------
| Iteration            | 1966       |
| ExpectedImprovement  | 0.018189   |
| ActualImprovement    | 0.018074   |
| ImprovementRatio     | 0.99364    |
| MeanKL               | 0.0088017  |
| Entropy              | -2.1637    |
| Perplexity           | 0.1149     |
| AveragePolicyStd     | 0.17109    |
| AveragePolicyStd[0]  | 0.18702    |
| AveragePolicyStd[1]  | 0.18687    |
| AveragePolicyStd[2]  | 0.1402     |
| AveragePolicyStd[3]  | 0.15287    |
| AveragePolicyStd[4]  | 0.14059    |
| AveragePolicyStd[5]  | 0.219      |
| AverageReturn        | 1846.5     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 406.03     |
| AverageEpisodeLength | 920.56     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 190.58     |
| TotalNEpisodes       | 26990      |
| TotalNSamples        | 9.8438e+06 |
| ExplainedVariance    | -0.03027   |
-------------------------------------
[2018-01-21 17:35:20.358518 UTC] Saving snapshot
[2018-01-21 17:35:20.358731 UTC] Starting iteration 1967
[2018-01-21 17:35:20.358862 UTC] Start collecting samples
[2018-01-21 17:35:22.760240 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:22.835287 UTC] Performing policy update
[2018-01-21 17:35:22.835890 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:22.912939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:23.790713 UTC] Performing line search
[2018-01-21 17:35:23.910236 UTC] Updating baseline
[2018-01-21 17:35:25.235842 UTC] Computing logging information
-------------------------------------
| Iteration            | 1967       |
| ExpectedImprovement  | 0.017915   |
| ActualImprovement    | 0.016981   |
| ImprovementRatio     | 0.94787    |
| MeanKL               | 0.0089712  |
| Entropy              | -2.1654    |
| Perplexity           | 0.1147     |
| AveragePolicyStd     | 0.17105    |
| AveragePolicyStd[0]  | 0.18684    |
| AveragePolicyStd[1]  | 0.1872     |
| AveragePolicyStd[2]  | 0.1401     |
| AveragePolicyStd[3]  | 0.15278    |
| AveragePolicyStd[4]  | 0.14049    |
| AveragePolicyStd[5]  | 0.2189     |
| AverageReturn        | 1840.2     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 416.89     |
| AverageEpisodeLength | 916.1      |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.37     |
| TotalNEpisodes       | 26996      |
| TotalNSamples        | 9.8493e+06 |
| ExplainedVariance    | 0.11408    |
-------------------------------------
[2018-01-21 17:35:25.766163 UTC] Saving snapshot
[2018-01-21 17:35:25.766345 UTC] Starting iteration 1968
[2018-01-21 17:35:25.766492 UTC] Start collecting samples
[2018-01-21 17:35:28.184514 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:28.258387 UTC] Performing policy update
[2018-01-21 17:35:28.258912 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:28.335932 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:29.262333 UTC] Performing line search
[2018-01-21 17:35:29.387389 UTC] Updating baseline
[2018-01-21 17:35:30.674522 UTC] Computing logging information
-------------------------------------
| Iteration            | 1968       |
| ExpectedImprovement  | 0.020161   |
| ActualImprovement    | 0.018762   |
| ImprovementRatio     | 0.93062    |
| MeanKL               | 0.0075411  |
| Entropy              | -2.1628    |
| Perplexity           | 0.11501    |
| AveragePolicyStd     | 0.17111    |
| AveragePolicyStd[0]  | 0.18715    |
| AveragePolicyStd[1]  | 0.18727    |
| AveragePolicyStd[2]  | 0.14017    |
| AveragePolicyStd[3]  | 0.15269    |
| AveragePolicyStd[4]  | 0.14076    |
| AveragePolicyStd[5]  | 0.21863    |
| AverageReturn        | 1841.2     |
| MinReturn            | 284.79     |
| MaxReturn            | 2089.8     |
| StdReturn            | 417.29     |
| AverageEpisodeLength | 916.1      |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.37     |
| TotalNEpisodes       | 26999      |
| TotalNSamples        | 9.8523e+06 |
| ExplainedVariance    | 0.18231    |
-------------------------------------
[2018-01-21 17:35:31.202949 UTC] Saving snapshot
[2018-01-21 17:35:31.203140 UTC] Starting iteration 1969
[2018-01-21 17:35:31.203277 UTC] Start collecting samples
[2018-01-21 17:35:33.598570 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:33.674138 UTC] Performing policy update
[2018-01-21 17:35:33.674710 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:33.761305 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:34.720629 UTC] Performing line search
[2018-01-21 17:35:34.841092 UTC] Updating baseline
[2018-01-21 17:35:36.078804 UTC] Computing logging information
------------------------------------
| Iteration            | 1969      |
| ExpectedImprovement  | 0.020111  |
| ActualImprovement    | 0.018602  |
| ImprovementRatio     | 0.92495   |
| MeanKL               | 0.0083019 |
| Entropy              | -2.1586   |
| Perplexity           | 0.11549   |
| AveragePolicyStd     | 0.17124   |
| AveragePolicyStd[0]  | 0.18699   |
| AveragePolicyStd[1]  | 0.18767   |
| AveragePolicyStd[2]  | 0.14039   |
| AveragePolicyStd[3]  | 0.15255   |
| AveragePolicyStd[4]  | 0.14087   |
| AveragePolicyStd[5]  | 0.21893   |
| AverageReturn        | 1795.5    |
| MinReturn            | 61.933    |
| MaxReturn            | 2092.8    |
| StdReturn            | 487.49    |
| AverageEpisodeLength | 893.05    |
| MinEpisodeLength     | 70        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 228.27    |
| TotalNEpisodes       | 27008     |
| TotalNSamples        | 9.859e+06 |
| ExplainedVariance    | 0.21675   |
------------------------------------
[2018-01-21 17:35:36.610052 UTC] Saving snapshot
[2018-01-21 17:35:36.610250 UTC] Starting iteration 1970
[2018-01-21 17:35:36.610404 UTC] Start collecting samples
[2018-01-21 17:35:39.390858 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:39.468537 UTC] Performing policy update
[2018-01-21 17:35:39.469033 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:39.544393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:40.504169 UTC] Performing line search
[2018-01-21 17:35:40.627667 UTC] Updating baseline
[2018-01-21 17:35:42.099217 UTC] Computing logging information
-------------------------------------
| Iteration            | 1970       |
| ExpectedImprovement  | 0.018444   |
| ActualImprovement    | 0.017893   |
| ImprovementRatio     | 0.97012    |
| MeanKL               | 0.0083997  |
| Entropy              | -2.1665    |
| Perplexity           | 0.11458    |
| AveragePolicyStd     | 0.17102    |
| AveragePolicyStd[0]  | 0.18637    |
| AveragePolicyStd[1]  | 0.18769    |
| AveragePolicyStd[2]  | 0.14035    |
| AveragePolicyStd[3]  | 0.15215    |
| AveragePolicyStd[4]  | 0.14059    |
| AveragePolicyStd[5]  | 0.219      |
| AverageReturn        | 1818.7     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 460.42     |
| AverageEpisodeLength | 902.05     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.29     |
| TotalNEpisodes       | 27015      |
| TotalNSamples        | 9.8655e+06 |
| ExplainedVariance    | 0.11218    |
-------------------------------------
[2018-01-21 17:35:42.642470 UTC] Saving snapshot
[2018-01-21 17:35:42.647969 UTC] Starting iteration 1971
[2018-01-21 17:35:42.648128 UTC] Start collecting samples
[2018-01-21 17:35:44.927311 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:44.998842 UTC] Performing policy update
[2018-01-21 17:35:44.999416 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:45.079334 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:46.043828 UTC] Performing line search
[2018-01-21 17:35:46.166083 UTC] Updating baseline
[2018-01-21 17:35:47.972641 UTC] Computing logging information
-------------------------------------
| Iteration            | 1971       |
| ExpectedImprovement  | 0.01958    |
| ActualImprovement    | 0.019623   |
| ImprovementRatio     | 1.0022     |
| MeanKL               | 0.0080624  |
| Entropy              | -2.1737    |
| Perplexity           | 0.11375    |
| AveragePolicyStd     | 0.17081    |
| AveragePolicyStd[0]  | 0.18619    |
| AveragePolicyStd[1]  | 0.18744    |
| AveragePolicyStd[2]  | 0.14044    |
| AveragePolicyStd[3]  | 0.15185    |
| AveragePolicyStd[4]  | 0.14029    |
| AveragePolicyStd[5]  | 0.21867    |
| AverageReturn        | 1818.7     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 460.42     |
| AverageEpisodeLength | 902.05     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.29     |
| TotalNEpisodes       | 27015      |
| TotalNSamples        | 9.8655e+06 |
| ExplainedVariance    | 0.097649   |
-------------------------------------
[2018-01-21 17:35:48.562588 UTC] Saving snapshot
[2018-01-21 17:35:48.562806 UTC] Starting iteration 1972
[2018-01-21 17:35:48.562948 UTC] Start collecting samples
[2018-01-21 17:35:51.234484 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:51.319203 UTC] Performing policy update
[2018-01-21 17:35:51.319770 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:51.401185 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:52.387093 UTC] Performing line search
[2018-01-21 17:35:52.518409 UTC] Updating baseline
[2018-01-21 17:35:53.800293 UTC] Computing logging information
-------------------------------------
| Iteration            | 1972       |
| ExpectedImprovement  | 0.017423   |
| ActualImprovement    | 0.016315   |
| ImprovementRatio     | 0.93646    |
| MeanKL               | 0.0087913  |
| Entropy              | -2.1753    |
| Perplexity           | 0.11357    |
| AveragePolicyStd     | 0.17077    |
| AveragePolicyStd[0]  | 0.18623    |
| AveragePolicyStd[1]  | 0.18705    |
| AveragePolicyStd[2]  | 0.14041    |
| AveragePolicyStd[3]  | 0.15192    |
| AveragePolicyStd[4]  | 0.14023    |
| AveragePolicyStd[5]  | 0.21877    |
| AverageReturn        | 1803.8     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 471.5      |
| AverageEpisodeLength | 894.24     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 220.04     |
| TotalNEpisodes       | 27022      |
| TotalNSamples        | 9.8717e+06 |
| ExplainedVariance    | 0.23329    |
-------------------------------------
[2018-01-21 17:35:54.364242 UTC] Saving snapshot
[2018-01-21 17:35:54.364428 UTC] Starting iteration 1973
[2018-01-21 17:35:54.364585 UTC] Start collecting samples
[2018-01-21 17:35:57.625021 UTC] Computing input variables for policy optimization
[2018-01-21 17:35:57.699706 UTC] Performing policy update
[2018-01-21 17:35:57.700282 UTC] Computing gradient in Euclidean space
[2018-01-21 17:35:57.778929 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:35:58.711227 UTC] Performing line search
[2018-01-21 17:35:58.833321 UTC] Updating baseline
[2018-01-21 17:36:00.041881 UTC] Computing logging information
-------------------------------------
| Iteration            | 1973       |
| ExpectedImprovement  | 0.018289   |
| ActualImprovement    | 0.017487   |
| ImprovementRatio     | 0.95616    |
| MeanKL               | 0.0086158  |
| Entropy              | -2.1783    |
| Perplexity           | 0.11324    |
| AveragePolicyStd     | 0.17069    |
| AveragePolicyStd[0]  | 0.186      |
| AveragePolicyStd[1]  | 0.187      |
| AveragePolicyStd[2]  | 0.14026    |
| AveragePolicyStd[3]  | 0.15192    |
| AveragePolicyStd[4]  | 0.14016    |
| AveragePolicyStd[5]  | 0.21879    |
| AverageReturn        | 1829.2     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 451.99     |
| AverageEpisodeLength | 905.35     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.11     |
| TotalNEpisodes       | 27030      |
| TotalNSamples        | 9.8797e+06 |
| ExplainedVariance    | -0.0018946 |
-------------------------------------
[2018-01-21 17:36:00.569769 UTC] Saving snapshot
[2018-01-21 17:36:00.569966 UTC] Starting iteration 1974
[2018-01-21 17:36:00.570098 UTC] Start collecting samples
[2018-01-21 17:36:03.006180 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:03.077022 UTC] Performing policy update
[2018-01-21 17:36:03.077532 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:03.151950 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:04.040054 UTC] Performing line search
[2018-01-21 17:36:04.156362 UTC] Updating baseline
[2018-01-21 17:36:05.574533 UTC] Computing logging information
-------------------------------------
| Iteration            | 1974       |
| ExpectedImprovement  | 0.018703   |
| ActualImprovement    | 0.015917   |
| ImprovementRatio     | 0.85104    |
| MeanKL               | 0.0090978  |
| Entropy              | -2.1778    |
| Perplexity           | 0.11329    |
| AveragePolicyStd     | 0.17071    |
| AveragePolicyStd[0]  | 0.18613    |
| AveragePolicyStd[1]  | 0.18725    |
| AveragePolicyStd[2]  | 0.14028    |
| AveragePolicyStd[3]  | 0.15171    |
| AveragePolicyStd[4]  | 0.14015    |
| AveragePolicyStd[5]  | 0.21872    |
| AverageReturn        | 1830       |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 452.22     |
| AverageEpisodeLength | 905.35     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.11     |
| TotalNEpisodes       | 27031      |
| TotalNSamples        | 9.8807e+06 |
| ExplainedVariance    | 0.028279   |
-------------------------------------
[2018-01-21 17:36:06.105821 UTC] Saving snapshot
[2018-01-21 17:36:06.106014 UTC] Starting iteration 1975
[2018-01-21 17:36:06.106142 UTC] Start collecting samples
[2018-01-21 17:36:08.640915 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:08.714902 UTC] Performing policy update
[2018-01-21 17:36:08.715446 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:08.790923 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:09.728858 UTC] Performing line search
[2018-01-21 17:36:09.848043 UTC] Updating baseline
[2018-01-21 17:36:11.637698 UTC] Computing logging information
-------------------------------------
| Iteration            | 1975       |
| ExpectedImprovement  | 0.015298   |
| ActualImprovement    | 0.014432   |
| ImprovementRatio     | 0.94343    |
| MeanKL               | 0.0095592  |
| Entropy              | -2.1796    |
| Perplexity           | 0.11309    |
| AveragePolicyStd     | 0.17066    |
| AveragePolicyStd[0]  | 0.18616    |
| AveragePolicyStd[1]  | 0.18746    |
| AveragePolicyStd[2]  | 0.1403     |
| AveragePolicyStd[3]  | 0.15157    |
| AveragePolicyStd[4]  | 0.13995    |
| AveragePolicyStd[5]  | 0.21855    |
| AverageReturn        | 1833.4     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 452.63     |
| AverageEpisodeLength | 907.05     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 211.39     |
| TotalNEpisodes       | 27037      |
| TotalNSamples        | 9.8867e+06 |
| ExplainedVariance    | 0.00036442 |
-------------------------------------
[2018-01-21 17:36:12.213520 UTC] Saving snapshot
[2018-01-21 17:36:12.213760 UTC] Starting iteration 1976
[2018-01-21 17:36:12.213940 UTC] Start collecting samples
[2018-01-21 17:36:14.798383 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:14.879169 UTC] Performing policy update
[2018-01-21 17:36:14.879730 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:14.956394 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:15.863331 UTC] Performing line search
[2018-01-21 17:36:15.983298 UTC] Updating baseline
[2018-01-21 17:36:17.441051 UTC] Computing logging information
------------------------------------
| Iteration            | 1976      |
| ExpectedImprovement  | 0.018592  |
| ActualImprovement    | 0.017603  |
| ImprovementRatio     | 0.94682   |
| MeanKL               | 0.0083351 |
| Entropy              | -2.1771   |
| Perplexity           | 0.11337   |
| AveragePolicyStd     | 0.17076   |
| AveragePolicyStd[0]  | 0.18596   |
| AveragePolicyStd[1]  | 0.18789   |
| AveragePolicyStd[2]  | 0.14052   |
| AveragePolicyStd[3]  | 0.15112   |
| AveragePolicyStd[4]  | 0.13997   |
| AveragePolicyStd[5]  | 0.21913   |
| AverageReturn        | 1831.4    |
| MinReturn            | 61.933    |
| MaxReturn            | 2092.8    |
| StdReturn            | 455.42    |
| AverageEpisodeLength | 906.49    |
| MinEpisodeLength     | 70        |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 212.86    |
| TotalNEpisodes       | 27044     |
| TotalNSamples        | 9.893e+06 |
| ExplainedVariance    | 0.091093  |
------------------------------------
[2018-01-21 17:36:17.999267 UTC] Saving snapshot
[2018-01-21 17:36:17.999452 UTC] Starting iteration 1977
[2018-01-21 17:36:17.999616 UTC] Start collecting samples
[2018-01-21 17:36:20.466213 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:20.544733 UTC] Performing policy update
[2018-01-21 17:36:20.545326 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:20.620950 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:21.528315 UTC] Performing line search
[2018-01-21 17:36:21.655188 UTC] Updating baseline
[2018-01-21 17:36:23.114542 UTC] Computing logging information
-------------------------------------
| Iteration            | 1977       |
| ExpectedImprovement  | 0.018486   |
| ActualImprovement    | 0.017845   |
| ImprovementRatio     | 0.96529    |
| MeanKL               | 0.00899    |
| Entropy              | -2.174     |
| Perplexity           | 0.11372    |
| AveragePolicyStd     | 0.17086    |
| AveragePolicyStd[0]  | 0.18622    |
| AveragePolicyStd[1]  | 0.18779    |
| AveragePolicyStd[2]  | 0.14056    |
| AveragePolicyStd[3]  | 0.15136    |
| AveragePolicyStd[4]  | 0.13985    |
| AveragePolicyStd[5]  | 0.21938    |
| AverageReturn        | 1802.8     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 481.28     |
| AverageEpisodeLength | 893.02     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 224.21     |
| TotalNEpisodes       | 27051      |
| TotalNSamples        | 9.8985e+06 |
| ExplainedVariance    | 0.33269    |
-------------------------------------
[2018-01-21 17:36:23.660742 UTC] Saving snapshot
[2018-01-21 17:36:23.660936 UTC] Starting iteration 1978
[2018-01-21 17:36:23.661068 UTC] Start collecting samples
[2018-01-21 17:36:25.975087 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:26.051600 UTC] Performing policy update
[2018-01-21 17:36:26.052131 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:26.141254 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:27.022575 UTC] Performing line search
[2018-01-21 17:36:27.141023 UTC] Updating baseline
[2018-01-21 17:36:28.636682 UTC] Computing logging information
-------------------------------------
| Iteration            | 1978       |
| ExpectedImprovement  | 0.017994   |
| ActualImprovement    | 0.016452   |
| ImprovementRatio     | 0.91434    |
| MeanKL               | 0.0091538  |
| Entropy              | -2.1742    |
| Perplexity           | 0.11369    |
| AveragePolicyStd     | 0.17085    |
| AveragePolicyStd[0]  | 0.18645    |
| AveragePolicyStd[1]  | 0.18776    |
| AveragePolicyStd[2]  | 0.14062    |
| AveragePolicyStd[3]  | 0.15159    |
| AveragePolicyStd[4]  | 0.13956    |
| AveragePolicyStd[5]  | 0.2191     |
| AverageReturn        | 1821.2     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 467.26     |
| AverageEpisodeLength | 902.49     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 217.82     |
| TotalNEpisodes       | 27055      |
| TotalNSamples        | 9.9025e+06 |
| ExplainedVariance    | -0.0060267 |
-------------------------------------
[2018-01-21 17:36:29.173059 UTC] Saving snapshot
[2018-01-21 17:36:29.173245 UTC] Starting iteration 1979
[2018-01-21 17:36:29.173385 UTC] Start collecting samples
[2018-01-21 17:36:31.587729 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:31.668160 UTC] Performing policy update
[2018-01-21 17:36:31.668784 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:31.745994 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:33.185946 UTC] Performing line search
[2018-01-21 17:36:33.349440 UTC] Updating baseline
[2018-01-21 17:36:34.963307 UTC] Computing logging information
-------------------------------------
| Iteration            | 1979       |
| ExpectedImprovement  | 0.016505   |
| ActualImprovement    | 0.015418   |
| ImprovementRatio     | 0.93415    |
| MeanKL               | 0.0083265  |
| Entropy              | -2.1752    |
| Perplexity           | 0.11358    |
| AveragePolicyStd     | 0.17084    |
| AveragePolicyStd[0]  | 0.18618    |
| AveragePolicyStd[1]  | 0.18827    |
| AveragePolicyStd[2]  | 0.14045    |
| AveragePolicyStd[3]  | 0.15172    |
| AveragePolicyStd[4]  | 0.13924    |
| AveragePolicyStd[5]  | 0.2192     |
| AverageReturn        | 1838.7     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 457.55     |
| AverageEpisodeLength | 911.15     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 213.12     |
| TotalNEpisodes       | 27061      |
| TotalNSamples        | 9.9079e+06 |
| ExplainedVariance    | 0.095946   |
-------------------------------------
[2018-01-21 17:36:35.544242 UTC] Saving snapshot
[2018-01-21 17:36:35.544433 UTC] Starting iteration 1980
[2018-01-21 17:36:35.544584 UTC] Start collecting samples
[2018-01-21 17:36:39.430509 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:39.558184 UTC] Performing policy update
[2018-01-21 17:36:39.558808 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:39.640050 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:40.570878 UTC] Performing line search
[2018-01-21 17:36:40.689011 UTC] Updating baseline
[2018-01-21 17:36:42.448972 UTC] Computing logging information
-------------------------------------
| Iteration            | 1980       |
| ExpectedImprovement  | 0.016855   |
| ActualImprovement    | 0.015976   |
| ImprovementRatio     | 0.94785    |
| MeanKL               | 0.0085582  |
| Entropy              | -2.1724    |
| Perplexity           | 0.11391    |
| AveragePolicyStd     | 0.17092    |
| AveragePolicyStd[0]  | 0.18679    |
| AveragePolicyStd[1]  | 0.1882     |
| AveragePolicyStd[2]  | 0.14045    |
| AveragePolicyStd[3]  | 0.15168    |
| AveragePolicyStd[4]  | 0.1394     |
| AveragePolicyStd[5]  | 0.21899    |
| AverageReturn        | 1857       |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 442.76     |
| AverageEpisodeLength | 920.37     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 206.58     |
| TotalNEpisodes       | 27068      |
| TotalNSamples        | 9.9149e+06 |
| ExplainedVariance    | -0.016172  |
-------------------------------------
[2018-01-21 17:36:42.997925 UTC] Saving snapshot
[2018-01-21 17:36:43.003911 UTC] Starting iteration 1981
[2018-01-21 17:36:43.004084 UTC] Start collecting samples
[2018-01-21 17:36:46.630503 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:46.705389 UTC] Performing policy update
[2018-01-21 17:36:46.705924 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:46.784034 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:47.718349 UTC] Performing line search
[2018-01-21 17:36:47.845286 UTC] Updating baseline
[2018-01-21 17:36:49.247284 UTC] Computing logging information
-------------------------------------
| Iteration            | 1981       |
| ExpectedImprovement  | 0.016792   |
| ActualImprovement    | 0.016139   |
| ImprovementRatio     | 0.96108    |
| MeanKL               | 0.008898   |
| Entropy              | -2.1692    |
| Perplexity           | 0.11426    |
| AveragePolicyStd     | 0.17099    |
| AveragePolicyStd[0]  | 0.18692    |
| AveragePolicyStd[1]  | 0.1885     |
| AveragePolicyStd[2]  | 0.14064    |
| AveragePolicyStd[3]  | 0.15178    |
| AveragePolicyStd[4]  | 0.13942    |
| AveragePolicyStd[5]  | 0.21871    |
| AverageReturn        | 1864       |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 436.49     |
| AverageEpisodeLength | 924.43     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.11     |
| TotalNEpisodes       | 27071      |
| TotalNSamples        | 9.9179e+06 |
| ExplainedVariance    | 0.0032579  |
-------------------------------------
[2018-01-21 17:36:49.845438 UTC] Saving snapshot
[2018-01-21 17:36:49.845659 UTC] Starting iteration 1982
[2018-01-21 17:36:49.845820 UTC] Start collecting samples
[2018-01-21 17:36:52.701504 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:52.774243 UTC] Performing policy update
[2018-01-21 17:36:52.774746 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:52.852410 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:36:53.795947 UTC] Performing line search
[2018-01-21 17:36:53.931226 UTC] Updating baseline
[2018-01-21 17:36:55.191514 UTC] Computing logging information
-------------------------------------
| Iteration            | 1982       |
| ExpectedImprovement  | 0.016348   |
| ActualImprovement    | 0.015234   |
| ImprovementRatio     | 0.93191    |
| MeanKL               | 0.0091577  |
| Entropy              | -2.171     |
| Perplexity           | 0.11407    |
| AveragePolicyStd     | 0.17098    |
| AveragePolicyStd[0]  | 0.1868     |
| AveragePolicyStd[1]  | 0.18844    |
| AveragePolicyStd[2]  | 0.14061    |
| AveragePolicyStd[3]  | 0.15135    |
| AveragePolicyStd[4]  | 0.13937    |
| AveragePolicyStd[5]  | 0.21929    |
| AverageReturn        | 1863.5     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 436.33     |
| AverageEpisodeLength | 924.43     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.11     |
| TotalNEpisodes       | 27075      |
| TotalNSamples        | 9.9219e+06 |
| ExplainedVariance    | 0.0039573  |
-------------------------------------
[2018-01-21 17:36:55.854947 UTC] Saving snapshot
[2018-01-21 17:36:55.855132 UTC] Starting iteration 1983
[2018-01-21 17:36:55.855274 UTC] Start collecting samples
[2018-01-21 17:36:58.777387 UTC] Computing input variables for policy optimization
[2018-01-21 17:36:58.864929 UTC] Performing policy update
[2018-01-21 17:36:58.865775 UTC] Computing gradient in Euclidean space
[2018-01-21 17:36:58.993984 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:00.015121 UTC] Performing line search
[2018-01-21 17:37:00.141584 UTC] Updating baseline
[2018-01-21 17:37:02.032824 UTC] Computing logging information
-------------------------------------
| Iteration            | 1983       |
| ExpectedImprovement  | 0.015611   |
| ActualImprovement    | 0.014819   |
| ImprovementRatio     | 0.94923    |
| MeanKL               | 0.0095892  |
| Entropy              | -2.17      |
| Perplexity           | 0.11418    |
| AveragePolicyStd     | 0.17102    |
| AveragePolicyStd[0]  | 0.18692    |
| AveragePolicyStd[1]  | 0.18875    |
| AveragePolicyStd[2]  | 0.14076    |
| AveragePolicyStd[3]  | 0.1511     |
| AveragePolicyStd[4]  | 0.13925    |
| AveragePolicyStd[5]  | 0.21931    |
| AverageReturn        | 1863       |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 436.12     |
| AverageEpisodeLength | 924.6      |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 204.16     |
| TotalNEpisodes       | 27082      |
| TotalNSamples        | 9.9289e+06 |
| ExplainedVariance    | 0.0027777  |
-------------------------------------
[2018-01-21 17:37:02.610377 UTC] Saving snapshot
[2018-01-21 17:37:02.610596 UTC] Starting iteration 1984
[2018-01-21 17:37:02.610744 UTC] Start collecting samples
[2018-01-21 17:37:05.599224 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:05.676467 UTC] Performing policy update
[2018-01-21 17:37:05.676975 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:05.756523 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:06.760289 UTC] Performing line search
[2018-01-21 17:37:06.887446 UTC] Updating baseline
[2018-01-21 17:37:08.070219 UTC] Computing logging information
-------------------------------------
| Iteration            | 1984       |
| ExpectedImprovement  | 0.017379   |
| ActualImprovement    | 0.016104   |
| ImprovementRatio     | 0.92662    |
| MeanKL               | 0.0085941  |
| Entropy              | -2.1708    |
| Perplexity           | 0.11408    |
| AveragePolicyStd     | 0.17099    |
| AveragePolicyStd[0]  | 0.18671    |
| AveragePolicyStd[1]  | 0.18858    |
| AveragePolicyStd[2]  | 0.14097    |
| AveragePolicyStd[3]  | 0.15123    |
| AveragePolicyStd[4]  | 0.139      |
| AveragePolicyStd[5]  | 0.21945    |
| AverageReturn        | 1875.8     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 419.01     |
| AverageEpisodeLength | 931.09     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.98     |
| TotalNEpisodes       | 27086      |
| TotalNSamples        | 9.9329e+06 |
| ExplainedVariance    | 0.0043929  |
-------------------------------------
[2018-01-21 17:37:08.605650 UTC] Saving snapshot
[2018-01-21 17:37:08.605859 UTC] Starting iteration 1985
[2018-01-21 17:37:08.605989 UTC] Start collecting samples
[2018-01-21 17:37:11.436167 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:11.509198 UTC] Performing policy update
[2018-01-21 17:37:11.509684 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:11.585131 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:12.600411 UTC] Performing line search
[2018-01-21 17:37:12.723671 UTC] Updating baseline
[2018-01-21 17:37:14.444329 UTC] Computing logging information
-------------------------------------
| Iteration            | 1985       |
| ExpectedImprovement  | 0.017092   |
| ActualImprovement    | 0.016142   |
| ImprovementRatio     | 0.94441    |
| MeanKL               | 0.0093027  |
| Entropy              | -2.1672    |
| Perplexity           | 0.1145     |
| AveragePolicyStd     | 0.17111    |
| AveragePolicyStd[0]  | 0.18641    |
| AveragePolicyStd[1]  | 0.18885    |
| AveragePolicyStd[2]  | 0.14094    |
| AveragePolicyStd[3]  | 0.15132    |
| AveragePolicyStd[4]  | 0.13915    |
| AveragePolicyStd[5]  | 0.21998    |
| AverageReturn        | 1875       |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 418.79     |
| AverageEpisodeLength | 931.09     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 195.98     |
| TotalNEpisodes       | 27090      |
| TotalNSamples        | 9.9369e+06 |
| ExplainedVariance    | 0.0050445  |
-------------------------------------
[2018-01-21 17:37:15.042220 UTC] Saving snapshot
[2018-01-21 17:37:15.042450 UTC] Starting iteration 1986
[2018-01-21 17:37:15.042613 UTC] Start collecting samples
[2018-01-21 17:37:17.967908 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:18.043647 UTC] Performing policy update
[2018-01-21 17:37:18.044185 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:18.123149 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:19.145046 UTC] Performing line search
[2018-01-21 17:37:19.277987 UTC] Updating baseline
[2018-01-21 17:37:20.802902 UTC] Computing logging information
-------------------------------------
| Iteration            | 1986       |
| ExpectedImprovement  | 0.018424   |
| ActualImprovement    | 0.017369   |
| ImprovementRatio     | 0.94277    |
| MeanKL               | 0.0093156  |
| Entropy              | -2.1716    |
| Perplexity           | 0.11399    |
| AveragePolicyStd     | 0.171      |
| AveragePolicyStd[0]  | 0.18678    |
| AveragePolicyStd[1]  | 0.18851    |
| AveragePolicyStd[2]  | 0.14067    |
| AveragePolicyStd[3]  | 0.15117    |
| AveragePolicyStd[4]  | 0.13892    |
| AveragePolicyStd[5]  | 0.21998    |
| AverageReturn        | 1876.1     |
| MinReturn            | 61.933     |
| MaxReturn            | 2092.8     |
| StdReturn            | 414.66     |
| AverageEpisodeLength | 932.11     |
| MinEpisodeLength     | 70         |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 193.92     |
| TotalNEpisodes       | 27099      |
| TotalNSamples        | 9.9455e+06 |
| ExplainedVariance    | 0.04827    |
-------------------------------------
[2018-01-21 17:37:21.363747 UTC] Saving snapshot
[2018-01-21 17:37:21.363939 UTC] Starting iteration 1987
[2018-01-21 17:37:21.364093 UTC] Start collecting samples
[2018-01-21 17:37:24.181427 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:24.254891 UTC] Performing policy update
[2018-01-21 17:37:24.255414 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:24.341948 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:25.318304 UTC] Performing line search
[2018-01-21 17:37:25.442357 UTC] Updating baseline
[2018-01-21 17:37:26.999128 UTC] Computing logging information
-------------------------------------
| Iteration            | 1987       |
| ExpectedImprovement  | 0.018171   |
| ActualImprovement    | 0.016606   |
| ImprovementRatio     | 0.91387    |
| MeanKL               | 0.0075677  |
| Entropy              | -2.1808    |
| Perplexity           | 0.11295    |
| AveragePolicyStd     | 0.17077    |
| AveragePolicyStd[0]  | 0.18656    |
| AveragePolicyStd[1]  | 0.18836    |
| AveragePolicyStd[2]  | 0.14055    |
| AveragePolicyStd[3]  | 0.15102    |
| AveragePolicyStd[4]  | 0.13816    |
| AveragePolicyStd[5]  | 0.22       |
| AverageReturn        | 1887.4     |
| MinReturn            | 573.96     |
| MaxReturn            | 2090.2     |
| StdReturn            | 370.97     |
| AverageEpisodeLength | 938.06     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 27105      |
| TotalNSamples        | 9.9498e+06 |
| ExplainedVariance    | 0.25116    |
-------------------------------------
[2018-01-21 17:37:27.549072 UTC] Saving snapshot
[2018-01-21 17:37:27.549306 UTC] Starting iteration 1988
[2018-01-21 17:37:27.549434 UTC] Start collecting samples
[2018-01-21 17:37:30.143110 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:30.219848 UTC] Performing policy update
[2018-01-21 17:37:30.220474 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:30.315212 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:31.481833 UTC] Performing line search
[2018-01-21 17:37:31.653553 UTC] Updating baseline
[2018-01-21 17:37:33.647530 UTC] Computing logging information
-------------------------------------
| Iteration            | 1988       |
| ExpectedImprovement  | 0.02056    |
| ActualImprovement    | 0.018999   |
| ImprovementRatio     | 0.92408    |
| MeanKL               | 0.009045   |
| Entropy              | -2.1806    |
| Perplexity           | 0.11297    |
| AveragePolicyStd     | 0.17079    |
| AveragePolicyStd[0]  | 0.18629    |
| AveragePolicyStd[1]  | 0.18854    |
| AveragePolicyStd[2]  | 0.14057    |
| AveragePolicyStd[3]  | 0.1512     |
| AveragePolicyStd[4]  | 0.1379     |
| AveragePolicyStd[5]  | 0.22027    |
| AverageReturn        | 1887.6     |
| MinReturn            | 573.96     |
| MaxReturn            | 2090.2     |
| StdReturn            | 370.99     |
| AverageEpisodeLength | 938.06     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 173.38     |
| TotalNEpisodes       | 27107      |
| TotalNSamples        | 9.9518e+06 |
| ExplainedVariance    | -0.058005  |
-------------------------------------
[2018-01-21 17:37:34.394051 UTC] Saving snapshot
[2018-01-21 17:37:34.394289 UTC] Starting iteration 1989
[2018-01-21 17:37:34.394463 UTC] Start collecting samples
[2018-01-21 17:37:37.710273 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:37.842397 UTC] Performing policy update
[2018-01-21 17:37:37.843532 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:37.979176 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:39.107394 UTC] Performing line search
[2018-01-21 17:37:39.246360 UTC] Updating baseline
[2018-01-21 17:37:40.573050 UTC] Computing logging information
-------------------------------------
| Iteration            | 1989       |
| ExpectedImprovement  | 0.018383   |
| ActualImprovement    | 0.017181   |
| ImprovementRatio     | 0.93462    |
| MeanKL               | 0.0094869  |
| Entropy              | -2.185     |
| Perplexity           | 0.11248    |
| AveragePolicyStd     | 0.17068    |
| AveragePolicyStd[0]  | 0.18616    |
| AveragePolicyStd[1]  | 0.18844    |
| AveragePolicyStd[2]  | 0.14073    |
| AveragePolicyStd[3]  | 0.15067    |
| AveragePolicyStd[4]  | 0.13776    |
| AveragePolicyStd[5]  | 0.22031    |
| AverageReturn        | 1895.8     |
| MinReturn            | 573.96     |
| MaxReturn            | 2075.4     |
| StdReturn            | 357.52     |
| AverageEpisodeLength | 942.72     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 167.24     |
| TotalNEpisodes       | 27113      |
| TotalNSamples        | 9.9578e+06 |
| ExplainedVariance    | 0.10768    |
-------------------------------------
[2018-01-21 17:37:41.135906 UTC] Saving snapshot
[2018-01-21 17:37:41.136102 UTC] Starting iteration 1990
[2018-01-21 17:37:41.136237 UTC] Start collecting samples
[2018-01-21 17:37:44.007193 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:44.086557 UTC] Performing policy update
[2018-01-21 17:37:44.087070 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:44.170228 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:45.211323 UTC] Performing line search
[2018-01-21 17:37:45.339171 UTC] Updating baseline
[2018-01-21 17:37:46.691424 UTC] Computing logging information
-------------------------------------
| Iteration            | 1990       |
| ExpectedImprovement  | 0.018589   |
| ActualImprovement    | 0.01794    |
| ImprovementRatio     | 0.96508    |
| MeanKL               | 0.0084928  |
| Entropy              | -2.1809    |
| Perplexity           | 0.11294    |
| AveragePolicyStd     | 0.17081    |
| AveragePolicyStd[0]  | 0.18634    |
| AveragePolicyStd[1]  | 0.18876    |
| AveragePolicyStd[2]  | 0.14089    |
| AveragePolicyStd[3]  | 0.15065    |
| AveragePolicyStd[4]  | 0.1377     |
| AveragePolicyStd[5]  | 0.22052    |
| AverageReturn        | 1911.3     |
| MinReturn            | 573.96     |
| MaxReturn            | 2075.4     |
| StdReturn            | 338.84     |
| AverageEpisodeLength | 950.53     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.57     |
| TotalNEpisodes       | 27119      |
| TotalNSamples        | 9.9638e+06 |
| ExplainedVariance    | 0.03511    |
-------------------------------------
[2018-01-21 17:37:47.331550 UTC] Saving snapshot
[2018-01-21 17:37:47.339386 UTC] Starting iteration 1991
[2018-01-21 17:37:47.339572 UTC] Start collecting samples
[2018-01-21 17:37:50.440397 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:50.524979 UTC] Performing policy update
[2018-01-21 17:37:50.525499 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:50.606612 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:51.681049 UTC] Performing line search
[2018-01-21 17:37:51.811256 UTC] Updating baseline
[2018-01-21 17:37:53.324245 UTC] Computing logging information
-------------------------------------
| Iteration            | 1991       |
| ExpectedImprovement  | 0.017778   |
| ActualImprovement    | 0.01684    |
| ImprovementRatio     | 0.94722    |
| MeanKL               | 0.0090463  |
| Entropy              | -2.1802    |
| Perplexity           | 0.11301    |
| AveragePolicyStd     | 0.17084    |
| AveragePolicyStd[0]  | 0.18639    |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.14095    |
| AveragePolicyStd[3]  | 0.15069    |
| AveragePolicyStd[4]  | 0.13753    |
| AveragePolicyStd[5]  | 0.22057    |
| AverageReturn        | 1909.4     |
| MinReturn            | 573.96     |
| MaxReturn            | 2083       |
| StdReturn            | 338.8      |
| AverageEpisodeLength | 949.65     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.54     |
| TotalNEpisodes       | 27123      |
| TotalNSamples        | 9.9677e+06 |
| ExplainedVariance    | 0.14411    |
-------------------------------------
[2018-01-21 17:37:53.880789 UTC] Saving snapshot
[2018-01-21 17:37:53.880980 UTC] Starting iteration 1992
[2018-01-21 17:37:53.881129 UTC] Start collecting samples
[2018-01-21 17:37:56.799046 UTC] Computing input variables for policy optimization
[2018-01-21 17:37:56.881882 UTC] Performing policy update
[2018-01-21 17:37:56.882566 UTC] Computing gradient in Euclidean space
[2018-01-21 17:37:56.982498 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:37:57.965838 UTC] Performing line search
[2018-01-21 17:37:58.123110 UTC] Updating baseline
[2018-01-21 17:37:59.975227 UTC] Computing logging information
-------------------------------------
| Iteration            | 1992       |
| ExpectedImprovement  | 0.01644    |
| ActualImprovement    | 0.015878   |
| ImprovementRatio     | 0.96582    |
| MeanKL               | 0.0085607  |
| Entropy              | -2.1814    |
| Perplexity           | 0.11289    |
| AveragePolicyStd     | 0.17082    |
| AveragePolicyStd[0]  | 0.18626    |
| AveragePolicyStd[1]  | 0.18908    |
| AveragePolicyStd[2]  | 0.14078    |
| AveragePolicyStd[3]  | 0.15089    |
| AveragePolicyStd[4]  | 0.13728    |
| AveragePolicyStd[5]  | 0.22063    |
| AverageReturn        | 1906.2     |
| MinReturn            | 573.96     |
| MaxReturn            | 2083       |
| StdReturn            | 338.64     |
| AverageEpisodeLength | 948.47     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.6      |
| TotalNEpisodes       | 27127      |
| TotalNSamples        | 9.9716e+06 |
| ExplainedVariance    | 0.13527    |
-------------------------------------
[2018-01-21 17:38:00.512998 UTC] Saving snapshot
[2018-01-21 17:38:00.513190 UTC] Starting iteration 1993
[2018-01-21 17:38:00.513356 UTC] Start collecting samples
[2018-01-21 17:38:04.187464 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:04.320603 UTC] Performing policy update
[2018-01-21 17:38:04.321302 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:04.448734 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:05.792477 UTC] Performing line search
[2018-01-21 17:38:05.922730 UTC] Updating baseline
[2018-01-21 17:38:07.495463 UTC] Computing logging information
-------------------------------------
| Iteration            | 1993       |
| ExpectedImprovement  | 0.020138   |
| ActualImprovement    | 0.018914   |
| ImprovementRatio     | 0.93921    |
| MeanKL               | 0.0084619  |
| Entropy              | -2.1747    |
| Perplexity           | 0.11364    |
| AveragePolicyStd     | 0.17099    |
| AveragePolicyStd[0]  | 0.18639    |
| AveragePolicyStd[1]  | 0.18915    |
| AveragePolicyStd[2]  | 0.14119    |
| AveragePolicyStd[3]  | 0.15094    |
| AveragePolicyStd[4]  | 0.13749    |
| AveragePolicyStd[5]  | 0.22081    |
| AverageReturn        | 1904.3     |
| MinReturn            | 573.96     |
| MaxReturn            | 2083       |
| StdReturn            | 339.51     |
| AverageEpisodeLength | 947.18     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.7      |
| TotalNEpisodes       | 27135      |
| TotalNSamples        | 9.9794e+06 |
| ExplainedVariance    | 0.069079   |
-------------------------------------
[2018-01-21 17:38:08.204096 UTC] Saving snapshot
[2018-01-21 17:38:08.204278 UTC] Starting iteration 1994
[2018-01-21 17:38:08.204403 UTC] Start collecting samples
[2018-01-21 17:38:11.226321 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:11.312837 UTC] Performing policy update
[2018-01-21 17:38:11.313438 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:11.393065 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:12.361759 UTC] Performing line search
[2018-01-21 17:38:12.481541 UTC] Updating baseline
[2018-01-21 17:38:14.310935 UTC] Computing logging information
-------------------------------------
| Iteration            | 1994       |
| ExpectedImprovement  | 0.018696   |
| ActualImprovement    | 0.017694   |
| ImprovementRatio     | 0.94643    |
| MeanKL               | 0.0085587  |
| Entropy              | -2.1722    |
| Perplexity           | 0.11393    |
| AveragePolicyStd     | 0.17106    |
| AveragePolicyStd[0]  | 0.18663    |
| AveragePolicyStd[1]  | 0.18879    |
| AveragePolicyStd[2]  | 0.14132    |
| AveragePolicyStd[3]  | 0.1508     |
| AveragePolicyStd[4]  | 0.13781    |
| AveragePolicyStd[5]  | 0.22098    |
| AverageReturn        | 1914.5     |
| MinReturn            | 573.96     |
| MaxReturn            | 2083       |
| StdReturn            | 317.51     |
| AverageEpisodeLength | 950.98     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.87     |
| TotalNEpisodes       | 27140      |
| TotalNSamples        | 9.9841e+06 |
| ExplainedVariance    | 0.088305   |
-------------------------------------
[2018-01-21 17:38:14.862213 UTC] Saving snapshot
[2018-01-21 17:38:14.862403 UTC] Starting iteration 1995
[2018-01-21 17:38:14.862534 UTC] Start collecting samples
[2018-01-21 17:38:18.112929 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:18.191451 UTC] Performing policy update
[2018-01-21 17:38:18.192047 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:18.312803 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:19.288123 UTC] Performing line search
[2018-01-21 17:38:19.408079 UTC] Updating baseline
[2018-01-21 17:38:21.109965 UTC] Computing logging information
-------------------------------------
| Iteration            | 1995       |
| ExpectedImprovement  | 0.016603   |
| ActualImprovement    | 0.015336   |
| ImprovementRatio     | 0.92369    |
| MeanKL               | 0.0087514  |
| Entropy              | -2.1761    |
| Perplexity           | 0.11349    |
| AveragePolicyStd     | 0.17094    |
| AveragePolicyStd[0]  | 0.18642    |
| AveragePolicyStd[1]  | 0.18884    |
| AveragePolicyStd[2]  | 0.14121    |
| AveragePolicyStd[3]  | 0.15075    |
| AveragePolicyStd[4]  | 0.13776    |
| AveragePolicyStd[5]  | 0.22064    |
| AverageReturn        | 1915.5     |
| MinReturn            | 573.96     |
| MaxReturn            | 2099.8     |
| StdReturn            | 317.96     |
| AverageEpisodeLength | 950.98     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.87     |
| TotalNEpisodes       | 27142      |
| TotalNSamples        | 9.9861e+06 |
| ExplainedVariance    | 0.0067884  |
-------------------------------------
[2018-01-21 17:38:21.847715 UTC] Saving snapshot
[2018-01-21 17:38:21.847901 UTC] Starting iteration 1996
[2018-01-21 17:38:21.848037 UTC] Start collecting samples
[2018-01-21 17:38:25.929554 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:26.018027 UTC] Performing policy update
[2018-01-21 17:38:26.018573 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:26.107243 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:27.038821 UTC] Performing line search
[2018-01-21 17:38:27.172310 UTC] Updating baseline
[2018-01-21 17:38:28.600668 UTC] Computing logging information
-------------------------------------
| Iteration            | 1996       |
| ExpectedImprovement  | 0.016533   |
| ActualImprovement    | 0.015823   |
| ImprovementRatio     | 0.95701    |
| MeanKL               | 0.0088812  |
| Entropy              | -2.18      |
| Perplexity           | 0.11304    |
| AveragePolicyStd     | 0.1708     |
| AveragePolicyStd[0]  | 0.18626    |
| AveragePolicyStd[1]  | 0.18879    |
| AveragePolicyStd[2]  | 0.14117    |
| AveragePolicyStd[3]  | 0.1507     |
| AveragePolicyStd[4]  | 0.13782    |
| AveragePolicyStd[5]  | 0.22006    |
| AverageReturn        | 1935.2     |
| MinReturn            | 573.96     |
| MaxReturn            | 2099.8     |
| StdReturn            | 287.85     |
| AverageEpisodeLength | 958.78     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 134.08     |
| TotalNEpisodes       | 27150      |
| TotalNSamples        | 9.9939e+06 |
| ExplainedVariance    | 0.075022   |
-------------------------------------
[2018-01-21 17:38:29.206057 UTC] Saving snapshot
[2018-01-21 17:38:29.206303 UTC] Starting iteration 1997
[2018-01-21 17:38:29.206490 UTC] Start collecting samples
[2018-01-21 17:38:31.960578 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:32.038666 UTC] Performing policy update
[2018-01-21 17:38:32.039259 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:32.116061 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:33.150301 UTC] Performing line search
[2018-01-21 17:38:33.269795 UTC] Updating baseline
[2018-01-21 17:38:34.591200 UTC] Computing logging information
-------------------------------------
| Iteration            | 1997       |
| ExpectedImprovement  | 0.017697   |
| ActualImprovement    | 0.017182   |
| ImprovementRatio     | 0.97091    |
| MeanKL               | 0.0094151  |
| Entropy              | -2.1816    |
| Perplexity           | 0.11286    |
| AveragePolicyStd     | 0.17076    |
| AveragePolicyStd[0]  | 0.18608    |
| AveragePolicyStd[1]  | 0.18883    |
| AveragePolicyStd[2]  | 0.14138    |
| AveragePolicyStd[3]  | 0.15047    |
| AveragePolicyStd[4]  | 0.1377     |
| AveragePolicyStd[5]  | 0.22008    |
| AverageReturn        | 1933.9     |
| MinReturn            | 523.87     |
| MaxReturn            | 2108.2     |
| StdReturn            | 303.21     |
| AverageEpisodeLength | 957.28     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 140.39     |
| TotalNEpisodes       | 27156      |
| TotalNSamples        | 9.9992e+06 |
| ExplainedVariance    | 0.092808   |
-------------------------------------
[2018-01-21 17:38:35.170735 UTC] Saving snapshot
[2018-01-21 17:38:35.171002 UTC] Starting iteration 1998
[2018-01-21 17:38:35.171213 UTC] Start collecting samples
[2018-01-21 17:38:38.527440 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:38.618656 UTC] Performing policy update
[2018-01-21 17:38:38.619223 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:38.694226 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:39.693052 UTC] Performing line search
[2018-01-21 17:38:39.816787 UTC] Updating baseline
[2018-01-21 17:38:41.220398 UTC] Computing logging information
-------------------------------------
| Iteration            | 1998       |
| ExpectedImprovement  | 0.018203   |
| ActualImprovement    | 0.016906   |
| ImprovementRatio     | 0.92876    |
| MeanKL               | 0.0082305  |
| Entropy              | -2.1819    |
| Perplexity           | 0.11283    |
| AveragePolicyStd     | 0.17075    |
| AveragePolicyStd[0]  | 0.186      |
| AveragePolicyStd[1]  | 0.18911    |
| AveragePolicyStd[2]  | 0.14138    |
| AveragePolicyStd[3]  | 0.15033    |
| AveragePolicyStd[4]  | 0.1377     |
| AveragePolicyStd[5]  | 0.21999    |
| AverageReturn        | 1916.1     |
| MinReturn            | 523.87     |
| MaxReturn            | 2108.2     |
| StdReturn            | 326.22     |
| AverageEpisodeLength | 948.69     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 151.01     |
| TotalNEpisodes       | 27160      |
| TotalNSamples        | 1.0002e+07 |
| ExplainedVariance    | 0.30767    |
-------------------------------------
[2018-01-21 17:38:41.804224 UTC] Saving snapshot
[2018-01-21 17:38:41.804415 UTC] Starting iteration 1999
[2018-01-21 17:38:41.804553 UTC] Start collecting samples
[2018-01-21 17:38:44.807402 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:44.888320 UTC] Performing policy update
[2018-01-21 17:38:44.888936 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:44.968461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:46.013601 UTC] Performing line search
[2018-01-21 17:38:46.136310 UTC] Updating baseline
[2018-01-21 17:38:47.511263 UTC] Computing logging information
--------------------------------------
| Iteration            | 1999        |
| ExpectedImprovement  | 0.018083    |
| ActualImprovement    | 0.017114    |
| ImprovementRatio     | 0.94642     |
| MeanKL               | 0.0085257   |
| Entropy              | -2.1805     |
| Perplexity           | 0.11298     |
| AveragePolicyStd     | 0.1708      |
| AveragePolicyStd[0]  | 0.18623     |
| AveragePolicyStd[1]  | 0.18909     |
| AveragePolicyStd[2]  | 0.14133     |
| AveragePolicyStd[3]  | 0.15047     |
| AveragePolicyStd[4]  | 0.13761     |
| AveragePolicyStd[5]  | 0.22006     |
| AverageReturn        | 1930.9      |
| MinReturn            | 523.87      |
| MaxReturn            | 2108.2      |
| StdReturn            | 306.27      |
| AverageEpisodeLength | 954.46      |
| MinEpisodeLength     | 306         |
| MaxEpisodeLength     | 1000        |
| StdEpisodeLength     | 141.54      |
| TotalNEpisodes       | 27166       |
| TotalNSamples        | 1.0008e+07  |
| ExplainedVariance    | -0.00052317 |
--------------------------------------
[2018-01-21 17:38:48.165385 UTC] Saving snapshot
[2018-01-21 17:38:48.165649 UTC] Starting iteration 2000
[2018-01-21 17:38:48.165850 UTC] Start collecting samples
[2018-01-21 17:38:52.058486 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:52.131578 UTC] Performing policy update
[2018-01-21 17:38:52.132176 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:52.219797 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:53.229319 UTC] Performing line search
[2018-01-21 17:38:53.372737 UTC] Updating baseline
[2018-01-21 17:38:54.730916 UTC] Computing logging information
-------------------------------------
| Iteration            | 2000       |
| ExpectedImprovement  | 0.017826   |
| ActualImprovement    | 0.016774   |
| ImprovementRatio     | 0.941      |
| MeanKL               | 0.008635   |
| Entropy              | -2.1794    |
| Perplexity           | 0.11311    |
| AveragePolicyStd     | 0.17083    |
| AveragePolicyStd[0]  | 0.18642    |
| AveragePolicyStd[1]  | 0.18921    |
| AveragePolicyStd[2]  | 0.14129    |
| AveragePolicyStd[3]  | 0.14996    |
| AveragePolicyStd[4]  | 0.13803    |
| AveragePolicyStd[5]  | 0.22009    |
| AverageReturn        | 1931       |
| MinReturn            | 523.87     |
| MaxReturn            | 2108.2     |
| StdReturn            | 306.26     |
| AverageEpisodeLength | 954.46     |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.54     |
| TotalNEpisodes       | 27171      |
| TotalNSamples        | 1.0013e+07 |
| ExplainedVariance    | 0.0010425  |
-------------------------------------
[2018-01-21 17:38:55.315396 UTC] Saving snapshot
[2018-01-21 17:38:55.322790 UTC] Starting iteration 2001
[2018-01-21 17:38:55.323103 UTC] Start collecting samples
[2018-01-21 17:38:58.199102 UTC] Computing input variables for policy optimization
[2018-01-21 17:38:58.278149 UTC] Performing policy update
[2018-01-21 17:38:58.278713 UTC] Computing gradient in Euclidean space
[2018-01-21 17:38:58.358920 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:38:59.284708 UTC] Performing line search
[2018-01-21 17:38:59.413203 UTC] Updating baseline
[2018-01-21 17:39:01.057274 UTC] Computing logging information
-------------------------------------
| Iteration            | 2001       |
| ExpectedImprovement  | 0.018089   |
| ActualImprovement    | 0.016549   |
| ImprovementRatio     | 0.91486    |
| MeanKL               | 0.0078684  |
| Entropy              | -2.1732    |
| Perplexity           | 0.11381    |
| AveragePolicyStd     | 0.17101    |
| AveragePolicyStd[0]  | 0.18658    |
| AveragePolicyStd[1]  | 0.18952    |
| AveragePolicyStd[2]  | 0.14157    |
| AveragePolicyStd[3]  | 0.15015    |
| AveragePolicyStd[4]  | 0.13801    |
| AveragePolicyStd[5]  | 0.22023    |
| AverageReturn        | 1922.1     |
| MinReturn            | 523.87     |
| MaxReturn            | 2108.2     |
| StdReturn            | 317.53     |
| AverageEpisodeLength | 950.2      |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.43     |
| TotalNEpisodes       | 27177      |
| TotalNSamples        | 1.0019e+07 |
| ExplainedVariance    | 0.096766   |
-------------------------------------
[2018-01-21 17:39:01.624320 UTC] Saving snapshot
[2018-01-21 17:39:01.624502 UTC] Starting iteration 2002
[2018-01-21 17:39:01.624641 UTC] Start collecting samples
[2018-01-21 17:39:04.813113 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:04.925242 UTC] Performing policy update
[2018-01-21 17:39:04.926063 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:05.031089 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:05.962489 UTC] Performing line search
[2018-01-21 17:39:06.086540 UTC] Updating baseline
[2018-01-21 17:39:07.399931 UTC] Computing logging information
-------------------------------------
| Iteration            | 2002       |
| ExpectedImprovement  | 0.017255   |
| ActualImprovement    | 0.016249   |
| ImprovementRatio     | 0.94171    |
| MeanKL               | 0.0087926  |
| Entropy              | -2.177     |
| Perplexity           | 0.11339    |
| AveragePolicyStd     | 0.17091    |
| AveragePolicyStd[0]  | 0.18641    |
| AveragePolicyStd[1]  | 0.18944    |
| AveragePolicyStd[2]  | 0.14126    |
| AveragePolicyStd[3]  | 0.14974    |
| AveragePolicyStd[4]  | 0.13831    |
| AveragePolicyStd[5]  | 0.22029    |
| AverageReturn        | 1920.6     |
| MinReturn            | 523.87     |
| MaxReturn            | 2108.2     |
| StdReturn            | 317.13     |
| AverageEpisodeLength | 950.2      |
| MinEpisodeLength     | 306        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 146.43     |
| TotalNEpisodes       | 27182      |
| TotalNSamples        | 1.0024e+07 |
| ExplainedVariance    | -0.012523  |
-------------------------------------
[2018-01-21 17:39:07.950823 UTC] Saving snapshot
[2018-01-21 17:39:07.951012 UTC] Starting iteration 2003
[2018-01-21 17:39:07.951132 UTC] Start collecting samples
[2018-01-21 17:39:10.778280 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:10.867486 UTC] Performing policy update
[2018-01-21 17:39:10.868000 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:10.946276 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:11.921429 UTC] Performing line search
[2018-01-21 17:39:12.081339 UTC] Updating baseline
[2018-01-21 17:39:13.234923 UTC] Computing logging information
-------------------------------------
| Iteration            | 2003       |
| ExpectedImprovement  | 0.01903    |
| ActualImprovement    | 0.017879   |
| ImprovementRatio     | 0.93948    |
| MeanKL               | 0.00829    |
| Entropy              | -2.1743    |
| Perplexity           | 0.11369    |
| AveragePolicyStd     | 0.17099    |
| AveragePolicyStd[0]  | 0.18635    |
| AveragePolicyStd[1]  | 0.1897     |
| AveragePolicyStd[2]  | 0.14145    |
| AveragePolicyStd[3]  | 0.1499     |
| AveragePolicyStd[4]  | 0.13809    |
| AveragePolicyStd[5]  | 0.22047    |
| AverageReturn        | 1903.2     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 346.83     |
| AverageEpisodeLength | 942.24     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.83     |
| TotalNEpisodes       | 27187      |
| TotalNSamples        | 1.0028e+07 |
| ExplainedVariance    | 0.18874    |
-------------------------------------
[2018-01-21 17:39:13.780501 UTC] Saving snapshot
[2018-01-21 17:39:13.780701 UTC] Starting iteration 2004
[2018-01-21 17:39:13.780844 UTC] Start collecting samples
[2018-01-21 17:39:16.419363 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:16.529042 UTC] Performing policy update
[2018-01-21 17:39:16.529945 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:16.630844 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:17.710994 UTC] Performing line search
[2018-01-21 17:39:17.828617 UTC] Updating baseline
[2018-01-21 17:39:19.402867 UTC] Computing logging information
-------------------------------------
| Iteration            | 2004       |
| ExpectedImprovement  | 0.017682   |
| ActualImprovement    | 0.017043   |
| ImprovementRatio     | 0.96387    |
| MeanKL               | 0.0081017  |
| Entropy              | -2.1813    |
| Perplexity           | 0.11289    |
| AveragePolicyStd     | 0.1708     |
| AveragePolicyStd[0]  | 0.18612    |
| AveragePolicyStd[1]  | 0.18916    |
| AveragePolicyStd[2]  | 0.14085    |
| AveragePolicyStd[3]  | 0.14994    |
| AveragePolicyStd[4]  | 0.13814    |
| AveragePolicyStd[5]  | 0.2206     |
| AverageReturn        | 1901.4     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 346.14     |
| AverageEpisodeLength | 942.24     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.83     |
| TotalNEpisodes       | 27192      |
| TotalNSamples        | 1.0033e+07 |
| ExplainedVariance    | -0.0064914 |
-------------------------------------
[2018-01-21 17:39:19.953951 UTC] Saving snapshot
[2018-01-21 17:39:19.954140 UTC] Starting iteration 2005
[2018-01-21 17:39:19.954274 UTC] Start collecting samples
[2018-01-21 17:39:22.716884 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:22.822458 UTC] Performing policy update
[2018-01-21 17:39:22.823116 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:22.947661 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:23.882710 UTC] Performing line search
[2018-01-21 17:39:24.007707 UTC] Updating baseline
[2018-01-21 17:39:25.666235 UTC] Computing logging information
------------------------------------
| Iteration            | 2005      |
| ExpectedImprovement  | 0.018506  |
| ActualImprovement    | 0.017425  |
| ImprovementRatio     | 0.94158   |
| MeanKL               | 0.0082004 |
| Entropy              | -2.1752   |
| Perplexity           | 0.11359   |
| AveragePolicyStd     | 0.17098   |
| AveragePolicyStd[0]  | 0.18622   |
| AveragePolicyStd[1]  | 0.18945   |
| AveragePolicyStd[2]  | 0.14117   |
| AveragePolicyStd[3]  | 0.15001   |
| AveragePolicyStd[4]  | 0.13815   |
| AveragePolicyStd[5]  | 0.2209    |
| AverageReturn        | 1903.9    |
| MinReturn            | 506.2     |
| MaxReturn            | 2108.2    |
| StdReturn            | 337.94    |
| AverageEpisodeLength | 944.55    |
| MinEpisodeLength     | 277       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 157.38    |
| TotalNEpisodes       | 27199     |
| TotalNSamples        | 1.004e+07 |
| ExplainedVariance    | 0.086312  |
------------------------------------
[2018-01-21 17:39:26.228801 UTC] Saving snapshot
[2018-01-21 17:39:26.229018 UTC] Starting iteration 2006
[2018-01-21 17:39:26.229185 UTC] Start collecting samples
[2018-01-21 17:39:29.396529 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:29.486303 UTC] Performing policy update
[2018-01-21 17:39:29.487143 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:29.580000 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:30.591223 UTC] Performing line search
[2018-01-21 17:39:30.743547 UTC] Updating baseline
[2018-01-21 17:39:32.227400 UTC] Computing logging information
-------------------------------------
| Iteration            | 2006       |
| ExpectedImprovement  | 0.019395   |
| ActualImprovement    | 0.017197   |
| ImprovementRatio     | 0.88668    |
| MeanKL               | 0.0080191  |
| Entropy              | -2.1778    |
| Perplexity           | 0.11329    |
| AveragePolicyStd     | 0.1709     |
| AveragePolicyStd[0]  | 0.1861     |
| AveragePolicyStd[1]  | 0.18932    |
| AveragePolicyStd[2]  | 0.14116    |
| AveragePolicyStd[3]  | 0.15004    |
| AveragePolicyStd[4]  | 0.13804    |
| AveragePolicyStd[5]  | 0.22077    |
| AverageReturn        | 1903.9     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 337.93     |
| AverageEpisodeLength | 944.55     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 157.38     |
| TotalNEpisodes       | 27200      |
| TotalNSamples        | 1.0041e+07 |
| ExplainedVariance    | -0.0029229 |
-------------------------------------
[2018-01-21 17:39:32.831127 UTC] Saving snapshot
[2018-01-21 17:39:32.831314 UTC] Starting iteration 2007
[2018-01-21 17:39:32.831454 UTC] Start collecting samples
[2018-01-21 17:39:35.921486 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:36.049868 UTC] Performing policy update
[2018-01-21 17:39:36.050755 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:36.159169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:37.109786 UTC] Performing line search
[2018-01-21 17:39:37.235145 UTC] Updating baseline
[2018-01-21 17:39:39.232642 UTC] Computing logging information
------------------------------------
| Iteration            | 2007      |
| ExpectedImprovement  | 0.017113  |
| ActualImprovement    | 0.016487  |
| ImprovementRatio     | 0.9634    |
| MeanKL               | 0.0087559 |
| Entropy              | -2.1834   |
| Perplexity           | 0.11266   |
| AveragePolicyStd     | 0.17075   |
| AveragePolicyStd[0]  | 0.18617   |
| AveragePolicyStd[1]  | 0.18954   |
| AveragePolicyStd[2]  | 0.14103   |
| AveragePolicyStd[3]  | 0.15      |
| AveragePolicyStd[4]  | 0.13755   |
| AveragePolicyStd[5]  | 0.22024   |
| AverageReturn        | 1933.6    |
| MinReturn            | 506.2     |
| MaxReturn            | 2108.2    |
| StdReturn            | 281.67    |
| AverageEpisodeLength | 958.7     |
| MinEpisodeLength     | 277       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 130.07    |
| TotalNEpisodes       | 27209     |
| TotalNSamples        | 1.005e+07 |
| ExplainedVariance    | 0.11249   |
------------------------------------
[2018-01-21 17:39:40.114350 UTC] Saving snapshot
[2018-01-21 17:39:40.114687 UTC] Starting iteration 2008
[2018-01-21 17:39:40.114879 UTC] Start collecting samples
[2018-01-21 17:39:43.965736 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:44.047970 UTC] Performing policy update
[2018-01-21 17:39:44.048646 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:44.128762 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:45.181190 UTC] Performing line search
[2018-01-21 17:39:45.378915 UTC] Updating baseline
[2018-01-21 17:39:47.346236 UTC] Computing logging information
-------------------------------------
| Iteration            | 2008       |
| ExpectedImprovement  | 0.019859   |
| ActualImprovement    | 0.018618   |
| ImprovementRatio     | 0.93752    |
| MeanKL               | 0.0093139  |
| Entropy              | -2.1793    |
| Perplexity           | 0.11312    |
| AveragePolicyStd     | 0.17085    |
| AveragePolicyStd[0]  | 0.18595    |
| AveragePolicyStd[1]  | 0.18964    |
| AveragePolicyStd[2]  | 0.14134    |
| AveragePolicyStd[3]  | 0.15018    |
| AveragePolicyStd[4]  | 0.13765    |
| AveragePolicyStd[5]  | 0.22037    |
| AverageReturn        | 1933.6     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 281.65     |
| AverageEpisodeLength | 958.7      |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 130.07     |
| TotalNEpisodes       | 27211      |
| TotalNSamples        | 1.0052e+07 |
| ExplainedVariance    | -0.038886  |
-------------------------------------
[2018-01-21 17:39:48.093150 UTC] Saving snapshot
[2018-01-21 17:39:48.093384 UTC] Starting iteration 2009
[2018-01-21 17:39:48.093572 UTC] Start collecting samples
[2018-01-21 17:39:52.624723 UTC] Computing input variables for policy optimization
[2018-01-21 17:39:52.729974 UTC] Performing policy update
[2018-01-21 17:39:52.730466 UTC] Computing gradient in Euclidean space
[2018-01-21 17:39:52.829958 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:39:54.047433 UTC] Performing line search
[2018-01-21 17:39:54.190307 UTC] Updating baseline
[2018-01-21 17:39:56.031750 UTC] Computing logging information
-------------------------------------
| Iteration            | 2009       |
| ExpectedImprovement  | 0.017916   |
| ActualImprovement    | 0.016875   |
| ImprovementRatio     | 0.94189    |
| MeanKL               | 0.0087892  |
| Entropy              | -2.1801    |
| Perplexity           | 0.11303    |
| AveragePolicyStd     | 0.17081    |
| AveragePolicyStd[0]  | 0.18605    |
| AveragePolicyStd[1]  | 0.18919    |
| AveragePolicyStd[2]  | 0.14148    |
| AveragePolicyStd[3]  | 0.15018    |
| AveragePolicyStd[4]  | 0.13771    |
| AveragePolicyStd[5]  | 0.22027    |
| AverageReturn        | 1924.2     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 294.91     |
| AverageEpisodeLength | 952.99     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 136.62     |
| TotalNEpisodes       | 27217      |
| TotalNSamples        | 1.0057e+07 |
| ExplainedVariance    | 0.31524    |
-------------------------------------
[2018-01-21 17:39:56.961979 UTC] Saving snapshot
[2018-01-21 17:39:56.962310 UTC] Starting iteration 2010
[2018-01-21 17:39:56.962584 UTC] Start collecting samples
[2018-01-21 17:40:01.317482 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:01.416331 UTC] Performing policy update
[2018-01-21 17:40:01.417169 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:01.518625 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:02.724319 UTC] Performing line search
[2018-01-21 17:40:02.921908 UTC] Updating baseline
[2018-01-21 17:40:04.652086 UTC] Computing logging information
-------------------------------------
| Iteration            | 2010       |
| ExpectedImprovement  | 0.018971   |
| ActualImprovement    | 0.017742   |
| ImprovementRatio     | 0.93522    |
| MeanKL               | 0.008052   |
| Entropy              | -2.1753    |
| Perplexity           | 0.11357    |
| AveragePolicyStd     | 0.17097    |
| AveragePolicyStd[0]  | 0.18595    |
| AveragePolicyStd[1]  | 0.18945    |
| AveragePolicyStd[2]  | 0.14181    |
| AveragePolicyStd[3]  | 0.1503     |
| AveragePolicyStd[4]  | 0.13749    |
| AveragePolicyStd[5]  | 0.2208     |
| AverageReturn        | 1902.8     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 334.56     |
| AverageEpisodeLength | 942.01     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.6      |
| TotalNEpisodes       | 27226      |
| TotalNSamples        | 1.0065e+07 |
| ExplainedVariance    | 0.1782     |
-------------------------------------
[2018-01-21 17:40:05.512229 UTC] Saving snapshot
[2018-01-21 17:40:05.518729 UTC] Starting iteration 2011
[2018-01-21 17:40:05.518928 UTC] Start collecting samples
[2018-01-21 17:40:09.930856 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:10.072925 UTC] Performing policy update
[2018-01-21 17:40:10.074765 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:10.173946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:11.534630 UTC] Performing line search
[2018-01-21 17:40:11.723712 UTC] Updating baseline
[2018-01-21 17:40:13.165658 UTC] Computing logging information
-------------------------------------
| Iteration            | 2011       |
| ExpectedImprovement  | 0.017534   |
| ActualImprovement    | 0.016814   |
| ImprovementRatio     | 0.95894    |
| MeanKL               | 0.0088222  |
| Entropy              | -2.1822    |
| Perplexity           | 0.11279    |
| AveragePolicyStd     | 0.1708     |
| AveragePolicyStd[0]  | 0.18602    |
| AveragePolicyStd[1]  | 0.18948    |
| AveragePolicyStd[2]  | 0.14151    |
| AveragePolicyStd[3]  | 0.14986    |
| AveragePolicyStd[4]  | 0.13725    |
| AveragePolicyStd[5]  | 0.22068    |
| AverageReturn        | 1905.6     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 334.56     |
| AverageEpisodeLength | 943.19     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.59     |
| TotalNEpisodes       | 27229      |
| TotalNSamples        | 1.0068e+07 |
| ExplainedVariance    | -0.10592   |
-------------------------------------
[2018-01-21 17:40:13.847305 UTC] Saving snapshot
[2018-01-21 17:40:13.847520 UTC] Starting iteration 2012
[2018-01-21 17:40:13.847642 UTC] Start collecting samples
[2018-01-21 17:40:17.982366 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:18.078597 UTC] Performing policy update
[2018-01-21 17:40:18.079161 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:18.162606 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:19.309164 UTC] Performing line search
[2018-01-21 17:40:19.445561 UTC] Updating baseline
[2018-01-21 17:40:20.898768 UTC] Computing logging information
-------------------------------------
| Iteration            | 2012       |
| ExpectedImprovement  | 0.01959    |
| ActualImprovement    | 0.018398   |
| ImprovementRatio     | 0.93911    |
| MeanKL               | 0.008871   |
| Entropy              | -2.1846    |
| Perplexity           | 0.11252    |
| AveragePolicyStd     | 0.17074    |
| AveragePolicyStd[0]  | 0.18597    |
| AveragePolicyStd[1]  | 0.18933    |
| AveragePolicyStd[2]  | 0.14158    |
| AveragePolicyStd[3]  | 0.14983    |
| AveragePolicyStd[4]  | 0.13693    |
| AveragePolicyStd[5]  | 0.22083    |
| AverageReturn        | 1896.1     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 344.93     |
| AverageEpisodeLength | 938.25     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 160.37     |
| TotalNEpisodes       | 27235      |
| TotalNSamples        | 1.0073e+07 |
| ExplainedVariance    | 0.25836    |
-------------------------------------
[2018-01-21 17:40:21.547429 UTC] Saving snapshot
[2018-01-21 17:40:21.547657 UTC] Starting iteration 2013
[2018-01-21 17:40:21.547810 UTC] Start collecting samples
[2018-01-21 17:40:24.734903 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:24.830779 UTC] Performing policy update
[2018-01-21 17:40:24.831388 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:24.952401 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:25.994145 UTC] Performing line search
[2018-01-21 17:40:26.142685 UTC] Updating baseline
[2018-01-21 17:40:27.597938 UTC] Computing logging information
-------------------------------------
| Iteration            | 2013       |
| ExpectedImprovement  | 0.017124   |
| ActualImprovement    | 0.016826   |
| ImprovementRatio     | 0.98257    |
| MeanKL               | 0.0084093  |
| Entropy              | -2.1836    |
| Perplexity           | 0.11264    |
| AveragePolicyStd     | 0.17079    |
| AveragePolicyStd[0]  | 0.18618    |
| AveragePolicyStd[1]  | 0.18962    |
| AveragePolicyStd[2]  | 0.14155    |
| AveragePolicyStd[3]  | 0.14986    |
| AveragePolicyStd[4]  | 0.13675    |
| AveragePolicyStd[5]  | 0.22076    |
| AverageReturn        | 1901.8     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 341.19     |
| AverageEpisodeLength | 941.23     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.71     |
| TotalNEpisodes       | 27240      |
| TotalNSamples        | 1.0078e+07 |
| ExplainedVariance    | -0.019422  |
-------------------------------------
[2018-01-21 17:40:28.182044 UTC] Saving snapshot
[2018-01-21 17:40:28.182254 UTC] Starting iteration 2014
[2018-01-21 17:40:28.182392 UTC] Start collecting samples
[2018-01-21 17:40:31.496363 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:31.578154 UTC] Performing policy update
[2018-01-21 17:40:31.578762 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:31.663711 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:32.665719 UTC] Performing line search
[2018-01-21 17:40:32.823887 UTC] Updating baseline
[2018-01-21 17:40:34.724496 UTC] Computing logging information
-------------------------------------
| Iteration            | 2014       |
| ExpectedImprovement  | 0.018654   |
| ActualImprovement    | 0.017232   |
| ImprovementRatio     | 0.92374    |
| MeanKL               | 0.0083485  |
| Entropy              | -2.1771    |
| Perplexity           | 0.11337    |
| AveragePolicyStd     | 0.171      |
| AveragePolicyStd[0]  | 0.18651    |
| AveragePolicyStd[1]  | 0.19008    |
| AveragePolicyStd[2]  | 0.14158    |
| AveragePolicyStd[3]  | 0.14998    |
| AveragePolicyStd[4]  | 0.1367     |
| AveragePolicyStd[5]  | 0.22114    |
| AverageReturn        | 1901.2     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 340.15     |
| AverageEpisodeLength | 941.3      |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.65     |
| TotalNEpisodes       | 27246      |
| TotalNSamples        | 1.0084e+07 |
| ExplainedVariance    | 0.10087    |
-------------------------------------
[2018-01-21 17:40:35.290139 UTC] Saving snapshot
[2018-01-21 17:40:35.290382 UTC] Starting iteration 2015
[2018-01-21 17:40:35.290584 UTC] Start collecting samples
[2018-01-21 17:40:38.272623 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:38.354647 UTC] Performing policy update
[2018-01-21 17:40:38.355182 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:38.444417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:39.581280 UTC] Performing line search
[2018-01-21 17:40:39.723941 UTC] Updating baseline
[2018-01-21 17:40:41.381036 UTC] Computing logging information
-------------------------------------
| Iteration            | 2015       |
| ExpectedImprovement  | 0.017386   |
| ActualImprovement    | 0.016472   |
| ImprovementRatio     | 0.94743    |
| MeanKL               | 0.0089685  |
| Entropy              | -2.177     |
| Perplexity           | 0.11338    |
| AveragePolicyStd     | 0.17098    |
| AveragePolicyStd[0]  | 0.18674    |
| AveragePolicyStd[1]  | 0.18967    |
| AveragePolicyStd[2]  | 0.14157    |
| AveragePolicyStd[3]  | 0.15013    |
| AveragePolicyStd[4]  | 0.13679    |
| AveragePolicyStd[5]  | 0.221      |
| AverageReturn        | 1898.8     |
| MinReturn            | 506.2      |
| MaxReturn            | 2108.2     |
| StdReturn            | 339.64     |
| AverageEpisodeLength | 940.94     |
| MinEpisodeLength     | 277        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 158.56     |
| TotalNEpisodes       | 27250      |
| TotalNSamples        | 1.0088e+07 |
| ExplainedVariance    | 0.15805    |
-------------------------------------
[2018-01-21 17:40:41.977039 UTC] Saving snapshot
[2018-01-21 17:40:41.977319 UTC] Starting iteration 2016
[2018-01-21 17:40:41.977466 UTC] Start collecting samples
[2018-01-21 17:40:44.997642 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:45.107896 UTC] Performing policy update
[2018-01-21 17:40:45.108591 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:45.191457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:46.130666 UTC] Performing line search
[2018-01-21 17:40:46.249115 UTC] Updating baseline
[2018-01-21 17:40:47.761699 UTC] Computing logging information
-------------------------------------
| Iteration            | 2016       |
| ExpectedImprovement  | 0.019342   |
| ActualImprovement    | 0.0189     |
| ImprovementRatio     | 0.97713    |
| MeanKL               | 0.0080153  |
| Entropy              | -2.1807    |
| Perplexity           | 0.11296    |
| AveragePolicyStd     | 0.17085    |
| AveragePolicyStd[0]  | 0.18641    |
| AveragePolicyStd[1]  | 0.18956    |
| AveragePolicyStd[2]  | 0.14145    |
| AveragePolicyStd[3]  | 0.15       |
| AveragePolicyStd[4]  | 0.13705    |
| AveragePolicyStd[5]  | 0.22065    |
| AverageReturn        | 1896       |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 343.92     |
| AverageEpisodeLength | 940.19     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 161.7      |
| TotalNEpisodes       | 27255      |
| TotalNSamples        | 1.0092e+07 |
| ExplainedVariance    | -0.013412  |
-------------------------------------
[2018-01-21 17:40:48.382587 UTC] Saving snapshot
[2018-01-21 17:40:48.382788 UTC] Starting iteration 2017
[2018-01-21 17:40:48.382933 UTC] Start collecting samples
[2018-01-21 17:40:51.468999 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:51.557854 UTC] Performing policy update
[2018-01-21 17:40:51.558491 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:51.641266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:52.650245 UTC] Performing line search
[2018-01-21 17:40:52.782937 UTC] Updating baseline
[2018-01-21 17:40:54.177726 UTC] Computing logging information
-------------------------------------
| Iteration            | 2017       |
| ExpectedImprovement  | 0.018506   |
| ActualImprovement    | 0.017293   |
| ImprovementRatio     | 0.93443    |
| MeanKL               | 0.0082014  |
| Entropy              | -2.1877    |
| Perplexity           | 0.11218    |
| AveragePolicyStd     | 0.17065    |
| AveragePolicyStd[0]  | 0.1864     |
| AveragePolicyStd[1]  | 0.1892     |
| AveragePolicyStd[2]  | 0.14116    |
| AveragePolicyStd[3]  | 0.14992    |
| AveragePolicyStd[4]  | 0.13697    |
| AveragePolicyStd[5]  | 0.22025    |
| AverageReturn        | 1914.3     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 323.55     |
| AverageEpisodeLength | 948.78     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.31     |
| TotalNEpisodes       | 27262      |
| TotalNSamples        | 1.0099e+07 |
| ExplainedVariance    | -0.0036738 |
-------------------------------------
[2018-01-21 17:40:54.810623 UTC] Saving snapshot
[2018-01-21 17:40:54.810822 UTC] Starting iteration 2018
[2018-01-21 17:40:54.810970 UTC] Start collecting samples
[2018-01-21 17:40:58.409936 UTC] Computing input variables for policy optimization
[2018-01-21 17:40:58.508628 UTC] Performing policy update
[2018-01-21 17:40:58.509189 UTC] Computing gradient in Euclidean space
[2018-01-21 17:40:58.601917 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:40:59.651389 UTC] Performing line search
[2018-01-21 17:40:59.787704 UTC] Updating baseline
[2018-01-21 17:41:01.455972 UTC] Computing logging information
-------------------------------------
| Iteration            | 2018       |
| ExpectedImprovement  | 0.017147   |
| ActualImprovement    | 0.015698   |
| ImprovementRatio     | 0.91545    |
| MeanKL               | 0.0089312  |
| Entropy              | -2.1929    |
| Perplexity           | 0.11159    |
| AveragePolicyStd     | 0.1705     |
| AveragePolicyStd[0]  | 0.18594    |
| AveragePolicyStd[1]  | 0.18908    |
| AveragePolicyStd[2]  | 0.14118    |
| AveragePolicyStd[3]  | 0.15006    |
| AveragePolicyStd[4]  | 0.13662    |
| AveragePolicyStd[5]  | 0.2201     |
| AverageReturn        | 1912.8     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 322.97     |
| AverageEpisodeLength | 948.78     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.31     |
| TotalNEpisodes       | 27265      |
| TotalNSamples        | 1.0102e+07 |
| ExplainedVariance    | 0.0099746  |
-------------------------------------
[2018-01-21 17:41:02.067460 UTC] Saving snapshot
[2018-01-21 17:41:02.067710 UTC] Starting iteration 2019
[2018-01-21 17:41:02.067886 UTC] Start collecting samples
[2018-01-21 17:41:05.208521 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:05.292222 UTC] Performing policy update
[2018-01-21 17:41:05.292790 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:05.377147 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:06.339995 UTC] Performing line search
[2018-01-21 17:41:06.463151 UTC] Updating baseline
[2018-01-21 17:41:08.583366 UTC] Computing logging information
-------------------------------------
| Iteration            | 2019       |
| ExpectedImprovement  | 0.018018   |
| ActualImprovement    | 0.017049   |
| ImprovementRatio     | 0.94624    |
| MeanKL               | 0.0093954  |
| Entropy              | -2.1894    |
| Perplexity           | 0.11198    |
| AveragePolicyStd     | 0.17062    |
| AveragePolicyStd[0]  | 0.18619    |
| AveragePolicyStd[1]  | 0.18906    |
| AveragePolicyStd[2]  | 0.14122    |
| AveragePolicyStd[3]  | 0.14991    |
| AveragePolicyStd[4]  | 0.1367     |
| AveragePolicyStd[5]  | 0.22062    |
| AverageReturn        | 1914.6     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 323.59     |
| AverageEpisodeLength | 948.78     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 152.31     |
| TotalNEpisodes       | 27271      |
| TotalNSamples        | 1.0108e+07 |
| ExplainedVariance    | 0.0019624  |
-------------------------------------
[2018-01-21 17:41:09.153459 UTC] Saving snapshot
[2018-01-21 17:41:09.153669 UTC] Starting iteration 2020
[2018-01-21 17:41:09.153817 UTC] Start collecting samples
[2018-01-21 17:41:12.559474 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:12.651310 UTC] Performing policy update
[2018-01-21 17:41:12.651901 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:12.729549 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:13.657212 UTC] Performing line search
[2018-01-21 17:41:13.776954 UTC] Updating baseline
[2018-01-21 17:41:15.155138 UTC] Computing logging information
-------------------------------------
| Iteration            | 2020       |
| ExpectedImprovement  | 0.01601    |
| ActualImprovement    | 0.015375   |
| ImprovementRatio     | 0.96031    |
| MeanKL               | 0.0087791  |
| Entropy              | -2.189     |
| Perplexity           | 0.11202    |
| AveragePolicyStd     | 0.17062    |
| AveragePolicyStd[0]  | 0.18627    |
| AveragePolicyStd[1]  | 0.18862    |
| AveragePolicyStd[2]  | 0.14134    |
| AveragePolicyStd[3]  | 0.15005    |
| AveragePolicyStd[4]  | 0.13668    |
| AveragePolicyStd[5]  | 0.22077    |
| AverageReturn        | 1925.8     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 313.5      |
| AverageEpisodeLength | 953.04     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.65     |
| TotalNEpisodes       | 27277      |
| TotalNSamples        | 1.0114e+07 |
| ExplainedVariance    | 0.0044274  |
-------------------------------------
[2018-01-21 17:41:15.732942 UTC] Saving snapshot
[2018-01-21 17:41:15.738664 UTC] Starting iteration 2021
[2018-01-21 17:41:15.738847 UTC] Start collecting samples
[2018-01-21 17:41:18.745069 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:18.818572 UTC] Performing policy update
[2018-01-21 17:41:18.819095 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:18.898333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:19.843098 UTC] Performing line search
[2018-01-21 17:41:19.964312 UTC] Updating baseline
[2018-01-21 17:41:22.266484 UTC] Computing logging information
-------------------------------------
| Iteration            | 2021       |
| ExpectedImprovement  | 0.019464   |
| ActualImprovement    | 0.018227   |
| ImprovementRatio     | 0.93646    |
| MeanKL               | 0.0080553  |
| Entropy              | -2.1906    |
| Perplexity           | 0.11185    |
| AveragePolicyStd     | 0.17053    |
| AveragePolicyStd[0]  | 0.18542    |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.14168    |
| AveragePolicyStd[3]  | 0.15016    |
| AveragePolicyStd[4]  | 0.1368     |
| AveragePolicyStd[5]  | 0.22025    |
| AverageReturn        | 1927.7     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 313.93     |
| AverageEpisodeLength | 953.04     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.65     |
| TotalNEpisodes       | 27281      |
| TotalNSamples        | 1.0118e+07 |
| ExplainedVariance    | 0.0092587  |
-------------------------------------
[2018-01-21 17:41:22.813156 UTC] Saving snapshot
[2018-01-21 17:41:22.813354 UTC] Starting iteration 2022
[2018-01-21 17:41:22.813492 UTC] Start collecting samples
[2018-01-21 17:41:26.014845 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:26.116775 UTC] Performing policy update
[2018-01-21 17:41:26.118091 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:26.198933 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:27.116639 UTC] Performing line search
[2018-01-21 17:41:27.241354 UTC] Updating baseline
[2018-01-21 17:41:29.081370 UTC] Computing logging information
-------------------------------------
| Iteration            | 2022       |
| ExpectedImprovement  | 0.015887   |
| ActualImprovement    | 0.014731   |
| ImprovementRatio     | 0.92721    |
| MeanKL               | 0.0094302  |
| Entropy              | -2.192     |
| Perplexity           | 0.11169    |
| AveragePolicyStd     | 0.1705     |
| AveragePolicyStd[0]  | 0.18551    |
| AveragePolicyStd[1]  | 0.18871    |
| AveragePolicyStd[2]  | 0.14148    |
| AveragePolicyStd[3]  | 0.15039    |
| AveragePolicyStd[4]  | 0.13663    |
| AveragePolicyStd[5]  | 0.22027    |
| AverageReturn        | 1943.1     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 279.75     |
| AverageEpisodeLength | 960.27     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 131.15     |
| TotalNEpisodes       | 27284      |
| TotalNSamples        | 1.0121e+07 |
| ExplainedVariance    | 0.0003299  |
-------------------------------------
[2018-01-21 17:41:29.672751 UTC] Saving snapshot
[2018-01-21 17:41:29.673048 UTC] Starting iteration 2023
[2018-01-21 17:41:29.673257 UTC] Start collecting samples
[2018-01-21 17:41:33.008840 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:33.120814 UTC] Performing policy update
[2018-01-21 17:41:33.121454 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:33.205456 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:34.203036 UTC] Performing line search
[2018-01-21 17:41:34.363589 UTC] Updating baseline
[2018-01-21 17:41:36.477124 UTC] Computing logging information
------------------------------------
| Iteration            | 2023      |
| ExpectedImprovement  | 0.018035  |
| ActualImprovement    | 0.016868  |
| ImprovementRatio     | 0.93534   |
| MeanKL               | 0.0085618 |
| Entropy              | -2.1876   |
| Perplexity           | 0.11219   |
| AveragePolicyStd     | 0.17062   |
| AveragePolicyStd[0]  | 0.18543   |
| AveragePolicyStd[1]  | 0.18902   |
| AveragePolicyStd[2]  | 0.14166   |
| AveragePolicyStd[3]  | 0.15061   |
| AveragePolicyStd[4]  | 0.1367    |
| AveragePolicyStd[5]  | 0.22029   |
| AverageReturn        | 1942      |
| MinReturn            | 404       |
| MaxReturn            | 2094.4    |
| StdReturn            | 285.63    |
| AverageEpisodeLength | 957.85    |
| MinEpisodeLength     | 231       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 133.95    |
| TotalNEpisodes       | 27293     |
| TotalNSamples        | 1.013e+07 |
| ExplainedVariance    | 0.061861  |
------------------------------------
[2018-01-21 17:41:37.091283 UTC] Saving snapshot
[2018-01-21 17:41:37.091491 UTC] Starting iteration 2024
[2018-01-21 17:41:37.091657 UTC] Start collecting samples
[2018-01-21 17:41:40.339241 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:40.431758 UTC] Performing policy update
[2018-01-21 17:41:40.432390 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:40.507104 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:41.428963 UTC] Performing line search
[2018-01-21 17:41:41.559633 UTC] Updating baseline
[2018-01-21 17:41:42.883329 UTC] Computing logging information
-------------------------------------
| Iteration            | 2024       |
| ExpectedImprovement  | 0.018861   |
| ActualImprovement    | 0.01808    |
| ImprovementRatio     | 0.95859    |
| MeanKL               | 0.0081754  |
| Entropy              | -2.1915    |
| Perplexity           | 0.11175    |
| AveragePolicyStd     | 0.17051    |
| AveragePolicyStd[0]  | 0.18528    |
| AveragePolicyStd[1]  | 0.18852    |
| AveragePolicyStd[2]  | 0.14163    |
| AveragePolicyStd[3]  | 0.1506     |
| AveragePolicyStd[4]  | 0.13658    |
| AveragePolicyStd[5]  | 0.22042    |
| AverageReturn        | 1922.2     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 327.81     |
| AverageEpisodeLength | 947.9      |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 153.55     |
| TotalNEpisodes       | 27298      |
| TotalNSamples        | 1.0134e+07 |
| ExplainedVariance    | 0.25       |
-------------------------------------
[2018-01-21 17:41:43.555484 UTC] Saving snapshot
[2018-01-21 17:41:43.555751 UTC] Starting iteration 2025
[2018-01-21 17:41:43.555898 UTC] Start collecting samples
[2018-01-21 17:41:46.853788 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:46.935375 UTC] Performing policy update
[2018-01-21 17:41:46.935970 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:47.017428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:48.042362 UTC] Performing line search
[2018-01-21 17:41:48.174981 UTC] Updating baseline
[2018-01-21 17:41:49.938511 UTC] Computing logging information
-------------------------------------
| Iteration            | 2025       |
| ExpectedImprovement  | 0.017825   |
| ActualImprovement    | 0.017073   |
| ImprovementRatio     | 0.95784    |
| MeanKL               | 0.0086403  |
| Entropy              | -2.1877    |
| Perplexity           | 0.11217    |
| AveragePolicyStd     | 0.17065    |
| AveragePolicyStd[0]  | 0.18579    |
| AveragePolicyStd[1]  | 0.18861    |
| AveragePolicyStd[2]  | 0.14151    |
| AveragePolicyStd[3]  | 0.15064    |
| AveragePolicyStd[4]  | 0.13644    |
| AveragePolicyStd[5]  | 0.2209     |
| AverageReturn        | 1906.5     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 354.99     |
| AverageEpisodeLength | 940.78     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 166.9      |
| TotalNEpisodes       | 27303      |
| TotalNSamples        | 1.0138e+07 |
| ExplainedVariance    | 0.093103   |
-------------------------------------
[2018-01-21 17:41:50.672936 UTC] Saving snapshot
[2018-01-21 17:41:50.673166 UTC] Starting iteration 2026
[2018-01-21 17:41:50.673313 UTC] Start collecting samples
[2018-01-21 17:41:55.046326 UTC] Computing input variables for policy optimization
[2018-01-21 17:41:55.130967 UTC] Performing policy update
[2018-01-21 17:41:55.131618 UTC] Computing gradient in Euclidean space
[2018-01-21 17:41:55.213854 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:41:56.205338 UTC] Performing line search
[2018-01-21 17:41:56.335564 UTC] Updating baseline
[2018-01-21 17:41:58.421475 UTC] Computing logging information
-------------------------------------
| Iteration            | 2026       |
| ExpectedImprovement  | 0.016601   |
| ActualImprovement    | 0.015522   |
| ImprovementRatio     | 0.93504    |
| MeanKL               | 0.0082871  |
| Entropy              | -2.1848    |
| Perplexity           | 0.1125     |
| AveragePolicyStd     | 0.17074    |
| AveragePolicyStd[0]  | 0.18594    |
| AveragePolicyStd[1]  | 0.1888     |
| AveragePolicyStd[2]  | 0.14175    |
| AveragePolicyStd[3]  | 0.15044    |
| AveragePolicyStd[4]  | 0.13645    |
| AveragePolicyStd[5]  | 0.22106    |
| AverageReturn        | 1912.3     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 351.79     |
| AverageEpisodeLength | 943.73     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.51     |
| TotalNEpisodes       | 27309      |
| TotalNSamples        | 1.0144e+07 |
| ExplainedVariance    | -0.0017162 |
-------------------------------------
[2018-01-21 17:41:59.056006 UTC] Saving snapshot
[2018-01-21 17:41:59.056200 UTC] Starting iteration 2027
[2018-01-21 17:41:59.056336 UTC] Start collecting samples
[2018-01-21 17:42:02.627483 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:02.709860 UTC] Performing policy update
[2018-01-21 17:42:02.710555 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:02.797936 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:04.014711 UTC] Performing line search
[2018-01-21 17:42:04.367208 UTC] Updating baseline
[2018-01-21 17:42:06.737572 UTC] Computing logging information
-------------------------------------
| Iteration            | 2027       |
| ExpectedImprovement  | 0.013855   |
| ActualImprovement    | 0.013298   |
| ImprovementRatio     | 0.9598     |
| MeanKL               | 0.0064436  |
| Entropy              | -2.1841    |
| Perplexity           | 0.11258    |
| AveragePolicyStd     | 0.17078    |
| AveragePolicyStd[0]  | 0.18627    |
| AveragePolicyStd[1]  | 0.18854    |
| AveragePolicyStd[2]  | 0.14185    |
| AveragePolicyStd[3]  | 0.15028    |
| AveragePolicyStd[4]  | 0.13627    |
| AveragePolicyStd[5]  | 0.22149    |
| AverageReturn        | 1910.7     |
| MinReturn            | 404        |
| MaxReturn            | 2094.4     |
| StdReturn            | 351.4      |
| AverageEpisodeLength | 943.73     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 165.51     |
| TotalNEpisodes       | 27313      |
| TotalNSamples        | 1.0148e+07 |
| ExplainedVariance    | 0.0034212  |
-------------------------------------
[2018-01-21 17:42:07.594194 UTC] Saving snapshot
[2018-01-21 17:42:07.594517 UTC] Starting iteration 2028
[2018-01-21 17:42:07.594732 UTC] Start collecting samples
[2018-01-21 17:42:10.898288 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:10.995574 UTC] Performing policy update
[2018-01-21 17:42:10.996444 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:11.109879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:12.184260 UTC] Performing line search
[2018-01-21 17:42:12.369720 UTC] Updating baseline
[2018-01-21 17:42:14.362139 UTC] Computing logging information
-------------------------------------
| Iteration            | 2028       |
| ExpectedImprovement  | 0.014499   |
| ActualImprovement    | 0.013744   |
| ImprovementRatio     | 0.94791    |
| MeanKL               | 0.0064631  |
| Entropy              | -2.182     |
| Perplexity           | 0.11281    |
| AveragePolicyStd     | 0.17084    |
| AveragePolicyStd[0]  | 0.18626    |
| AveragePolicyStd[1]  | 0.18855    |
| AveragePolicyStd[2]  | 0.14188    |
| AveragePolicyStd[3]  | 0.15035    |
| AveragePolicyStd[4]  | 0.13642    |
| AveragePolicyStd[5]  | 0.22155    |
| AverageReturn        | 1937.8     |
| MinReturn            | 404        |
| MaxReturn            | 2085.3     |
| StdReturn            | 314.23     |
| AverageEpisodeLength | 956.71     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 147.78     |
| TotalNEpisodes       | 27319      |
| TotalNSamples        | 1.0154e+07 |
| ExplainedVariance    | 0.0024943  |
-------------------------------------
[2018-01-21 17:42:14.952500 UTC] Saving snapshot
[2018-01-21 17:42:14.952753 UTC] Starting iteration 2029
[2018-01-21 17:42:14.952928 UTC] Start collecting samples
[2018-01-21 17:42:18.744042 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:18.848489 UTC] Performing policy update
[2018-01-21 17:42:18.849074 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:18.940179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:20.130098 UTC] Performing line search
[2018-01-21 17:42:20.269879 UTC] Updating baseline
[2018-01-21 17:42:22.296803 UTC] Computing logging information
-------------------------------------
| Iteration            | 2029       |
| ExpectedImprovement  | 0.017938   |
| ActualImprovement    | 0.016751   |
| ImprovementRatio     | 0.93383    |
| MeanKL               | 0.0086426  |
| Entropy              | -2.1802    |
| Perplexity           | 0.11302    |
| AveragePolicyStd     | 0.17087    |
| AveragePolicyStd[0]  | 0.18624    |
| AveragePolicyStd[1]  | 0.18871    |
| AveragePolicyStd[2]  | 0.14217    |
| AveragePolicyStd[3]  | 0.15027    |
| AveragePolicyStd[4]  | 0.13649    |
| AveragePolicyStd[5]  | 0.22135    |
| AverageReturn        | 1925       |
| MinReturn            | 404        |
| MaxReturn            | 2085.3     |
| StdReturn            | 330.03     |
| AverageEpisodeLength | 952.27     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 154.21     |
| TotalNEpisodes       | 27325      |
| TotalNSamples        | 1.0159e+07 |
| ExplainedVariance    | 0.13592    |
-------------------------------------
[2018-01-21 17:42:22.864227 UTC] Saving snapshot
[2018-01-21 17:42:22.864454 UTC] Starting iteration 2030
[2018-01-21 17:42:22.864590 UTC] Start collecting samples
[2018-01-21 17:42:27.014924 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:27.105127 UTC] Performing policy update
[2018-01-21 17:42:27.105722 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:27.187139 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:28.227871 UTC] Performing line search
[2018-01-21 17:42:28.367486 UTC] Updating baseline
[2018-01-21 17:42:29.995046 UTC] Computing logging information
-------------------------------------
| Iteration            | 2030       |
| ExpectedImprovement  | 0.018144   |
| ActualImprovement    | 0.017284   |
| ImprovementRatio     | 0.95262    |
| MeanKL               | 0.0086032  |
| Entropy              | -2.1843    |
| Perplexity           | 0.11256    |
| AveragePolicyStd     | 0.17077    |
| AveragePolicyStd[0]  | 0.18621    |
| AveragePolicyStd[1]  | 0.18887    |
| AveragePolicyStd[2]  | 0.14178    |
| AveragePolicyStd[3]  | 0.15024    |
| AveragePolicyStd[4]  | 0.13631    |
| AveragePolicyStd[5]  | 0.22122    |
| AverageReturn        | 1920       |
| MinReturn            | 404        |
| MaxReturn            | 2085.3     |
| StdReturn            | 332.73     |
| AverageEpisodeLength | 949.83     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.36     |
| TotalNEpisodes       | 27330      |
| TotalNSamples        | 1.0164e+07 |
| ExplainedVariance    | 0.08067    |
-------------------------------------
[2018-01-21 17:42:30.793312 UTC] Saving snapshot
[2018-01-21 17:42:30.803875 UTC] Starting iteration 2031
[2018-01-21 17:42:30.804122 UTC] Start collecting samples
[2018-01-21 17:42:34.389476 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:34.483171 UTC] Performing policy update
[2018-01-21 17:42:34.483785 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:34.576203 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:35.593858 UTC] Performing line search
[2018-01-21 17:42:35.770361 UTC] Updating baseline
[2018-01-21 17:42:38.089055 UTC] Computing logging information
-------------------------------------
| Iteration            | 2031       |
| ExpectedImprovement  | 0.018626   |
| ActualImprovement    | 0.017365   |
| ImprovementRatio     | 0.9323     |
| MeanKL               | 0.0079922  |
| Entropy              | -2.1834    |
| Perplexity           | 0.11266    |
| AveragePolicyStd     | 0.17081    |
| AveragePolicyStd[0]  | 0.18647    |
| AveragePolicyStd[1]  | 0.18886    |
| AveragePolicyStd[2]  | 0.14184    |
| AveragePolicyStd[3]  | 0.15037    |
| AveragePolicyStd[4]  | 0.13602    |
| AveragePolicyStd[5]  | 0.22133    |
| AverageReturn        | 1919.2     |
| MinReturn            | 404        |
| MaxReturn            | 2085.3     |
| StdReturn            | 332.43     |
| AverageEpisodeLength | 949.83     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.36     |
| TotalNEpisodes       | 27333      |
| TotalNSamples        | 1.0167e+07 |
| ExplainedVariance    | 0.0061252  |
-------------------------------------
[2018-01-21 17:42:38.844656 UTC] Saving snapshot
[2018-01-21 17:42:38.844935 UTC] Starting iteration 2032
[2018-01-21 17:42:38.845119 UTC] Start collecting samples
[2018-01-21 17:42:42.991158 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:43.126202 UTC] Performing policy update
[2018-01-21 17:42:43.126865 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:43.247973 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:44.685949 UTC] Performing line search
[2018-01-21 17:42:44.886005 UTC] Updating baseline
[2018-01-21 17:42:46.824935 UTC] Computing logging information
-------------------------------------
| Iteration            | 2032       |
| ExpectedImprovement  | 0.018851   |
| ActualImprovement    | 0.017613   |
| ImprovementRatio     | 0.93435    |
| MeanKL               | 0.0081458  |
| Entropy              | -2.1826    |
| Perplexity           | 0.11275    |
| AveragePolicyStd     | 0.17084    |
| AveragePolicyStd[0]  | 0.18641    |
| AveragePolicyStd[1]  | 0.18901    |
| AveragePolicyStd[2]  | 0.14153    |
| AveragePolicyStd[3]  | 0.15052    |
| AveragePolicyStd[4]  | 0.13616    |
| AveragePolicyStd[5]  | 0.22143    |
| AverageReturn        | 1929.6     |
| MinReturn            | 404        |
| MaxReturn            | 2085.3     |
| StdReturn            | 320.54     |
| AverageEpisodeLength | 955.57     |
| MinEpisodeLength     | 231        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 149.81     |
| TotalNEpisodes       | 27340      |
| TotalNSamples        | 1.0174e+07 |
| ExplainedVariance    | 0.082426   |
-------------------------------------
[2018-01-21 17:42:47.383510 UTC] Saving snapshot
[2018-01-21 17:42:47.383701 UTC] Starting iteration 2033
[2018-01-21 17:42:47.383848 UTC] Start collecting samples
[2018-01-21 17:42:51.557632 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:51.632821 UTC] Performing policy update
[2018-01-21 17:42:51.633491 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:51.710250 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:52.786819 UTC] Performing line search
[2018-01-21 17:42:52.937237 UTC] Updating baseline
[2018-01-21 17:42:54.593113 UTC] Computing logging information
------------------------------------
| Iteration            | 2033      |
| ExpectedImprovement  | 0.020583  |
| ActualImprovement    | 0.018593  |
| ImprovementRatio     | 0.90333   |
| MeanKL               | 0.0076968 |
| Entropy              | -2.1836   |
| Perplexity           | 0.11263   |
| AveragePolicyStd     | 0.17082   |
| AveragePolicyStd[0]  | 0.18671   |
| AveragePolicyStd[1]  | 0.18864   |
| AveragePolicyStd[2]  | 0.14143   |
| AveragePolicyStd[3]  | 0.15045   |
| AveragePolicyStd[4]  | 0.13616   |
| AveragePolicyStd[5]  | 0.22153   |
| AverageReturn        | 1929.3    |
| MinReturn            | 404       |
| MaxReturn            | 2085.3    |
| StdReturn            | 318.83    |
| AverageEpisodeLength | 956.35    |
| MinEpisodeLength     | 231       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 149.25    |
| TotalNEpisodes       | 27346     |
| TotalNSamples        | 1.018e+07 |
| ExplainedVariance    | 0.14413   |
------------------------------------
[2018-01-21 17:42:55.156140 UTC] Saving snapshot
[2018-01-21 17:42:55.156322 UTC] Starting iteration 2034
[2018-01-21 17:42:55.156459 UTC] Start collecting samples
[2018-01-21 17:42:57.995399 UTC] Computing input variables for policy optimization
[2018-01-21 17:42:58.091266 UTC] Performing policy update
[2018-01-21 17:42:58.091821 UTC] Computing gradient in Euclidean space
[2018-01-21 17:42:58.180764 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:42:59.169110 UTC] Performing line search
[2018-01-21 17:42:59.333292 UTC] Updating baseline
[2018-01-21 17:43:00.640152 UTC] Computing logging information
-------------------------------------
| Iteration            | 2034       |
| ExpectedImprovement  | 0.017976   |
| ActualImprovement    | 0.017368   |
| ImprovementRatio     | 0.96616    |
| MeanKL               | 0.0087024  |
| Entropy              | -2.1887    |
| Perplexity           | 0.11206    |
| AveragePolicyStd     | 0.17068    |
| AveragePolicyStd[0]  | 0.1867     |
| AveragePolicyStd[1]  | 0.18849    |
| AveragePolicyStd[2]  | 0.14132    |
| AveragePolicyStd[3]  | 0.15031    |
| AveragePolicyStd[4]  | 0.13596    |
| AveragePolicyStd[5]  | 0.22131    |
| AverageReturn        | 1885       |
| MinReturn            | 317.53     |
| MaxReturn            | 2085.3     |
| StdReturn            | 400.62     |
| AverageEpisodeLength | 934.88     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 189.56     |
| TotalNEpisodes       | 27351      |
| TotalNSamples        | 1.0183e+07 |
| ExplainedVariance    | 0.29748    |
-------------------------------------
[2018-01-21 17:43:01.191748 UTC] Saving snapshot
[2018-01-21 17:43:01.191947 UTC] Starting iteration 2035
[2018-01-21 17:43:01.192098 UTC] Start collecting samples
[2018-01-21 17:43:04.030241 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:04.106067 UTC] Performing policy update
[2018-01-21 17:43:04.106615 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:04.186358 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:05.190876 UTC] Performing line search
[2018-01-21 17:43:05.364764 UTC] Updating baseline
[2018-01-21 17:43:06.641572 UTC] Computing logging information
-------------------------------------
| Iteration            | 2035       |
| ExpectedImprovement  | 0.018251   |
| ActualImprovement    | 0.017542   |
| ImprovementRatio     | 0.96114    |
| MeanKL               | 0.0082673  |
| Entropy              | -2.1922    |
| Perplexity           | 0.11167    |
| AveragePolicyStd     | 0.1706     |
| AveragePolicyStd[0]  | 0.18679    |
| AveragePolicyStd[1]  | 0.18841    |
| AveragePolicyStd[2]  | 0.14111    |
| AveragePolicyStd[3]  | 0.15006    |
| AveragePolicyStd[4]  | 0.13589    |
| AveragePolicyStd[5]  | 0.22133    |
| AverageReturn        | 1894.3     |
| MinReturn            | 317.53     |
| MaxReturn            | 2089.5     |
| StdReturn            | 377.61     |
| AverageEpisodeLength | 938.88     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.56     |
| TotalNEpisodes       | 27358      |
| TotalNSamples        | 1.0189e+07 |
| ExplainedVariance    | 0.076289   |
-------------------------------------
[2018-01-21 17:43:07.328032 UTC] Saving snapshot
[2018-01-21 17:43:07.328215 UTC] Starting iteration 2036
[2018-01-21 17:43:07.328332 UTC] Start collecting samples
[2018-01-21 17:43:10.500420 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:10.618211 UTC] Performing policy update
[2018-01-21 17:43:10.619041 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:10.730238 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:11.796788 UTC] Performing line search
[2018-01-21 17:43:11.922489 UTC] Updating baseline
[2018-01-21 17:43:13.164339 UTC] Computing logging information
-------------------------------------
| Iteration            | 2036       |
| ExpectedImprovement  | 0.01935    |
| ActualImprovement    | 0.017975   |
| ImprovementRatio     | 0.92893    |
| MeanKL               | 0.0091236  |
| Entropy              | -2.1935    |
| Perplexity           | 0.11153    |
| AveragePolicyStd     | 0.17057    |
| AveragePolicyStd[0]  | 0.18683    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.14101    |
| AveragePolicyStd[3]  | 0.15006    |
| AveragePolicyStd[4]  | 0.13591    |
| AveragePolicyStd[5]  | 0.22148    |
| AverageReturn        | 1893.4     |
| MinReturn            | 317.53     |
| MaxReturn            | 2089.5     |
| StdReturn            | 377.58     |
| AverageEpisodeLength | 938.88     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 178.56     |
| TotalNEpisodes       | 27362      |
| TotalNSamples        | 1.0193e+07 |
| ExplainedVariance    | -0.023694  |
-------------------------------------
[2018-01-21 17:43:13.729956 UTC] Saving snapshot
[2018-01-21 17:43:13.730155 UTC] Starting iteration 2037
[2018-01-21 17:43:13.730319 UTC] Start collecting samples
[2018-01-21 17:43:17.069242 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:17.149847 UTC] Performing policy update
[2018-01-21 17:43:17.150360 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:17.230838 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:18.183536 UTC] Performing line search
[2018-01-21 17:43:18.317148 UTC] Updating baseline
[2018-01-21 17:43:19.677192 UTC] Computing logging information
-------------------------------------
| Iteration            | 2037       |
| ExpectedImprovement  | 0.021192   |
| ActualImprovement    | 0.02012    |
| ImprovementRatio     | 0.94943    |
| MeanKL               | 0.0080897  |
| Entropy              | -2.2011    |
| Perplexity           | 0.11068    |
| AveragePolicyStd     | 0.17037    |
| AveragePolicyStd[0]  | 0.18709    |
| AveragePolicyStd[1]  | 0.18753    |
| AveragePolicyStd[2]  | 0.14096    |
| AveragePolicyStd[3]  | 0.14988    |
| AveragePolicyStd[4]  | 0.1354     |
| AveragePolicyStd[5]  | 0.22136    |
| AverageReturn        | 1882.5     |
| MinReturn            | 317.53     |
| MaxReturn            | 2089.5     |
| StdReturn            | 380.12     |
| AverageEpisodeLength | 935.09     |
| MinEpisodeLength     | 184        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 179.24     |
| TotalNEpisodes       | 27368      |
| TotalNSamples        | 1.0199e+07 |
| ExplainedVariance    | 0.19147    |
-------------------------------------
[2018-01-21 17:43:20.234633 UTC] Saving snapshot
[2018-01-21 17:43:20.234833 UTC] Starting iteration 2038
[2018-01-21 17:43:20.234993 UTC] Start collecting samples
[2018-01-21 17:43:23.200716 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:23.280494 UTC] Performing policy update
[2018-01-21 17:43:23.281112 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:23.369369 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:24.334296 UTC] Performing line search
[2018-01-21 17:43:24.456722 UTC] Updating baseline
[2018-01-21 17:43:25.765877 UTC] Computing logging information
-------------------------------------
| Iteration            | 2038       |
| ExpectedImprovement  | 0.022964   |
| ActualImprovement    | 0.02199    |
| ImprovementRatio     | 0.95758    |
| MeanKL               | 0.0079283  |
| Entropy              | -2.2062    |
| Perplexity           | 0.11012    |
| AveragePolicyStd     | 0.17025    |
| AveragePolicyStd[0]  | 0.18736    |
| AveragePolicyStd[1]  | 0.18732    |
| AveragePolicyStd[2]  | 0.14074    |
| AveragePolicyStd[3]  | 0.14942    |
| AveragePolicyStd[4]  | 0.13525    |
| AveragePolicyStd[5]  | 0.22141    |
| AverageReturn        | 1851.8     |
| MinReturn            | 133.07     |
| MaxReturn            | 2089.5     |
| StdReturn            | 423.5      |
| AverageEpisodeLength | 921.99     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 199.99     |
| TotalNEpisodes       | 27374      |
| TotalNSamples        | 1.0203e+07 |
| ExplainedVariance    | 0.044652   |
-------------------------------------
[2018-01-21 17:43:26.329132 UTC] Saving snapshot
[2018-01-21 17:43:26.329335 UTC] Starting iteration 2039
[2018-01-21 17:43:26.329489 UTC] Start collecting samples
[2018-01-21 17:43:29.336897 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:29.425074 UTC] Performing policy update
[2018-01-21 17:43:29.425602 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:29.507233 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:30.422672 UTC] Performing line search
[2018-01-21 17:43:30.560211 UTC] Updating baseline
[2018-01-21 17:43:32.146859 UTC] Computing logging information
-------------------------------------
| Iteration            | 2039       |
| ExpectedImprovement  | 0.020669   |
| ActualImprovement    | 0.01946    |
| ImprovementRatio     | 0.94152    |
| MeanKL               | 0.0079028  |
| Entropy              | -2.2106    |
| Perplexity           | 0.10964    |
| AveragePolicyStd     | 0.17013    |
| AveragePolicyStd[0]  | 0.18722    |
| AveragePolicyStd[1]  | 0.18699    |
| AveragePolicyStd[2]  | 0.14062    |
| AveragePolicyStd[3]  | 0.14937    |
| AveragePolicyStd[4]  | 0.13514    |
| AveragePolicyStd[5]  | 0.22146    |
| AverageReturn        | 1821       |
| MinReturn            | 133.07     |
| MaxReturn            | 2089.5     |
| StdReturn            | 464.71     |
| AverageEpisodeLength | 907.34     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 219.7      |
| TotalNEpisodes       | 27381      |
| TotalNSamples        | 1.0209e+07 |
| ExplainedVariance    | 0.14711    |
-------------------------------------
[2018-01-21 17:43:32.722947 UTC] Saving snapshot
[2018-01-21 17:43:32.723151 UTC] Starting iteration 2040
[2018-01-21 17:43:32.723304 UTC] Start collecting samples
[2018-01-21 17:43:35.568099 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:35.667270 UTC] Performing policy update
[2018-01-21 17:43:35.667914 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:35.743845 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:36.706217 UTC] Performing line search
[2018-01-21 17:43:36.826321 UTC] Updating baseline
[2018-01-21 17:43:38.280943 UTC] Computing logging information
-------------------------------------
| Iteration            | 2040       |
| ExpectedImprovement  | 0.018793   |
| ActualImprovement    | 0.01755    |
| ImprovementRatio     | 0.93387    |
| MeanKL               | 0.0084757  |
| Entropy              | -2.221     |
| Perplexity           | 0.1085     |
| AveragePolicyStd     | 0.16987    |
| AveragePolicyStd[0]  | 0.18687    |
| AveragePolicyStd[1]  | 0.18686    |
| AveragePolicyStd[2]  | 0.14007    |
| AveragePolicyStd[3]  | 0.14933    |
| AveragePolicyStd[4]  | 0.13467    |
| AveragePolicyStd[5]  | 0.22141    |
| AverageReturn        | 1811.7     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 470.57     |
| AverageEpisodeLength | 902.76     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.48     |
| TotalNEpisodes       | 27385      |
| TotalNSamples        | 1.0213e+07 |
| ExplainedVariance    | 0.21403    |
-------------------------------------
[2018-01-21 17:43:38.851523 UTC] Saving snapshot
[2018-01-21 17:43:38.857200 UTC] Starting iteration 2041
[2018-01-21 17:43:38.857393 UTC] Start collecting samples
[2018-01-21 17:43:41.719744 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:41.813846 UTC] Performing policy update
[2018-01-21 17:43:41.814480 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:41.914091 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:42.862722 UTC] Performing line search
[2018-01-21 17:43:42.995020 UTC] Updating baseline
[2018-01-21 17:43:44.377479 UTC] Computing logging information
-------------------------------------
| Iteration            | 2041       |
| ExpectedImprovement  | 0.018406   |
| ActualImprovement    | 0.017419   |
| ImprovementRatio     | 0.9464     |
| MeanKL               | 0.0086855  |
| Entropy              | -2.2247    |
| Perplexity           | 0.1081     |
| AveragePolicyStd     | 0.16974    |
| AveragePolicyStd[0]  | 0.18652    |
| AveragePolicyStd[1]  | 0.1867     |
| AveragePolicyStd[2]  | 0.14017    |
| AveragePolicyStd[3]  | 0.1492     |
| AveragePolicyStd[4]  | 0.13472    |
| AveragePolicyStd[5]  | 0.22116    |
| AverageReturn        | 1814       |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 470.33     |
| AverageEpisodeLength | 903.29     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 222.02     |
| TotalNEpisodes       | 27392      |
| TotalNSamples        | 1.0219e+07 |
| ExplainedVariance    | 0.12888    |
-------------------------------------
[2018-01-21 17:43:44.986202 UTC] Saving snapshot
[2018-01-21 17:43:44.986423 UTC] Starting iteration 2042
[2018-01-21 17:43:44.986591 UTC] Start collecting samples
[2018-01-21 17:43:47.763037 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:47.845895 UTC] Performing policy update
[2018-01-21 17:43:47.846480 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:47.926938 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:48.863266 UTC] Performing line search
[2018-01-21 17:43:49.001651 UTC] Updating baseline
[2018-01-21 17:43:50.160259 UTC] Computing logging information
-------------------------------------
| Iteration            | 2042       |
| ExpectedImprovement  | 0.018481   |
| ActualImprovement    | 0.017426   |
| ImprovementRatio     | 0.94294    |
| MeanKL               | 0.0083051  |
| Entropy              | -2.2262    |
| Perplexity           | 0.10794    |
| AveragePolicyStd     | 0.16973    |
| AveragePolicyStd[0]  | 0.18675    |
| AveragePolicyStd[1]  | 0.18673    |
| AveragePolicyStd[2]  | 0.14012    |
| AveragePolicyStd[3]  | 0.14887    |
| AveragePolicyStd[4]  | 0.13459    |
| AveragePolicyStd[5]  | 0.22128    |
| AverageReturn        | 1825.2     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 458.43     |
| AverageEpisodeLength | 909.3      |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 216.35     |
| TotalNEpisodes       | 27396      |
| TotalNSamples        | 1.0223e+07 |
| ExplainedVariance    | -0.069656  |
-------------------------------------
[2018-01-21 17:43:50.723004 UTC] Saving snapshot
[2018-01-21 17:43:50.723194 UTC] Starting iteration 2043
[2018-01-21 17:43:50.723334 UTC] Start collecting samples
[2018-01-21 17:43:53.572849 UTC] Computing input variables for policy optimization
[2018-01-21 17:43:53.656434 UTC] Performing policy update
[2018-01-21 17:43:53.657157 UTC] Computing gradient in Euclidean space
[2018-01-21 17:43:53.736823 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:43:54.690484 UTC] Performing line search
[2018-01-21 17:43:54.825184 UTC] Updating baseline
[2018-01-21 17:43:56.404962 UTC] Computing logging information
-------------------------------------
| Iteration            | 2043       |
| ExpectedImprovement  | 0.016786   |
| ActualImprovement    | 0.015815   |
| ImprovementRatio     | 0.94215    |
| MeanKL               | 0.0089047  |
| Entropy              | -2.2256    |
| Perplexity           | 0.10801    |
| AveragePolicyStd     | 0.16973    |
| AveragePolicyStd[0]  | 0.18675    |
| AveragePolicyStd[1]  | 0.18699    |
| AveragePolicyStd[2]  | 0.14019    |
| AveragePolicyStd[3]  | 0.1492     |
| AveragePolicyStd[4]  | 0.13436    |
| AveragePolicyStd[5]  | 0.2209     |
| AverageReturn        | 1852.4     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 427.5      |
| AverageEpisodeLength | 922.15     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.55     |
| TotalNEpisodes       | 27400      |
| TotalNSamples        | 1.0227e+07 |
| ExplainedVariance    | 0.023018   |
-------------------------------------
[2018-01-21 17:43:56.997426 UTC] Saving snapshot
[2018-01-21 17:43:56.997637 UTC] Starting iteration 2044
[2018-01-21 17:43:56.997768 UTC] Start collecting samples
[2018-01-21 17:43:59.988050 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:00.064178 UTC] Performing policy update
[2018-01-21 17:44:00.064692 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:00.141843 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:01.144464 UTC] Performing line search
[2018-01-21 17:44:01.263791 UTC] Updating baseline
[2018-01-21 17:44:02.568608 UTC] Computing logging information
-------------------------------------
| Iteration            | 2044       |
| ExpectedImprovement  | 0.017714   |
| ActualImprovement    | 0.016754   |
| ImprovementRatio     | 0.94583    |
| MeanKL               | 0.0081033  |
| Entropy              | -2.2219    |
| Perplexity           | 0.1084     |
| AveragePolicyStd     | 0.16986    |
| AveragePolicyStd[0]  | 0.18703    |
| AveragePolicyStd[1]  | 0.1876     |
| AveragePolicyStd[2]  | 0.1401     |
| AveragePolicyStd[3]  | 0.14895    |
| AveragePolicyStd[4]  | 0.13444    |
| AveragePolicyStd[5]  | 0.22103    |
| AverageReturn        | 1847.6     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 426.89     |
| AverageEpisodeLength | 922.15     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 201.55     |
| TotalNEpisodes       | 27407      |
| TotalNSamples        | 1.0234e+07 |
| ExplainedVariance    | 0.026485   |
-------------------------------------
[2018-01-21 17:44:03.173562 UTC] Saving snapshot
[2018-01-21 17:44:03.173786 UTC] Starting iteration 2045
[2018-01-21 17:44:03.173952 UTC] Start collecting samples
[2018-01-21 17:44:06.128948 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:06.213738 UTC] Performing policy update
[2018-01-21 17:44:06.214340 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:06.295924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:07.295311 UTC] Performing line search
[2018-01-21 17:44:07.425681 UTC] Updating baseline
[2018-01-21 17:44:08.817754 UTC] Computing logging information
------------------------------------
| Iteration            | 2045      |
| ExpectedImprovement  | 0.01896   |
| ActualImprovement    | 0.018243  |
| ImprovementRatio     | 0.96218   |
| MeanKL               | 0.0088301 |
| Entropy              | -2.2286   |
| Perplexity           | 0.10768   |
| AveragePolicyStd     | 0.1697    |
| AveragePolicyStd[0]  | 0.18705   |
| AveragePolicyStd[1]  | 0.18767   |
| AveragePolicyStd[2]  | 0.13988   |
| AveragePolicyStd[3]  | 0.14841   |
| AveragePolicyStd[4]  | 0.13418   |
| AveragePolicyStd[5]  | 0.22103   |
| AverageReturn        | 1816.1    |
| MinReturn            | 133.07    |
| MaxReturn            | 2105.3    |
| StdReturn            | 467.07    |
| AverageEpisodeLength | 907.52    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 221.32    |
| TotalNEpisodes       | 27414     |
| TotalNSamples        | 1.024e+07 |
| ExplainedVariance    | 0.15938   |
------------------------------------
[2018-01-21 17:44:09.384031 UTC] Saving snapshot
[2018-01-21 17:44:09.384235 UTC] Starting iteration 2046
[2018-01-21 17:44:09.384375 UTC] Start collecting samples
[2018-01-21 17:44:12.254557 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:12.332618 UTC] Performing policy update
[2018-01-21 17:44:12.333174 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:12.409484 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:13.381405 UTC] Performing line search
[2018-01-21 17:44:13.510799 UTC] Updating baseline
[2018-01-21 17:44:14.778649 UTC] Computing logging information
-------------------------------------
| Iteration            | 2046       |
| ExpectedImprovement  | 0.01866    |
| ActualImprovement    | 0.018198   |
| ImprovementRatio     | 0.97523    |
| MeanKL               | 0.0082225  |
| Entropy              | -2.231     |
| Perplexity           | 0.10742    |
| AveragePolicyStd     | 0.16963    |
| AveragePolicyStd[0]  | 0.18706    |
| AveragePolicyStd[1]  | 0.18774    |
| AveragePolicyStd[2]  | 0.13979    |
| AveragePolicyStd[3]  | 0.14844    |
| AveragePolicyStd[4]  | 0.13405    |
| AveragePolicyStd[5]  | 0.2207     |
| AverageReturn        | 1816.2     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 467.12     |
| AverageEpisodeLength | 907.52     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 221.32     |
| TotalNEpisodes       | 27416      |
| TotalNSamples        | 1.0242e+07 |
| ExplainedVariance    | 0.040235   |
-------------------------------------
[2018-01-21 17:44:15.340424 UTC] Saving snapshot
[2018-01-21 17:44:15.340669 UTC] Starting iteration 2047
[2018-01-21 17:44:15.340843 UTC] Start collecting samples
[2018-01-21 17:44:18.285697 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:18.380314 UTC] Performing policy update
[2018-01-21 17:44:18.380830 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:18.467192 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:19.450824 UTC] Performing line search
[2018-01-21 17:44:19.590529 UTC] Updating baseline
[2018-01-21 17:44:21.157891 UTC] Computing logging information
-------------------------------------
| Iteration            | 2047       |
| ExpectedImprovement  | 0.018954   |
| ActualImprovement    | 0.018045   |
| ImprovementRatio     | 0.95203    |
| MeanKL               | 0.0096026  |
| Entropy              | -2.2382    |
| Perplexity           | 0.10665    |
| AveragePolicyStd     | 0.16942    |
| AveragePolicyStd[0]  | 0.18657    |
| AveragePolicyStd[1]  | 0.18741    |
| AveragePolicyStd[2]  | 0.13988    |
| AveragePolicyStd[3]  | 0.14814    |
| AveragePolicyStd[4]  | 0.13394    |
| AveragePolicyStd[5]  | 0.22058    |
| AverageReturn        | 1821       |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 460.45     |
| AverageEpisodeLength | 912.18     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 218.3      |
| TotalNEpisodes       | 27422      |
| TotalNSamples        | 1.0248e+07 |
| ExplainedVariance    | 0.027573   |
-------------------------------------
[2018-01-21 17:44:21.754480 UTC] Saving snapshot
[2018-01-21 17:44:21.754699 UTC] Starting iteration 2048
[2018-01-21 17:44:21.754878 UTC] Start collecting samples
[2018-01-21 17:44:24.595110 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:24.676679 UTC] Performing policy update
[2018-01-21 17:44:24.677198 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:24.757021 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:25.727119 UTC] Performing line search
[2018-01-21 17:44:25.847404 UTC] Updating baseline
[2018-01-21 17:44:27.311944 UTC] Computing logging information
-------------------------------------
| Iteration            | 2048       |
| ExpectedImprovement  | 0.017913   |
| ActualImprovement    | 0.01691    |
| ImprovementRatio     | 0.94402    |
| MeanKL               | 0.0079761  |
| Entropy              | -2.2383    |
| Perplexity           | 0.10664    |
| AveragePolicyStd     | 0.16944    |
| AveragePolicyStd[0]  | 0.18639    |
| AveragePolicyStd[1]  | 0.18764    |
| AveragePolicyStd[2]  | 0.13958    |
| AveragePolicyStd[3]  | 0.14813    |
| AveragePolicyStd[4]  | 0.13399    |
| AveragePolicyStd[5]  | 0.2209     |
| AverageReturn        | 1825.6     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 450.91     |
| AverageEpisodeLength | 917.13     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 214.67     |
| TotalNEpisodes       | 27429      |
| TotalNSamples        | 1.0255e+07 |
| ExplainedVariance    | 0.088453   |
-------------------------------------
[2018-01-21 17:44:27.891625 UTC] Saving snapshot
[2018-01-21 17:44:27.891819 UTC] Starting iteration 2049
[2018-01-21 17:44:27.891950 UTC] Start collecting samples
[2018-01-21 17:44:30.968799 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:31.065103 UTC] Performing policy update
[2018-01-21 17:44:31.065743 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:31.156957 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:32.207020 UTC] Performing line search
[2018-01-21 17:44:32.412740 UTC] Updating baseline
[2018-01-21 17:44:33.883692 UTC] Computing logging information
-------------------------------------
| Iteration            | 2049       |
| ExpectedImprovement  | 0.01564    |
| ActualImprovement    | 0.01391    |
| ImprovementRatio     | 0.88941    |
| MeanKL               | 0.0089627  |
| Entropy              | -2.2378    |
| Perplexity           | 0.10669    |
| AveragePolicyStd     | 0.16946    |
| AveragePolicyStd[0]  | 0.18624    |
| AveragePolicyStd[1]  | 0.18754    |
| AveragePolicyStd[2]  | 0.13953    |
| AveragePolicyStd[3]  | 0.14805    |
| AveragePolicyStd[4]  | 0.13411    |
| AveragePolicyStd[5]  | 0.22131    |
| AverageReturn        | 1819       |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 450.82     |
| AverageEpisodeLength | 914.69     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.1      |
| TotalNEpisodes       | 27432      |
| TotalNSamples        | 1.0257e+07 |
| ExplainedVariance    | 0.23155    |
-------------------------------------
[2018-01-21 17:44:34.481240 UTC] Saving snapshot
[2018-01-21 17:44:34.481430 UTC] Starting iteration 2050
[2018-01-21 17:44:34.481587 UTC] Start collecting samples
[2018-01-21 17:44:37.157660 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:37.245205 UTC] Performing policy update
[2018-01-21 17:44:37.245795 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:37.327039 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:38.381725 UTC] Performing line search
[2018-01-21 17:44:38.578248 UTC] Updating baseline
[2018-01-21 17:44:40.716422 UTC] Computing logging information
-------------------------------------
| Iteration            | 2050       |
| ExpectedImprovement  | 0.014799   |
| ActualImprovement    | 0.014013   |
| ImprovementRatio     | 0.94688    |
| MeanKL               | 0.0064568  |
| Entropy              | -2.2358    |
| Perplexity           | 0.1069     |
| AveragePolicyStd     | 0.16954    |
| AveragePolicyStd[0]  | 0.18642    |
| AveragePolicyStd[1]  | 0.18767    |
| AveragePolicyStd[2]  | 0.13942    |
| AveragePolicyStd[3]  | 0.14824    |
| AveragePolicyStd[4]  | 0.13393    |
| AveragePolicyStd[5]  | 0.22158    |
| AverageReturn        | 1819       |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 450.51     |
| AverageEpisodeLength | 914.69     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 215.1      |
| TotalNEpisodes       | 27438      |
| TotalNSamples        | 1.0263e+07 |
| ExplainedVariance    | 0.0049324  |
-------------------------------------
[2018-01-21 17:44:41.300504 UTC] Saving snapshot
[2018-01-21 17:44:41.306117 UTC] Starting iteration 2051
[2018-01-21 17:44:41.306287 UTC] Start collecting samples
[2018-01-21 17:44:44.015388 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:44.091157 UTC] Performing policy update
[2018-01-21 17:44:44.091699 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:44.166819 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:45.170882 UTC] Performing line search
[2018-01-21 17:44:45.289842 UTC] Updating baseline
[2018-01-21 17:44:46.688404 UTC] Computing logging information
------------------------------------
| Iteration            | 2051      |
| ExpectedImprovement  | 0.018204  |
| ActualImprovement    | 0.017069  |
| ImprovementRatio     | 0.93764   |
| MeanKL               | 0.0083304 |
| Entropy              | -2.2408   |
| Perplexity           | 0.10638   |
| AveragePolicyStd     | 0.16941   |
| AveragePolicyStd[0]  | 0.18614   |
| AveragePolicyStd[1]  | 0.18799   |
| AveragePolicyStd[2]  | 0.13928   |
| AveragePolicyStd[3]  | 0.14815   |
| AveragePolicyStd[4]  | 0.13369   |
| AveragePolicyStd[5]  | 0.22119   |
| AverageReturn        | 1820.1    |
| MinReturn            | 133.07    |
| MaxReturn            | 2105.3    |
| StdReturn            | 451.36    |
| AverageEpisodeLength | 915.08    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 1000      |
| StdEpisodeLength     | 215.25    |
| TotalNEpisodes       | 27445     |
| TotalNSamples        | 1.027e+07 |
| ExplainedVariance    | 0.068497  |
------------------------------------
[2018-01-21 17:44:47.263849 UTC] Saving snapshot
[2018-01-21 17:44:47.264042 UTC] Starting iteration 2052
[2018-01-21 17:44:47.264189 UTC] Start collecting samples
[2018-01-21 17:44:50.201305 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:50.282426 UTC] Performing policy update
[2018-01-21 17:44:50.283127 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:50.363631 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:51.361484 UTC] Performing line search
[2018-01-21 17:44:51.488252 UTC] Updating baseline
[2018-01-21 17:44:53.074161 UTC] Computing logging information
-------------------------------------
| Iteration            | 2052       |
| ExpectedImprovement  | 0.01673    |
| ActualImprovement    | 0.015791   |
| ImprovementRatio     | 0.94391    |
| MeanKL               | 0.0084518  |
| Entropy              | -2.2407    |
| Perplexity           | 0.10639    |
| AveragePolicyStd     | 0.16944    |
| AveragePolicyStd[0]  | 0.18645    |
| AveragePolicyStd[1]  | 0.18812    |
| AveragePolicyStd[2]  | 0.13926    |
| AveragePolicyStd[3]  | 0.14792    |
| AveragePolicyStd[4]  | 0.13345    |
| AveragePolicyStd[5]  | 0.22146    |
| AverageReturn        | 1845.2     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 418.09     |
| AverageEpisodeLength | 926.7      |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 198.11     |
| TotalNEpisodes       | 27449      |
| TotalNSamples        | 1.0274e+07 |
| ExplainedVariance    | 0.19474    |
-------------------------------------
[2018-01-21 17:44:53.629559 UTC] Saving snapshot
[2018-01-21 17:44:53.629798 UTC] Starting iteration 2053
[2018-01-21 17:44:53.629977 UTC] Start collecting samples
[2018-01-21 17:44:56.535701 UTC] Computing input variables for policy optimization
[2018-01-21 17:44:56.615248 UTC] Performing policy update
[2018-01-21 17:44:56.615839 UTC] Computing gradient in Euclidean space
[2018-01-21 17:44:56.717227 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:44:57.810396 UTC] Performing line search
[2018-01-21 17:44:58.005015 UTC] Updating baseline
[2018-01-21 17:44:59.925122 UTC] Computing logging information
-------------------------------------
| Iteration            | 2053       |
| ExpectedImprovement  | 0.020258   |
| ActualImprovement    | 0.019328   |
| ImprovementRatio     | 0.95407    |
| MeanKL               | 0.008858   |
| Entropy              | -2.2399    |
| Perplexity           | 0.10647    |
| AveragePolicyStd     | 0.16945    |
| AveragePolicyStd[0]  | 0.18633    |
| AveragePolicyStd[1]  | 0.18811    |
| AveragePolicyStd[2]  | 0.13921    |
| AveragePolicyStd[3]  | 0.14803    |
| AveragePolicyStd[4]  | 0.13364    |
| AveragePolicyStd[5]  | 0.2214     |
| AverageReturn        | 1867.6     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 384.25     |
| AverageEpisodeLength | 938.55     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.17     |
| TotalNEpisodes       | 27454      |
| TotalNSamples        | 1.0279e+07 |
| ExplainedVariance    | 0.00082806 |
-------------------------------------
[2018-01-21 17:45:00.508997 UTC] Saving snapshot
[2018-01-21 17:45:00.509178 UTC] Starting iteration 2054
[2018-01-21 17:45:00.509283 UTC] Start collecting samples
[2018-01-21 17:45:04.367517 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:04.447274 UTC] Performing policy update
[2018-01-21 17:45:04.447862 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:04.528502 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:05.756252 UTC] Performing line search
[2018-01-21 17:45:05.899067 UTC] Updating baseline
[2018-01-21 17:45:07.432170 UTC] Computing logging information
-------------------------------------
| Iteration            | 2054       |
| ExpectedImprovement  | 0.01842    |
| ActualImprovement    | 0.01713    |
| ImprovementRatio     | 0.92997    |
| MeanKL               | 0.0087146  |
| Entropy              | -2.2406    |
| Perplexity           | 0.1064     |
| AveragePolicyStd     | 0.16943    |
| AveragePolicyStd[0]  | 0.18653    |
| AveragePolicyStd[1]  | 0.188      |
| AveragePolicyStd[2]  | 0.13928    |
| AveragePolicyStd[3]  | 0.14799    |
| AveragePolicyStd[4]  | 0.13358    |
| AveragePolicyStd[5]  | 0.22117    |
| AverageReturn        | 1866.3     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 383.69     |
| AverageEpisodeLength | 938.55     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 181.17     |
| TotalNEpisodes       | 27458      |
| TotalNSamples        | 1.0283e+07 |
| ExplainedVariance    | -0.001239  |
-------------------------------------
[2018-01-21 17:45:08.076768 UTC] Saving snapshot
[2018-01-21 17:45:08.076962 UTC] Starting iteration 2055
[2018-01-21 17:45:08.077104 UTC] Start collecting samples
[2018-01-21 17:45:11.652434 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:11.780451 UTC] Performing policy update
[2018-01-21 17:45:11.781364 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:11.897236 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:13.202069 UTC] Performing line search
[2018-01-21 17:45:13.414629 UTC] Updating baseline
[2018-01-21 17:45:15.161710 UTC] Computing logging information
-------------------------------------
| Iteration            | 2055       |
| ExpectedImprovement  | 0.01821    |
| ActualImprovement    | 0.017396   |
| ImprovementRatio     | 0.95531    |
| MeanKL               | 0.0087475  |
| Entropy              | -2.2377    |
| Perplexity           | 0.1067     |
| AveragePolicyStd     | 0.16951    |
| AveragePolicyStd[0]  | 0.18673    |
| AveragePolicyStd[1]  | 0.18821    |
| AveragePolicyStd[2]  | 0.13904    |
| AveragePolicyStd[3]  | 0.14801    |
| AveragePolicyStd[4]  | 0.1339     |
| AveragePolicyStd[5]  | 0.22115    |
| AverageReturn        | 1871.9     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 382.31     |
| AverageEpisodeLength | 940.52     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.76     |
| TotalNEpisodes       | 27464      |
| TotalNSamples        | 1.0289e+07 |
| ExplainedVariance    | 0.045965   |
-------------------------------------
[2018-01-21 17:45:15.749349 UTC] Saving snapshot
[2018-01-21 17:45:15.749543 UTC] Starting iteration 2056
[2018-01-21 17:45:15.749693 UTC] Start collecting samples
[2018-01-21 17:45:19.342930 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:19.420342 UTC] Performing policy update
[2018-01-21 17:45:19.420869 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:19.496273 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:20.589219 UTC] Performing line search
[2018-01-21 17:45:20.742345 UTC] Updating baseline
[2018-01-21 17:45:22.128907 UTC] Computing logging information
-------------------------------------
| Iteration            | 2056       |
| ExpectedImprovement  | 0.015502   |
| ActualImprovement    | 0.014602   |
| ImprovementRatio     | 0.94191    |
| MeanKL               | 0.0078581  |
| Entropy              | -2.2357    |
| Perplexity           | 0.10692    |
| AveragePolicyStd     | 0.16957    |
| AveragePolicyStd[0]  | 0.18648    |
| AveragePolicyStd[1]  | 0.18838    |
| AveragePolicyStd[2]  | 0.13931    |
| AveragePolicyStd[3]  | 0.14793    |
| AveragePolicyStd[4]  | 0.13385    |
| AveragePolicyStd[5]  | 0.22146    |
| AverageReturn        | 1873.6     |
| MinReturn            | 133.07     |
| MaxReturn            | 2105.3     |
| StdReturn            | 382.93     |
| AverageEpisodeLength | 940.52     |
| MinEpisodeLength     | 104        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 180.76     |
| TotalNEpisodes       | 27467      |
| TotalNSamples        | 1.0292e+07 |
| ExplainedVariance    | -0.040527  |
-------------------------------------
[2018-01-21 17:45:22.756743 UTC] Saving snapshot
[2018-01-21 17:45:22.757033 UTC] Starting iteration 2057
[2018-01-21 17:45:22.757218 UTC] Start collecting samples
[2018-01-21 17:45:26.159562 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:26.260383 UTC] Performing policy update
[2018-01-21 17:45:26.261032 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:26.334794 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:27.410751 UTC] Performing line search
[2018-01-21 17:45:27.555764 UTC] Updating baseline
[2018-01-21 17:45:28.885808 UTC] Computing logging information
-------------------------------------
| Iteration            | 2057       |
| ExpectedImprovement  | 0.016329   |
| ActualImprovement    | 0.015698   |
| ImprovementRatio     | 0.96137    |
| MeanKL               | 0.0093302  |
| Entropy              | -2.2403    |
| Perplexity           | 0.10642    |
| AveragePolicyStd     | 0.16944    |
| AveragePolicyStd[0]  | 0.18613    |
| AveragePolicyStd[1]  | 0.18836    |
| AveragePolicyStd[2]  | 0.13918    |
| AveragePolicyStd[3]  | 0.14758    |
| AveragePolicyStd[4]  | 0.13397    |
| AveragePolicyStd[5]  | 0.22141    |
| AverageReturn        | 1908.7     |
| MinReturn            | 299.44     |
| MaxReturn            | 2105.3     |
| StdReturn            | 330.52     |
| AverageEpisodeLength | 955.44     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 155.41     |
| TotalNEpisodes       | 27474      |
| TotalNSamples        | 1.0299e+07 |
| ExplainedVariance    | 0.00088152 |
-------------------------------------
[2018-01-21 17:45:29.638321 UTC] Saving snapshot
[2018-01-21 17:45:29.638537 UTC] Starting iteration 2058
[2018-01-21 17:45:29.638666 UTC] Start collecting samples
[2018-01-21 17:45:32.492389 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:32.567604 UTC] Performing policy update
[2018-01-21 17:45:32.568125 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:32.645804 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:33.604079 UTC] Performing line search
[2018-01-21 17:45:33.729257 UTC] Updating baseline
[2018-01-21 17:45:35.684412 UTC] Computing logging information
-------------------------------------
| Iteration            | 2058       |
| ExpectedImprovement  | 0.018977   |
| ActualImprovement    | 0.017815   |
| ImprovementRatio     | 0.93878    |
| MeanKL               | 0.0088066  |
| Entropy              | -2.2352    |
| Perplexity           | 0.10697    |
| AveragePolicyStd     | 0.1696     |
| AveragePolicyStd[0]  | 0.18635    |
| AveragePolicyStd[1]  | 0.18894    |
| AveragePolicyStd[2]  | 0.13914    |
| AveragePolicyStd[3]  | 0.14749    |
| AveragePolicyStd[4]  | 0.13408    |
| AveragePolicyStd[5]  | 0.22161    |
| AverageReturn        | 1914.6     |
| MinReturn            | 299.44     |
| MaxReturn            | 2105.3     |
| StdReturn            | 301.6      |
| AverageEpisodeLength | 962.31     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 141.41     |
| TotalNEpisodes       | 27479      |
| TotalNSamples        | 1.0304e+07 |
| ExplainedVariance    | 0.018091   |
-------------------------------------
[2018-01-21 17:45:36.295590 UTC] Saving snapshot
[2018-01-21 17:45:36.295817 UTC] Starting iteration 2059
[2018-01-21 17:45:36.296005 UTC] Start collecting samples
[2018-01-21 17:45:39.249908 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:39.412141 UTC] Performing policy update
[2018-01-21 17:45:39.413213 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:39.523374 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:40.598979 UTC] Performing line search
[2018-01-21 17:45:40.730553 UTC] Updating baseline
[2018-01-21 17:45:42.898561 UTC] Computing logging information
-------------------------------------
| Iteration            | 2059       |
| ExpectedImprovement  | 0.015627   |
| ActualImprovement    | 0.014679   |
| ImprovementRatio     | 0.9393     |
| MeanKL               | 0.0088071  |
| Entropy              | -2.2417    |
| Perplexity           | 0.10628    |
| AveragePolicyStd     | 0.16938    |
| AveragePolicyStd[0]  | 0.18601    |
| AveragePolicyStd[1]  | 0.18882    |
| AveragePolicyStd[2]  | 0.13914    |
| AveragePolicyStd[3]  | 0.1473     |
| AveragePolicyStd[4]  | 0.13418    |
| AveragePolicyStd[5]  | 0.22084    |
| AverageReturn        | 1930.1     |
| MinReturn            | 299.44     |
| MaxReturn            | 2105.3     |
| StdReturn            | 259.51     |
| AverageEpisodeLength | 970.09     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 120.29     |
| TotalNEpisodes       | 27483      |
| TotalNSamples        | 1.0308e+07 |
| ExplainedVariance    | -0.010495  |
-------------------------------------
[2018-01-21 17:45:43.841258 UTC] Saving snapshot
[2018-01-21 17:45:43.841543 UTC] Starting iteration 2060
[2018-01-21 17:45:43.841721 UTC] Start collecting samples
[2018-01-21 17:45:48.365192 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:48.453526 UTC] Performing policy update
[2018-01-21 17:45:48.454121 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:48.538161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:49.468896 UTC] Performing line search
[2018-01-21 17:45:49.587842 UTC] Updating baseline
[2018-01-21 17:45:50.943669 UTC] Computing logging information
-------------------------------------
| Iteration            | 2060       |
| ExpectedImprovement  | 0.016052   |
| ActualImprovement    | 0.015558   |
| ImprovementRatio     | 0.96922    |
| MeanKL               | 0.0081381  |
| Entropy              | -2.2383    |
| Perplexity           | 0.10664    |
| AveragePolicyStd     | 0.16946    |
| AveragePolicyStd[0]  | 0.18601    |
| AveragePolicyStd[1]  | 0.18905    |
| AveragePolicyStd[2]  | 0.13953    |
| AveragePolicyStd[3]  | 0.14726    |
| AveragePolicyStd[4]  | 0.13422    |
| AveragePolicyStd[5]  | 0.2207     |
| AverageReturn        | 1916.5     |
| MinReturn            | 299.44     |
| MaxReturn            | 2098.6     |
| StdReturn            | 277.67     |
| AverageEpisodeLength | 965.55     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.93     |
| TotalNEpisodes       | 27491      |
| TotalNSamples        | 1.0315e+07 |
| ExplainedVariance    | 0.20641    |
-------------------------------------
[2018-01-21 17:45:51.538641 UTC] Saving snapshot
[2018-01-21 17:45:51.544802 UTC] Starting iteration 2061
[2018-01-21 17:45:51.544991 UTC] Start collecting samples
[2018-01-21 17:45:54.029378 UTC] Computing input variables for policy optimization
[2018-01-21 17:45:54.101662 UTC] Performing policy update
[2018-01-21 17:45:54.102231 UTC] Computing gradient in Euclidean space
[2018-01-21 17:45:54.180116 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:45:55.057908 UTC] Performing line search
[2018-01-21 17:45:55.181069 UTC] Updating baseline
[2018-01-21 17:45:56.546385 UTC] Computing logging information
-------------------------------------
| Iteration            | 2061       |
| ExpectedImprovement  | 0.019524   |
| ActualImprovement    | 0.018424   |
| ImprovementRatio     | 0.94369    |
| MeanKL               | 0.0078626  |
| Entropy              | -2.2396    |
| Perplexity           | 0.10651    |
| AveragePolicyStd     | 0.16941    |
| AveragePolicyStd[0]  | 0.18608    |
| AveragePolicyStd[1]  | 0.1891     |
| AveragePolicyStd[2]  | 0.13952    |
| AveragePolicyStd[3]  | 0.14736    |
| AveragePolicyStd[4]  | 0.13414    |
| AveragePolicyStd[5]  | 0.22028    |
| AverageReturn        | 1914.4     |
| MinReturn            | 299.44     |
| MaxReturn            | 2098.6     |
| StdReturn            | 280.65     |
| AverageEpisodeLength | 965.55     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.93     |
| TotalNEpisodes       | 27495      |
| TotalNSamples        | 1.0319e+07 |
| ExplainedVariance    | 0.05533    |
-------------------------------------
[2018-01-21 17:45:57.204162 UTC] Saving snapshot
[2018-01-21 17:45:57.204393 UTC] Starting iteration 2062
[2018-01-21 17:45:57.204554 UTC] Start collecting samples
[2018-01-21 17:46:00.857101 UTC] Computing input variables for policy optimization
[2018-01-21 17:46:00.961150 UTC] Performing policy update
[2018-01-21 17:46:00.962047 UTC] Computing gradient in Euclidean space
[2018-01-21 17:46:01.078099 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:46:02.037684 UTC] Performing line search
[2018-01-21 17:46:02.161693 UTC] Updating baseline
[2018-01-21 17:46:03.858975 UTC] Computing logging information
-------------------------------------
| Iteration            | 2062       |
| ExpectedImprovement  | 0.01586    |
| ActualImprovement    | 0.014876   |
| ImprovementRatio     | 0.93793    |
| MeanKL               | 0.0088392  |
| Entropy              | -2.241     |
| Perplexity           | 0.10636    |
| AveragePolicyStd     | 0.16938    |
| AveragePolicyStd[0]  | 0.18604    |
| AveragePolicyStd[1]  | 0.18928    |
| AveragePolicyStd[2]  | 0.13959    |
| AveragePolicyStd[3]  | 0.14725    |
| AveragePolicyStd[4]  | 0.134      |
| AveragePolicyStd[5]  | 0.22009    |
| AverageReturn        | 1914.7     |
| MinReturn            | 299.44     |
| MaxReturn            | 2098.6     |
| StdReturn            | 280.79     |
| AverageEpisodeLength | 965.55     |
| MinEpisodeLength     | 181        |
| MaxEpisodeLength     | 1000       |
| StdEpisodeLength     | 127.93     |
| TotalNEpisodes       | 27497      |
| TotalNSamples        | 1.0321e+07 |
| ExplainedVariance    | 0.0068998  |
-------------------------------------
[2018-01-21 17:46:04.398920 UTC] Saving snapshot
[2018-01-21 17:46:04.399132 UTC] Starting iteration 2063
[2018-01-21 17:46:04.399295 UTC] Start collecting samples
[2018-01-21 17:46:06.966850 UTC] Computing input variables for policy optimization
[2018-01-21 17:46:07.045141 UTC] Performing policy update
[2018-01-21 17:46:07.045682 UTC] Computing gradient in Euclidean space
[2018-01-21 17:46:07.122033 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-01-21 17:46:07.986626 UTC] Performing line search
[2018-01-21 17:46:08.106919 UTC] Updating baseline
